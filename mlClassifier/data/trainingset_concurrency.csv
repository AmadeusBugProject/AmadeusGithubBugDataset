,created,user,body,closed,commitsDetails,url,ttf,labels,title,numCommits,changesInPackagesSPOON,filteredCommits,statsSkippedReason,changesInPackagesGIT,gitStatsSummary.insertions,gitStatsSummary.deletions,gitStatsSummary.lines,gitStatsSummary.gitFilesChange,spoonStatsSummary.UPD,spoonStatsSummary.spoonMethodsChanged,spoonStatsSummary.TOT,spoonStatsSummary.MOV,spoonStatsSummary.INS,spoonStatsSummary.DEL,spoonStatsSummary.spoonFilesChanged,filteredCommitsReason.unavailable,filteredCommitsReason.moreThanOneParent,filteredCommitsReason.mergeCommitUsed,filteredCommitsReason.duplicated,filteredCommitsReason.multipleIssueFixes,filteredCommitsReason.alsoFixesPhrase,projectName
5,2015-03-27 16:37:49,codenameone,"Original [issue 1409](https://code.google.com/p/codenameone/issues/detail?id=1409) created by codenameone on 2015-03-23T18:05:45.000Z:

I encountered the problem that getProducts never returns when there is no internet connection

<b>What steps will reproduce the problem?</b>
1. set device info flight mode
2. start app and call getProducts to get product infos like localized description and price
3. wait for the response of getProducts

<b>What is the expected output? What do you see instead?</b>
I would expect to get a null or empty response or some exception.
But none of these happen. The method never returns anything.

<b>What version of the product are you using? On what operating system?</b>
libVersion=84
iOS 8.1.3 on iPhone 5

<b>Please provide any additional information below.</b>
When there _is_ an internet connection, getProducts works fine.
",2015-03-30 22:56:39,"[{'nameRev': '7f3a34555a221975d191477f88282e425facabd9 remotes/origin/Version-3.0~35', 'commitMessage': 'Fixed issues with Purchase.getProducts() hanging if there is a network error or if a product name comes back as null. Issue https://github.com/codenameone/CodenameOne/issues/1408\n', 'commitParents': ['f4683118e5746a94b44da99d9592c690be703e84'], 'spoonStatsSkippedReason': '', 'commitHash': '7f3a34555a221975d191477f88282e425facabd9', 'authoredDateTime': '2015-03-30 15:56:27', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 23, 'deletions': 2, 'lines': 25, 'filePath': 'Ports/iOSPort/src/com/codename1/impl/ios/ZoozPurchase.java'}, {'insertions': 10, 'deletions': 0, 'lines': 10, 'filePath': 'Ports/iOSPort/nativeSources/CodenameOne_GLViewController.m'}], 'commitDateTime': '2015-03-30 15:56:27', 'commitUser': 'shannah', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 3, 'MOV': 0, 'INS': 3, 'DEL': 0, 'spoonMethodName': 'com.codename1.impl.ios.ZoozPurchase'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.codename1.impl.ios.ZoozPurchase.fetchProductsCanceledOrFailed(java.lang.String)'}, {'UPD': 0, 'TOT': 4, 'MOV': 0, 'INS': 4, 'DEL': 0, 'spoonMethodName': 'com.codename1.impl.ios.ZoozPurchase.getProducts(java.lang.String[])'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.codename1.impl.ios.ZoozPurchase.fetchProductsComplete()'}, {'UPD': 0, 'TOT': 2, 'MOV': 0, 'INS': 1, 'DEL': 1, 'spoonMethodName': 'com.codename1.impl.ios.ZoozPurchase.getProducts(java.lang.String[]).1.run()'}], 'spoonFilePath': 'ZoozPurchase.java'}]}]",https://github.com/codenameone/CodenameOne/issues/1408,3.000277777777778,"['Priority-Medium', 'Type-Defect']",getProducts never returns when no internet connection,1.0,"['com.codename1.impl.ios.ZoozPurchase.fetchProductsCanceledOrFailed(java.lang.String)', 'com.codename1.impl.ios.ZoozPurchase', 'com.codename1.impl.ios.ZoozPurchase.getProducts(java.lang.String[]).1.run()', 'com.codename1.impl.ios.ZoozPurchase.fetchProductsComplete()', 'com.codename1.impl.ios.ZoozPurchase.getProducts(java.lang.String[])']",['7f3a34555a221975d191477f88282e425facabd9'],,"['Ports/iOSPort/src/com/codename1/impl/ios', 'Ports/iOSPort/nativeSources/CodenameOne_GLViewController.m']",33.0,2.0,35.0,2.0,0.0,5.0,11.0,0.0,10.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,CodenameOne
1272,2020-04-21 09:58:39,tibor-leo-safar-accedo,"
### [REQUIRED] Issue description
Regarding an already filed issue (https://github.com/google/ExoPlayer/issues/6348) what is happening also with the streams and devices we are working with, we have done some testing with the **dev-2** branch (checked out on 12 of April 2020). It turned out that the playback of live stream is not starting. Looking into the logs the issue was identified in SampleQueue.java function onFormatResult line 800.
The crash was because the assertion have failed for Looper.myLooper().
Replacing it with Looper.getMainLooper() didn't crash and playback works nicely.

### [REQUIRED] Reproduction steps
This is an application what we develop for one of our clients.
There are Live DASH streams with protected with WideVine L3.


### [REQUIRED] Link to test content
A test stream and additional will be shared separately in an e-mail.

### [REQUIRED] A full bug report captured from the device

### [REQUIRED] Version of ExoPlayer being used
[ExoPlayerLib/2.11.4] from **dev-2** branch, checked out on 18 of April 2020.


### [REQUIRED] Device(s) and version(s) of Android being used
Samsung Galaxy S10 with Android 10.
Crash happened in 100%
",2020-04-28 18:03:53,"[{'nameRev': '30c55d117e5fc3822f732ac3f875e0e6e7df9079 remotes/origin/dev-v2~326', 'commitMessage': 'Fix NPE when reading from a SampleQueue from a loading thread\n\nIssue: #7273\nPiperOrigin-RevId: 308238035\n', 'commitParents': ['4df7470da5483074a0db74a1d969e9b9cf3c9ef6'], 'spoonStatsSkippedReason': '', 'commitHash': '30c55d117e5fc3822f732ac3f875e0e6e7df9079', 'authoredDateTime': '2020-04-24 13:32:05', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 30, 'deletions': 5, 'lines': 35, 'filePath': 'library/core/src/test/java/com/google/android/exoplayer2/source/SampleQueueTest.java'}, {'insertions': 1, 'deletions': 0, 'lines': 1, 'filePath': 'library/dash/src/main/java/com/google/android/exoplayer2/source/dash/PlayerEmsgHandler.java'}, {'insertions': 6, 'deletions': 1, 'lines': 7, 'filePath': 'library/core/src/main/java/com/google/android/exoplayer2/source/ProgressiveMediaPeriod.java'}, {'insertions': 2, 'deletions': 0, 'lines': 2, 'filePath': 'RELEASENOTES.md'}, {'insertions': 9, 'deletions': 2, 'lines': 11, 'filePath': 'library/hls/src/main/java/com/google/android/exoplayer2/source/hls/HlsSampleStreamWrapper.java'}, {'insertions': 5, 'deletions': 2, 'lines': 7, 'filePath': 'library/core/src/main/java/com/google/android/exoplayer2/source/SampleQueue.java'}, {'insertions': 11, 'deletions': 2, 'lines': 13, 'filePath': 'library/core/src/main/java/com/google/android/exoplayer2/source/chunk/ChunkSampleStream.java'}], 'commitDateTime': '2020-04-27 10:41:50', 'commitUser': 'icbaker', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.SampleQueueTest.setUp()'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.SampleQueueTest.isReadyReturnsTrueForClearSampleAndPlayClearSamplesWithoutKeysIsTrue()'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.SampleQueueTest.adjustUpstreamFormat()'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.SampleQueueTest.invalidateUpstreamFormatAdjustment()'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.SampleQueueTest.allowPlayClearSamplesWithoutKeysReadsClearSamples()'}], 'spoonFilePath': 'SampleQueueTest.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.dash.PlayerEmsgHandler.PlayerTrackEmsgHandler'}], 'spoonFilePath': 'PlayerEmsgHandler.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 4, 'MOV': 1, 'INS': 3, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.SampleQueue'}, {'UPD': 0, 'TOT': 5, 'MOV': 0, 'INS': 2, 'DEL': 3, 'spoonMethodName': 'com.google.android.exoplayer2.source.SampleQueue.onFormatResult(com.google.android.exoplayer2.Format,com.google.android.exoplayer2.FormatHolder)'}], 'spoonFilePath': 'SampleQueue.java'}, {'spoonMethods': [{'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.hls.HlsSampleStreamWrapper.createSampleQueue(int,int)'}, {'UPD': 0, 'TOT': 2, 'MOV': 0, 'INS': 2, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.hls.HlsSampleStreamWrapper.HlsSampleQueue'}], 'spoonFilePath': 'HlsSampleStreamWrapper.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 2, 'MOV': 0, 'INS': 2, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.chunk.ChunkSampleStream'}], 'spoonFilePath': 'ChunkSampleStream.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.ProgressiveMediaPeriod.prepareTrackOutput(com.google.android.exoplayer2.source.ProgressiveMediaPeriod$TrackId)'}], 'spoonFilePath': 'ProgressiveMediaPeriod.java'}]}, {'nameRev': '49e5e66033c497fdd26c756d97c94b595ec35eb6 tags/r2.11.5~1^2~61', 'commitMessage': 'Fix NPE when reading from a SampleQueue from a loading thread\n\nIssue: #7273\nPiperOrigin-RevId: 308238035\n', 'commitParents': ['7ee08f09d28d4d3e13cdaf13689f463dd9d1bd5c'], 'spoonStatsSkippedReason': '', 'commitHash': '49e5e66033c497fdd26c756d97c94b595ec35eb6', 'authoredDateTime': '2020-04-24 13:32:05', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 22, 'deletions': 5, 'lines': 27, 'filePath': 'library/core/src/test/java/com/google/android/exoplayer2/source/SampleQueueTest.java'}, {'insertions': 4, 'deletions': 1, 'lines': 5, 'filePath': 'library/dash/src/main/java/com/google/android/exoplayer2/source/dash/PlayerEmsgHandler.java'}, {'insertions': 2, 'deletions': 1, 'lines': 3, 'filePath': 'library/core/src/main/java/com/google/android/exoplayer2/source/ProgressiveMediaPeriod.java'}, {'insertions': 2, 'deletions': 1, 'lines': 3, 'filePath': 'RELEASENOTES.md'}, {'insertions': 8, 'deletions': 2, 'lines': 10, 'filePath': 'library/hls/src/main/java/com/google/android/exoplayer2/source/hls/HlsSampleStreamWrapper.java'}, {'insertions': 5, 'deletions': 3, 'lines': 8, 'filePath': 'library/core/src/main/java/com/google/android/exoplayer2/source/SampleQueue.java'}, {'insertions': 9, 'deletions': 2, 'lines': 11, 'filePath': 'library/core/src/main/java/com/google/android/exoplayer2/source/chunk/ChunkSampleStream.java'}], 'commitDateTime': '2020-05-27 20:22:32', 'commitUser': 'ojw28', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.SampleQueueTest.setUp()'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.SampleQueueTest.testAllowPlayClearSamplesWithoutKeysReadsClearSamples()'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.SampleQueueTest.testInvalidateUpstreamFormatAdjustment()'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.SampleQueueTest.testAdjustUpstreamFormat()'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.SampleQueueTest.testIsReadyReturnsTrueForClearSampleAndPlayClearSamplesWithoutKeysIsTrue()'}], 'spoonFilePath': 'SampleQueueTest.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.dash.PlayerEmsgHandler.PlayerTrackEmsgHandler'}], 'spoonFilePath': 'PlayerEmsgHandler.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 3, 'MOV': 0, 'INS': 3, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.SampleQueue'}, {'UPD': 0, 'TOT': 6, 'MOV': 0, 'INS': 3, 'DEL': 3, 'spoonMethodName': 'com.google.android.exoplayer2.source.SampleQueue.onFormatResult(com.google.android.exoplayer2.Format,com.google.android.exoplayer2.FormatHolder)'}], 'spoonFilePath': 'SampleQueue.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 2, 'MOV': 0, 'INS': 2, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.hls.HlsSampleStreamWrapper.FormatAdjustingSampleQueue'}, {'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.hls.HlsSampleStreamWrapper.createSampleQueue(int,int)'}], 'spoonFilePath': 'HlsSampleStreamWrapper.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 2, 'MOV': 0, 'INS': 2, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.chunk.ChunkSampleStream'}], 'spoonFilePath': 'ChunkSampleStream.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.ProgressiveMediaPeriod.prepareTrackOutput(com.google.android.exoplayer2.source.ProgressiveMediaPeriod$TrackId)'}], 'spoonFilePath': 'ProgressiveMediaPeriod.java'}]}]",https://github.com/google/ExoPlayer/issues/7273,7.000277777777778,['bug'],ExoPlayer Looper.myLooper() internal crash on dev-2 branch with DASH Live stream WV L3,1.0,"['com.google.android.exoplayer2.source.ProgressiveMediaPeriod.prepareTrackOutput(com.google.android.exoplayer2.source.ProgressiveMediaPeriod$TrackId)', 'com.google.android.exoplayer2.source.hls.HlsSampleStreamWrapper.FormatAdjustingSampleQueue', 'com.google.android.exoplayer2.source.SampleQueue.onFormatResult(com.google.android.exoplayer2.Format,com.google.android.exoplayer2.FormatHolder)', 'com.google.android.exoplayer2.source.chunk.ChunkSampleStream', 'com.google.android.exoplayer2.source.dash.PlayerEmsgHandler.PlayerTrackEmsgHandler', 'com.google.android.exoplayer2.source.hls.HlsSampleStreamWrapper.createSampleQueue(int,int)', 'com.google.android.exoplayer2.source.SampleQueue']",['49e5e66033c497fdd26c756d97c94b595ec35eb6'],,"['library/hls/src/main/java/com/google/android/exoplayer2/source', 'library/core/src/main/java/com/google/android/exoplayer2/source', 'library/dash/src/main/java/com/google/android/exoplayer2/source']",28.0,9.0,37.0,5.0,1.0,7.0,16.0,0.0,12.0,3.0,5.0,0.0,0.0,0.0,1.0,0.0,0.0,ExoPlayer
1405,2019-07-09 05:17:53,drayan85,"### [REQUIRED] Issue description
When I try to run ExoPlayer library core test cases keep failing

### [REQUIRED] Reproduction steps
When try to Run `DownloadHelperTest.java` is keep failing 

### [REQUIRED] Link to test content
N/A

### [REQUIRED] A full bug report captured from the device
```
[Robolectric] com.google.android.exoplayer2.offline.DownloadHelperTest.getTrackSelections_afterReplaceTrackSelections_returnsNewSelections: sdk=28; resources=BINARY
Exception in thread ""DownloadHelper"" java.lang.NullPointerException
	at com.google.android.exoplayer2.offline.DownloadHelper$MediaPreparer.onSourceInfoRefreshed(DownloadHelper.java:918)
	at com.google.android.exoplayer2.source.BaseMediaSource.refreshSourceInfo(BaseMediaSource.java:74)
	at com.google.android.exoplayer2.testutil.FakeMediaSource.finishSourcePreparation(FakeMediaSource.java:215)
	at com.google.android.exoplayer2.testutil.FakeMediaSource.prepareSourceInternal(FakeMediaSource.java:105)
	at com.google.android.exoplayer2.source.BaseMediaSource.prepareSource(BaseMediaSource.java:140)
	at com.google.android.exoplayer2.offline.DownloadHelper$MediaPreparer.handleMessage(DownloadHelper.java:858)
	at android.os.Handler.dispatchMessage(Handler.java:102)
	at com.google.android.exoplayer2.testutil.RobolectricUtil$CustomLooper.doLoop(RobolectricUtil.java:129)
	at com.google.android.exoplayer2.testutil.RobolectricUtil$CustomLooper.loop(RobolectricUtil.java:67)
	at android.os.Looper.loop(Looper.java)
	at android.os.HandlerThread.run(HandlerThread.java:65)
```

### [REQUIRED] Version of ExoPlayer being used
### 2.10.2 ###

### [REQUIRED] Device(s) and version(s) of Android being used
N/A

<!-- DO NOT DELETE
validate_template=true
template_path=.github/ISSUE_TEMPLATE/bug.md
-->
",2019-07-09 11:00:03,"[{'nameRev': 'b3d258b6cf5953acf643688390157b1efae805a2 tags/r2.11.0^2~618', 'commitMessage': 'Fix race condition in DownloadHelper\n\nSending MESSAGE_PREPARE_SOURCE should happen last in the constructor.\nIt was previously happening before initialization finished (and in\nparticular before pendingMediaPeriods was instantiated).\n\nIssue: #6146\nPiperOrigin-RevId: 257158275\n', 'commitParents': ['92fb654ab6c3ee3af1ef7992b36f10e23e7733a4'], 'spoonStatsSkippedReason': '', 'commitHash': 'b3d258b6cf5953acf643688390157b1efae805a2', 'authoredDateTime': '2019-07-09 11:18:50', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 3, 'deletions': 3, 'lines': 6, 'filePath': 'library/core/src/main/java/com/google/android/exoplayer2/offline/DownloadHelper.java'}], 'commitDateTime': '2019-07-09 11:41:54', 'commitUser': 'ojw28', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 3, 'MOV': 3, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.offline.DownloadHelper.MediaPreparer'}], 'spoonFilePath': 'DownloadHelper.java'}]}, {'nameRev': 'be9fea89a6a669a2d7125a90a90054810c9a4d0b tags/r2.10.3^2~4', 'commitMessage': 'Fix race condition in DownloadHelper\n\nSending MESSAGE_PREPARE_SOURCE should happen last in the constructor.\nIt was previously happening before initialization finished (and in\nparticular before pendingMediaPeriods was instantiated).\n\nIssue: #6146\nPiperOrigin-RevId: 257158275\n', 'commitParents': ['dbabb7c9a3ba6640c51230bc7932c7ab471284d8'], 'spoonStatsSkippedReason': '', 'commitHash': 'be9fea89a6a669a2d7125a90a90054810c9a4d0b', 'authoredDateTime': '2019-07-09 11:18:50', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 3, 'deletions': 3, 'lines': 6, 'filePath': 'library/core/src/main/java/com/google/android/exoplayer2/offline/DownloadHelper.java'}], 'commitDateTime': '2019-07-09 11:48:54', 'commitUser': 'ojw28', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 3, 'MOV': 3, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.offline.DownloadHelper.MediaPreparer'}], 'spoonFilePath': 'DownloadHelper.java'}]}]",https://github.com/google/ExoPlayer/issues/6146,0.0002777777777777778,['bug'],DownloadHelperTest Failed,1.0,['com.google.android.exoplayer2.offline.DownloadHelper.MediaPreparer'],['b3d258b6cf5953acf643688390157b1efae805a2'],,['library/core/src/main/java/com/google/android/exoplayer2/offline'],3.0,3.0,6.0,1.0,0.0,1.0,3.0,3.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,ExoPlayer
1590,2018-01-28 08:57:43,eneim,"### Issue description
The following Exception is thrown with I tried to use SimpleCache with multiple SimpleExoPlayer:

```
6439-6696/packagename E/LoadTask: Unexpected exception loading stream
                                          java.lang.IllegalStateException: Top bit not zero: -2021886842
                                              at com.google.android.exoplayer2.util.ParsableByteArray.readUnsignedIntToInt(ParsableByteArray.java:388)
                                              at com.google.android.exoplayer2.extractor.mp4.Mp4Extractor.readSample(Mp4Extractor.java:474)
                                              at com.google.android.exoplayer2.extractor.mp4.Mp4Extractor.read(Mp4Extractor.java:179)
                                              at com.google.android.exoplayer2.source.ExtractorMediaPeriod$ExtractingLoadable.load(ExtractorMediaPeriod.java:749)
                                              at com.google.android.exoplayer2.upstream.Loader$LoadTask.run(Loader.java:315)
                                              at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
                                              at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:636)
                                              at java.lang.Thread.run(Thread.java:764)
6439-6538/packagename E/ExoPlayerImplInternal: Source error.
                                                       com.google.android.exoplayer2.upstream.Loader$UnexpectedLoaderException: Unexpected IllegalStateException: Top bit not zero: -2021886842
                                                           at com.google.android.exoplayer2.upstream.Loader$LoadTask.run(Loader.java:337)
                                                           at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
                                                           at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:636)
                                                           at java.lang.Thread.run(Thread.java:764)
                                                        Caused by: java.lang.IllegalStateException: Top bit not zero: -2021886842
                                                           at com.google.android.exoplayer2.util.ParsableByteArray.readUnsignedIntToInt(ParsableByteArray.java:388)
                                                           at com.google.android.exoplayer2.extractor.mp4.Mp4Extractor.readSample(Mp4Extractor.java:474)
                                                           at com.google.android.exoplayer2.extractor.mp4.Mp4Extractor.read(Mp4Extractor.java:179)
                                                           at com.google.android.exoplayer2.source.ExtractorMediaPeriod$ExtractingLoadable.load(ExtractorMediaPeriod.java:749)
                                                           at com.google.android.exoplayer2.upstream.Loader$LoadTask.run(Loader.java:315)
                                                           at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162) 
                                                           at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:636) 
                                                           at java.lang.Thread.run(Thread.java:764) 
6439-6703/packagename D/SurfaceUtils: disconnecting from surface 0x79fc077010, reason disconnectFromSurface
```

The term ""multiple"" is my assumption, as in this demo, I used a pool with a factory method that uses a fixed Cache (as factory option) to create SimpleExoPlayer instances on demand and cache for reuse. This is not always reproducible but the rate of failure is high.

When the Cache is set to null I get no error.

### Link to test content
All videos are get from this link (the video/mp4 variants, not webm)

https://material.io/guidelines/motion/material-motion.html

### Version of ExoPlayer being used: 2.6.1

### Device(s) and version(s) of Android being used: Nexus 5X, Android 27 (8.1).

I can provide the code for various part if need. But hope the log above says something.",2018-02-12 11:58:27,"[{'nameRev': 'b210c20e8405ff5d9ec95a1c74b0b777694b7f8e tags/r2.7.0^2~47', 'commitMessage': ""Fix not thread safe static buffer usage\n\nDefaultExtractorInput.SCRATCH_SPACE buffer is used to skip data by\nreading it into this buffer and discarding.\n\nSimultaneous use of skip methods corrupts this buffer. Normally the\nread data is discarded so it doesn't matter but the underlying\nDataSource may use the buffer too. If it's a CacheDataSource it uses\nthis buffer to read data from upstream then write to cache.\n\nIssue: #3762\n\n-------------\nCreated by MOE: https://github.com/google/moe\nMOE_MIGRATED_REVID=184502170\n"", 'commitParents': ['2d76d63c3e9adde31c35100ea71196b061adafd1'], 'spoonStatsSkippedReason': '', 'commitHash': 'b210c20e8405ff5d9ec95a1c74b0b777694b7f8e', 'authoredDateTime': '2018-02-05 02:36:34', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 7, 'deletions': 4, 'lines': 11, 'filePath': 'library/core/src/main/java/com/google/android/exoplayer2/extractor/DefaultExtractorInput.java'}], 'commitDateTime': '2018-02-20 10:45:26', 'commitUser': 'ojw28', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 2, 'TOT': 4, 'MOV': 0, 'INS': 2, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.extractor.DefaultExtractorInput'}, {'UPD': 1, 'TOT': 10, 'MOV': 6, 'INS': 2, 'DEL': 1, 'spoonMethodName': 'com.google.android.exoplayer2.extractor.DefaultExtractorInput.skipFully(int,boolean)'}, {'UPD': 2, 'TOT': 2, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.extractor.DefaultExtractorInput.skip(int)'}], 'spoonFilePath': 'DefaultExtractorInput.java'}]}, {'nameRev': 'de293af3a42b15b1ac809d55d31fde756f7a565d tags/r2.8.0^2~362', 'commitMessage': ""Fix not thread safe static buffer usage\n\nDefaultExtractorInput.SCRATCH_SPACE buffer is used to skip data by\nreading it into this buffer and discarding.\n\nSimultaneous use of skip methods corrupts this buffer. Normally the\nread data is discarded so it doesn't matter but the underlying\nDataSource may use the buffer too. If it's a CacheDataSource it uses\nthis buffer to read data from upstream then write to cache.\n\nIssue: #3762\n\n-------------\nCreated by MOE: https://github.com/google/moe\nMOE_MIGRATED_REVID=184502170\n"", 'commitParents': ['e437248f4f9f8e9c06d580715503b7d1129f61d7'], 'spoonStatsSkippedReason': '', 'commitHash': 'de293af3a42b15b1ac809d55d31fde756f7a565d', 'authoredDateTime': '2018-02-05 02:36:34', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 7, 'deletions': 4, 'lines': 11, 'filePath': 'library/core/src/main/java/com/google/android/exoplayer2/extractor/DefaultExtractorInput.java'}], 'commitDateTime': '2018-02-08 14:49:00', 'commitUser': 'andrewlewis', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 2, 'TOT': 4, 'MOV': 0, 'INS': 2, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.extractor.DefaultExtractorInput'}, {'UPD': 1, 'TOT': 10, 'MOV': 6, 'INS': 2, 'DEL': 1, 'spoonMethodName': 'com.google.android.exoplayer2.extractor.DefaultExtractorInput.skipFully(int,boolean)'}, {'UPD': 2, 'TOT': 2, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.extractor.DefaultExtractorInput.skip(int)'}], 'spoonFilePath': 'DefaultExtractorInput.java'}]}]",https://github.com/google/ExoPlayer/issues/3762,15.000277777777777,['bug'],Error when using multiple SimpleExoPlayer instances share the same Cache instances (SimpleCache),1.0,"['com.google.android.exoplayer2.extractor.DefaultExtractorInput', 'com.google.android.exoplayer2.extractor.DefaultExtractorInput.skipFully(int,boolean)', 'com.google.android.exoplayer2.extractor.DefaultExtractorInput.skip(int)']",['de293af3a42b15b1ac809d55d31fde756f7a565d'],,['library/core/src/main/java/com/google/android/exoplayer2/extractor'],7.0,4.0,11.0,1.0,5.0,3.0,16.0,6.0,4.0,1.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,ExoPlayer
1763,2016-11-25 05:37:21,gpinigin,"Caught by Crashlytics (most recent dev-v2, 2.0.4). No steps to reproduce — cannot reproduce by myself. Issue doesn't depend on device or OS version. 

Stacktrace:
```
com.google.android.exoplayer2.util.Assertions.checkState (Assertions.java:79)
com.google.android.exoplayer2.upstream.Loader$LoadTask.start (Loader.java:258)
com.google.android.exoplayer2.upstream.Loader$LoadTask.handleMessage (Loader.java:364)
android.os.Handler.dispatchMessage (Handler.java:102)
android.os.Looper.loop (Looper.java:146)
android.app.ActivityThread.main (ActivityThread.java:5603)
java.lang.reflect.Method.invokeNative (Method.java)
java.lang.reflect.Method.invoke (Method.java:515)
com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run (ZygoteInit.java:1283)
com.android.internal.os.ZygoteInit.main (ZygoteInit.java:1099)
dalvik.system.NativeStart.main (NativeStart.java)
```

The step discovered from logs:
1. Exoplayerv2.prepare
2. Exoplayer state changes to STATE_BUFFERING
3. onVideoDecoderInitialized, onAudioDecoderInitialized methods called
4. Exoplayer state changes to STATE_READY
5. ...
6. Unhandled exception thrown

",2016-11-25 20:22:23,"[{'nameRev': '97a23ce5726757d82eba0b64cc2cb5d0cfd011f4 tags/r2.1.0~1^2~77', 'commitMessage': 'Fix main thread playlist refreshes in HlsPlaylistTracker\n\nThe refresh handler in HlsPlaylistTracker was being instantiated in the\nsame thread as the MediaSource (i.e. Main thread).\n\nIssue:#2108\n\n-------------\nCreated by MOE: https://github.com/google/moe\nMOE_MIGRATED_REVID=140197553\n', 'commitParents': ['894ae1a310cc2e1fe5ba59eb56c7fbeb751036d5'], 'spoonStatsSkippedReason': '', 'commitHash': '97a23ce5726757d82eba0b64cc2cb5d0cfd011f4', 'authoredDateTime': '2016-11-25 06:13:35', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 7, 'deletions': 3, 'lines': 10, 'filePath': 'library/src/main/java/com/google/android/exoplayer2/source/hls/HlsMediaSource.java'}], 'commitDateTime': '2016-11-25 20:18:32', 'commitUser': 'ojw28', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 2, 'MOV': 0, 'INS': 2, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.hls.HlsMediaSource.prepareSource(com.google.android.exoplayer2.source.MediaSource$Listener)'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.android.exoplayer2.source.hls.HlsMediaSource.releaseSource()'}, {'UPD': 2, 'TOT': 10, 'MOV': 4, 'INS': 3, 'DEL': 1, 'spoonMethodName': 'com.google.android.exoplayer2.source.hls.HlsMediaSource'}], 'spoonFilePath': 'HlsMediaSource.java'}]}]",https://github.com/google/ExoPlayer/issues/2108,0.0002777777777777778,['bug'],Unhandled exception IllegalStateException during playback live HLS (media index),1.0,"['com.google.android.exoplayer2.source.hls.HlsMediaSource.prepareSource(com.google.android.exoplayer2.source.MediaSource$Listener)', 'com.google.android.exoplayer2.source.hls.HlsMediaSource.releaseSource()', 'com.google.android.exoplayer2.source.hls.HlsMediaSource']",['97a23ce5726757d82eba0b64cc2cb5d0cfd011f4'],,['library/src/main/java/com/google/android/exoplayer2/source/hls'],7.0,3.0,10.0,1.0,2.0,3.0,13.0,4.0,6.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,ExoPlayer
2509,2017-05-02 11:06:44,binakot,"#### Environment
```
HikariCP version: 2.6.1
JDK version     : 1.8.0_71-b15
Database        : PostgreSQL 9.6.1 on x86_64-pc-linux-gnu, compiled by gcc, 64-bit
Driver version  : 9.4-1201-jdbc41
```
-----------------------------------------------------------------------------------------
#### HikariCP Config
```
config.setAutoCommit(false);
config.setMinimumIdle(10);
config.setMaximumPoolSize(100);
Other options are default.
```
-----------------------------------------------------------------------------------------
#### Trace
```
Unexpected exception in housekeeping task
java.lang.IllegalArgumentException: Comparison method violates its general contract!
	at java.util.TimSort.mergeLo(TimSort.java:777) ~[?:1.8.0_71]
	at java.util.TimSort.mergeAt(TimSort.java:514) ~[?:1.8.0_71]
	at java.util.TimSort.mergeForceCollapse(TimSort.java:457) ~[?:1.8.0_71]
	at java.util.TimSort.sort(TimSort.java:254) ~[?:1.8.0_71]
	at java.util.Arrays.sort(Arrays.java:1512) ~[?:1.8.0_71]
	at java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:348) ~[?:1.8.0_71]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_71]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[?:1.8.0_71]
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) ~[?:1.8.0_71]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) ~[?:1.8.0_71]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_71]
	at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:423) ~[?:1.8.0_71]
	at com.zaxxer.hikari.pool.HikariPool$HouseKeeper.run(HikariPool.java:704) [NavServ.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_71]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [?:1.8.0_71]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_71]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [?:1.8.0_71]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_71]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_71]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_71]
```
-----------------------------------------------------------------------------------------

I got this exception only 3 times: 2017-04-30 07:26:53.399, 2017-04-30 15:42:25.314, 2017-04-30 15:42:55.675 after about 4 days application non-stop working. 

Really don't know what's wrong, because in the stack trace there is no my code.",2017-05-03 15:36:09,"[{'nameRev': '61be9b923d8608605eb97264129d4f0467a0a2cc tags/HikariCP-2.6.2~9', 'commitMessage': 'Fixes #880 Fix race condition caused by sorting collection\nwhile the condition of sort can change.\n', 'commitParents': ['275c3d70bc7a0e41d8b3ad9913bef01535817e11'], 'spoonStatsSkippedReason': '', 'commitHash': '61be9b923d8608605eb97264129d4f0467a0a2cc', 'authoredDateTime': '2017-05-04 00:36:03', 'commitGHEventType': 'closed', 'commitGitStats': [{'insertions': 7, 'deletions': 5, 'lines': 12, 'filePath': 'src/main/java/com/zaxxer/hikari/util/ConcurrentBag.java'}, {'insertions': 2, 'deletions': 2, 'lines': 4, 'filePath': 'src/test/java/com/zaxxer/hikari/pool/ConnectionPoolSizeVsThreadsTest.java'}, {'insertions': 5, 'deletions': 13, 'lines': 18, 'filePath': 'src/main/java/com/zaxxer/hikari/pool/PoolEntry.java'}, {'insertions': 14, 'deletions': 13, 'lines': 27, 'filePath': 'src/main/java/com/zaxxer/hikari/pool/HikariPool.java'}], 'commitDateTime': '2017-05-04 00:36:03', 'commitUser': 'brettwooldridge', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 1, 'TOT': 5, 'MOV': 2, 'INS': 0, 'DEL': 2, 'spoonMethodName': 'com.zaxxer.hikari.pool.HikariPool.addBagItem(int)'}, {'UPD': 3, 'TOT': 17, 'MOV': 8, 'INS': 4, 'DEL': 2, 'spoonMethodName': 'com.zaxxer.hikari.pool.HikariPool.HouseKeeper.run()'}], 'spoonFilePath': 'HikariPool.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 2, 'MOV': 0, 'INS': 0, 'DEL': 2, 'spoonMethodName': 'com.zaxxer.hikari.util.ConcurrentBag'}, {'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'com.zaxxer.hikari.util.ConcurrentBag.addBagItem(int)'}, {'UPD': 0, 'TOT': 4, 'MOV': 1, 'INS': 3, 'DEL': 0, 'spoonMethodName': 'com.zaxxer.hikari.util.ConcurrentBag.values(int)'}], 'spoonFilePath': 'ConcurrentBag.java'}, {'spoonMethods': [{'UPD': 2, 'TOT': 2, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'com.zaxxer.hikari.pool.ConnectionPoolSizeVsThreadsTest'}], 'spoonFilePath': 'ConnectionPoolSizeVsThreadsTest.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 2, 'MOV': 0, 'INS': 0, 'DEL': 2, 'spoonMethodName': 'com.zaxxer.hikari.pool.PoolEntry'}], 'spoonFilePath': 'PoolEntry.java'}]}]",https://github.com/brettwooldridge/HikariCP/issues/880,1.0002777777777778,['bug'],Unexpected IllegalArgumentException in housekeeping task.,1.0,"['com.zaxxer.hikari.pool.HikariPool.addBagItem(int)', 'com.zaxxer.hikari.util.ConcurrentBag.addBagItem(int)', 'com.zaxxer.hikari.pool.HikariPool.HouseKeeper.run()', 'com.zaxxer.hikari.util.ConcurrentBag', 'com.zaxxer.hikari.pool.PoolEntry', 'com.zaxxer.hikari.util.ConcurrentBag.values(int)']",['61be9b923d8608605eb97264129d4f0467a0a2cc'],,"['src/main/java/com/zaxxer/hikari/pool', 'src/main/java/com/zaxxer/hikari/util']",26.0,31.0,57.0,3.0,5.0,6.0,31.0,11.0,7.0,8.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,HikariCP
2551,2015-03-03 14:56:45,yanivoren,"When trying to suspend 2 pools, the suspend is stuck on the 2nd suspend command.

Notes:
Both pools access the same database.

jetty-web.xml

`<New id=""mainDS"" class=""org.eclipse.jetty.plus.jndi.Resource"">`
        `<Arg>jdbc/mainDataSource</Arg>`
        `<Arg>`
            `<New class=""com.zaxxer.hikari.HikariDataSource"">`
              `<Arg>`
                `<New class=""com.zaxxer.hikari.HikariConfig"">`
                    `<Set name=""dataSourceClassName"">org.postgresql.ds.PGSimpleDataSource</Set>`
                    `<Set name=""username""></Set>`
                    `<Set name=""password""></Set>`
                    `<Set name=""maximumPoolSize"">30</Set>`
                    `<Set name=""allowPoolSuspension"">true</Set>`
                    `<Set name=""registerMbeans"">true</Set>`
                    `<Set name=""poolName"">MainPool</Set>`
                    `<Call name=""addDataSourceProperty"">`
                        `<Arg>url</Arg>`
                        `<Arg>jdbc:postgresql://localhost/db</Arg>`
                    `</Call>`
                `</New>`
              `</Arg>`
            `</New>`
        `</Arg>`
    `</New>`

```
`<!-- For Quartz job execution. -->`

`<New id=""QuartzDS"" class=""org.eclipse.jetty.plus.jndi.Resource"">`
    `<Arg>jdbc/QuartzDataSource</Arg>`
    `<Arg>`
        `<New class=""com.zaxxer.hikari.HikariDataSource"">`
          `<Arg>`
            `<New class=""com.zaxxer.hikari.HikariConfig"">`
                `<Set name=""dataSourceClassName"">org.postgresql.ds.PGSimpleDataSource</Set>`
                `<Set name=""username""></Set>`
                `<Set name=""password""></Set>`
                `<Set name=""maximumPoolSize"">30</Set>`
                `<Set name=""allowPoolSuspension"">true</Set>`
                `<Set name=""registerMbeans"">true</Set>`
                `<Set name=""poolName"">QuartzPool</Set>`
                `<Call name=""addDataSourceProperty"">`
                    `<Arg>url</Arg>`
                    `<Arg>jdbc:postgresql://localhost/db</Arg>`
                `</Call>`
            `</New>`
          `</Arg>`
        `</New>`
    `</Arg>`
`</New>`
```
",2015-03-04 14:37:00,"[{'nameRev': 'f7df7cd0b0a6455c1678350bb5f14050fcaa0d47 tags/HikariCP-2.3.4~12', 'commitMessage': 'Fixes #275 make global lock non-static\n', 'commitParents': ['e9e7f05c4200382095c7bd4a480071fa7cd1579b'], 'spoonStatsSkippedReason': '', 'commitHash': 'f7df7cd0b0a6455c1678350bb5f14050fcaa0d47', 'authoredDateTime': '2015-03-04 23:36:55', 'commitGHEventType': 'closed', 'commitGitStats': [{'insertions': 1, 'deletions': 1, 'lines': 2, 'filePath': 'hikaricp-common/src/main/java/com/zaxxer/hikari/pool/BaseHikariPool.java'}, {'insertions': 3, 'deletions': 5, 'lines': 8, 'filePath': 'hikaricp-common/src/main/java/com/zaxxer/hikari/pool/GlobalPoolLock.java'}, {'insertions': 1, 'deletions': 1, 'lines': 2, 'filePath': 'hikaricp-common/pom.xml'}], 'commitDateTime': '2015-03-04 23:36:55', 'commitUser': 'brettwooldridge', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 2, 'MOV': 0, 'INS': 1, 'DEL': 1, 'spoonMethodName': 'com.zaxxer.hikari.pool.BaseHikariPool'}], 'spoonFilePath': 'BaseHikariPool.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 4, 'MOV': 0, 'INS': 0, 'DEL': 4, 'spoonMethodName': 'com.zaxxer.hikari.pool.GlobalPoolLock'}], 'spoonFilePath': 'GlobalPoolLock.java'}]}]",https://github.com/brettwooldridge/HikariCP/issues/275,0.0002777777777777778,['bug'],Fail to suspend 2 pools,1.0,"['com.zaxxer.hikari.pool.BaseHikariPool', 'com.zaxxer.hikari.pool.GlobalPoolLock']",['f7df7cd0b0a6455c1678350bb5f14050fcaa0d47'],,['hikaricp-common/src/main/java/com/zaxxer/hikari/pool'],4.0,6.0,10.0,2.0,0.0,2.0,6.0,0.0,1.0,5.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,HikariCP
2556,2015-01-22 09:36:37,ghost,"I am creating this for the one from the google group https://groups.google.com/forum/#!topic/hikari-cp/RgS7J0Xs05U

quote:

I'm getting this funny exceptions. It only appears in my console, not on my logs where HikariCP messages appear.
It seems to appear every few minutes, maybe after inactivity.

end quote

The stacktraces seem to appear without anything happening in particular, while the various dev/testing web apps where HikariCP is used still function as normal so far but the connection may possible fail in the middle of a long series of database operations when this happens. I am not really sure.

Exception in thread ""HikariCP utility thread (pool HikariPool-0)"" java.lang.NullPointerException
    at com.mysql.jdbc.MysqlIO.setSocketTimeout(MysqlIO.java:4890)
    at com.mysql.jdbc.ConnectionImpl$12.run(ConnectionImpl.java:5540)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Exception in thread ""HikariCP utility thread (pool HikariPool-0)"" java.lang.NullPointerException
    at com.mysql.jdbc.MysqlIO.setSocketTimeout(MysqlIO.java:4890)
    at com.mysql.jdbc.ConnectionImpl$12.run(ConnectionImpl.java:5540)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Exception in thread ""HikariCP utility thread (pool HikariPool-0)"" java.lang.NullPointerException
    at com.mysql.jdbc.MysqlIO.setSocketTimeout(MysqlIO.java:4890)
    at com.mysql.jdbc.ConnectionImpl$12.run(ConnectionImpl.java:5540)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
",2015-01-24 06:33:25,"[{'nameRev': 'c24e6d56bbf79d4a6315199b1193eff3498c0d7b tags/HikariCP-2.3.1~10', 'commitMessage': 'Fix #236 use a synchronous Executor to execute setNetworkConnection() to avoid race conditions.\n', 'commitParents': ['2eefb6d4cd753f786988ace3c84c95865ce94411'], 'spoonStatsSkippedReason': '', 'commitHash': 'c24e6d56bbf79d4a6315199b1193eff3498c0d7b', 'authoredDateTime': '2015-01-23 10:10:44', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 20, 'deletions': 6, 'lines': 26, 'filePath': 'hikaricp-common/src/main/java/com/zaxxer/hikari/pool/PoolUtilities.java'}], 'commitDateTime': '2015-01-23 10:10:44', 'commitUser': 'brettwooldridge', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'com.zaxxer.hikari.pool.PoolUtilities.getAndSetNetworkTimeout(java.sql.Connection,long)'}, {'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'com.zaxxer.hikari.pool.PoolUtilities.setNetworkTimeout(java.sql.Connection,long)'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.zaxxer.hikari.pool.PoolUtilities.SynchronousExecutor'}, {'UPD': 3, 'TOT': 5, 'MOV': 0, 'INS': 1, 'DEL': 1, 'spoonMethodName': 'com.zaxxer.hikari.pool.PoolUtilities'}], 'spoonFilePath': 'PoolUtilities.java'}]}, {'nameRev': '8af2bc551af8f5ca6d36318b9afc2f94b917129b tags/HikariCP-2.3.1~8', 'commitMessage': 'Fix #236 via workaround for MySQL issue http://bugs.mysql.com/bug.php?id=75615\n', 'commitParents': ['34e58dfb2b05a3eab75b1f3585b89485191a086e'], 'spoonStatsSkippedReason': '', 'commitHash': '8af2bc551af8f5ca6d36318b9afc2f94b917129b', 'authoredDateTime': '2015-01-24 15:15:49', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 34, 'deletions': 11, 'lines': 45, 'filePath': 'hikaricp-common/src/main/java/com/zaxxer/hikari/pool/PoolUtilities.java'}], 'commitDateTime': '2015-01-24 15:15:49', 'commitUser': 'brettwooldridge', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'com.zaxxer.hikari.pool.PoolUtilities.getAndSetNetworkTimeout(java.sql.Connection,long)'}, {'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'com.zaxxer.hikari.pool.PoolUtilities.setNetworkTimeout(java.sql.Connection,long)'}, {'UPD': 1, 'TOT': 3, 'MOV': 0, 'INS': 0, 'DEL': 2, 'spoonMethodName': 'com.zaxxer.hikari.pool.PoolUtilities'}, {'UPD': 0, 'TOT': 3, 'MOV': 2, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.zaxxer.hikari.pool.PoolUtilities.initializeDataSource(java.lang.String,javax.sql.DataSource,java.util.Properties,java.lang.String,java.lang.String,java.lang.String)'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.zaxxer.hikari.pool.PoolUtilities.createNetworkTimeoutExecutor(javax.sql.DataSource,java.lang.String,java.lang.String)'}], 'spoonFilePath': 'PoolUtilities.java'}]}]",https://github.com/brettwooldridge/HikariCP/issues/236,1.0002777777777778,['bug'],"NullPointerException in utility thread, only appearing on console",2.0,"['com.zaxxer.hikari.pool.PoolUtilities.getAndSetNetworkTimeout(java.sql.Connection,long)', 'com.zaxxer.hikari.pool.PoolUtilities.setNetworkTimeout(java.sql.Connection,long)', 'com.zaxxer.hikari.pool.PoolUtilities', 'com.zaxxer.hikari.pool.PoolUtilities.createNetworkTimeoutExecutor(javax.sql.DataSource,java.lang.String,java.lang.String)', 'com.zaxxer.hikari.pool.PoolUtilities.SynchronousExecutor', 'com.zaxxer.hikari.pool.PoolUtilities.initializeDataSource(java.lang.String,javax.sql.DataSource,java.util.Properties,java.lang.String,java.lang.String,java.lang.String)']","['8af2bc551af8f5ca6d36318b9afc2f94b917129b', 'c24e6d56bbf79d4a6315199b1193eff3498c0d7b']",,['hikaricp-common/src/main/java/com/zaxxer/hikari/pool'],54.0,17.0,71.0,1.0,8.0,6.0,17.0,2.0,4.0,3.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,HikariCP
3276,2013-01-06 19:14:40,tfmorris,"User Ryan Elenbaum reported this exception when shutting down the Refine server

java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:894)
        at java.util.HashMap$EntryIterator.next(HashMap.java:934)
        at java.util.HashMap$EntryIterator.next(HashMap.java:932)
        at com.google.refine.InterProjectModel.flushJoinsInvolvingProject(InterProjectModel.java:109)
        at com.google.refine.model.Project.dispose(Project.java:109)
        at com.google.refine.ProjectManager.saveProjects(ProjectManager.java:265) 
        at com.google.refine.ProjectManager.save(ProjectManager.java:206)
        at com.google.refine.ProjectManager.dispose(ProjectManager.java:99)
        at com.google.refine.RefineServlet.destroy(RefineServlet.java:151)
        at org.mortbay.jetty.servlet.ServletHolder.destroyInstance(ServletHolder.java:318)
        at org.mortbay.jetty.servlet.ServletHolder.doStop(ServletHolder.java:289)
        at org.mortbay.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:76)
",2013-08-17 17:57:03,"[{'nameRev': 'e93bfa798e3e37517a3dc3355ffabc265389c65b tags/2.6-beta.1~10', 'commitMessage': 'Use iterator when removing to avoid ConcurrentModificationException -\nfixes #652', 'commitParents': ['3315136681c799be7e6147d246c4e7291d14c319'], 'spoonStatsSkippedReason': '', 'commitHash': 'e93bfa798e3e37517a3dc3355ffabc265389c65b', 'authoredDateTime': '2013-08-17 13:45:22', 'commitGHEventType': 'closed', 'commitGitStats': [{'insertions': 7, 'deletions': 4, 'lines': 11, 'filePath': 'main/src/com/google/refine/InterProjectModel.java'}], 'commitDateTime': '2013-08-17 13:45:22', 'commitUser': 'tfmorris', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 1, 'TOT': 10, 'MOV': 5, 'INS': 1, 'DEL': 3, 'spoonMethodName': 'com.google.refine.InterProjectModel.flushJoinsInvolvingProjectColumn(long,java.lang.String)'}, {'UPD': 1, 'TOT': 11, 'MOV': 6, 'INS': 1, 'DEL': 3, 'spoonMethodName': 'com.google.refine.InterProjectModel.flushJoinsInvolvingProject(long)'}], 'spoonFilePath': 'InterProjectModel.java'}]}]",https://github.com/OpenRefine/OpenRefine/issues/652,222.00027777777777,"['bug', 'priority: Critical']",ConcurrentModificationException when flushing projects at shutdown,1.0,"['com.google.refine.InterProjectModel.flushJoinsInvolvingProjectColumn(long,java.lang.String)', 'com.google.refine.InterProjectModel.flushJoinsInvolvingProject(long)']",['e93bfa798e3e37517a3dc3355ffabc265389c65b'],,['main/src/com/google/refine'],7.0,4.0,11.0,1.0,2.0,2.0,21.0,11.0,2.0,6.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,OpenRefine
4645,2017-03-11 15:16:49,isopov,"It seems that NettyResponseFuture.cancel() is written with concurency support with CAS to prevent double execution. But assigning null to timeoutsHolder is not protected with CAS or storing it to the local. This may result in NPE. Here it the test, that may reproduce it (or may not):
```
package com.sopovs.moradanen.jcstress.asynchttpclient;

import java.util.concurrent.BlockingQueue;
import java.util.concurrent.LinkedBlockingQueue;

import org.asynchttpclient.DefaultAsyncHttpClient;
import org.asynchttpclient.ListenableFuture;
import org.asynchttpclient.Response;

public class RaceCancelTest {
	public static void main(String[] args) throws InterruptedException {
		try (DefaultAsyncHttpClient httpClient = new DefaultAsyncHttpClient()) {
			BlockingQueue<ListenableFuture<Response>> queue1 = new LinkedBlockingQueue<>(),
					queue2 = new LinkedBlockingQueue<>();
			Thread thread1 = new CancellingThread(queue1), thread2 = new CancellingThread(queue2);
			thread1.start();
			thread2.start();
			for (int i = 0; i < 100; i++) {
				ListenableFuture<Response> future = httpClient.prepareGet(""http://google.com"").execute();
				queue1.add(future);
				queue2.add(future);
				while(!queue1.isEmpty() || !queue2.isEmpty()){
					Thread.yield();
				}
			}
			thread1.interrupt();
			thread2.interrupt();
		}
	}

	static class CancellingThread extends Thread {
		private final BlockingQueue<ListenableFuture<Response>> queue;

		public CancellingThread(BlockingQueue<ListenableFuture<Response>> queue) {
			this.queue = queue;
		}

		@Override
		public void run() {
			while (!this.isInterrupted()) {
				try {
					ListenableFuture<Response> future = queue.take();
					future.cancel(true);
				} catch (InterruptedException e) {
					this.interrupt();
				} catch (Exception e) {
					e.printStackTrace();
				}
			}
		}
	}
}
```
And here is the stacktrace:
```
java.lang.NullPointerException
	at org.asynchttpclient.netty.NettyResponseFuture.cancelTimeouts(NettyResponseFuture.java:286)
	at org.asynchttpclient.netty.NettyResponseFuture.cancel(NettyResponseFuture.java:143)
	at com.sopovs.moradanen.jcstress.asynchttpclient.RaceCancelTest$CancellingThread.run(RaceCancelTest.java:44)
```",2017-03-11 20:06:38,"[{'nameRev': '', 'commitMessage': '', 'commitParents': [], 'spoonStatsSkippedReason': '', 'commitHash': '16bd7473380b8e2c6061135aa1575b2eeaae2098', 'authoredDateTime': '', 'commitGHEventType': 'referenced', 'commitGitStats': [], 'commitDateTime': '', 'commitUser': 'slandelle', 'commitSpoonAstDiffStats': []}, {'nameRev': '', 'commitMessage': '', 'commitParents': [], 'spoonStatsSkippedReason': '', 'commitHash': '7743c664edafc633266af00a04d77dd01670c34a', 'authoredDateTime': '', 'commitGHEventType': 'closed', 'commitGitStats': [], 'commitDateTime': '', 'commitUser': 'slandelle', 'commitSpoonAstDiffStats': []}, {'nameRev': '5bc7c5058514f4239f5d19d3a807e9cf8803f39a tags/async-http-client-project-2.1.0-alpha7~1', 'commitMessage': 'Make cancelTimeouts threadsafe, close #1365\n\nMotivation:\n\n`cancelTimeouts` is currently not threadsafe. We can end up with a NPE\n, eg when request completes at the same time user cancel the future.\n\nModification:\n\nUse CAS to perform `cancelTimeouts`.\n\nResult:\n\nNo more NPE on `cancelTimeouts`.\n', 'commitParents': ['40f35bae1ae983343b28ebc726cf2b391c8e0a52'], 'spoonStatsSkippedReason': '', 'commitHash': '5bc7c5058514f4239f5d19d3a807e9cf8803f39a', 'authoredDateTime': '2017-03-11 21:03:08', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 11, 'deletions': 7, 'lines': 18, 'filePath': 'client/src/main/java/org/asynchttpclient/netty/NettyResponseFuture.java'}], 'commitDateTime': '2017-03-11 21:18:36', 'commitUser': 'slandelle', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 3, 'MOV': 1, 'INS': 1, 'DEL': 1, 'spoonMethodName': 'org.asynchttpclient.netty.NettyResponseFuture.setTimeoutsHolder(org.asynchttpclient.netty.timeout.TimeoutsHolder)'}, {'UPD': 0, 'TOT': 3, 'MOV': 1, 'INS': 2, 'DEL': 0, 'spoonMethodName': 'org.asynchttpclient.netty.NettyResponseFuture'}, {'UPD': 0, 'TOT': 2, 'MOV': 0, 'INS': 1, 'DEL': 1, 'spoonMethodName': 'org.asynchttpclient.netty.NettyResponseFuture.getTimeoutsHolder()'}, {'UPD': 0, 'TOT': 6, 'MOV': 0, 'INS': 3, 'DEL': 3, 'spoonMethodName': 'org.asynchttpclient.netty.NettyResponseFuture.cancelTimeouts()'}, {'UPD': 0, 'TOT': 2, 'MOV': 0, 'INS': 1, 'DEL': 1, 'spoonMethodName': 'org.asynchttpclient.netty.NettyResponseFuture.toString()'}], 'spoonFilePath': 'NettyResponseFuture.java'}]}, {'nameRev': 'cda5ac54881d9825bfebcddec2b655e6e2d08a14 tags/async-http-client-project-2.0.31~3', 'commitMessage': 'Make cancelTimeouts threadsafe, close #1365\n\nMotivation:\n\n`cancelTimeouts` is currently not threadsafe. We can end up with a NPE\n, eg when request completes at the same time user cancel the future.\n\nModification:\n\nUse CAS to perform `cancelTimeouts`.\n\nResult:\n\nNo more NPE on `cancelTimeouts`.\n', 'commitParents': ['5e2a9098e97705553261c052dfb0e155bfb2bd75'], 'spoonStatsSkippedReason': '', 'commitHash': 'cda5ac54881d9825bfebcddec2b655e6e2d08a14', 'authoredDateTime': '2017-03-11 21:03:08', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 11, 'deletions': 7, 'lines': 18, 'filePath': 'client/src/main/java/org/asynchttpclient/netty/NettyResponseFuture.java'}], 'commitDateTime': '2017-03-11 21:18:19', 'commitUser': 'slandelle', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 3, 'MOV': 1, 'INS': 1, 'DEL': 1, 'spoonMethodName': 'org.asynchttpclient.netty.NettyResponseFuture.setTimeoutsHolder(org.asynchttpclient.netty.timeout.TimeoutsHolder)'}, {'UPD': 0, 'TOT': 3, 'MOV': 1, 'INS': 2, 'DEL': 0, 'spoonMethodName': 'org.asynchttpclient.netty.NettyResponseFuture'}, {'UPD': 0, 'TOT': 2, 'MOV': 0, 'INS': 1, 'DEL': 1, 'spoonMethodName': 'org.asynchttpclient.netty.NettyResponseFuture.getTimeoutsHolder()'}, {'UPD': 0, 'TOT': 6, 'MOV': 0, 'INS': 3, 'DEL': 3, 'spoonMethodName': 'org.asynchttpclient.netty.NettyResponseFuture.cancelTimeouts()'}, {'UPD': 0, 'TOT': 2, 'MOV': 0, 'INS': 1, 'DEL': 1, 'spoonMethodName': 'org.asynchttpclient.netty.NettyResponseFuture.toString()'}], 'spoonFilePath': 'NettyResponseFuture.java'}]}]",https://github.com/AsyncHttpClient/async-http-client/issues/1365,0.0002777777777777778,['Defect'],Race NettyResponseFuture.cancel() may produce NPE,1.0,"['org.asynchttpclient.netty.NettyResponseFuture.setTimeoutsHolder(org.asynchttpclient.netty.timeout.TimeoutsHolder)', 'org.asynchttpclient.netty.NettyResponseFuture', 'org.asynchttpclient.netty.NettyResponseFuture.getTimeoutsHolder()', 'org.asynchttpclient.netty.NettyResponseFuture.cancelTimeouts()', 'org.asynchttpclient.netty.NettyResponseFuture.toString()']",['5bc7c5058514f4239f5d19d3a807e9cf8803f39a'],,['client/src/main/java/org/asynchttpclient/netty'],11.0,7.0,18.0,1.0,0.0,5.0,16.0,2.0,8.0,6.0,1.0,0.0,0.0,0.0,3.0,0.0,0.0,async-http-client
4659,2016-11-28 15:41:40,markus-s24,"The ThrottleRequestFilter wraps the AsyncHandler with an AsyncHandlerWrapper. This way all additional interfaces implemented by the originating AsyncHandler get lost. E.g. ProgressAsyncHandler, ResumableAsyncHandler or any custom interfaces.

Suggestion: Maybe use a dynamic proxy which implements all interfaces of the originating AsyncHandler instead of using AsyncHandlerWrapper.
",2016-11-30 13:02:57,"[{'nameRev': '4dbf11c19805c4a474dbe05ce61824ffb1602e5c tags/async-http-client-project-2.1.0-alpha2~2', 'commitMessage': 'Fix semaphore not being released on Throwable, see #1314\n\nMotivation:\n\nFix for #1314 introduced a regression where the Semaphore leaks and is\nnot released on Throwable.\n\nModification:\n\nRelease Semaphore when called handler method is onThrowable too, not\nonly onComplete.\n\nResult\n\nNo more Semaphore leak\n', 'commitParents': ['fd907c003fa1fb59d0cbcd3e0567814bdb2ded6a'], 'spoonStatsSkippedReason': '', 'commitHash': '4dbf11c19805c4a474dbe05ce61824ffb1602e5c', 'authoredDateTime': '2017-01-12 22:17:55', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 11, 'deletions': 3, 'lines': 14, 'filePath': 'client/src/main/java/org/asynchttpclient/filter/ReleasePermitOnComplete.java'}], 'commitDateTime': '2017-01-12 22:17:55', 'commitUser': 'slandelle', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 4, 'MOV': 2, 'INS': 1, 'DEL': 1, 'spoonMethodName': 'org.asynchttpclient.filter.ReleasePermitOnComplete.wrap(org.asynchttpclient.AsyncHandler,java.util.concurrent.Semaphore).1.invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])'}], 'spoonFilePath': 'ReleasePermitOnComplete.java'}]}, {'nameRev': '3e78a04d58ab904fe668d0cf4c09b31ba7437500 tags/async-http-client-project-2.1.0-alpha2~18', 'commitMessage': 'Make ThrottleRequestFilter preserve interfaces of AsyncHandlers, close #1314\n\n', 'commitParents': ['d59fd205a4eca4c9514ce74440ababd93e74b0bd'], 'spoonStatsSkippedReason': '', 'commitHash': '3e78a04d58ab904fe668d0cf4c09b31ba7437500', 'authoredDateTime': '2016-11-30 14:02:55', 'commitGHEventType': 'closed', 'commitGitStats': [{'insertions': 13, 'deletions': 12, 'lines': 25, 'filePath': 'client/src/main/java/org/asynchttpclient/filter/ThrottleRequestFilter.java'}, {'insertions': 50, 'deletions': 0, 'lines': 50, 'filePath': 'client/src/main/java/org/asynchttpclient/filter/ReleasePermitOnComplete.java'}, {'insertions': 4, 'deletions': 3, 'lines': 7, 'filePath': 'extras/guava/src/main/java/org/asynchttpclient/extras/guava/RateLimitedThrottleRequestFilter.java'}, {'insertions': 0, 'deletions': 79, 'lines': 79, 'filePath': 'client/src/main/java/org/asynchttpclient/filter/AsyncHandlerWrapper.java'}], 'commitDateTime': '2016-11-30 14:02:55', 'commitUser': 'slandelle', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 1, 'spoonMethodName': 'org.asynchttpclient.filter.AsyncHandlerWrapper'}], 'spoonFilePath': 'AsyncHandlerWrapper.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 4, 'MOV': 2, 'INS': 1, 'DEL': 1, 'spoonMethodName': 'org.asynchttpclient.extras.guava.RateLimitedThrottleRequestFilter.filter(org.asynchttpclient.filter.FilterContext)'}], 'spoonFilePath': 'RateLimitedThrottleRequestFilter.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 4, 'MOV': 2, 'INS': 1, 'DEL': 1, 'spoonMethodName': 'org.asynchttpclient.filter.ThrottleRequestFilter.filter(org.asynchttpclient.filter.FilterContext)'}], 'spoonFilePath': 'ThrottleRequestFilter.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'org.asynchttpclient.filter.ReleasePermitOnComplete'}], 'spoonFilePath': 'ReleasePermitOnComplete.java'}]}]",https://github.com/AsyncHttpClient/async-http-client/issues/1314,1.0002777777777778,"['Contributions Welcome!', 'Defect']",ThrottleRequestFilter swallows interfaces,2.0,"['org.asynchttpclient.extras.guava.RateLimitedThrottleRequestFilter.filter(org.asynchttpclient.filter.FilterContext)', 'org.asynchttpclient.filter.ReleasePermitOnComplete', 'org.asynchttpclient.filter.AsyncHandlerWrapper', 'org.asynchttpclient.filter.ReleasePermitOnComplete.wrap(org.asynchttpclient.AsyncHandler,java.util.concurrent.Semaphore).1.invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])', 'org.asynchttpclient.filter.ThrottleRequestFilter.filter(org.asynchttpclient.filter.FilterContext)']","['4dbf11c19805c4a474dbe05ce61824ffb1602e5c', '3e78a04d58ab904fe668d0cf4c09b31ba7437500']",,"['extras/guava/src/main/java/org/asynchttpclient/extras/guava', 'client/src/main/java/org/asynchttpclient/filter']",78.0,97.0,175.0,4.0,0.0,5.0,14.0,6.0,4.0,4.0,4.0,0.0,0.0,0.0,0.0,0.0,0.0,async-http-client
4733,2015-12-16 22:31:28,slandelle,"This happens because the future is still set on the channel so AHC tries to recover or abort it.
",2015-12-16 22:32:11,"[{'nameRev': 'e1ec1d1a0043b82d547347e72848307c399a13e0 tags/async-http-client-project-2.0.0-RC3~2', 'commitMessage': 'Fix race condition on connection reuse, close #1059\n', 'commitParents': ['7c140df8286f92fce3173fa7724d6aeb3498d81f'], 'spoonStatsSkippedReason': '', 'commitHash': 'e1ec1d1a0043b82d547347e72848307c399a13e0', 'authoredDateTime': '2015-12-16 23:32:05', 'commitGHEventType': 'closed', 'commitGitStats': [{'insertions': 3, 'deletions': 1, 'lines': 4, 'filePath': 'client/src/main/java/org/asynchttpclient/netty/request/NettyRequestSender.java'}], 'commitDateTime': '2015-12-16 23:32:05', 'commitUser': 'slandelle', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'org.asynchttpclient.netty.request.NettyRequestSender.sendNextRequest(org.asynchttpclient.Request,org.asynchttpclient.netty.NettyResponseFuture)'}], 'spoonFilePath': 'NettyRequestSender.java'}]}]",https://github.com/AsyncHttpClient/async-http-client/issues/1059,0.0002777777777777778,['Defect'],Race condition when channel gets closed during a reuse before sending next request,1.0,"['org.asynchttpclient.netty.request.NettyRequestSender.sendNextRequest(org.asynchttpclient.Request,org.asynchttpclient.netty.NettyResponseFuture)']",['e1ec1d1a0043b82d547347e72848307c399a13e0'],,['client/src/main/java/org/asynchttpclient/netty/request'],3.0,1.0,4.0,1.0,0.0,1.0,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,async-http-client
4737,2015-11-27 13:06:08,slandelle,"```
java.lang.NullPointerException: null
    at org.asynchttpclient.netty.NettyResponseFuture.getChannelRemoteAddress(NettyResponseFuture.java:414) ~[async-http-client-2.0.0-alpha27.jar:na]
    at org.asynchttpclient.netty.timeout.TimeoutTimerTask.<init>(TimeoutTimerTask.java:42) ~[async-http-client-2.0.0-alpha27.jar:na]
    at org.asynchttpclient.netty.timeout.RequestTimeoutTimerTask.<init>(RequestTimeoutTimerTask.java:31) ~[async-http-client-2.0.0-alpha27.jar:na]
    at org.asynchttpclient.netty.request.NettyRequestSender.scheduleTimeouts(NettyRequestSender.java:365) [async-http-client-2.0.0-alpha27.jar:na]
    at org.asynchttpclient.netty.request.NettyRequestSender.writeRequest(NettyRequestSender.java:346) [async-http-client-2.0.0-alpha27.jar:na]
    at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithOpenChannel(NettyRequestSender.java:228) [async-http-client-2.0.0-alpha27.jar:na]
    at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithCertainForceConnect(NettyRequestSender.java:139) [async-http-client-2.0.0-alpha27.jar:na]
    at org.asynchttpclient.netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:112) [async-http-client-2.0.0-alpha27.jar:na]
    at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:220) [async-http-client-2.0.0-alpha27.jar:na]
    at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:188) [async-http-client-2.0.0-alpha27.jar:na]
```
",2015-11-27 13:07:13,"[{'nameRev': 'fecb1167f056447bbc9e53ff989bf27c638fe978 tags/async-http-client-project-2.0.0-RC1~3', 'commitMessage': 'Fix race condition causing NPE, close #1049\n', 'commitParents': ['2e81bcc6cbfe04312a9b9d997bedfd3730e151b0'], 'spoonStatsSkippedReason': '', 'commitHash': 'fecb1167f056447bbc9e53ff989bf27c638fe978', 'authoredDateTime': '2015-11-27 14:07:06', 'commitGHEventType': 'closed', 'commitGitStats': [{'insertions': 4, 'deletions': 2, 'lines': 6, 'filePath': 'client/src/main/java/org/asynchttpclient/netty/timeout/TimeoutTimerTask.java'}, {'insertions': 0, 'deletions': 5, 'lines': 5, 'filePath': 'client/src/main/java/org/asynchttpclient/netty/NettyResponseFuture.java'}], 'commitDateTime': '2015-11-27 14:07:06', 'commitUser': 'slandelle', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 1, 'spoonMethodName': 'org.asynchttpclient.netty.NettyResponseFuture.getChannelRemoteAddress()'}], 'spoonFilePath': 'NettyResponseFuture.java'}, {'spoonMethods': [{'UPD': 2, 'TOT': 7, 'MOV': 2, 'INS': 2, 'DEL': 1, 'spoonMethodName': 'org.asynchttpclient.netty.timeout.TimeoutTimerTask'}], 'spoonFilePath': 'TimeoutTimerTask.java'}]}]",https://github.com/AsyncHttpClient/async-http-client/issues/1049,0.0002777777777777778,['Defect'],Race condition causing NPE when a timeout gets scheduled as the ResponseFuture gets terminated,1.0,"['org.asynchttpclient.netty.NettyResponseFuture.getChannelRemoteAddress()', 'org.asynchttpclient.netty.timeout.TimeoutTimerTask']",['fecb1167f056447bbc9e53ff989bf27c638fe978'],,"['client/src/main/java/org/asynchttpclient/netty', 'client/src/main/java/org/asynchttpclient/netty/timeout']",4.0,7.0,11.0,2.0,2.0,2.0,8.0,2.0,2.0,2.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,async-http-client
4871,2014-04-12 20:51:45,slandelle,"Causing NPE:

```
java.lang.NullPointerException
    at com.ning.http.client.providers.netty.timeout.TimeoutsHolder.cancel(TimeoutsHolder.java:27)
    at com.ning.http.client.providers.netty.timeout.IdleConnectionTimeoutTimerTask.run(IdleConnectionTimeoutTimerTask.java:66)
```
",2014-04-18 08:32:32,"[{'nameRev': '63c51410d8a73e9406f996a06d4431ac70c14a9e tags/2.0.0-alpha1~441', 'commitMessage': 'idleConnectionTimeout might have been dropped or not set, close #534\n', 'commitParents': ['56b7228f52e34300ce209c25fdc55775dff90e16'], 'spoonStatsSkippedReason': '', 'commitHash': '63c51410d8a73e9406f996a06d4431ac70c14a9e', 'authoredDateTime': '2014-04-18 10:32:33', 'commitGHEventType': 'closed', 'commitGitStats': [{'insertions': 9, 'deletions': 5, 'lines': 14, 'filePath': 'providers/netty/src/main/java/org/asynchttpclient/providers/netty/request/timeout/TimeoutsHolder.java'}], 'commitDateTime': '2014-04-18 10:32:33', 'commitUser': 'slandelle', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 6, 'MOV': 4, 'INS': 1, 'DEL': 1, 'spoonMethodName': 'org.asynchttpclient.providers.netty.request.timeout.TimeoutsHolder.cancel()'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'org.asynchttpclient.providers.netty.request.timeout.TimeoutsHolder'}], 'spoonFilePath': 'TimeoutsHolder.java'}]}, {'nameRev': '1643fa23e827ea550a1c59917f6d24be2710eac7 tags/async-http-client-1.8.7~1', 'commitMessage': 'MakeTimeoutsHolder threadsafe, close #534\n', 'commitParents': ['e025988b591add12a819e9a4af8ec09ea3d3cb0e'], 'spoonStatsSkippedReason': '', 'commitHash': '1643fa23e827ea550a1c59917f6d24be2710eac7', 'authoredDateTime': '2014-04-12 22:52:09', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 5, 'deletions': 4, 'lines': 9, 'filePath': 'src/main/java/com/ning/http/client/providers/netty/timeout/TimeoutsHolder.java'}], 'commitDateTime': '2014-04-12 22:52:09', 'commitUser': 'slandelle', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 6, 'MOV': 3, 'INS': 1, 'DEL': 2, 'spoonMethodName': 'com.ning.http.client.providers.netty.timeout.TimeoutsHolder.cancel()'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.ning.http.client.providers.netty.timeout.TimeoutsHolder'}], 'spoonFilePath': 'TimeoutsHolder.java'}]}, {'nameRev': '3b41defac647ead03343da8b81c7fae9da8b5cb1 tags/2.0.0-alpha1~444', 'commitMessage': 'MakeTimeoutsHolder threadsafe, close #534\n', 'commitParents': ['710a19fd4a048208f4924f9514e0a97b086230d0'], 'spoonStatsSkippedReason': '', 'commitHash': '3b41defac647ead03343da8b81c7fae9da8b5cb1', 'authoredDateTime': '2014-04-12 22:54:03', 'commitGHEventType': 'closed', 'commitGitStats': [{'insertions': 5, 'deletions': 4, 'lines': 9, 'filePath': 'providers/netty/src/main/java/org/asynchttpclient/providers/netty/request/timeout/TimeoutsHolder.java'}], 'commitDateTime': '2014-04-12 22:54:03', 'commitUser': 'slandelle', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 6, 'MOV': 3, 'INS': 1, 'DEL': 2, 'spoonMethodName': 'org.asynchttpclient.providers.netty.request.timeout.TimeoutsHolder.cancel()'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'org.asynchttpclient.providers.netty.request.timeout.TimeoutsHolder'}], 'spoonFilePath': 'TimeoutsHolder.java'}]}, {'nameRev': '051792f0f4a3cf709c334761e133598075217caa tags/async-http-client-1.8.8~5', 'commitMessage': 'idleConnectionTimeout might have been dropped or not set, close #534\n', 'commitParents': ['93652c385e2462211ea2586ec41cffc2da0803ac'], 'spoonStatsSkippedReason': '', 'commitHash': '051792f0f4a3cf709c334761e133598075217caa', 'authoredDateTime': '2014-04-18 10:31:04', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 9, 'deletions': 5, 'lines': 14, 'filePath': 'src/main/java/com/ning/http/client/providers/netty/timeout/TimeoutsHolder.java'}], 'commitDateTime': '2014-04-18 10:31:04', 'commitUser': 'slandelle', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 6, 'MOV': 4, 'INS': 1, 'DEL': 1, 'spoonMethodName': 'com.ning.http.client.providers.netty.timeout.TimeoutsHolder.cancel()'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.ning.http.client.providers.netty.timeout.TimeoutsHolder'}], 'spoonFilePath': 'TimeoutsHolder.java'}]}]",https://github.com/AsyncHttpClient/async-http-client/issues/534,5.000277777777778,"['Defect', 'Netty']",TimeoutsHolder isn't threadsafe,2.0,"['com.ning.http.client.providers.netty.timeout.TimeoutsHolder.cancel()', 'com.ning.http.client.providers.netty.timeout.TimeoutsHolder', 'org.asynchttpclient.providers.netty.request.timeout.TimeoutsHolder.cancel()', 'org.asynchttpclient.providers.netty.request.timeout.TimeoutsHolder']","['63c51410d8a73e9406f996a06d4431ac70c14a9e', '1643fa23e827ea550a1c59917f6d24be2710eac7']",,"['providers/netty/src/main/java/org/asynchttpclient/providers/netty/request', 'src/main/java/com/ning/http/client/providers/netty/timeout']",14.0,9.0,23.0,2.0,0.0,4.0,14.0,7.0,4.0,3.0,1.0,0.0,0.0,0.0,2.0,0.0,0.0,async-http-client
4979,2017-10-16 20:30:44,a18250,"<!--
Hi there! thank you for discovering and submitting an issue!

Please first tell us a little bit about the environment you're running:
The commands in the comments can be run directly in a command prompt.
-->


- **OS and version used:** <Android 7.1.1> <!-- Windows 10, Ubuntu 15.04... -->

- **Java runtime used:** <Java 8> <!-- Please include runtime and version -->

- **SDK version used:** <1.5.36> <!-- Please include the SDK version -->


# Description of the issue:
<!-- please be as detailed as possible: which feature has a problem, how often does it fail,  -->
My code on mobile device calls SDK in following order:

0. network is available
1. DeviceClient.open()
2. DeviceClient.setMessageCallback()
3. subscribeToDeviceMethod()
4. device is now listening for incoming IoT message/method
5. network connection lost
6. DeviceClient.closeNow()

Problem is that at step 6, the closeNow() never returns and the calling thread is blocked forever (at least I waited for minutes).

If I remove step 3, then there is no block at step 6. This seems indicate the issue is related with direct method handling.

Step into SDK code with Android Studio, it appears the code is blocked on a wait object at:

```
void unsubscribe(String topic) throws IOException {
    synchronized(this.mqttLock) { <<<< lock here
        try {
            if(!this.mqttConnection.getMqttAsyncClient().isConnected()) {
                throw new IOException(""Cannot unsubscribe when mqtt client is disconnected"");
            }

            if(this.userSpecifiedSASTokenExpiredOnRetry) {
                throw new IOException(""Cannot unsubscribe when user supplied SAS token has expired"");
            }

            IMqttToken e = this.mqttConnection.getMqttAsyncClient().unsubscribe(topic);
            e.waitForCompletion();
        } catch (MqttException var5) {
            throw new IOException(""Unable to unsubscribe to topic :"" + topic + ""because "" + var5.getCause() + var5.getMessage());
        }

    }
}
```


# Code sample exhibiting the issue:
<!-- Please remove any connection string information! -->
Just the 6 steps above
# Console log of the issue:
<!-- Consider setting the DEBUG environment variable to '*'. This will produce a much more verbose output that will help debugging -->
<!-- Don't forget to remove any connection string information! -->

",2017-12-05 18:03:11,"[{'nameRev': '', 'commitMessage': '', 'commitParents': [], 'spoonStatsSkippedReason': '', 'commitHash': '098428e3d637c1095236cbf423f97e693e164799', 'authoredDateTime': '', 'commitGHEventType': 'referenced', 'commitGitStats': [], 'commitDateTime': '', 'commitUser': 'timtay-microsoft', 'commitSpoonAstDiffStats': []}, {'nameRev': '', 'commitMessage': '', 'commitParents': [], 'spoonStatsSkippedReason': '', 'commitHash': '22e78f7417e0c4805009a55aaa21a652dafe95aa', 'authoredDateTime': '', 'commitGHEventType': 'referenced', 'commitGitStats': [], 'commitDateTime': '', 'commitUser': 'timtay-microsoft', 'commitSpoonAstDiffStats': []}, {'nameRev': '', 'commitMessage': '', 'commitParents': [], 'spoonStatsSkippedReason': '', 'commitHash': 'dddacf43ed99a88b16d44e1d3de48be8cb138f3d', 'authoredDateTime': '', 'commitGHEventType': 'referenced', 'commitGitStats': [], 'commitDateTime': '', 'commitUser': 'timtay-microsoft', 'commitSpoonAstDiffStats': []}, {'nameRev': '', 'commitMessage': '', 'commitParents': [], 'spoonStatsSkippedReason': '', 'commitHash': '795d5d1b56d3a3f1be170a2bbda87877bb499c29', 'authoredDateTime': '', 'commitGHEventType': 'referenced', 'commitGitStats': [], 'commitDateTime': '', 'commitUser': 'timtay-microsoft', 'commitSpoonAstDiffStats': []}, {'nameRev': '', 'commitMessage': '', 'commitParents': [], 'spoonStatsSkippedReason': '', 'commitHash': 'cbf23a179809bd0dae505afaed3d9f7213872c71', 'authoredDateTime': '', 'commitGHEventType': 'referenced', 'commitGitStats': [], 'commitDateTime': '', 'commitUser': 'timtay-microsoft', 'commitSpoonAstDiffStats': []}, {'nameRev': '', 'commitMessage': '', 'commitParents': [], 'spoonStatsSkippedReason': '', 'commitHash': '5efa24ffbeb26d1b5801a4325fe3e0beed99b804', 'authoredDateTime': '', 'commitGHEventType': 'referenced', 'commitGitStats': [], 'commitDateTime': '', 'commitUser': 'timtay-microsoft', 'commitSpoonAstDiffStats': []}, {'nameRev': '5315b893ec130cdc09c56bdf538e20faaa44f79b tags/2017-12-2~21^2', 'commitMessage': 'Fixing a bug where deadlock can occur in MQTT when internet connection is lost\n\ngithub bug #167\n', 'commitParents': ['da34ccf610e1e6584c8e7c6063e5eb6cbe555247'], 'spoonStatsSkippedReason': '', 'commitHash': '5315b893ec130cdc09c56bdf538e20faaa44f79b', 'authoredDateTime': '2017-11-16 10:11:48', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 0, 'deletions': 63, 'lines': 63, 'filePath': 'device/iot-device-client/src/test/java/tests/unit/com/microsoft/azure/sdk/iot/device/transport/mqtt/MqttDeviceMethodTest.java'}, {'insertions': 2, 'deletions': 8, 'lines': 10, 'filePath': 'device/iot-device-client/src/main/java/com/microsoft/azure/sdk/iot/device/transport/mqtt/MqttDeviceMethod.java'}, {'insertions': 0, 'deletions': 50, 'lines': 50, 'filePath': 'device/iot-device-client/src/test/java/tests/unit/com/microsoft/azure/sdk/iot/device/transport/mqtt/MqttDeviceTwinTest.java'}, {'insertions': 0, 'deletions': 1, 'lines': 1, 'filePath': 'device/iot-device-client/devdoc/requirement_docs/com/microsoft/azure/iothub/transport/mqtt/mqttdevicetwin_requirements.md'}, {'insertions': 0, 'deletions': 44, 'lines': 44, 'filePath': 'device/iot-device-client/src/main/java/com/microsoft/azure/sdk/iot/device/transport/mqtt/Mqtt.java'}, {'insertions': 1, 'deletions': 8, 'lines': 9, 'filePath': 'device/iot-device-client/src/main/java/com/microsoft/azure/sdk/iot/device/transport/mqtt/MqttDeviceTwin.java'}, {'insertions': 7, 'deletions': 23, 'lines': 30, 'filePath': 'device/iot-device-client/devdoc/requirement_docs/com/microsoft/azure/iothub/transport/mqtt/mqtt_requirements.md'}, {'insertions': 0, 'deletions': 25, 'lines': 25, 'filePath': 'device/iot-device-client/src/test/java/tests/unit/com/microsoft/azure/sdk/iot/device/transport/mqtt/MqttTest.java'}, {'insertions': 0, 'deletions': 1, 'lines': 1, 'filePath': 'device/iot-device-client/devdoc/requirement_docs/com/microsoft/azure/iothub/transport/mqtt/mqttdevicemethod_requirements.md'}], 'commitDateTime': '2017-11-16 10:11:48', 'commitUser': 'timtay-microsoft', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 1, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttTest.unsubscribeThrowsExceptionWhenUserSuppliedSASTokenHasExpired()'}], 'spoonFilePath': 'MqttTest.java'}, {'spoonMethods': [{'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceTwinTest.sendDoesNotThrowsIoExceptionIfMessageIsEmpty(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt,com.microsoft.azure.sdk.iot.device.transport.IotHubTransportMessage).20'}, {'UPD': 1, 'TOT': 2, 'MOV': 1, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceTwinTest.sendPublishesMessageForUpdateReportedPropertiesOnCorrectTopic(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt,com.microsoft.azure.sdk.iot.device.transport.IotHubTransportMessage).11'}, {'UPD': 1, 'TOT': 2, 'MOV': 1, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceTwinTest.sendPublishesMessageForGetTwinOnCorrectTopic(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt,com.microsoft.azure.sdk.iot.device.transport.IotHubTransportMessage).7'}, {'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceTwinTest.sendDoesNotPublishesMessageForSubscribeToDesiredPropertiesOnCorrectTopic(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt,com.microsoft.azure.sdk.iot.device.transport.IotHubTransportMessage).15'}, {'UPD': 0, 'TOT': 1, 'MOV': 1, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceTwinTest.stopUnsubscribesFromDeviceTwinResponse(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt).4'}, {'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceTwinTest.sendThrowsIoExceptionIfMessageIsNull(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt,com.microsoft.azure.sdk.iot.device.transport.IotHubTransportMessage).18'}, {'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceTwinTest.sendThrowsExceptionForGetTwinOnCorrectTopicIfReqIdIsNullOrEmpty(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt,com.microsoft.azure.sdk.iot.device.transport.IotHubTransportMessage).8'}, {'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceTwinTest.sendSubscribesMessageForSubscribeToDesiredPropertiesOnCorrectTopic(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt,com.microsoft.azure.sdk.iot.device.transport.IotHubTransportMessage).17'}, {'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceTwinTest.sendPublishesMessageForUpdateReportedPropertiesOnCorrectTopic(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt,com.microsoft.azure.sdk.iot.device.transport.IotHubTransportMessage).10'}, {'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceTwinTest.sendDoesNotThrowsIoExceptionIfMessageIsEmpty(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt,com.microsoft.azure.sdk.iot.device.transport.IotHubTransportMessage).19'}, {'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceTwinTest.sendDoesNotPublishesMessageForSubscribeToDesiredPropertiesOnCorrectTopic(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt,com.microsoft.azure.sdk.iot.device.transport.IotHubTransportMessage).14'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 1, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceTwinTest.stopThrowsExceptionIfUnSubscribesFails(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt)'}, {'UPD': 1, 'TOT': 2, 'MOV': 1, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceTwinTest.sendThrowsExceptionForGetTwinOnCorrectTopicIfReqIdIsNullOrEmpty(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt,com.microsoft.azure.sdk.iot.device.transport.IotHubTransportMessage).9'}, {'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceTwinTest.sendSubscribesMessageForSubscribeToDesiredPropertiesOnCorrectTopic(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt,com.microsoft.azure.sdk.iot.device.transport.IotHubTransportMessage).16'}, {'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceTwinTest.sendThrowsExceptionForUpdateReportedPropertiesOnCorrectTopicIfReqIdIsNullOrEmpty(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt,com.microsoft.azure.sdk.iot.device.transport.IotHubTransportMessage).12'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 1, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceTwinTest.stopUnsubscribesFromDeviceTwinResponse(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt)'}, {'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceTwinTest.sendPublishesMessageForGetTwinOnCorrectTopic(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt,com.microsoft.azure.sdk.iot.device.transport.IotHubTransportMessage).6'}, {'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceTwinTest.sendThrowsExceptionForUpdateReportedPropertiesOnCorrectTopicIfReqIdIsNullOrEmpty(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt,com.microsoft.azure.sdk.iot.device.transport.IotHubTransportMessage).13'}], 'spoonFilePath': 'MqttDeviceTwinTest.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 2, 'MOV': 1, 'INS': 0, 'DEL': 1, 'spoonMethodName': 'com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceTwin.stop()'}], 'spoonFilePath': 'MqttDeviceTwin.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 2, 'MOV': 1, 'INS': 0, 'DEL': 1, 'spoonMethodName': 'com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceMethod.stop()'}], 'spoonFilePath': 'MqttDeviceMethod.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 1, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceMethodTest.stopSucceedsDoesNotCallUnSubscribeIfStopped(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt)'}, {'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceMethodTest.sendDoesNotSendOnDifferentMessageType(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt).9'}, {'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceMethodTest.sendSucceedsCallsSubscribe(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt).7'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 1, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceMethodTest.stopSucceedsCallsUnSubscribe(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt)'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 1, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceMethodTest.stopSucceedsDoesNotCallUnSubscribeIfNotStarted(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt)'}, {'UPD': 1, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'tests.unit.com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceMethodTest.sendSucceedsCallsPublish(com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt).8'}], 'spoonFilePath': 'MqttDeviceMethodTest.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 1, 'spoonMethodName': 'com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt.unsubscribe(java.lang.String)'}], 'spoonFilePath': 'Mqtt.java'}]}, {'nameRev': '', 'commitMessage': '', 'commitParents': [], 'spoonStatsSkippedReason': '', 'commitHash': '1dfb8859fe4029ba2d3764bcda129a6a4d2b2015', 'authoredDateTime': '', 'commitGHEventType': 'referenced', 'commitGitStats': [], 'commitDateTime': '', 'commitUser': 'timtay-microsoft', 'commitSpoonAstDiffStats': []}]",https://github.com/Azure/azure-iot-sdk-java/issues/167,49.000277777777775,"['bug', 'fix checked in']",DeviceClient.closeNow() blocks forever,1.0,"['com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceMethod.stop()', 'com.microsoft.azure.sdk.iot.device.transport.mqtt.Mqtt.unsubscribe(java.lang.String)', 'com.microsoft.azure.sdk.iot.device.transport.mqtt.MqttDeviceTwin.stop()']",['5315b893ec130cdc09c56bdf538e20faaa44f79b'],,['device/iot-device-client/src/main/java/com/microsoft/azure/sdk/iot'],3.0,60.0,63.0,3.0,0.0,3.0,5.0,2.0,0.0,3.0,3.0,0.0,0.0,0.0,7.0,0.0,0.0,azure-iot-sdk-java
5042,2020-05-30 23:17:33,kingwang-git,"IntelliJ build version: 2019.3.4 IU-193.6911.18
OS: Windows 10
JDK: JetBrains s.r.o 11.0.6
Plugin version: 3.37.0-2019.3
Error message: 
```
java.util.ConcurrentModificationException
	at java.base/java.util.HashMap.computeIfAbsent(HashMap.java:1134)
	at com.microsoft.azuretools.sdkmanage.AccessTokenAzureManager.getAzureSpringCloudClient(AccessTokenAzureManager.java:135)
	at com.microsoft.azuretools.authmanage.AuthMethodManager.getAzureSpringCloudClient(AuthMethodManager.java:84)
	at com.microsoft.azuretools.core.mvp.model.springcloud.AzureSpringCloudMvpModel.getSpringManager(AzureSpringCloudMvpModel.java:262)
	at com.microsoft.azuretools.core.mvp.model.springcloud.AzureSpringCloudMvpModel.listAllSpringCloudClustersBySubscription(AzureSpringCloudMvpModel.java:91)
	at com.microsoft.azuretools.core.mvp.model.springcloud.AzureSpringCloudMvpModel.lambda$null$0(AzureSpringCloudMvpModel.java:77)
	at rx.Observable.unsafeSubscribe(Observable.java:10327)
	at rx.internal.operators.OperatorSubscribeOn$SubscribeOnSubscriber.call(OperatorSubscribeOn.java:100)
	at rx.internal.schedulers.CachedThreadScheduler$EventLoopWorker$1.call(CachedThreadScheduler.java:230)
	at rx.internal.schedulers.ScheduledAction.run(ScheduledAction.java:55)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run",2020-06-04 01:07:54,"[{'nameRev': '6a979b46c65eb45677e713e1f8e90954f7d89d5d remotes/origin/master~11', 'commitMessage': 'Fix ConcurrentModificationException (#4426)\n\nFix issue https://github.com/microsoft/azure-tools-for-java/issues/4422', 'commitParents': ['6a46ec5a4a4ba150c455140c4f2d60f8f9ef0ce9'], 'spoonStatsSkippedReason': '', 'commitHash': '6a979b46c65eb45677e713e1f8e90954f7d89d5d', 'authoredDateTime': '2020-06-03 17:14:07', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 4, 'deletions': 5, 'lines': 9, 'filePath': 'Utils/azuretools-core/src/com/microsoft/azuretools/sdkmanage/AzureManagerBase.java'}], 'commitDateTime': '2020-06-03 17:14:07', 'commitUser': 'andxu', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 2, 'TOT': 2, 'MOV': 0, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'com.microsoft.azuretools.sdkmanage.AzureManagerBase'}], 'spoonFilePath': 'AzureManagerBase.java'}]}, {'nameRev': '', 'commitMessage': '', 'commitParents': [], 'spoonStatsSkippedReason': '', 'commitHash': 'c958e8d5ccd4fd24ab92e19e993ff87f7e9fc70d', 'authoredDateTime': '', 'commitGHEventType': 'referenced', 'commitGitStats': [], 'commitDateTime': '', 'commitUser': 'andxu', 'commitSpoonAstDiffStats': []}]",https://github.com/microsoft/azure-tools-for-java/issues/4422,4.000277777777778,"['bug', 'fixed', 'springcloud']",[IntelliJ][ReportedByUser] MS Azure Explorer - Error Loading Spring Cloud(Preview),1.0,['com.microsoft.azuretools.sdkmanage.AzureManagerBase'],['6a979b46c65eb45677e713e1f8e90954f7d89d5d'],,['Utils/azuretools-core/src/com/microsoft/azuretools/sdkmanage'],4.0,5.0,9.0,1.0,2.0,1.0,2.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,azure-tools-for-java
5611,2019-05-22 08:27:02,philwo,"> ATTENTION! Please read and follow:
> - if this is a _question_ about how to build / test / query / deploy using Bazel, ask it on StackOverflow instead: https://stackoverflow.com/questions/tagged/bazel
> - if this is a _discussion starter_, send it to bazel-discuss@googlegroups.com
> - if this is a _bug_ or _feature request_, fill the form below as best as you can.

### Description of the problem / feature request:

Observed here: https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/115#c9e1d6df-0d66-40a1-ba84-f829f85f1ed6

Bazel 0.25.2 was hanging after running a few `bazel test` builds with various incompatible flags.

I captured the `ps -axuf` and `jstack` of the container. The suspicious part is this:

```
	at com.google.common.util.concurrent.Uninterruptibles.joinUninterruptibly(Uninterruptibles.java:132)
	at com.google.devtools.build.lib.runtime.ExperimentalEventHandler.stopUpdateThread(ExperimentalEventHandler.java:1008)
	at com.google.devtools.build.lib.runtime.ExperimentalEventHandler.completeBuild(ExperimentalEventHandler.java:679)
	at com.google.devtools.build.lib.runtime.ExperimentalEventHandler.afterCommand(ExperimentalEventHandler.java:741)
```

Here's the full log:

```
root@bk-docker-n5kr:~# docker exec -it sweet_hodgkin bash
buildkite-agent@bk-docker-n5kr:/workdir$ ps axuf
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
buildki+ 28086  1.0  0.0  18204  2620 pts/1    Ss   08:17   0:00 bash
buildki+ 28093  0.0  0.0  15576  1604 pts/1    R+   08:17   0:00  \_ ps axuf
buildki+     1  0.0  0.0   1076     8 pts/0    Ss   03:02   0:00 /sbin/docker-init -- /bin/sh -e -c curl -sS https://raw.githubusercontent.com/bazelbuild/continuous-integration/master/buildkite/bazelci.py?1558494111 -o bazelci.py python3.6 bazelci.py runner --task=ubuntu1404 --http_config=https://raw.githubusercontent.com/bazelbuild/rules_go/master/.bazelci
buildki+     6  0.0  0.0   4456   924 pts/0    S+   03:02   0:00 /bin/sh -e -c curl -sS https://raw.githubusercontent.com/bazelbuild/continuous-integration/master/buildkite/bazelci.py?1558494111 -o bazelci.py python3.6 bazelci.py runner --task=ubuntu1404 --http_config=https://raw.githubusercontent.com/bazelbuild/rules_go/master/.bazelci/presubmit.yml --git_
buildki+     9  0.0  0.0  88572 17480 pts/0    S+   03:02   0:00  \_ python3.6 bazelci.py runner --task=ubuntu1404 --http_config=https://raw.githubusercontent.com/bazelbuild/rules_go/master/.bazelci/presubmit.yml --git_repository=https://github.com/bazelbuild/rules_go.git --git_commit=e9ba5a46e82a1b5d14c41b3e5f6e2841507b36d9 --incompatible_flag=--incompatib
buildki+ 16488  0.0  0.0 368060  8336 pts/0    Sl+  03:05   0:00      \_ bazel --migrate test --flaky_test_attempts=3 --build_tests_only --local_test_jobs=12 --show_progress_rate_limit=5 --curses=yes --color=yes --verbose_failures --keep_going --jobs=32 --announce_rc --experimental_multi_threaded_digest --experimental_repository_cache_hardlinks --disk_cache
buildki+ 27475  0.0  0.0 212060  6608 pts/0    Sl+  03:08   0:02          \_ /var/lib/buildkite-agent/.cache/bazelisk/bin/bazel-0.25.2-linux-x86_64 test --flaky_test_attempts=3 --build_tests_only --local_test_jobs=12 --show_progress_rate_limit=5 --curses=yes --color=yes --verbose_failures --keep_going --jobs=32 --announce_rc --experimental_multi_threaded_di
buildki+   187  8.8  8.3 51239872 10374388 ?   Ssl  03:02  27:48 bazel(rules_go) -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/var/lib/buildkite-agent/.cache/bazel/_bazel_buildkite-agent/05c08a510716cf145d3810bd22200732 -XX:+UseParallelOldGC --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED -Xverify:none -Djava.util.

buildkite-agent@bk-docker-n5kr:/workdir$ jstack 187
2019-05-22 08:18:04
Full thread dump OpenJDK 64-Bit Server VM (11.0.2+7-LTS mixed mode):

Threads class SMR info:
_java_thread_list=0x00007f06d4001c60, length=58, elements={
0x00007f0748012000, 0x00007f07483f2000, 0x00007f07483f4000, 0x00007f074840f800,
0x00007f0748411800, 0x00007f0748413800, 0x00007f0748416000, 0x00007f074848c000,
0x00007f0748492800, 0x00007f0748b84000, 0x00007f0748d1b000, 0x00007f0748d0f800,
0x00007f063c006800, 0x00007f06b01a9800, 0x00007f06b0265000, 0x00007f06b0267800,
0x00007f063c008800, 0x00007f063c009800, 0x00007f041c001000, 0x00007f063c00b000,
0x00007f063c00c800, 0x00007f063c00d800, 0x00007f063c00e000, 0x00007f063c00f800,
0x00007f063c010800, 0x00007f063c011800, 0x00007f063c012800, 0x00007f063c014000,
0x00007f063c015000, 0x00007f063c016000, 0x00007f063c017000, 0x00007f063c018800,
0x00007f063c019800, 0x00007f063c01a800, 0x00007f063c01b800, 0x00007f063c01d000,
0x00007f063c01e000, 0x00007f063c01f000, 0x00007f063c020800, 0x00007f063c021800,
0x00007f063c022800, 0x00007f063c023800, 0x00007f063c025000, 0x00007f063c026000,
0x00007f063c027000, 0x00007f063c028000, 0x00007f063c029800, 0x00007f063c02a800,
0x00007f063c02b800, 0x00007f063c02c800, 0x00007f063c02e000, 0x00007f063c02f000,
0x00007f063c030000, 0x00007f063c031000, 0x00007f063c032800, 0x00007f063c033800,
0x00007f02f4477000, 0x00007f06d4001000
}

""main"" #1 prio=5 os_prio=0 cpu=1034.63ms elapsed=18923.62s tid=0x00007f0748012000 nid=0xbc in Object.wait()  [0x00007f0751c65000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(java.base@11.0.2/Native Method)
	- waiting on <0x000000009fde0ea0> (a java.lang.Object)
	at java.lang.Object.wait(java.base@11.0.2/Unknown Source)
	at io.grpc.internal.ServerImpl.awaitTermination(ServerImpl.java:265)
	- waiting to re-lock in wait() <0x000000009fde0ea0> (a java.lang.Object)
	at com.google.devtools.build.lib.server.GrpcServerImpl.serve(GrpcServerImpl.java:620)
	at com.google.devtools.build.lib.runtime.BlazeRuntime.serverMain(BlazeRuntime.java:1036)
	at com.google.devtools.build.lib.runtime.BlazeRuntime.main(BlazeRuntime.java:738)
	at com.google.devtools.build.lib.bazel.Bazel.main(Bazel.java:71)

""Reference Handler"" #2 daemon prio=10 os_prio=0 cpu=180.73ms elapsed=18923.60s tid=0x00007f07483f2000 nid=0xbf waiting on condition  [0x00007f072c550000]
   java.lang.Thread.State: RUNNABLE
	at java.lang.ref.Reference.waitForReferencePendingList(java.base@11.0.2/Native Method)
	at java.lang.ref.Reference.processPendingReferences(java.base@11.0.2/Unknown Source)
	at java.lang.ref.Reference$ReferenceHandler.run(java.base@11.0.2/Unknown Source)

""Finalizer"" #3 daemon prio=8 os_prio=0 cpu=80.99ms elapsed=18923.59s tid=0x00007f07483f4000 nid=0xc0 in Object.wait()  [0x00007f072c44f000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(java.base@11.0.2/Native Method)
	- waiting on <no object reference available>
	at java.lang.ref.ReferenceQueue.remove(java.base@11.0.2/Unknown Source)
	- waiting to re-lock in wait() <0x000000009f627800> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(java.base@11.0.2/Unknown Source)
	at java.lang.ref.Finalizer$FinalizerThread.run(java.base@11.0.2/Unknown Source)

""Signal Dispatcher"" #4 daemon prio=9 os_prio=0 cpu=0.95ms elapsed=18923.58s tid=0x00007f074840f800 nid=0xc1 runnable  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread0"" #5 daemon prio=9 os_prio=0 cpu=48852.42ms elapsed=18923.58s tid=0x00007f0748411800 nid=0xc2 waiting on condition  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
   No compile task

""C1 CompilerThread0"" #15 daemon prio=9 os_prio=0 cpu=5645.14ms elapsed=18923.58s tid=0x00007f0748413800 nid=0xc3 waiting on condition  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
   No compile task

""Sweeper thread"" #20 daemon prio=9 os_prio=0 cpu=1573.24ms elapsed=18923.58s tid=0x00007f0748416000 nid=0xc4 runnable  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Common-Cleaner"" #21 daemon prio=8 os_prio=0 cpu=33.92ms elapsed=18923.55s tid=0x00007f074848c000 nid=0xc7 in Object.wait()  [0x00007f06e91a6000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(java.base@11.0.2/Native Method)
	- waiting on <no object reference available>
	at java.lang.ref.ReferenceQueue.remove(java.base@11.0.2/Unknown Source)
	- waiting to re-lock in wait() <0x000000009f6181a8> (a java.lang.ref.ReferenceQueue$Lock)
	at jdk.internal.ref.CleanerImpl.run(java.base@11.0.2/Unknown Source)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)
	at jdk.internal.misc.InnocuousThread.run(java.base@11.0.2/Unknown Source)

""Service Thread"" #22 daemon prio=9 os_prio=0 cpu=0.21ms elapsed=18923.55s tid=0x00007f0748492800 nid=0xc8 runnable  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""pid-file-watcher"" #25 daemon prio=5 os_prio=0 cpu=1004.97ms elapsed=18922.62s tid=0x00007f0748b84000 nid=0xe4 waiting on condition  [0x00007f06a96c8000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(java.base@11.0.2/Native Method)
	at java.lang.Thread.sleep(java.base@11.0.2/Unknown Source)
	at java.util.concurrent.TimeUnit.sleep(java.base@11.0.2/Unknown Source)
	at com.google.common.util.concurrent.Uninterruptibles.sleepUninterruptibly(Uninterruptibles.java:308)
	at com.google.devtools.build.lib.server.GrpcServerImpl$PidFileWatcherThread.run(GrpcServerImpl.java:435)

""grpc-default-boss-ELG-1-1"" #27 daemon prio=5 os_prio=0 cpu=628.67ms elapsed=18922.48s tid=0x00007f0748d1b000 nid=0xe6 runnable  [0x00007f06a90c6000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x000000009fe69040> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x000000009fe68fe8> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-timeout-and-memory"" #28 daemon prio=5 os_prio=0 cpu=6.95ms elapsed=18922.48s tid=0x00007f0748d0f800 nid=0xe7 in Object.wait()  [0x00007f06a8fc5000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(java.base@11.0.2/Native Method)
	- waiting on <no object reference available>
	at java.lang.Object.wait(java.base@11.0.2/Unknown Source)
	at com.google.devtools.build.lib.server.CommandManager.waitForChange(CommandManager.java:82)
	- waiting to re-lock in wait() <0x000000009fe59750> (a java.util.HashMap)
	at com.google.devtools.build.lib.server.ServerWatcherRunnable.run(ServerWatcherRunnable.java:98)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-1"" #29 daemon prio=5 os_prio=0 cpu=829.34ms elapsed=18922.41s tid=0x00007f063c006800 nid=0xeb runnable  [0x00007f06e8879000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x000000009fe4df70> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x000000009fe4df18> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""threadDeathWatcher-4-1"" #30 daemon prio=1 os_prio=0 cpu=1610.85ms elapsed=18922.37s tid=0x00007f06b01a9800 nid=0xec waiting on condition  [0x00007f06e897a000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(java.base@11.0.2/Native Method)
	at io.netty.util.ThreadDeathWatcher$Watcher.run(ThreadDeathWatcher.java:152)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-stream-0"" #32 daemon prio=5 os_prio=0 cpu=414.78ms elapsed=18922.31s tid=0x00007f06b0265000 nid=0xef waiting on condition  [0x00007f06a8cc4000]
   java.lang.Thread.State: WAITING (parking)
	at jdk.internal.misc.Unsafe.park(java.base@11.0.2/Native Method)
	- parking to wait for  <0x00000005cf3261b8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(java.base@11.0.2/Unknown Source)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(java.base@11.0.2/Unknown Source)
	at java.util.concurrent.LinkedBlockingQueue.take(java.base@11.0.2/Unknown Source)
	at com.google.common.util.concurrent.Uninterruptibles.takeUninterruptibly(Uninterruptibles.java:258)
	at com.google.devtools.build.lib.server.GrpcServerImpl$GrpcSink.call(GrpcServerImpl.java:302)
	at com.google.devtools.build.lib.server.GrpcServerImpl$GrpcSink$$Lambda$224/0x0000000800317040.run(Unknown Source)
	at java.util.concurrent.Executors$RunnableAdapter.call(java.base@11.0.2/Unknown Source)
	at java.util.concurrent.FutureTask.run(java.base@11.0.2/Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@11.0.2/Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@11.0.2/Unknown Source)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-command-0"" #33 daemon prio=5 os_prio=0 cpu=37132.62ms elapsed=18922.31s tid=0x00007f06b0267800 nid=0xf0 in Object.wait()  [0x00007f06a8bc2000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(java.base@11.0.2/Native Method)
	- waiting on <no object reference available>
	at java.lang.Thread.join(java.base@11.0.2/Unknown Source)
	- waiting to re-lock in wait() <0x00000006c81f5d60> (a java.lang.Thread)
	at java.lang.Thread.join(java.base@11.0.2/Unknown Source)
	at com.google.common.util.concurrent.Uninterruptibles.joinUninterruptibly(Uninterruptibles.java:132)
	at com.google.devtools.build.lib.runtime.ExperimentalEventHandler.stopUpdateThread(ExperimentalEventHandler.java:1008)
	at com.google.devtools.build.lib.runtime.ExperimentalEventHandler.completeBuild(ExperimentalEventHandler.java:679)
	at com.google.devtools.build.lib.runtime.ExperimentalEventHandler.afterCommand(ExperimentalEventHandler.java:741)
	at jdk.internal.reflect.GeneratedMethodAccessor513.invoke(Unknown Source)
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(java.base@11.0.2/Unknown Source)
	at java.lang.reflect.Method.invoke(java.base@11.0.2/Unknown Source)
	at com.google.common.eventbus.Subscriber.invokeSubscriberMethod(Subscriber.java:87)
	at com.google.common.eventbus.Subscriber$SynchronizedSubscriber.invokeSubscriberMethod(Subscriber.java:144)
	- locked <0x00000005cb9983a8> (a com.google.common.eventbus.Subscriber$SynchronizedSubscriber)
	at com.google.common.eventbus.Subscriber$1.run(Subscriber.java:72)
	at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:398)
	at com.google.common.eventbus.Subscriber.dispatchEvent(Subscriber.java:67)
	at com.google.common.eventbus.Dispatcher$PerThreadQueuedDispatcher.dispatch(Dispatcher.java:108)
	at com.google.common.eventbus.EventBus.post(EventBus.java:212)
	at com.google.devtools.build.lib.runtime.BlazeRuntime.afterCommand(BlazeRuntime.java:554)
	at com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.execExclusively(BlazeCommandDispatcher.java:518)
	at com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.exec(BlazeCommandDispatcher.java:207)
	at com.google.devtools.build.lib.server.GrpcServerImpl.executeCommand(GrpcServerImpl.java:749)
	at com.google.devtools.build.lib.server.GrpcServerImpl.access$1600(GrpcServerImpl.java:103)
	at com.google.devtools.build.lib.server.GrpcServerImpl$2.lambda$run$0(GrpcServerImpl.java:818)
	at com.google.devtools.build.lib.server.GrpcServerImpl$2$$Lambda$225/0x0000000800316840.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@11.0.2/Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@11.0.2/Unknown Source)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-2"" #35 daemon prio=5 os_prio=0 cpu=656.66ms elapsed=18922.19s tid=0x00007f063c008800 nid=0xfe runnable  [0x00007f06a88c2000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x000000009fe69748> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x000000009fe696f0> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-3"" #300 daemon prio=5 os_prio=0 cpu=865.10ms elapsed=18917.79s tid=0x00007f063c009800 nid=0x265 runnable  [0x00007f06e8a7b000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a0151d10> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a0152d38> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""globbing pool-52"" #645 daemon prio=5 os_prio=0 cpu=77.73ms elapsed=18892.77s tid=0x00007f041c001000 nid=0x1745 waiting on condition  [0x00007f051a4e3000]
   java.lang.Thread.State: WAITING (parking)
	at jdk.internal.misc.Unsafe.park(java.base@11.0.2/Native Method)
	- parking to wait for  <0x000000009facd500> (a com.google.devtools.build.lib.concurrent.NamedForkJoinPool)
	at java.util.concurrent.locks.LockSupport.park(java.base@11.0.2/Unknown Source)
	at java.util.concurrent.ForkJoinPool.runWorker(java.base@11.0.2/Unknown Source)
	at java.util.concurrent.ForkJoinWorkerThread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-4"" #761 daemon prio=5 os_prio=0 cpu=618.08ms elapsed=18877.76s tid=0x00007f063c00b000 nid=0x1850 runnable  [0x00007f06e93a8000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a014e4f8> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a014f520> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-5"" #1200 daemon prio=5 os_prio=0 cpu=660.05ms elapsed=18851.36s tid=0x00007f063c00c800 nid=0x1c00 runnable  [0x00007f051a5e4000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a013ae88> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a013beb0> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-6"" #1701 daemon prio=5 os_prio=0 cpu=644.47ms elapsed=18847.30s tid=0x00007f063c00d800 nid=0x1e05 runnable  [0x00007f02d66b4000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a0137670> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a0138698> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-7"" #2201 daemon prio=5 os_prio=0 cpu=737.08ms elapsed=18842.82s tid=0x00007f063c00e000 nid=0x200f runnable  [0x00007f02d58a6000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a010fd60> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a0110d88> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-8"" #2701 daemon prio=5 os_prio=0 cpu=724.57ms elapsed=18838.97s tid=0x00007f063c00f800 nid=0x2216 runnable  [0x00007f031b4f5000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a010c548> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a010d570> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-9"" #3103 daemon prio=5 os_prio=0 cpu=616.44ms elapsed=18834.28s tid=0x00007f063c010800 nid=0x23b4 runnable  [0x00007f02d519f000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a00f1028> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a00f2050> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-10"" #3505 daemon prio=5 os_prio=0 cpu=723.83ms elapsed=18830.07s tid=0x00007f063c011800 nid=0x2559 runnable  [0x00007f02d0c5a000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a00ed810> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a00ee838> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-11"" #3907 daemon prio=5 os_prio=0 cpu=630.36ms elapsed=18825.37s tid=0x00007f063c012800 nid=0x26fe runnable  [0x00007f02d77d4000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a00e9ff8> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a00eb020> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-12"" #4309 daemon prio=5 os_prio=0 cpu=760.55ms elapsed=18821.11s tid=0x00007f063c014000 nid=0x28a0 runnable  [0x00007f02d7dda000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a00e67e0> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a00e7808> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-13"" #4809 daemon prio=5 os_prio=0 cpu=645.87ms elapsed=18817.21s tid=0x00007f063c015000 nid=0x2aa5 runnable  [0x00007f051b6f5000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a00cb148> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a00cc170> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-14"" #5309 daemon prio=5 os_prio=0 cpu=606.09ms elapsed=18812.56s tid=0x00007f063c016000 nid=0x2ca7 runnable  [0x00007f02d5fad000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a00c7930> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a00c8958> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-15"" #5712 daemon prio=5 os_prio=0 cpu=625.09ms elapsed=18807.76s tid=0x00007f063c017000 nid=0x2e45 runnable  [0x00007f0629090000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a00ba358> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a00bb380> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-16"" #6063 daemon prio=5 os_prio=0 cpu=840.21ms elapsed=18804.06s tid=0x00007f063c018800 nid=0x2fac runnable  [0x00007f02d509e000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a00b6b40> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a00b7b68> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-17"" #6465 daemon prio=5 os_prio=0 cpu=746.83ms elapsed=18799.95s tid=0x00007f063c019800 nid=0x314a runnable  [0x00007f062a8f6000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a009f4a0> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a00a04c8> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-18"" #6965 daemon prio=5 os_prio=0 cpu=685.15ms elapsed=18795.83s tid=0x00007f063c01a800 nid=0x334d runnable  [0x00007f05187c6000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a009bc88> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a009ccb0> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-19"" #7465 daemon prio=5 os_prio=0 cpu=715.48ms elapsed=18792.10s tid=0x00007f063c01b800 nid=0x3549 runnable  [0x00007f02da8f6000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a00931c8> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a00941f0> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-20"" #7964 daemon prio=5 os_prio=0 cpu=723.81ms elapsed=18790.32s tid=0x00007f063c01d000 nid=0x3748 runnable  [0x00007f02d75d2000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a008f9b0> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a00909d8> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-21"" #8399 daemon prio=5 os_prio=0 cpu=706.33ms elapsed=18786.03s tid=0x00007f063c01e000 nid=0x3902 runnable  [0x00007f02d6cc9000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a00812c8> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a00822f0> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-22"" #8749 daemon prio=5 os_prio=0 cpu=642.83ms elapsed=18782.44s tid=0x00007f063c01f000 nid=0x3a67 runnable  [0x00007f02d4694000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a007dab0> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a007ead8> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-23"" #9100 daemon prio=5 os_prio=0 cpu=617.44ms elapsed=18779.07s tid=0x00007f063c020800 nid=0x3bce runnable  [0x00007f02d69c6000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a006ae10> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a006be38> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-24"" #9502 daemon prio=5 os_prio=0 cpu=605.38ms elapsed=18774.86s tid=0x00007f063c021800 nid=0x3d67 runnable  [0x00007f02d0553000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a00675f8> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a0068620> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-25"" #9905 daemon prio=5 os_prio=0 cpu=707.91ms elapsed=18770.40s tid=0x00007f063c022800 nid=0x3f02 runnable  [0x00007f02d0856000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a0053e20> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a0054e48> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-26"" #10258 daemon prio=5 os_prio=0 cpu=635.00ms elapsed=18766.89s tid=0x00007f063c023800 nid=0x4074 runnable  [0x00007f04184c5000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a0050608> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a0051630> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-27"" #10661 daemon prio=5 os_prio=0 cpu=605.94ms elapsed=18764.84s tid=0x00007f063c025000 nid=0x4211 runnable  [0x00007f0629b9f000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a003e658> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a003f680> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-28"" #11124 daemon prio=5 os_prio=0 cpu=624.28ms elapsed=18641.60s tid=0x00007f063c026000 nid=0x52af runnable  [0x00007f02d86e3000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a003ae40> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a003be68> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-29"" #11724 daemon prio=5 os_prio=0 cpu=606.70ms elapsed=18637.84s tid=0x00007f063c027000 nid=0x5512 runnable  [0x00007f02b54bf000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a002baa0> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a002cac8> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-30"" #12321 daemon prio=5 os_prio=0 cpu=725.15ms elapsed=18633.64s tid=0x00007f063c028000 nid=0x5771 runnable  [0x00007f02d68c5000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a0028288> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a00292b0> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-31"" #12918 daemon prio=5 os_prio=0 cpu=586.69ms elapsed=18629.74s tid=0x00007f063c029800 nid=0x59d4 runnable  [0x00007f02d5eac000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a001cbf0> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a001dc18> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-32"" #13379 daemon prio=5 os_prio=0 cpu=600.98ms elapsed=18625.23s tid=0x00007f063c02a800 nid=0x5baf runnable  [0x00007f02d6ecb000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a00193d8> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a001a400> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-33"" #13840 daemon prio=5 os_prio=0 cpu=620.04ms elapsed=18621.28s tid=0x00007f063c02b800 nid=0x5d89 runnable  [0x00007f02d9cea000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a00071d8> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a0008200> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-34"" #14301 daemon prio=5 os_prio=0 cpu=633.10ms elapsed=18617.19s tid=0x00007f063c02c800 nid=0x5f64 runnable  [0x00007f02d4e9c000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x00000000a00039c0> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000a00049e8> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-35"" #14762 daemon prio=5 os_prio=0 cpu=757.24ms elapsed=18613.15s tid=0x00007f063c02e000 nid=0x6139 runnable  [0x00007f0418acd000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x000000009ffef760> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x000000009fff0788> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-36"" #15360 daemon prio=5 os_prio=0 cpu=699.40ms elapsed=18609.36s tid=0x00007f063c02f000 nid=0x6398 runnable  [0x00007f02c05da000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x000000009ffebf48> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x000000009ffecf70> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-37"" #15957 daemon prio=5 os_prio=0 cpu=651.12ms elapsed=18605.68s tid=0x00007f063c030000 nid=0x65f5 runnable  [0x00007f02da9f7000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x000000009ffe07a0> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x000000009ffe17c8> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-38"" #16419 daemon prio=5 os_prio=0 cpu=687.41ms elapsed=18601.58s tid=0x00007f063c031000 nid=0x67ce runnable  [0x00007f02d4795000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x000000009ffdcf88> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x000000009ffddfb0> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-39"" #16849 daemon prio=5 os_prio=0 cpu=625.21ms elapsed=18598.31s tid=0x00007f063c032800 nid=0x6982 runnable  [0x00007f02d0250000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x000000009ffc9950> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x000000009ffca978> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""grpc-default-worker-ELG-3-40"" #17310 daemon prio=5 os_prio=0 cpu=34.73ms elapsed=18594.28s tid=0x00007f063c033800 nid=0x6b57 runnable  [0x00007f02d9fed000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPoll.wait(java.base@11.0.2/Native Method)
	at sun.nio.ch.EPollSelectorImpl.doSelect(java.base@11.0.2/Unknown Source)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(java.base@11.0.2/Unknown Source)
	- locked <0x000000009ffc6138> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x000000009ffc7160> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(java.base@11.0.2/Unknown Source)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:753)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:409)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""cli-update-thread"" #17906 prio=5 os_prio=0 cpu=214.82ms elapsed=18590.71s tid=0x00007f02f4477000 nid=0x6dae waiting on condition  [0x00007f02c02d7000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(java.base@11.0.2/Native Method)
	at com.google.devtools.build.lib.runtime.ExperimentalEventHandler.lambda$startUpdateThread$0(ExperimentalEventHandler.java:981)
	at com.google.devtools.build.lib.runtime.ExperimentalEventHandler$$Lambda$374/0x0000000800527840.run(Unknown Source)
	at java.lang.Thread.run(java.base@11.0.2/Unknown Source)

""Attach Listener"" #17908 daemon prio=9 os_prio=0 cpu=0.49ms elapsed=0.20s tid=0x00007f06d4001000 nid=0x6e1a waiting on condition  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""VM Thread"" os_prio=0 cpu=2334.50ms elapsed=18923.60s tid=0x00007f07483ea000 nid=0xbe runnable  

""ParGC Thread#0"" os_prio=0 cpu=1532.06ms elapsed=18923.61s tid=0x00007f0748028000 nid=0xbd runnable  

""ParGC Thread#1"" os_prio=0 cpu=1534.93ms elapsed=18922.75s tid=0x00007f06e4001000 nid=0xce runnable  

""ParGC Thread#2"" os_prio=0 cpu=1534.09ms elapsed=18922.75s tid=0x00007f06e4002800 nid=0xcf runnable  

""ParGC Thread#3"" os_prio=0 cpu=1532.89ms elapsed=18922.75s tid=0x00007f06e4004000 nid=0xd0 runnable  

""ParGC Thread#4"" os_prio=0 cpu=1530.62ms elapsed=18922.75s tid=0x00007f06e4005800 nid=0xd1 runnable  

""ParGC Thread#5"" os_prio=0 cpu=1534.20ms elapsed=18922.74s tid=0x00007f06e4007000 nid=0xd2 runnable  

""ParGC Thread#6"" os_prio=0 cpu=1534.00ms elapsed=18922.74s tid=0x00007f06e4008800 nid=0xd3 runnable  

""ParGC Thread#7"" os_prio=0 cpu=1534.79ms elapsed=18922.74s tid=0x00007f06e400a000 nid=0xd4 runnable  

""ParGC Thread#8"" os_prio=0 cpu=1536.33ms elapsed=18922.74s tid=0x00007f06e400b800 nid=0xd5 runnable  

""ParGC Thread#9"" os_prio=0 cpu=1532.57ms elapsed=18922.74s tid=0x00007f06e400d800 nid=0xd6 runnable  

""ParGC Thread#10"" os_prio=0 cpu=1529.70ms elapsed=18922.74s tid=0x00007f06e400f000 nid=0xd7 runnable  

""ParGC Thread#11"" os_prio=0 cpu=1533.68ms elapsed=18922.74s tid=0x00007f06e4010800 nid=0xd8 runnable  

""ParGC Thread#12"" os_prio=0 cpu=1534.26ms elapsed=18922.74s tid=0x00007f06e4012000 nid=0xd9 runnable  

""ParGC Thread#13"" os_prio=0 cpu=1532.17ms elapsed=18922.74s tid=0x00007f06e4013800 nid=0xda runnable  

""ParGC Thread#14"" os_prio=0 cpu=1532.45ms elapsed=18922.74s tid=0x00007f06e4015000 nid=0xdb runnable  

""ParGC Thread#15"" os_prio=0 cpu=1532.01ms elapsed=18922.74s tid=0x00007f06e4016800 nid=0xdc runnable  

""ParGC Thread#16"" os_prio=0 cpu=1528.83ms elapsed=18922.74s tid=0x00007f06e4018000 nid=0xdd runnable  

""ParGC Thread#17"" os_prio=0 cpu=1533.78ms elapsed=18922.74s tid=0x00007f06e4019800 nid=0xde runnable  

""ParGC Thread#18"" os_prio=0 cpu=1532.22ms elapsed=18922.74s tid=0x00007f06e401b000 nid=0xdf runnable  

""ParGC Thread#19"" os_prio=0 cpu=1530.22ms elapsed=18922.74s tid=0x00007f06e401c800 nid=0xe0 runnable  

""ParGC Thread#20"" os_prio=0 cpu=1526.86ms elapsed=18922.74s tid=0x00007f06e401e000 nid=0xe1 runnable  

""ParGC Thread#21"" os_prio=0 cpu=1533.70ms elapsed=18922.74s tid=0x00007f06e4020000 nid=0xe2 runnable  

""ParGC Thread#22"" os_prio=0 cpu=1526.75ms elapsed=18922.74s tid=0x00007f06e4022000 nid=0xe3 runnable  

""VM Periodic Task Thread"" os_prio=0 cpu=13872.91ms elapsed=18923.55s tid=0x00007f0748495000 nid=0xc9 waiting on condition  

JNI global refs: 42, weak refs: 0
```

### What operating system are you running Bazel on?

Ubuntu 19.04 host VM, Ubuntu 14.04 Docker container

### What's the output of `bazel info release`?

Bazel 0.25.2
",2019-08-27 14:19:06,"[{'nameRev': '39f31abb7c923bc9b7e24a19edf69427cbe7e8b5 remotes/origin/plf-test-not-splitting-cmdline~1', 'commitMessage': ""ExperimentalEventHandler: add a shutdown flag\n\nWe've observed hangs of Bazel on shutdown with thread stacks indicating\nthat the cli-update-thread does not exit properly. We don't know why\nthis is happening, and have no easy way to reproduce.\n\nMy best guess is that the interrupt is swallowed somewhere, although I\nwas unable to pinpoint a specific place where that could happen. As a\ndefensive mechanism, this change adds a boolean flag that signals to the\nCLI thread to shutdown in case the interrupt signal gets lost. This also\nhelps with a potential race between startUpdateThread and\nstopUpdateThread.\n\nPossible fix for #8432.\n\nPiperOrigin-RevId: 249612495\n"", 'commitParents': ['dff9a2fc12067498503187c658bad089aee4d910'], 'spoonStatsSkippedReason': '', 'commitHash': '39f31abb7c923bc9b7e24a19edf69427cbe7e8b5', 'authoredDateTime': '2019-05-23 03:16:27', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 3, 'deletions': 1, 'lines': 4, 'filePath': 'src/main/java/com/google/devtools/build/lib/runtime/ExperimentalEventHandler.java'}], 'commitDateTime': '2019-05-23 03:17:37', 'commitUser': 'bazel-io', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 2, 'MOV': 0, 'INS': 1, 'DEL': 1, 'spoonMethodName': 'com.google.devtools.build.lib.runtime.ExperimentalEventHandler.startUpdateThread()'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.devtools.build.lib.runtime.ExperimentalEventHandler.stopUpdateThread()'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.devtools.build.lib.runtime.ExperimentalEventHandler'}], 'spoonFilePath': 'ExperimentalEventHandler.java'}]}]",https://github.com/bazelbuild/bazel/issues/8432,97.00027777777778,['type: bug'],Bazel 0.25.2 hangs after running a few jobs,1.0,"['com.google.devtools.build.lib.runtime.ExperimentalEventHandler.startUpdateThread()', 'com.google.devtools.build.lib.runtime.ExperimentalEventHandler', 'com.google.devtools.build.lib.runtime.ExperimentalEventHandler.stopUpdateThread()']",['39f31abb7c923bc9b7e24a19edf69427cbe7e8b5'],,['src/main/java/com/google/devtools/build/lib/runtime'],3.0,1.0,4.0,1.0,0.0,3.0,4.0,0.0,3.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,bazel
5636,2019-04-26 11:05:32,buchgr,"With Bazel from HEAD we see the below error quite a bit

```
ERROR: The Build Event Protocol upload failed: INVALID_ARGUMENT: INVALID_ARGUMENT: Non-consecutive sequence number
```

The error appeared on most of the failed steps in the below downstream run

https://buildkite.com/bazel/bazel-at-head-plus-downstream/builds/932",2019-04-26 14:22:31,"[{'nameRev': '4b092cdc34e81ac4d4337f549a96a022d6ff7cce remotes/origin/plf-test-libraries_to_link_depset_2~164', 'commitMessage': ""Restore atomicity of sequence number generation and addition to the event queue in the BES uploader.\n\nThe BES backends expect the sequence number and the order in the queue to coincide and the atomicity was removed in https://github.com/bazelbuild/bazel/commit/7c7957607642b4dead24c8c1fceae214eeb8ff37 so we're restoring it now with this change.\n\nFixes https://github.com/bazelbuild/bazel/issues/8160\n\nPiperOrigin-RevId: 245416462\n"", 'commitParents': ['d002e450a8d23f84a22345e7c41599bbadfb5d1a'], 'spoonStatsSkippedReason': '', 'commitHash': '4b092cdc34e81ac4d4337f549a96a022d6ff7cce', 'authoredDateTime': '2019-04-26 07:20:53', 'commitGHEventType': 'closed', 'commitGitStats': [{'insertions': 31, 'deletions': 12, 'lines': 43, 'filePath': 'src/main/java/com/google/devtools/build/lib/buildeventservice/BuildEventServiceUploader.java'}], 'commitDateTime': '2019-04-26 07:21:56', 'commitUser': 'bazel-io', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 2, 'MOV': 1, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.devtools.build.lib.buildeventservice.BuildEventServiceUploader.close()'}, {'UPD': 0, 'TOT': 7, 'MOV': 5, 'INS': 1, 'DEL': 1, 'spoonMethodName': 'com.google.devtools.build.lib.buildeventservice.BuildEventServiceUploader.enqueueEvent(com.google.devtools.build.lib.buildeventstream.BuildEvent)'}], 'spoonFilePath': 'BuildEventServiceUploader.java'}]}]",https://github.com/bazelbuild/bazel/issues/8160,0.0002777777777777778,"['P1', 'category: BEP', 'team-Core', 'type: bug']",BES fails with Non-consecutive sequence number error,1.0,"['com.google.devtools.build.lib.buildeventservice.BuildEventServiceUploader.close()', 'com.google.devtools.build.lib.buildeventservice.BuildEventServiceUploader.enqueueEvent(com.google.devtools.build.lib.buildeventstream.BuildEvent)']",['4b092cdc34e81ac4d4337f549a96a022d6ff7cce'],,['src/main/java/com/google/devtools/build/lib/buildeventservice'],31.0,12.0,43.0,1.0,0.0,2.0,9.0,6.0,2.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,bazel
5756,2018-12-11 09:30:25,laszlocsomor,"### Description of the problem / feature request:

On Bazel CI, I'm repeatedly getting the following error:
```
ERROR: D:/b/bk-worker-windows-java8-w528/bazel/google-bazel-presubmit/third_party/protobuf/3.6.1/BUILD:123:1: Couldn't build file third_party/protobuf/3.6.1/_objs/protobuf_lite/stringpiece.obj: C++ compilation of rule '//third_party/protobuf/3.6.1:protobuf_lite' failed: Failed to delete output files after incomplete download. Cannot continue with local execution.: D:/b/f622aej4/execroot/io_bazel/bazel-out/host/bin/third_party/protobuf/3.6.1/_objs/protobuf_lite/stringpiece.obj (Permission denied)
--
  | Target //src:bazel failed to build
```

(https://buildkite.com/bazel/google-bazel-presubmit/builds/12883#61fdc327-7d64-4979-a3b4-9d35f2c729a2)

### Bugs: what's the simplest, easiest way to reproduce this bug? Please provide a minimal example if possible.

I don't know. I saw the error on CI for my change https://bazel-review.googlesource.com/c/bazel/+/84151.

Retrying the job doesn't seem to help.

Looks like this is where the error comes from: https://github.com/bazelbuild/bazel/blob/b3f28d3da12ea2d97d838f66765bf21d5dee2f7f/src/main/java/com/google/devtools/build/lib/remote/AbstractRemoteActionCache.java#L280-L284

And I suppose the reason is that `outErr` has open streams for stdout and stderr, and those streams should be closed before attempting to delete the files here: https://github.com/bazelbuild/bazel/blob/b3f28d3da12ea2d97d838f66765bf21d5dee2f7f/src/main/java/com/google/devtools/build/lib/remote/AbstractRemoteActionCache.java#L273-L274

### What operating system are you running Bazel on?

> Replace this line with your answer.

### What's the output of `bazel info release`?

`0.20.0`
",2019-01-07 11:51:04,"[{'nameRev': 'a3a5975dca3ad04c19dc7d063fcf490a8cd612fd tags/0.23.0~508', 'commitMessage': 'Fix a race condition in remote cache\n\nPreviously, outerF.setExeception was set before closing the output stream of the download file when download fails. This was causing a permission error when trying to delete the file on Windows.\n\nFixes https://github.com/bazelbuild/bazel/issues/6890\n\nRELNOTES: None\nPiperOrigin-RevId: 228138102\n', 'commitParents': ['95110991e154f84c6b08a38d65a684d18bb6adcc'], 'spoonStatsSkippedReason': '', 'commitHash': 'a3a5975dca3ad04c19dc7d063fcf490a8cd612fd', 'authoredDateTime': '2019-01-07 03:48:37', 'commitGHEventType': 'closed', 'commitGitStats': [{'insertions': 2, 'deletions': 1, 'lines': 3, 'filePath': 'src/main/java/com/google/devtools/build/lib/remote/AbstractRemoteActionCache.java'}], 'commitDateTime': '2019-01-07 03:50:06', 'commitUser': 'bazel-io', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 1, 'MOV': 1, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'com.google.devtools.build.lib.remote.AbstractRemoteActionCache.downloadFile(com.google.devtools.build.lib.vfs.Path,build.bazel.remote.execution.v2.Digest).3.onFailure(java.lang.Throwable)'}], 'spoonFilePath': 'AbstractRemoteActionCache.java'}]}, {'nameRev': '3ef03fe16e1912d0bee7a4b55e2862c9cc7926e3 tags/0.22.0~7', 'commitMessage': 'Fix a race condition in remote cache\n\nPreviously, outerF.setExeception was set before closing the output stream of the download file when download fails. This was causing a permission error when trying to delete the file on Windows.\n\nFixes https://github.com/bazelbuild/bazel/issues/6890\n\nRELNOTES: None\nPiperOrigin-RevId: 228138102\n', 'commitParents': ['deb028e3fb30b4e2953df16f35ab1f55a08ea8fa'], 'spoonStatsSkippedReason': '', 'commitHash': '3ef03fe16e1912d0bee7a4b55e2862c9cc7926e3', 'authoredDateTime': '2019-01-07 03:48:37', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 2, 'deletions': 1, 'lines': 3, 'filePath': 'src/main/java/com/google/devtools/build/lib/remote/AbstractRemoteActionCache.java'}], 'commitDateTime': '2019-01-23 18:25:58', 'commitUser': 'aehlig', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 1, 'MOV': 1, 'INS': 0, 'DEL': 0, 'spoonMethodName': 'com.google.devtools.build.lib.remote.AbstractRemoteActionCache.downloadFile(com.google.devtools.build.lib.vfs.Path,build.bazel.remote.execution.v2.Digest).3.onFailure(java.lang.Throwable)'}], 'spoonFilePath': 'AbstractRemoteActionCache.java'}]}, {'nameRev': '', 'commitMessage': '', 'commitParents': [], 'spoonStatsSkippedReason': '', 'commitHash': 'f1df03e091a3a355f2005b99962a3813a71a9242', 'authoredDateTime': '', 'commitGHEventType': 'referenced', 'commitGitStats': [], 'commitDateTime': '', 'commitUser': 'aehlig', 'commitSpoonAstDiffStats': []}, {'nameRev': '', 'commitMessage': '', 'commitParents': [], 'spoonStatsSkippedReason': '', 'commitHash': '8d07b6048abf9acf9f16710b279e728872cabc19', 'authoredDateTime': '', 'commitGHEventType': 'referenced', 'commitGitStats': [], 'commitDateTime': '', 'commitUser': 'aehlig', 'commitSpoonAstDiffStats': []}]",https://github.com/bazelbuild/bazel/issues/6890,27.00027777777778,"['P0', 'type: bug', 'untriaged']","Windows, CI: getting ""Failed to delete output files after incomplete download""",1.0,"['com.google.devtools.build.lib.remote.AbstractRemoteActionCache.downloadFile(com.google.devtools.build.lib.vfs.Path,build.bazel.remote.execution.v2.Digest).3.onFailure(java.lang.Throwable)']",['a3a5975dca3ad04c19dc7d063fcf490a8cd612fd'],,['src/main/java/com/google/devtools/build/lib/remote'],2.0,1.0,3.0,1.0,0.0,1.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,3.0,0.0,0.0,bazel
5875,2018-07-04 11:55:36,laszlocsomor,"### Description of the problem / feature request:

There's a race condition in: https://github.com/bazelbuild/bazel/blob/a320e4460f35767da7fc8eb824ce1f4b304b5e0f/src/main/java/com/google/devtools/build/lib/vfs/JavaIoFileSystem.java#L353

If the directory is deleted or replaced by a file between the `file.isDirectory()` check and the `file.list()` call, then `file.list()` returns null and throws an NPE.

There may be other such race conditions in Bazel's VFS library.

### What's the output of `bazel info release`?

release 0.15.0",2018-07-09 07:55:42,"[{'nameRev': '49d2031743231a979297bfc5d0a4463fabc0e21f tags/0.17.1~791', 'commitMessage': ""vfs: fix race condition in JavaIoFileSystem.delete\n\nJavaIoFileSystem.delete() now uses\nFiles.deleteIfExists() to delete files and empty\ndirectories.\n\nThe old code used to have a race condition: a\nfile could have been deleted or changed to a\nnon-directory between checking it's a directory\nand listing its contents.\n\nWindowsFileSystem.delete() now uses the\nDeletePath JNI method (wrapped by\nWindowsFileOperations.deletePath) which is more\nrobust than JavaIoFileSystem.delete(), because it\ncan delete readonly files and can tolerate some\nconcurrent filesystem modifications.\n\nFixes https://github.com/bazelbuild/bazel/issues/5513\n\nChange-Id: I5d2720afff8dfd3725880a6d7408d22de5935c3d\n\nCloses #5527.\n\nChange-Id: I5d2720afff8dfd3725880a6d7408d22de5935c3d\nPiperOrigin-RevId: 203720575\n"", 'commitParents': ['7d9eedf969a326320e996baccccd4ae04f1e2deb'], 'spoonStatsSkippedReason': '', 'commitHash': '49d2031743231a979297bfc5d0a4463fabc0e21f', 'authoredDateTime': '2018-07-09 00:53:34', 'commitGHEventType': 'closed', 'commitGitStats': [{'insertions': 32, 'deletions': 12, 'lines': 44, 'filePath': 'src/main/java/com/google/devtools/build/lib/vfs/JavaIoFileSystem.java'}, {'insertions': 1, 'deletions': 0, 'lines': 1, 'filePath': 'src/main/java/com/google/devtools/build/lib/windows/BUILD'}, {'insertions': 27, 'deletions': 0, 'lines': 27, 'filePath': 'src/main/java/com/google/devtools/build/lib/windows/jni/WindowsFileOperations.java'}, {'insertions': 16, 'deletions': 0, 'lines': 16, 'filePath': 'src/main/java/com/google/devtools/build/lib/windows/WindowsFileSystem.java'}], 'commitDateTime': '2018-07-09 00:54:56', 'commitUser': 'bazel-io', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.devtools.build.lib.windows.jni.WindowsFileOperations.deletePath(java.lang.String)'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.devtools.build.lib.windows.jni.WindowsFileOperations.nativeDeletePath(java.lang.String,java.lang.String[])'}, {'UPD': 0, 'TOT': 5, 'MOV': 0, 'INS': 5, 'DEL': 0, 'spoonMethodName': 'com.google.devtools.build.lib.windows.jni.WindowsFileOperations'}], 'spoonFilePath': 'WindowsFileOperations.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.devtools.build.lib.windows.WindowsFileSystem.delete(com.google.devtools.build.lib.vfs.Path)'}], 'spoonFilePath': 'WindowsFileSystem.java'}, {'spoonMethods': [{'UPD': 12, 'TOT': 34, 'MOV': 7, 'INS': 9, 'DEL': 6, 'spoonMethodName': 'com.google.devtools.build.lib.vfs.JavaIoFileSystem.delete(com.google.devtools.build.lib.vfs.Path)'}], 'spoonFilePath': 'JavaIoFileSystem.java'}]}, {'nameRev': 'b57ba275c0d67482f8bedc2888c34215721ee50e tags/0.17.1~811', 'commitMessage': 'Windows,JNI: implement native DeletePath method\n\nImplement a native DeletePath method that can\ndelete files, directories, and junctions. The\nmethod should tolerate when concurrent\nprocesses delete the file.\n\nThe new JNI function is more robust than Java IO\nfile deletion function because it can also delete\nreadonly files.\n\nSee https://github.com/bazelbuild/bazel/issues/5513\n\nCloses #5520.\n\nChange-Id: I21ea36dd64960b294e2b51600273bf4290ad7c0f\nPiperOrigin-RevId: 203448581\n', 'commitParents': ['bc52a18a16d378193fd18a76fb58cc5e50414e37'], 'spoonStatsSkippedReason': '', 'commitHash': 'b57ba275c0d67482f8bedc2888c34215721ee50e', 'authoredDateTime': '2018-07-06 02:16:34', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 103, 'deletions': 0, 'lines': 103, 'filePath': 'src/main/native/windows/file.cc'}, {'insertions': 16, 'deletions': 0, 'lines': 16, 'filePath': 'src/main/native/windows/file-jni.cc'}, {'insertions': 17, 'deletions': 0, 'lines': 17, 'filePath': 'src/main/native/windows/file.h'}, {'insertions': 127, 'deletions': 0, 'lines': 127, 'filePath': 'src/test/native/windows/file_test.cc'}], 'commitDateTime': '2018-07-06 02:17:54', 'commitUser': 'bazel-io', 'commitSpoonAstDiffStats': []}]",https://github.com/bazelbuild/bazel/issues/5513,4.000277777777778,"['type: bug', 'z-category: service APIs']",Bazel server: race condition in JavaIoFileSystem.delete(),2.0,"['com.google.devtools.build.lib.windows.jni.WindowsFileOperations.deletePath(java.lang.String)', 'com.google.devtools.build.lib.vfs.JavaIoFileSystem.delete(com.google.devtools.build.lib.vfs.Path)', 'com.google.devtools.build.lib.windows.WindowsFileSystem.delete(com.google.devtools.build.lib.vfs.Path)', 'com.google.devtools.build.lib.windows.jni.WindowsFileOperations.nativeDeletePath(java.lang.String,java.lang.String[])', 'com.google.devtools.build.lib.windows.jni.WindowsFileOperations']","['b57ba275c0d67482f8bedc2888c34215721ee50e', '49d2031743231a979297bfc5d0a4463fabc0e21f']",,"['src/main/java/com/google/devtools/build/lib/windows/jni', 'src/main/java/com/google/devtools/build/lib/vfs', 'src/main/java/com/google/devtools/build/lib/windows/BUILD', 'src/main/native/windows/file.h', 'src/main/native/windows', 'src/main/java/com/google/devtools/build/lib/windows']",212.0,12.0,224.0,7.0,12.0,5.0,42.0,7.0,17.0,6.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,bazel
5888,2018-06-15 00:50:59,EricBurnett,"### Description of the problem / feature request:

Bazel can deadlock while writing to the BES. The issue is that sendBuildEvent() is reentrant, which in turn can cause multiple lock orderings. One such ordering pair is PrintStream causing a BuildEvent to be sent, and a BuildEvent being sent causing a file digest to be computed, causing a PrintStream to be called if it is slow.

The first path:

	at com.google.devtools.build.lib.buildeventservice.BuildEventServiceTransport.sendBuildEvent(BuildEventServiceTransport.java:289)
	- waiting to lock <0x000000070139db90> (a com.google.devtools.build.lib.buildeventservice.BuildEventServiceTransport)
	at com.google.devtools.build.lib.runtime.BuildEventStreamer.flush(BuildEventStreamer.java:540)
	at com.google.devtools.build.lib.runtime.SynchronizedOutputStream.write(SynchronizedOutputStream.java:129)
	at com.google.devtools.build.lib.util.io.DelegatingOutErr$DelegatingOutputStream.write(DelegatingOutErr.java:103)
	at com.google.devtools.build.lib.util.AnsiStrippingOutputStream.write(AnsiStrippingOutputStream.java:67)
	- locked <0x000000070139fdd0> (a com.google.devtools.build.lib.util.AnsiStrippingOutputStream)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	- locked <0x000000070139fdb8> (a java.io.BufferedOutputStream)
	at com.google.devtools.build.lib.runtime.ExperimentalEventHandler.handle(ExperimentalEventHandler.java:365)
	- locked <0x000000070139fd20> (a com.google.devtools.build.lib.runtime.ExperimentalEventHandler)
	at com.google.devtools.build.lib.events.Reporter.handle(Reporter.java:113)
	- locked <0x000000070139fca8> (a com.google.devtools.build.lib.events.Reporter)
	at com.google.devtools.build.lib.events.ReporterStream.write(ReporterStream.java:55)
	at java.io.PrintStream.write(PrintStream.java:480)
	- locked <0x00000007019ca958> (a java.io.PrintStream)
	at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221)
	at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291)
	at sun.nio.cs.StreamEncoder.flushBuffer(StreamEncoder.java:104)
	- locked <0x00000007019ca9a0> (a java.io.OutputStreamWriter)
	at java.io.OutputStreamWriter.flushBuffer(OutputStreamWriter.java:185)
	at java.io.PrintStream.newLine(PrintStream.java:546)
	- locked <0x00000007019ca958> (a java.io.PrintStream)
	at java.io.PrintStream.println(PrintStream.java:807)
	- locked <0x00000007019ca958> (a java.io.PrintStream)
	at com.google.devtools.build.lib.actions.cache.DigestUtils.getDigestInternal(DigestUtils.java:162)

The second path:

	at java.io.PrintStream.println(PrintStream.java:805)
	- waiting to lock <0x00000007019ca958> (a java.io.PrintStream)
	at com.google.devtools.build.lib.actions.cache.DigestUtils.getDigestInternal(DigestUtils.java:162)
	at com.google.devtools.build.lib.actions.cache.DigestUtils.getDigestOrFail(DigestUtils.java:249)
	at com.google.devtools.build.lib.remote.util.DigestUtil.compute(DigestUtil.java:64)
	at com.google.devtools.build.lib.remote.RemoteModule$CasPathConverter.apply(RemoteModule.java:69)
	at com.google.devtools.build.lib.runtime.ServerBuilder$1.apply(ServerBuilder.java:101)
	at com.google.devtools.build.lib.runtime.NamedArtifactGroup.asStreamProto(NamedArtifactGroup.java:63)
	at com.google.devtools.build.lib.buildeventservice.BuildEventServiceTransport.sendBuildEvent(BuildEventServiceTransport.java:289)
	- locked <0x000000070139db90> (a com.google.devtools.build.lib.buildeventservice.BuildEventServiceTransport)
	at com.google.devtools.build.lib.runtime.BuildEventStreamer.post(BuildEventStreamer.java:273)
	at com.google.devtools.build.lib.runtime.BuildEventStreamer.maybeReportArtifactSet(BuildEventStreamer.java:407)
	at com.google.devtools.build.lib.runtime.BuildEventStreamer.maybeReportArtifactSet(BuildEventStreamer.java:405)
	at com.google.devtools.build.lib.runtime.BuildEventStreamer.maybeReportArtifactSet(BuildEventStreamer.java:411)
	at com.google.devtools.build.lib.runtime.BuildEventStreamer.buildEvent(BuildEventStreamer.java:479)

I'm unsure what the correct fix is. This specific scenario can be avoided by removing the log from getDigestInternal, but that may not be the only path in which different lock orderings can arise. Probably better would be for BuildEventServiceTransport to not call anything that might produce a log line while in synchronized code, and then also handle the case of new build events being produced while preparing one to be sent.

### Bugs: what's the simplest, easiest way to reproduce this bug? Please provide a minimal example if possible.

At a guess: do a build with many large input files, high --jobs, on a machine with a slow disk and/or severely CPU constrained, to trigger that specific log message to be hit a lot. Then hope you get lucky.

### What operating system are you running Bazel on?

Linux.

### What's the output of `bazel info release`?

Pulled from java.log instead: 0.13.1-201805171359+f41dd9a

###  Have you found anything relevant by searching the web?

Nope.

### Any other information, logs, or outputs that you want to share?

I have copies of the java.log and jstack output if need be, but I'm not at liberty to publically share them. Googlers: poke me if you want to see them, or more context on the exact scenario in which this was found. The person who actually found this bug (I'm just reporting it) encounters it with enough regularity that more details from a hung bazel can probably be extracted if necessary.",2018-07-17 15:33:43,"[{'nameRev': '59f915949e262ac526e8f7e3153502400d95b0ff tags/0.17.1~653', 'commitMessage': 'fix deadlock in the bes upload. Fixes #5402\n\nThe synchronized block in sendBuildEvent is\nnot needed at all, since all the logic in\nthere is local to the method and thread safe.\n\nSee the issue for more details.\n\nRELNOTES: None\nPiperOrigin-RevId: 204915541\n', 'commitParents': ['b8977a4cd6cd3b2a83b0596d8e477f8d89988d8a'], 'spoonStatsSkippedReason': '', 'commitHash': '59f915949e262ac526e8f7e3153502400d95b0ff', 'authoredDateTime': '2018-07-17 08:31:58', 'commitGHEventType': 'closed', 'commitGitStats': [{'insertions': 3, 'deletions': 3, 'lines': 6, 'filePath': 'src/main/java/com/google/devtools/build/lib/buildeventservice/BuildEventServiceTransport.java'}], 'commitDateTime': '2018-07-17 08:33:22', 'commitUser': 'bazel-io', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.devtools.build.lib.buildeventservice.BuildEventServiceTransport'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 1, 'spoonMethodName': 'com.google.devtools.build.lib.buildeventservice.BuildEventServiceTransport.sendBuildEvent(com.google.devtools.build.lib.buildeventstream.BuildEvent,com.google.devtools.build.lib.buildeventstream.ArtifactGroupNamer)'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 0, 'DEL': 1, 'spoonMethodName': 'com.google.devtools.build.lib.buildeventservice.BuildEventServiceTransport.getInvocationResult()'}], 'spoonFilePath': 'BuildEventServiceTransport.java'}]}]",https://github.com/bazelbuild/bazel/issues/5402,32.000277777777775,"['P1', 'category: misc > misc', 'type: bug']",Deadlock in BuildEventServiceTransport.java,1.0,"['com.google.devtools.build.lib.buildeventservice.BuildEventServiceTransport.sendBuildEvent(com.google.devtools.build.lib.buildeventstream.BuildEvent,com.google.devtools.build.lib.buildeventstream.ArtifactGroupNamer)', 'com.google.devtools.build.lib.buildeventservice.BuildEventServiceTransport.getInvocationResult()', 'com.google.devtools.build.lib.buildeventservice.BuildEventServiceTransport']",['59f915949e262ac526e8f7e3153502400d95b0ff'],,['src/main/java/com/google/devtools/build/lib/buildeventservice'],3.0,3.0,6.0,1.0,0.0,3.0,3.0,0.0,1.0,2.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,bazel
6141,2017-10-19 14:28:09,meteorcloudy,"I sometimes get a `failed to create output directory` error when building bazel with head bazel.
```
pcloudy@pcloudy0-w MSYS ~/workspace/my_tests/bazel
$ bazel clean && bazel build src:bazel
Extracting Bazel installation...
...........
INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.
WARNING: C:/tools/msys64/home/pcloudy/workspace/my_tests/bazel/src/main/cpp/BUILD:79:12: in srcs attribute of cc_binary rule //src/main/cpp:client: please do not import '//src/main/native/windows:resources.o' directly. You should either move the file to this package or depend on an appropriate rule there
INFO: Analysed target //src:bazel (164 packages loaded).
INFO: Found 1 target...
ERROR: C:/tools/msys64/home/pcloudy/workspace/my_tests/bazel/third_party/grpc/BUILD:77:1: failed to create output directory 'C:/tmp/_bazel_pcloudy/2a_d8ear/execroot/io_bazel/bazel-out/x64_windows-fastbuild/bin/third_party/grpc/_objs/grpc_unsecure/third_party/grpc/src/core/profiling': C:/tmp/_bazel_pcloudy/2a_d8ear/execroot/io_bazel/bazel-out/x64_windows-fastbuild/bin/third_party/grpc/_objs/grpc_unsecure/third_party/grpc/src (Not a directory)
Target //src:bazel failed to build
INFO: Elapsed time: 113.590s, Critical Path: 9.66s
FAILED: Build did NOT complete successfully
```

Suspect culprit: 3ce58ce65663b89a1209d9d9767752a11445edfe, I'll try to confirm it.",2017-11-14 10:45:35,"[{'nameRev': 'e28d3af92227cd60287d27d9efa7593ae3e0509f tags/0.8.0~144', 'commitMessage': 'Automated rollback of commit 5cca89f970bcaddb98972055f2d9539e9d225e67.\n\n*** Reason for rollback ***\n\nBreak CI Windows nightly build: https://github.com/bazelbuild/bazel/issues/3927\nhttps://ci.bazel.io/blue/organizations/jenkins/Global%2Fbazel-tests/detail/bazel-tests/314/pipeline/31\n\n*** Original change description ***\n\nRemove synchronization from FileSystem implementations.\n\nPiperOrigin-RevId: 173243523\n', 'commitParents': ['5f5e2b471cef3fdb5e4998c7947cbd657305a48f'], 'spoonStatsSkippedReason': '', 'commitHash': 'e28d3af92227cd60287d27d9efa7593ae3e0509f', 'authoredDateTime': '2017-10-24 14:16:12', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 40, 'deletions': 17, 'lines': 57, 'filePath': 'src/main/java/com/google/devtools/build/lib/vfs/JavaIoFileSystem.java'}, {'insertions': 11, 'deletions': 9, 'lines': 20, 'filePath': 'src/main/java/com/google/devtools/build/lib/vfs/AbstractFileSystem.java'}], 'commitDateTime': '2017-10-24 15:38:45', 'commitUser': 'bazel-io', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 2, 'MOV': 1, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.devtools.build.lib.vfs.AbstractFileSystem.getOutputStream(com.google.devtools.build.lib.vfs.Path,boolean)'}], 'spoonFilePath': 'AbstractFileSystem.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 10, 'MOV': 8, 'INS': 1, 'DEL': 1, 'spoonMethodName': 'com.google.devtools.build.lib.vfs.JavaIoFileSystem.createDirectory(com.google.devtools.build.lib.vfs.Path)'}, {'UPD': 0, 'TOT': 2, 'MOV': 1, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.devtools.build.lib.vfs.JavaIoFileSystem.delete(com.google.devtools.build.lib.vfs.Path)'}, {'UPD': 0, 'TOT': 4, 'MOV': 3, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.devtools.build.lib.vfs.JavaIoFileSystem.renameTo(com.google.devtools.build.lib.vfs.Path,com.google.devtools.build.lib.vfs.Path)'}], 'spoonFilePath': 'JavaIoFileSystem.java'}]}]",https://github.com/bazelbuild/bazel/issues/3927,25.00027777777778,"['P1', 'platform: windows', 'type: bug']",Windows: `failed to create output directory` error,1.0,"['com.google.devtools.build.lib.vfs.JavaIoFileSystem.createDirectory(com.google.devtools.build.lib.vfs.Path)', 'com.google.devtools.build.lib.vfs.JavaIoFileSystem.delete(com.google.devtools.build.lib.vfs.Path)', 'com.google.devtools.build.lib.vfs.AbstractFileSystem.getOutputStream(com.google.devtools.build.lib.vfs.Path,boolean)', 'com.google.devtools.build.lib.vfs.JavaIoFileSystem.renameTo(com.google.devtools.build.lib.vfs.Path,com.google.devtools.build.lib.vfs.Path)']",['e28d3af92227cd60287d27d9efa7593ae3e0509f'],,['src/main/java/com/google/devtools/build/lib/vfs'],51.0,26.0,77.0,2.0,0.0,4.0,18.0,13.0,4.0,1.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,bazel
6812,2016-07-25 12:53:56,damienmg,"http://ci.bazel.io/job/bazel-tests/BAZEL_VERSION=latest,PLATFORM_NAME=darwin-x86_64/139/console
",2016-08-02 11:17:24,"[{'nameRev': 'c4e132c6b27cc5d77e56f642b18bbb300f0f7659 tags/0.3.2~839', 'commitMessage': 'Mark //src/test/shell/bazel:experimental_ui_test as manual\n\nThis test is timeout flaky (#1560) and the flake seems like genuine. While\nwaiting for a proper fix, deactivate this test on ci.bazel.io.\n\n--\nMOS_MIGRATED_REVID=128571273\n', 'commitParents': ['8ef37409879c3f5b93af3a9713f79c9d8a8ea79b'], 'spoonStatsSkippedReason': '', 'commitHash': 'c4e132c6b27cc5d77e56f642b18bbb300f0f7659', 'authoredDateTime': '2016-07-27 11:06:27', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 1, 'deletions': 0, 'lines': 1, 'filePath': 'src/test/shell/integration/BUILD'}], 'commitDateTime': '2016-07-27 11:15:23', 'commitUser': 'bazel-io', 'commitSpoonAstDiffStats': []}, {'nameRev': 'fbe4221dca7715edd784c2fc598877fb102cb8a3 tags/0.3.2~762', 'commitMessage': 'ExperimentalEventHandler: narrow scope of synchronized blocks\n\n...to in particular never own a lock while waiting for the\nupdate thread to finish. This avoids a deadlock when the update\nthread tries to enter the synchronized part of doRefresh while\nthe thread trying to stop the update thread holds the lock.\n\nAlso renable experimental_ui_test now that the race condition leading\nto the deadlock is fixed. The absence of flakiness has been verified\nby running the test locally 100 times. Fixes #1560.\n\n--\nChange-Id: I5d85b347e6cb7a1da8d5a724d6f9cd7461e33e5b\nReviewed-on: https://bazel-review.googlesource.com/#/c/4225\nMOS_MIGRATED_REVID=129079398\n', 'commitParents': ['2a8d73109f5e539fc20e956e8139dc1bb5b65111'], 'spoonStatsSkippedReason': '', 'commitHash': 'fbe4221dca7715edd784c2fc598877fb102cb8a3', 'authoredDateTime': '2016-08-02 09:59:25', 'commitGHEventType': 'closed', 'commitGitStats': [{'insertions': 18, 'deletions': 12, 'lines': 30, 'filePath': 'src/main/java/com/google/devtools/build/lib/runtime/ExperimentalEventHandler.java'}, {'insertions': 0, 'deletions': 1, 'lines': 1, 'filePath': 'src/test/shell/integration/BUILD'}], 'commitDateTime': '2016-08-02 11:16:29', 'commitUser': 'bazel-io', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 6, 'MOV': 4, 'INS': 1, 'DEL': 1, 'spoonMethodName': 'com.google.devtools.build.lib.runtime.ExperimentalEventHandler.buildComplete(com.google.devtools.build.lib.buildtool.buildevent.BuildCompleteEvent)'}, {'UPD': 0, 'TOT': 3, 'MOV': 1, 'INS': 1, 'DEL': 1, 'spoonMethodName': 'com.google.devtools.build.lib.runtime.ExperimentalEventHandler.afterCommand(com.google.devtools.build.lib.runtime.AfterCommandEvent)'}, {'UPD': 0, 'TOT': 3, 'MOV': 1, 'INS': 1, 'DEL': 1, 'spoonMethodName': 'com.google.devtools.build.lib.runtime.ExperimentalEventHandler.noBuild(com.google.devtools.build.lib.analysis.NoBuildEvent)'}], 'spoonFilePath': 'ExperimentalEventHandler.java'}]}]",https://github.com/bazelbuild/bazel/issues/1560,7.000277777777778,"['P1', 'type: bug']",//src/test/shell/integration:experimental_ui_test is timeout flaky,1.0,"['com.google.devtools.build.lib.runtime.ExperimentalEventHandler.afterCommand(com.google.devtools.build.lib.runtime.AfterCommandEvent)', 'com.google.devtools.build.lib.runtime.ExperimentalEventHandler.buildComplete(com.google.devtools.build.lib.buildtool.buildevent.BuildCompleteEvent)', 'com.google.devtools.build.lib.runtime.ExperimentalEventHandler.noBuild(com.google.devtools.build.lib.analysis.NoBuildEvent)']","['c4e132c6b27cc5d77e56f642b18bbb300f0f7659', 'fbe4221dca7715edd784c2fc598877fb102cb8a3']",,['src/main/java/com/google/devtools/build/lib/runtime'],18.0,12.0,30.0,1.0,0.0,3.0,12.0,6.0,3.0,3.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,bazel
6968,2016-04-19 21:48:10,sudeepgaddam,"I tried to build tensorflow source using bazel. I was assuming bazel would take care of parallelisation in a smart way by having a balance between performance and judicious usage. But it was using 100% of all the cores and 95% RAM. This is in no way a good approach. I had to hard boot the  system.

Under default cases, Bazel should use resources in more smarter way.
",2018-07-19 12:28:22,"[{'nameRev': '222b03cd7562e8c44f07e6decd900a4d52b4f95f tags/0.5.0~728', 'commitMessage': 'Automatically set --jobs based on the number of CPU threads.\n\nThis change adds the new magic value ""auto"" to --jobs and makes this the\ndefault.  When --jobs=auto, we determine the number of available CPU\nthreads and set a reasonable value for --jobs based on this number.  I\'m\nexplicitly not defining what ""reasonable"" means because we may want to\nchange the heuristics later on.\n\nThe goal here is to reduce the load on the system when running Bazel\nwhile not adversely affecting build times significantly.  Previous\nversions of Bazel defaulted --jobs to 200, which could easily overload\nthe local machine with a lot of processes.  This value was derived from\nBlaze\'s default, which makes sense because most jobs are network-bound\ndue to distributed execution; however, in the Bazel case, this never\nmade sense and is actually harmful.\n\nThis change was initiated by problems observed on Macs where Bazel would\nbring machines to their knees due to system resource overload.  It\'s\nlikely that the overload is caused by too much RAM usage rather than\nCPU, but both of these should go down with a more limited jobs value.\nShould help alleviate issue #1160.\n\nRELNOTES: The --jobs flag now defaults to ""auto"", which causes Bazel to\nuse a reasonable degree of parallelism based on the local machine\'s\ncapacity.\n\n--\nPiperOrigin-RevId: 150466088\nMOS_MIGRATED_REVID=150466088\n', 'commitParents': ['17dee66cba5480c7b70abb3530aeb309ee9c8847'], 'spoonStatsSkippedReason': '', 'commitHash': '222b03cd7562e8c44f07e6decd900a4d52b4f95f', 'authoredDateTime': '2017-03-17 18:52:48', 'commitGHEventType': 'referenced', 'commitGitStats': [{'insertions': 10, 'deletions': 0, 'lines': 10, 'filePath': 'src/main/java/com/google/devtools/build/lib/actions/LocalHostCapacity.java'}, {'insertions': 55, 'deletions': 16, 'lines': 71, 'filePath': 'src/main/java/com/google/devtools/build/lib/buildtool/BuildRequest.java'}, {'insertions': 12, 'deletions': 0, 'lines': 12, 'filePath': 'src/test/shell/integration/execution_phase_tests.sh'}], 'commitDateTime': '2017-03-20 11:40:07', 'commitUser': 'bazel-io', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.devtools.build.lib.actions.LocalHostCapacity.setLocalHostCapacity(com.google.devtools.build.lib.actions.ResourceSet)'}], 'spoonFilePath': 'LocalHostCapacity.java'}, {'spoonMethods': [{'UPD': 0, 'TOT': 4, 'MOV': 1, 'INS': 3, 'DEL': 0, 'spoonMethodName': 'com.google.devtools.build.lib.buildtool.BuildRequest'}, {'UPD': 0, 'TOT': 3, 'MOV': 2, 'INS': 0, 'DEL': 1, 'spoonMethodName': 'com.google.devtools.build.lib.buildtool.BuildRequest.validateOptions()'}, {'UPD': 3, 'TOT': 6, 'MOV': 1, 'INS': 2, 'DEL': 0, 'spoonMethodName': 'com.google.devtools.build.lib.buildtool.BuildRequest.BuildRequestOptions'}, {'UPD': 0, 'TOT': 1, 'MOV': 0, 'INS': 1, 'DEL': 0, 'spoonMethodName': 'com.google.devtools.build.lib.buildtool.BuildRequest.JobsConverter'}], 'spoonFilePath': 'BuildRequest.java'}]}]",https://github.com/bazelbuild/bazel/issues/1160,820.0002777777778,"['P2', 'type: bug', 'z-category: performance']",Insane(100%) consumption of resources,1.0,"['com.google.devtools.build.lib.buildtool.BuildRequest.BuildRequestOptions', 'com.google.devtools.build.lib.buildtool.BuildRequest.validateOptions()', 'com.google.devtools.build.lib.buildtool.BuildRequest', 'com.google.devtools.build.lib.buildtool.BuildRequest.JobsConverter', 'com.google.devtools.build.lib.actions.LocalHostCapacity.setLocalHostCapacity(com.google.devtools.build.lib.actions.ResourceSet)']",['222b03cd7562e8c44f07e6decd900a4d52b4f95f'],,"['src/main/java/com/google/devtools/build/lib/buildtool', 'src/main/java/com/google/devtools/build/lib/actions']",65.0,16.0,81.0,2.0,3.0,5.0,15.0,4.0,7.0,1.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,bazel
7483,2017-09-09 16:58:36,kageiit,"Seems like this option may not be fully ready yet? @yiding 

Stacktrace
```
Not using buckd because NO_BUCKD is set.
PARSING BUCK FILES... 3.1s (100%)
CREATING ACTION GRAPH: FINISHED IN 2.2s
BUILDING: FINISHED IN 10.9s
DOWNLOADED 0.00 BYTES/SEC AVG, 0 ARTIFACTS, 0.00 BYTES
[2017-09-09 07:29:26.321][error][command:null][tid:01][com.facebook.buck.cli.Main] Uncaught exception at top level
com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: A build rule for this target has already been created: //.okbuck/cache:com.android.support.animated-vector-drawable-26.0.2.aar#aar_prebuilt_jar,dex
	at com.google.common.util.concurrent.Futures.wrapAndThrowUnchecked(Futures.java:1329)
	at com.google.common.util.concurrent.Futures.getUnchecked(Futures.java:1314)
	at com.facebook.buck.rules.MultiThreadedBuildRuleResolver.requireRule(MultiThreadedBuildRuleResolver.java:105)
	at com.facebook.buck.rules.ActionGraphCache$1.lambda$2(ActionGraphCache.java:215)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture$Completion.exec(CompletableFuture.java:443)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: java.lang.IllegalStateException: A build rule for this target has already been created: //.okbuck/cache:com.android.support.animated-vector-drawable-26.0.2.aar#aar_prebuilt_jar,dex
	at com.google.common.base.Preconditions.checkState(Preconditions.java:444)
	at com.facebook.buck.rules.MultiThreadedBuildRuleResolver.addToIndex(MultiThreadedBuildRuleResolver.java:118)
	at com.facebook.buck.android.AndroidBinaryGraphEnhancer.createPreDexRulesForLibraries(AndroidBinaryGraphEnhancer.java:594)
	at com.facebook.buck.android.AndroidBinaryGraphEnhancer.createAdditionalBuildables(AndroidBinaryGraphEnhancer.java:341)
	at com.facebook.buck.android.AndroidBinaryDescription.createBuildRule(AndroidBinaryDescription.java:305)
	at com.facebook.buck.android.AndroidBinaryDescription.createBuildRule(AndroidBinaryDescription.java:1)
	at com.facebook.buck.rules.DefaultTargetNodeToBuildRuleTransformer.transform(DefaultTargetNodeToBuildRuleTransformer.java:88)
	at com.facebook.buck.rules.MultiThreadedBuildRuleResolver.lambda$1(MultiThreadedBuildRuleResolver.java:108)
	at com.facebook.buck.rules.MultiThreadedBuildRuleResolver$2.compute(MultiThreadedBuildRuleResolver.java:155)
	at java.util.concurrent.RecursiveTask.exec(RecursiveTask.java:94)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:389)
	at java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1001)
	at com.facebook.buck.util.concurrent.WorkThreadTrackingFutureDecorator.get(WorkThreadTrackingFutureDecorator.java:52)
	at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:142)
	at com.google.common.util.concurrent.Futures.getUnchecked(Futures.java:1312)
```",2017-12-07 04:28:20,"[{'commitSpoonAstDiffStats': [{'spoonFilePath': 'AndroidBinaryGraphEnhancer.java', 'spoonMethods': [{'INS': 1, 'MOV': 12, 'DEL': 11, 'UPD': 2, 'TOT': 26, 'spoonMethodName': 'com.facebook.buck.android.AndroidBinaryGraphEnhancer.createPreDexRulesForLibraries(java.lang.Iterable,com.facebook.buck.android.packageable.AndroidPackageableCollection)'}]}], 'nameRev': 'c70c2f363789e43c7561eeaf7931ec12f5c4d7c8 tags/v2017.10.01.01~313', 'commitUser': 'facebook-github-bot', 'commitHash': 'c70c2f363789e43c7561eeaf7931ec12f5c4d7c8', 'commitDateTime': '2017-09-12 16:02:02', 'commitMessage': 'Replace getRuleOptional..addToIndex with computeIfAbsent for createPreDexRulesForLibraries\n\nSummary:\nThis fixes a potential race condition when doing parallelized action graph creation.\n\nReported in: https://github.com/facebook/buck/issues/1518\n\nTest Plan: CI\n\nReviewed By: ttsugriy\n\nfbshipit-source-id: 2fc49b1\n', 'commitGitStats': [{'filePath': 'src/com/facebook/buck/android/AndroidBinaryGraphEnhancer.java', 'deletions': 21, 'lines': 32, 'insertions': 11}], 'commitParents': ['671fa23ba0e820c4910d785af489a46316a050be'], 'authoredDateTime': '2017-09-12 14:42:14', 'spoonStatsSkippedReason': '', 'commitGHEventType': 'referenced'}]",https://github.com/facebook/buck/issues/1518,88.00027777777778,['bug'],"""A build rule for this target has already been created"" errors after enabling action_graph_parallelization_enabled",1.0,"['com.facebook.buck.android.AndroidBinaryGraphEnhancer.createPreDexRulesForLibraries(java.lang.Iterable,com.facebook.buck.android.packageable.AndroidPackageableCollection)']",['c70c2f363789e43c7561eeaf7931ec12f5c4d7c8'],,['src/com/facebook/buck/android'],11.0,21.0,32.0,1.0,2.0,1.0,26.0,12.0,1.0,11.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,buck
16470,2017-08-14 16:28:46,soon,"The [`JavadocPackageCheck`](https://github.com/checkstyle/checkstyle/blob/master/src/main/java/com/puppycrawl/tools/checkstyle/checks/javadoc/JavadocPackageCheck.java) contains a field `directoriesChecked`, which contains all processed directories. 

This field should be updated across the application lifetime (https://github.com/checkstyle/checkstyle/blob/master/src/main/java/com/puppycrawl/tools/checkstyle/checks/javadoc/JavadocPackageCheck.java#L73), therefore it should be thread-safe",2017-08-17 13:25:44,"[{'commitMessage': 'Issue #4945: JavadocPackageCheck should be thread-safe\n', 'commitParents': ['25b31e7210a50a2cd545682d8c100ca0571ec98f'], 'commitHash': 'ff85e714498a63c4060121f0625be5c76b741a5a', 'commitSpoonAstDiffStats': [{'spoonFilePath': 'JavadocPackageCheck.java', 'spoonMethods': [{'spoonMethodName': 'com.puppycrawl.tools.checkstyle.checks.javadoc.JavadocPackageCheck', 'DEL': 1, 'INS': 1, 'UPD': 0, 'MOV': 0, 'TOT': 2}, {'spoonMethodName': 'com.puppycrawl.tools.checkstyle.checks.javadoc.JavadocPackageCheck.processFiltered(java.io.File,com.puppycrawl.tools.checkstyle.api.FileText)', 'DEL': 1, 'INS': 2, 'UPD': 0, 'MOV': 1, 'TOT': 4}]}], 'nameRev': 'ff85e714498a63c4060121f0625be5c76b741a5a tags/checkstyle-8.2~59', 'commitUser': 'romani', 'authoredDateTime': '2017-08-14 21:38:34', 'commitGHEventType': 'referenced', 'commitGitStats': [{'lines': 9, 'filePath': 'src/main/java/com/puppycrawl/tools/checkstyle/checks/javadoc/JavadocPackageCheck.java', 'insertions': 4, 'deletions': 5}], 'commitDateTime': '2017-08-17 06:25:09', 'spoonStatsSkippedReason': ''}]",https://github.com/checkstyle/checkstyle/issues/4945,2.000277777777778,"['approved', 'bug']",JavadocPackageCheck should be thread-safe,1.0,"['com.puppycrawl.tools.checkstyle.checks.javadoc.JavadocPackageCheck', 'com.puppycrawl.tools.checkstyle.checks.javadoc.JavadocPackageCheck.processFiltered(java.io.File,com.puppycrawl.tools.checkstyle.api.FileText)']",['ff85e714498a63c4060121f0625be5c76b741a5a'],,['src/main/java/com/puppycrawl/tools/checkstyle/checks/javadoc'],4.0,5.0,9.0,1.0,0.0,2.0,6.0,1.0,3.0,2.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,checkstyle
16473,2017-08-10 13:18:49,soon,"Now the `SeverityLevelCounter` uses `int count` to track the number of errors with the specified `SeverityLevel`. However, this is not thread-safe, because `int` incrementation is not an atomic operation and therefore may lead to unpredictable results.",2017-08-18 09:13:05,"[{'commitMessage': 'Issue #4927: Use atomic counter to allow multi-thread access\n', 'commitParents': ['ff85e714498a63c4060121f0625be5c76b741a5a'], 'commitHash': '290b5b083e2d00795b08446e6d0dd14f71df9bfd', 'commitSpoonAstDiffStats': [{'spoonFilePath': 'SeverityLevelCounter.java', 'spoonMethods': [{'spoonMethodName': 'com.puppycrawl.tools.checkstyle.api.SeverityLevelCounter.getCount()', 'DEL': 0, 'INS': 1, 'UPD': 0, 'MOV': 1, 'TOT': 2}, {'spoonMethodName': 'com.puppycrawl.tools.checkstyle.api.SeverityLevelCounter.addError(com.puppycrawl.tools.checkstyle.api.AuditEvent)', 'DEL': 1, 'INS': 1, 'UPD': 0, 'MOV': 0, 'TOT': 2}, {'spoonMethodName': 'com.puppycrawl.tools.checkstyle.api.SeverityLevelCounter.addException(com.puppycrawl.tools.checkstyle.api.AuditEvent,java.lang.Throwable)', 'DEL': 1, 'INS': 1, 'UPD': 0, 'MOV': 0, 'TOT': 2}, {'spoonMethodName': 'com.puppycrawl.tools.checkstyle.api.SeverityLevelCounter.auditStarted(com.puppycrawl.tools.checkstyle.api.AuditEvent)', 'DEL': 1, 'INS': 1, 'UPD': 0, 'MOV': 0, 'TOT': 2}, {'spoonMethodName': 'com.puppycrawl.tools.checkstyle.api.SeverityLevelCounter', 'DEL': 0, 'INS': 2, 'UPD': 1, 'MOV': 0, 'TOT': 3}]}], 'nameRev': '290b5b083e2d00795b08446e6d0dd14f71df9bfd tags/checkstyle-8.2~58', 'commitUser': 'romani', 'authoredDateTime': '2017-08-10 18:20:27', 'commitGHEventType': 'referenced', 'commitGitStats': [{'lines': 12, 'filePath': 'src/main/java/com/puppycrawl/tools/checkstyle/api/SeverityLevelCounter.java', 'insertions': 7, 'deletions': 5}], 'commitDateTime': '2017-08-17 06:32:13', 'spoonStatsSkippedReason': ''}]",https://github.com/checkstyle/checkstyle/issues/4927,7.000277777777778,"['approved', 'bug']",SeverityLevelCounter should be thread-safe,1.0,"['com.puppycrawl.tools.checkstyle.api.SeverityLevelCounter.addError(com.puppycrawl.tools.checkstyle.api.AuditEvent)', 'com.puppycrawl.tools.checkstyle.api.SeverityLevelCounter.addException(com.puppycrawl.tools.checkstyle.api.AuditEvent,java.lang.Throwable)', 'com.puppycrawl.tools.checkstyle.api.SeverityLevelCounter.getCount()', 'com.puppycrawl.tools.checkstyle.api.SeverityLevelCounter.auditStarted(com.puppycrawl.tools.checkstyle.api.AuditEvent)', 'com.puppycrawl.tools.checkstyle.api.SeverityLevelCounter']",['290b5b083e2d00795b08446e6d0dd14f71df9bfd'],,['src/main/java/com/puppycrawl/tools/checkstyle/api'],7.0,5.0,12.0,1.0,1.0,5.0,11.0,1.0,6.0,3.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,checkstyle
16474,2017-08-10 12:38:10,soon,"Now the `AbstractJavadocCheck` class contains some thread-unsafe context, therefore we cannot use the same check instance across multiple threads.

All thread-unsafe members should be rewritten so that they could be used from the separate threads:

1. `parser` - the `JavadocDetailNodeParser` isn't a thread-safe class
1. `blockCommentAst` - the `AbstractJavadocCheck` updates this field in the `visitToken` method",2017-08-18 09:12:37,"[{'commitMessage': 'Issue #4925: Remove thread-unsafe context from the `AbstractJavadocCheck` class\n', 'commitParents': ['290b5b083e2d00795b08446e6d0dd14f71df9bfd'], 'commitHash': '29901f78ef65899c4c9912b6d49079bdf658d9aa', 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AbstractJavadocCheck.java', 'spoonMethods': [{'spoonMethodName': 'com.puppycrawl.tools.checkstyle.checks.javadoc.AbstractJavadocCheck.FileContext', 'DEL': 0, 'INS': 1, 'UPD': 0, 'MOV': 0, 'TOT': 1}, {'spoonMethodName': 'com.puppycrawl.tools.checkstyle.checks.javadoc.AbstractJavadocCheck.getBlockCommentAst()', 'DEL': 0, 'INS': 1, 'UPD': 0, 'MOV': 1, 'TOT': 2}, {'spoonMethodName': 'com.puppycrawl.tools.checkstyle.checks.javadoc.AbstractJavadocCheck.visitToken(com.puppycrawl.tools.checkstyle.api.DetailAST)', 'DEL': 1, 'INS': 2, 'UPD': 0, 'MOV': 1, 'TOT': 4}, {'spoonMethodName': 'com.puppycrawl.tools.checkstyle.checks.javadoc.AbstractJavadocCheck', 'DEL': 0, 'INS': 1, 'UPD': 0, 'MOV': 2, 'TOT': 3}]}], 'nameRev': '29901f78ef65899c4c9912b6d49079bdf658d9aa tags/checkstyle-8.2~57', 'commitUser': 'romani', 'authoredDateTime': '2017-08-10 17:38:50', 'commitGHEventType': 'referenced', 'commitGitStats': [{'lines': 32, 'filePath': 'src/main/java/com/puppycrawl/tools/checkstyle/checks/javadoc/AbstractJavadocCheck.java', 'insertions': 21, 'deletions': 11}], 'commitDateTime': '2017-08-17 06:36:20', 'spoonStatsSkippedReason': ''}]",https://github.com/checkstyle/checkstyle/issues/4925,7.000277777777778,"['approved', 'bug']",Remove thread-unsafe context from the AbstractJavadocCheck class ,1.0,"['com.puppycrawl.tools.checkstyle.checks.javadoc.AbstractJavadocCheck.getBlockCommentAst()', 'com.puppycrawl.tools.checkstyle.checks.javadoc.AbstractJavadocCheck.FileContext', 'com.puppycrawl.tools.checkstyle.checks.javadoc.AbstractJavadocCheck.visitToken(com.puppycrawl.tools.checkstyle.api.DetailAST)', 'com.puppycrawl.tools.checkstyle.checks.javadoc.AbstractJavadocCheck']",['29901f78ef65899c4c9912b6d49079bdf658d9aa'],,['src/main/java/com/puppycrawl/tools/checkstyle/checks/javadoc'],21.0,11.0,32.0,1.0,0.0,4.0,10.0,4.0,5.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,checkstyle
16475,2017-08-07 15:04:03,soon,"Now the `AbstractCheck` class contains some thread-unsafe context, therefore we cannot use the same check instance across multiple threads.

All thread-unsafe members should be rewritten so that they could be used from the separate threads:

1. `messages` - it is updated via `clearMessages` and `log` methods.
1. `fileContents` - it is updated via `setFileContents` and queried via `getLine`, `getFileContents`",2017-09-05 13:49:31,"[{'commitMessage': 'Issue #4908: Remove thread-unsafe context from AbstractCheck\n', 'commitParents': ['a0df7245867da31daafc1ecb785eaf4d7bc7baa3'], 'commitHash': 'd029afafbf71d412747884a37f8ca239d352ef8d', 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AbstractCheck.java', 'spoonMethods': [{'spoonMethodName': 'com.puppycrawl.tools.checkstyle.api.AbstractCheck.getLine(int)', 'DEL': 0, 'INS': 1, 'UPD': 0, 'MOV': 1, 'TOT': 2}, {'spoonMethodName': 'com.puppycrawl.tools.checkstyle.api.AbstractCheck.getMessages()', 'DEL': 0, 'INS': 1, 'UPD': 0, 'MOV': 1, 'TOT': 2}, {'spoonMethodName': 'com.puppycrawl.tools.checkstyle.api.AbstractCheck.getLines()', 'DEL': 0, 'INS': 1, 'UPD': 0, 'MOV': 1, 'TOT': 2}, {'spoonMethodName': 'com.puppycrawl.tools.checkstyle.api.AbstractCheck', 'DEL': 0, 'INS': 1, 'UPD': 0, 'MOV': 2, 'TOT': 3}, {'spoonMethodName': 'com.puppycrawl.tools.checkstyle.api.AbstractCheck.setFileContents(com.puppycrawl.tools.checkstyle.api.FileContents)', 'DEL': 1, 'INS': 1, 'UPD': 0, 'MOV': 0, 'TOT': 2}, {'spoonMethodName': 'com.puppycrawl.tools.checkstyle.api.AbstractCheck.log(int,java.lang.String,java.lang.Object[])', 'DEL': 0, 'INS': 1, 'UPD': 0, 'MOV': 1, 'TOT': 2}, {'spoonMethodName': 'com.puppycrawl.tools.checkstyle.api.AbstractCheck.clearMessages()', 'DEL': 0, 'INS': 1, 'UPD': 0, 'MOV': 1, 'TOT': 2}, {'spoonMethodName': 'com.puppycrawl.tools.checkstyle.api.AbstractCheck.FileContext', 'DEL': 0, 'INS': 1, 'UPD': 0, 'MOV': 0, 'TOT': 1}, {'spoonMethodName': 'com.puppycrawl.tools.checkstyle.api.AbstractCheck.getFileContents()', 'DEL': 0, 'INS': 1, 'UPD': 0, 'MOV': 1, 'TOT': 2}, {'spoonMethodName': 'com.puppycrawl.tools.checkstyle.api.AbstractCheck.log(int,int,java.lang.String,java.lang.Object[])', 'DEL': 0, 'INS': 1, 'UPD': 0, 'MOV': 1, 'TOT': 2}]}, {'spoonFilePath': 'AbstractCheckTest.java', 'spoonMethods': [{'spoonMethodName': 'com.puppycrawl.tools.checkstyle.api.AbstractCheckTest.testClearMessages()', 'DEL': 2, 'INS': 2, 'UPD': 2, 'MOV': 2, 'TOT': 8}]}], 'nameRev': 'd029afafbf71d412747884a37f8ca239d352ef8d tags/checkstyle-8.3~71', 'commitUser': 'romani', 'authoredDateTime': '2017-08-08 12:36:29', 'commitGHEventType': 'referenced', 'commitGitStats': [{'lines': 9, 'filePath': 'src/test/java/com/puppycrawl/tools/checkstyle/api/AbstractCheckTest.java', 'insertions': 2, 'deletions': 7}, {'lines': 39, 'filePath': 'src/main/java/com/puppycrawl/tools/checkstyle/api/AbstractCheck.java', 'insertions': 25, 'deletions': 14}], 'commitDateTime': '2017-09-05 06:48:17', 'spoonStatsSkippedReason': ''}]",https://github.com/checkstyle/checkstyle/issues/4908,28.00027777777778,"['approved', 'bug']",Remove thread-unsafe context from the AbstractCheck class,1.0,"['com.puppycrawl.tools.checkstyle.api.AbstractCheck.getLine(int)', 'com.puppycrawl.tools.checkstyle.api.AbstractCheck.getFileContents()', 'com.puppycrawl.tools.checkstyle.api.AbstractCheck.getLines()', 'com.puppycrawl.tools.checkstyle.api.AbstractCheck.log(int,int,java.lang.String,java.lang.Object[])', 'com.puppycrawl.tools.checkstyle.api.AbstractCheck.setFileContents(com.puppycrawl.tools.checkstyle.api.FileContents)', 'com.puppycrawl.tools.checkstyle.api.AbstractCheck.log(int,java.lang.String,java.lang.Object[])', 'com.puppycrawl.tools.checkstyle.api.AbstractCheck.clearMessages()', 'com.puppycrawl.tools.checkstyle.api.AbstractCheck.FileContext', 'com.puppycrawl.tools.checkstyle.api.AbstractCheck', 'com.puppycrawl.tools.checkstyle.api.AbstractCheck.getMessages()']",['d029afafbf71d412747884a37f8ca239d352ef8d'],,['src/main/java/com/puppycrawl/tools/checkstyle/api'],25.0,14.0,39.0,1.0,0.0,10.0,20.0,9.0,10.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,checkstyle
16581,2016-05-22 19:06:31,eric-milles,"```
/var/tmp $ cat TestClass.java 
@SuppressWarnings(value = ""rawtypes"") // 'value=' is superflous (non-compact) - VIOLATION
public class Example {
    @SuppressWarnings({""unchecked""}) // array is superflous - VIOLATION 
    public void somePublicMethod() {
    }

    @SomeAnnotation(attr1=""value1"", arrt2={""value2""}) // array is superfluous  - VIOLATION
    public Object[] stuff;
}

/var/tmp $ cat my_check.xml 
<?xml version=""1.0""?>
<!DOCTYPE module PUBLIC
          ""-//Puppy Crawl//DTD Check Configuration 1.3//EN""
          ""http://www.puppycrawl.com/dtds/configuration_1_3.dtd"">
<module name = ""Checker"">
    <module name=""TreeWalker"">
        <module name=""AnnotationUseStyle"">
            <property name=""elementStyle"" value=""compact_no_array"" />
            <property name=""trailingArrayComma"" value=""ignore"" />
        </module>
    </module>
</module>

/var/tmp $ java -jar checkstyle-7.0-all.jar -c my_check.xml TestClass.java 
Starting audit...
[ERROR] /var/tmp/TestClass.java:3: Annotation style must be 'COMPACT_NO_ARRAY'. [AnnotationUseStyle]
Audit done.
Checkstyle ends with 1 errors.
```

Expected violations are marked at source code - there should be 3 of them as all of them are not compact.

Attention: ""compact"" mode should give 1 violation 1st line.

http://checkstyle.sourceforge.net/config_annotation.html#AnnotationUseStyle
http://checkstyle.sourceforge.net/property_types.html#elementStyle
## 
",2019-06-14 14:09:17,"[{'commitMessage': 'Issue #3210: correct xdocs for AnnotationUseStyle elementStyle property\n', 'commitParents': ['a6c70a79e031b2b2f9543af36f4ebcc631c36845'], 'commitHash': '49022de2183f359af6be4c0de57528d797b3d201', 'commitSpoonAstDiffStats': [], 'nameRev': '49022de2183f359af6be4c0de57528d797b3d201 tags/checkstyle-8.22~32', 'commitUser': 'romani', 'authoredDateTime': '2019-06-13 10:22:27', 'commitGHEventType': 'referenced', 'commitGitStats': [{'lines': 34, 'filePath': 'src/xdocs/property_types.xml', 'insertions': 25, 'deletions': 9}], 'commitDateTime': '2019-06-14 07:08:41', 'spoonStatsSkippedReason': ''}, {'commitMessage': 'Issue #3210: fix compact_no_array in AnnotationUseStyleCheck\n', 'commitParents': ['a1546b7177c89c4f0eb573be409d3623235edf84'], 'commitHash': 'a6c70a79e031b2b2f9543af36f4ebcc631c36845', 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AnnotationUseStyleCheckTest.java', 'spoonMethods': [{'spoonMethodName': 'com.puppycrawl.tools.checkstyle.checks.annotation.AnnotationUseStyleCheckTest.testStyleCompactNoArray()', 'DEL': 0, 'INS': 4, 'UPD': 0, 'MOV': 0, 'TOT': 4}, {'spoonMethodName': 'com.puppycrawl.tools.checkstyle.checks.annotation.AnnotationUseStyleCheckTest.testTrailingArrayIgnore()', 'DEL': 0, 'INS': 2, 'UPD': 0, 'MOV': 0, 'TOT': 2}, {'spoonMethodName': 'com.puppycrawl.tools.checkstyle.checks.annotation.AnnotationUseStyleCheckTest.testDefault()', 'DEL': 0, 'INS': 4, 'UPD': 0, 'MOV': 0, 'TOT': 4}]}, {'spoonFilePath': 'AnnotationUseStyleCheck.java', 'spoonMethods': [{'spoonMethodName': 'com.puppycrawl.tools.checkstyle.checks.annotation.AnnotationUseStyleCheck.checkCompactNoArrayStyle(com.puppycrawl.tools.checkstyle.api.DetailAST)', 'DEL': 3, 'INS': 3, 'UPD': 2, 'MOV': 8, 'TOT': 16}]}], 'nameRev': 'a6c70a79e031b2b2f9543af36f4ebcc631c36845 tags/checkstyle-8.22~33', 'commitUser': 'romani', 'authoredDateTime': '2019-06-04 15:44:10', 'commitGHEventType': 'referenced', 'commitGitStats': [{'lines': 30, 'filePath': 'src/main/java/com/puppycrawl/tools/checkstyle/checks/annotation/AnnotationUseStyleCheck.java', 'insertions': 12, 'deletions': 18}, {'lines': 10, 'filePath': 'src/test/java/com/puppycrawl/tools/checkstyle/checks/annotation/AnnotationUseStyleCheckTest.java', 'insertions': 10, 'deletions': 0}, {'lines': 2, 'filePath': 'src/xdocs/config_annotation.xml', 'insertions': 0, 'deletions': 2}], 'commitDateTime': '2019-06-14 07:08:41', 'spoonStatsSkippedReason': ''}]",https://github.com/checkstyle/checkstyle/issues/3210,1117.0002777777777,"['approved', 'bug', 'easy']",AnnotationUseStyle: compact_no_array does not violate extended and multi param annotations,1.0,['com.puppycrawl.tools.checkstyle.checks.annotation.AnnotationUseStyleCheck.checkCompactNoArrayStyle(com.puppycrawl.tools.checkstyle.api.DetailAST)'],"['a6c70a79e031b2b2f9543af36f4ebcc631c36845', '49022de2183f359af6be4c0de57528d797b3d201']",,['src/main/java/com/puppycrawl/tools/checkstyle/checks/annotation'],12.0,18.0,30.0,1.0,2.0,1.0,16.0,8.0,3.0,3.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,checkstyle
18197,2018-08-21 17:37:43,jon-wei,"We noticed deadlocks on the overlord in our test cluster in HttpRemoteTaskRunner:

```
""hrtr-pending-tasks-runner-0"" #193 daemon prio=5 os_prio=0 tid=0x00007fc5a8020000 nid=0x72ec waiting for monitor entry [0x00007fc5644b4000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.runTaskOnWorker(HttpRemoteTaskRunner.java:379)
        - waiting to lock <0x00000000c1544c10> (a java.lang.Object)
        at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.lambda$addPendingTaskToExecutor$7(HttpRemoteTaskRunner.java:1005)
        at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner$$Lambda$133/748833484.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
```

```
java.lang.Thread.State: BLOCKED (on object monitor)
        at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.getKnownTasks(HttpRemoteTaskRunner.java:1111)
        - waiting to lock <0x00000000c1544c10> (a java.lang.Object)
        at io.druid.indexing.overlord.TaskQueue.manage(TaskQueue.java:232)
        at io.druid.indexing.overlord.TaskQueue.access$000(TaskQueue.java:69)
        at io.druid.indexing.overlord.TaskQueue$1.run(TaskQueue.java:136)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
```

```
""HttpRemoteTaskRunner-worker-sync-4"" #116 daemon prio=5 os_prio=0 tid=0x00007fc588018800 nid=0x724c waiting for monitor entry [0x00007fc569801000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at io.druid.server.coordination.ChangeRequestHttpSyncer.stop(ChangeRequestHttpSyncer.java:138)
        - waiting to lock <0x00000000c1544980> (a io.druid.concurrent.LifecycleLock)
        at io.druid.indexing.overlord.hrtr.WorkerHolder.stop(WorkerHolder.java:333)
        at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.removeWorker(HttpRemoteTaskRunner.java:532)
        - locked <0x00000000c1540300> (a java.util.concurrent.ConcurrentHashMap)
        at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.lambda$scheduleSyncMonitoring$2(HttpRemoteTaskRunner.java:642)
        - locked <0x00000000c1540300> (a java.util.concurrent.ConcurrentHashMap)
        at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner$$Lambda$90/1851975649.run(Unknown Source)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
```

```
java.lang.Thread.State: BLOCKED (on object monitor)
        at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.taskAddedOrUpdated(HttpRemoteTaskRunner.java:1174)
        - waiting to lock <0x00000000c1544c10> (a java.lang.Object)
        at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner$$Lambda$87/935105358.taskAddedOrUpdated(Unknown Source)
        at io.druid.indexing.overlord.hrtr.WorkerHolder$2.notifyListener(WorkerHolder.java:449)
        at io.druid.indexing.overlord.hrtr.WorkerHolder$2.deltaSync(WorkerHolder.java:442)
        at io.druid.server.coordination.ChangeRequestHttpSyncer$1.onSuccess(ChangeRequestHttpSyncer.java:269)
        - locked <0x00000000c1543b50> (a io.druid.concurrent.LifecycleLock)
        at io.druid.server.coordination.ChangeRequestHttpSyncer$1.onSuccess(ChangeRequestHttpSyncer.java:225)
        at com.google.common.util.concurrent.Futures$4.run(Futures.java:1181)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
```",2018-08-25 21:15:57,"[{'commitGitStats': [{'filePath': 'indexing-service/src/main/java/io/druid/indexing/overlord/hrtr/HttpRemoteTaskRunner.java', 'deletions': 10, 'insertions': 25, 'lines': 35}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'HttpRemoteTaskRunner.java', 'spoonMethods': [{'INS': 4, 'UPD': 0, 'DEL': 1, 'spoonMethodName': 'io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.runTaskOnWorker(io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner$HttpRemoteTaskRunnerWorkItem,java.lang.String)', 'MOV': 3, 'TOT': 8}, {'INS': 1, 'UPD': 0, 'DEL': 0, 'spoonMethodName': 'io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.taskComplete(io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner$HttpRemoteTaskRunnerWorkItem,io.druid.indexing.overlord.hrtr.WorkerHolder,io.druid.indexer.TaskStatus)', 'MOV': 0, 'TOT': 1}, {'INS': 3, 'UPD': 0, 'DEL': 0, 'spoonMethodName': 'io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.taskAddedOrUpdated(io.druid.indexing.worker.TaskAnnouncement,io.druid.indexing.overlord.hrtr.WorkerHolder)', 'MOV': 1, 'TOT': 4}]}], 'spoonStatsSkippedReason': '', 'authoredDateTime': '2018-08-25 14:15:57', 'commitMessage': 'fix TaskQueue-HRTR deadlock (#6212)\n\n* fix TaskQueue-HRTR deadlock causing https://github.com/apache/incubator-druid/issues/6201\r\n\r\n* address review comments\r\n', 'commitUser': 'gianm', 'commitDateTime': '2018-08-25 14:15:57', 'commitParents': ['28e6ae3664b02b17ee691968469905f8df5a8e28'], 'commitGHEventType': 'referenced', 'nameRev': 'c3aaf8122d658fce578e75ccbba18f7c74b8114f tags/druid-0.13.0-incubating-rc1~130', 'commitHash': 'c3aaf8122d658fce578e75ccbba18f7c74b8114f'}]",https://github.com/apache/druid/issues/6201,4.000277777777778,"['Bug', 'HTTP']",Deadlock on overlord with HttpRemoteTaskRunner,1.0,"['io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.taskAddedOrUpdated(io.druid.indexing.worker.TaskAnnouncement,io.druid.indexing.overlord.hrtr.WorkerHolder)', 'io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.taskComplete(io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner$HttpRemoteTaskRunnerWorkItem,io.druid.indexing.overlord.hrtr.WorkerHolder,io.druid.indexer.TaskStatus)', 'io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.runTaskOnWorker(io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner$HttpRemoteTaskRunnerWorkItem,java.lang.String)']",['c3aaf8122d658fce578e75ccbba18f7c74b8114f'],,['indexing-service/src/main/java/io/druid/indexing/overlord/hrtr'],25.0,10.0,35.0,1.0,0.0,3.0,13.0,4.0,8.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,druid
18868,2014-01-13 20:51:18,s1monw,"InterruptedExceptions should be handled by either rethrowing or restoring the interrupt state (i.e. calling `Thread.currentThread().interrupt()`). This is important since the caller of the is method or subequent method calls might also be interested in this exception. If we ignore the interrupt state the caller might be left unaware of the exception and blocks again on a subsequent method.
",2014-01-13 21:34:49,"[{'commitGitStats': [{'lines': 8, 'insertions': 5, 'deletions': 3, 'filePath': 'src/main/java/org/elasticsearch/action/support/AdapterActionFuture.java'}, {'lines': 36, 'insertions': 0, 'deletions': 36, 'filePath': 'src/main/java/org/elasticsearch/ElasticsearchInterruptedException.java'}, {'lines': 4, 'insertions': 2, 'deletions': 2, 'filePath': 'src/main/java/org/elasticsearch/index/service/InternalIndexService.java'}, {'lines': 30, 'insertions': 15, 'deletions': 15, 'filePath': 'src/main/java/org/elasticsearch/action/ActionFuture.java'}, {'lines': 8, 'insertions': 5, 'deletions': 3, 'filePath': 'src/main/java/org/elasticsearch/transport/PlainTransportFuture.java'}, {'lines': 12, 'insertions': 8, 'deletions': 4, 'filePath': 'src/main/java/org/elasticsearch/tribe/TribeService.java'}, {'lines': 4, 'insertions': 2, 'deletions': 2, 'filePath': 'src/main/java/org/elasticsearch/common/util/concurrent/EsAbortPolicy.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['0751f0b7c66429382be01bbab9007f043f1adb3c'], 'commitUser': 's1monw', 'commitDateTime': '2014-01-13 22:17:11', 'commitHash': 'a1efa1f7aa24631040af8f988135277ff20d2326', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 2, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.common.util.concurrent.EsAbortPolicy.rejectedExecution(java.lang.Runnable,java.util.concurrent.ThreadPoolExecutor)', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'EsAbortPolicy.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.index.service.InternalIndexService.close(java.lang.String,java.util.concurrent.Executor)', 'INS': 2, 'MOV': 1}], 'spoonFilePath': 'InternalIndexService.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 2, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.tribe.TribeService.doStart()', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 3, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.tribe.TribeService.doStart().1.onFailure(java.lang.String,java.lang.Throwable)', 'INS': 1, 'MOV': 2}], 'spoonFilePath': 'TribeService.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 2, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.transport.PlainTransportFuture.txGet()', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 2, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.transport.PlainTransportFuture.txGet(long,java.util.concurrent.TimeUnit)', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'PlainTransportFuture.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.ElasticsearchInterruptedException', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'ElasticsearchInterruptedException.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 2, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.action.support.AdapterActionFuture.actionGet(long,java.util.concurrent.TimeUnit)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 2, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.action.support.AdapterActionFuture.actionGet()', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'AdapterActionFuture.java'}, {'spoonMethods': [], 'spoonFilePath': 'ActionFuture.java'}], 'nameRev': 'a1efa1f7aa24631040af8f988135277ff20d2326 tags/v2.0.0-beta1~5786', 'commitGHEventType': 'closed', 'commitMessage': 'Remove `ElasticsearchInterruptedException` and handle interrupt state\ncorrectly.\n\nInterruptedExceptions should be handled by either rethrowing or\nrestoring the interrupt state (i.e. calling\n`Thread.currentThread().interrupt()`). This is important since the\ncaller of the is method or subequent method calls might also be\ninterested in this exception. If we ignore the interrupt state the\ncaller might be left unaware of the exception and blocks again on\na subsequent method.\n\nCloses #4712\n', 'authoredDateTime': '2014-01-13 21:45:42'}]",https://github.com/elastic/elasticsearch/issues/4712,0.0002777777777777778,"['>breaking', '>enhancement', 'v1.0.0.RC1']",Remove `ElasticsearchInterruptedException` and handle interrupt state  correctly.,1.0,"['org.elasticsearch.index.service.InternalIndexService.close(java.lang.String,java.util.concurrent.Executor)', 'org.elasticsearch.action.support.AdapterActionFuture.actionGet(long,java.util.concurrent.TimeUnit)', 'org.elasticsearch.action.support.AdapterActionFuture.actionGet()', 'org.elasticsearch.transport.PlainTransportFuture.txGet(long,java.util.concurrent.TimeUnit)', 'org.elasticsearch.common.util.concurrent.EsAbortPolicy.rejectedExecution(java.lang.Runnable,java.util.concurrent.ThreadPoolExecutor)', 'org.elasticsearch.transport.PlainTransportFuture.txGet()', 'org.elasticsearch.tribe.TribeService.doStart().1.onFailure(java.lang.String,java.lang.Throwable)', 'org.elasticsearch.tribe.TribeService.doStart()', 'org.elasticsearch.ElasticsearchInterruptedException']",['a1efa1f7aa24631040af8f988135277ff20d2326'],,"['src/main/java/org/elasticsearch/transport', 'src/main/java/org/elasticsearch', 'src/main/java/org/elasticsearch/action/support', 'src/main/java/org/elasticsearch/action', 'src/main/java/org/elasticsearch/index/service', 'src/main/java/org/elasticsearch/common/util/concurrent', 'src/main/java/org/elasticsearch/tribe']",37.0,65.0,102.0,7.0,6.0,9.0,20.0,3.0,9.0,2.0,6.0,0.0,0.0,0.0,0.0,0.0,0.0,elasticsearch
19459,2019-07-01 18:47:36,paulward24,"I submitted a CR for this issue:

https://github.com/elastic/elasticsearch/pull/43839

The field ```fileDetails``` (a ```HashMap```, i.e., not thread safe)

https://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java#L679

is used only in synchronzied methods (in about 20 locations), e.g.,:

https://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java#L767-L768

i.e., including ```.size()```.

This is correct, because according to JDK:

```
If multiple threads access a hash map concurrently, and at least
one of the threads modifies the map structurally, it must be
synchronized externally.
```

https://docs.oracle.com/javase/8/docs/api/java/util/HashMap.html

However, in the 21st location, here:

https://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java#L958

the method is not synchronized.

This CR simply adds the keyword synchronized to the method, just like for all the other places.
",2019-07-03 11:38:57,"[{'commitGitStats': [{'lines': 2, 'insertions': 1, 'deletions': 1, 'filePath': 'server/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['7059224668cf42032e795170f7f8012195f7b421'], 'commitUser': 'dnhatn', 'commitDateTime': '2019-07-03 07:39:58', 'commitHash': 'cff027499a56c2a3d0370888f5fc4c3e6e624f6b', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.recovery.RecoveryState.Index.getFileDetails(java.lang.String)', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'RecoveryState.java'}], 'nameRev': 'cff027499a56c2a3d0370888f5fc4c3e6e624f6b tags/v7.3.0~165', 'commitGHEventType': 'referenced', 'commitMessage': 'Ensure to access RecoveryState#fileDetails under lock\n\nCloses #43840', 'authoredDateTime': '2019-07-03 04:37:07'}, {'commitGitStats': [{'lines': 2, 'insertions': 1, 'deletions': 1, 'filePath': 'server/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['0a41b13cd81eac588c7d46a7e1063c6928b9e220'], 'commitUser': 'dnhatn', 'commitDateTime': '2019-07-03 07:37:07', 'commitHash': '8e413f85e8978da83db107aae51e5543d4779dba', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.recovery.RecoveryState.Index.getFileDetails(java.lang.String)', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'RecoveryState.java'}], 'nameRev': '8e413f85e8978da83db107aae51e5543d4779dba remotes/origin/reindex_v2~392', 'commitGHEventType': 'closed', 'commitMessage': 'Ensure to access RecoveryState#fileDetails under lock\n\nCloses #43840', 'authoredDateTime': '2019-07-03 04:37:07'}]",https://github.com/elastic/elasticsearch/issues/43840,1.0002777777777778,"[':Distributed/Recovery', '>bug']","HashMap is is not thread safe. Field fileDetails is synchronized in 20 locations, but not in 1 (the 21st) location",1.0,['org.elasticsearch.indices.recovery.RecoveryState.Index.getFileDetails(java.lang.String)'],['cff027499a56c2a3d0370888f5fc4c3e6e624f6b'],,['server/src/main/java/org/elasticsearch/indices/recovery'],1.0,1.0,2.0,1.0,0.0,1.0,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,elasticsearch
20574,2017-02-08 14:26:00,s1monw,"We use ExpandCollapseSearchResponseListener for field collapsing but this listener executes heavy operations in a blocking fashion. This could happen on a transport thread. We should do this in an async way.

I just spoke with @jimczi how to fix it and he is taking it over. For now I will put a fix into 5.x and master since this triggers quite often due to a recent change.

Note: this code is unreleased",2017-02-09 17:06:11,"[{'commitGitStats': [{'lines': 26, 'insertions': 7, 'deletions': 19, 'filePath': 'core/src/main/java/org/elasticsearch/action/search/AbstractSearchAsyncAction.java'}, {'lines': 119, 'insertions': 0, 'deletions': 119, 'filePath': 'core/src/main/java/org/elasticsearch/search/collapse/ExpandCollapseSearchResponseListener.java'}, {'lines': 10, 'insertions': 9, 'deletions': 1, 'filePath': 'docs/reference/search/request/collapse.asciidoc'}, {'lines': 9, 'insertions': 9, 'deletions': 0, 'filePath': 'core/src/main/java/org/elasticsearch/action/search/SearchTransportService.java'}, {'lines': 15, 'insertions': 0, 'deletions': 15, 'filePath': 'core/src/test/java/org/elasticsearch/search/SearchModuleTests.java'}, {'lines': 15, 'insertions': 0, 'deletions': 15, 'filePath': 'core/src/main/java/org/elasticsearch/action/search/SearchPhaseController.java'}, {'lines': 75, 'insertions': 0, 'deletions': 75, 'filePath': 'core/src/test/java/org/elasticsearch/search/ResponseListenerPluginIT.java'}, {'lines': 1, 'insertions': 1, 'deletions': 0, 'filePath': 'core/src/test/java/org/elasticsearch/search/collapse/CollapseBuilderTests.java'}, {'lines': 33, 'insertions': 31, 'deletions': 2, 'filePath': 'core/src/main/java/org/elasticsearch/search/collapse/CollapseBuilder.java'}, {'lines': 28, 'insertions': 0, 'deletions': 28, 'filePath': 'core/src/main/java/org/elasticsearch/search/SearchModule.java'}, {'lines': 111, 'insertions': 103, 'deletions': 8, 'filePath': 'core/src/main/java/org/elasticsearch/action/search/TransportSearchAction.java'}, {'lines': 4, 'insertions': 2, 'deletions': 2, 'filePath': 'core/src/main/java/org/elasticsearch/node/Node.java'}, {'lines': 60, 'insertions': 55, 'deletions': 5, 'filePath': 'rest-api-spec/src/main/resources/rest-api-spec/test/search/110_field_collapsing.yaml'}], 'spoonStatsSkippedReason': '', 'commitParents': ['9aea2d63b8756e44458e1d426a608951b4bec64c'], 'commitUser': 'jimczi', 'commitDateTime': '2017-02-09 18:46:28', 'commitHash': 'ae74ae3853b99850e149129f67a6c30242a2a8b9', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.SearchModuleTests.testRegisterSearchResponseListener()', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'SearchModuleTests.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.SearchModule.registerSearchResponseListener(java.util.function.BiConsumer)', 'INS': 0, 'MOV': 0}, {'DEL': 4, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.SearchModule', 'INS': 0, 'MOV': 0}, {'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.SearchModule.getSearchResponseListeners()', 'INS': 0, 'MOV': 0}, {'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.SearchModule.registerSearchResponseListeners(org.elasticsearch.client.Client,java.util.List)', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'SearchModule.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 4, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.action.search.AbstractSearchAsyncAction.FetchPhase.sendResponse(org.elasticsearch.action.search.SearchPhaseController,org.apache.lucene.search.ScoreDoc[],java.lang.String,org.elasticsearch.action.search.SearchPhaseController$ReducedQueryPhase,org.elasticsearch.common.util.concurrent.AtomicArray).3.doRun()', 'INS': 0, 'MOV': 3}, {'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.AbstractSearchAsyncAction.FetchPhase.sendResponse(org.elasticsearch.action.search.SearchPhaseController,org.apache.lucene.search.ScoreDoc[],java.lang.String,org.elasticsearch.action.search.SearchPhaseController$ReducedQueryPhase,org.elasticsearch.common.util.concurrent.AtomicArray)', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'AbstractSearchAsyncAction.java'}, {'spoonMethods': [{'DEL': 4, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.SearchPhaseController', 'INS': 0, 'MOV': 0}, {'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.SearchPhaseController.getSearchResponseListener()', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'SearchPhaseController.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 9, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder.equals(java.lang.Object)', 'INS': 3, 'MOV': 4}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder.innerToXContent(org.elasticsearch.common.xcontent.XContentBuilder)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder.setMaxConcurrentGroupRequests(int)', 'INS': 1, 'MOV': 0}, {'DEL': 1, 'TOT': 11, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder.hashCode()', 'INS': 6, 'MOV': 4}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder.writeTo(org.elasticsearch.common.io.stream.StreamOutput)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder.getMaxConcurrentGroupRequests()', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder', 'INS': 4, 'MOV': 0}], 'spoonFilePath': 'CollapseBuilder.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.ExpandCollapseSearchResponseListener', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'ExpandCollapseSearchResponseListener.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilderTests.randomCollapseBuilder()', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'CollapseBuilderTests.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.TransportSearchAction.buildExpandSearchSourceBuilder(org.elasticsearch.index.query.InnerHitBuilder)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.TransportSearchAction.expandCollapsedHits(org.elasticsearch.cluster.node.DiscoveryNode,org.elasticsearch.action.search.SearchTask,org.elasticsearch.action.search.SearchRequest,org.elasticsearch.action.search.SearchResponse,org.elasticsearch.action.ActionListener)', 'INS': 1, 'MOV': 0}, {'DEL': 3, 'TOT': 15, 'UPD': 4, 'spoonMethodName': 'org.elasticsearch.action.search.TransportSearchAction.executeSearch(org.elasticsearch.action.search.SearchTask,long,org.elasticsearch.action.search.SearchRequest,java.lang.String[],java.util.List,java.util.function.Function,org.elasticsearch.cluster.ClusterState,java.util.Map,org.elasticsearch.action.ActionListener)', 'INS': 2, 'MOV': 6}], 'spoonFilePath': 'TransportSearchAction.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.ResponseListenerPluginIT', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'ResponseListenerPluginIT.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.SearchTransportService.sendExecuteMultiSearch(org.elasticsearch.cluster.node.DiscoveryNode,org.elasticsearch.action.search.MultiSearchRequest,org.elasticsearch.action.search.SearchTask,org.elasticsearch.action.ActionListener)', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'SearchTransportService.java'}, {'spoonMethods': [{'DEL': 2, 'TOT': 2, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.node.Node', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'Node.java'}], 'nameRev': 'ae74ae3853b99850e149129f67a6c30242a2a8b9 tags/v5.4.0~597', 'commitGHEventType': 'referenced', 'commitMessage': 'Removes ExpandCollapseSearchResponseListener, search response listeners and blocking calls\n\nThis changes removes the SearchResponseListener that was used by the ExpandCollapseSearchResponseListener to expand collapsed hits.\nThe removal of SearchResponseListener is not a breaking change because it was never released.\nThis change also replace the blocking call in ExpandCollapseSearchResponseListener by a single asynchronous multi search request. The parallelism of the expand request can be set via CollapseBuilder#max_concurrent_group_searches\n\nCloses #23048\n', 'authoredDateTime': '2017-02-09 18:06:10'}, {'commitGitStats': [{'lines': 26, 'insertions': 19, 'deletions': 7, 'filePath': 'core/src/main/java/org/elasticsearch/action/search/AbstractSearchAsyncAction.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['ecb01c15b9a6645f22f153eb099a377e70e398c8'], 'commitUser': 's1monw', 'commitDateTime': '2017-02-08 15:27:08', 'commitHash': 'd45761e4885400ab64653484f2330599e7c2d40a', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 5, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.action.search.AbstractSearchAsyncAction.FetchPhase.sendResponse(org.elasticsearch.action.search.SearchPhaseController,org.apache.lucene.search.ScoreDoc[],java.lang.String,org.elasticsearch.action.search.SearchPhaseController$ReducedQueryPhase,org.elasticsearch.common.util.concurrent.AtomicArray)', 'INS': 1, 'MOV': 3}], 'spoonFilePath': 'AbstractSearchAsyncAction.java'}], 'nameRev': 'd45761e4885400ab64653484f2330599e7c2d40a tags/v6.0.0-alpha1~852', 'commitGHEventType': 'referenced', 'commitMessage': 'Fork off a search thread before sending back fetched responses\n\nThis is just a temporary fix until #23048 is fixed. FieldCollapsing\nis executing blocking calls on a network thread which causes potential deadlocks\nand trips assertions.\n\nRelates to #23048\n', 'authoredDateTime': '2017-02-08 15:27:08'}, {'commitGitStats': [{'lines': 26, 'insertions': 19, 'deletions': 7, 'filePath': 'core/src/main/java/org/elasticsearch/action/search/AbstractSearchAsyncAction.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['572ce266be2abdb0bdba9ac064ce0ee8a0eff5ed'], 'commitUser': 's1monw', 'commitDateTime': '2017-02-08 15:35:10', 'commitHash': 'fd5de00e87e84a244e3e6a6220f1084aa94ec367', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 5, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.action.search.AbstractSearchAsyncAction.FetchPhase.sendResponse(org.elasticsearch.action.search.SearchPhaseController,org.apache.lucene.search.ScoreDoc[],java.lang.String,org.elasticsearch.action.search.SearchPhaseController$ReducedQueryPhase,org.elasticsearch.common.util.concurrent.AtomicArray)', 'INS': 1, 'MOV': 3}], 'spoonFilePath': 'AbstractSearchAsyncAction.java'}], 'nameRev': 'fd5de00e87e84a244e3e6a6220f1084aa94ec367 tags/v5.4.0~618', 'commitGHEventType': 'referenced', 'commitMessage': 'Fork off a search thread before sending back fetched responses\n\nThis is just a temporary fix until #23048 is fixed. FieldCollapsing\nis executing blocking calls on a network thread which causes potential deadlocks\nand trips assertions.\n\nRelates to #23048\n', 'authoredDateTime': '2017-02-08 15:27:08'}, {'commitGitStats': [{'lines': 1, 'insertions': 0, 'deletions': 1, 'filePath': 'core/src/main/java/org/elasticsearch/action/search/AbstractSearchAsyncAction.java'}, {'lines': 119, 'insertions': 0, 'deletions': 119, 'filePath': 'core/src/main/java/org/elasticsearch/search/collapse/ExpandCollapseSearchResponseListener.java'}, {'lines': 10, 'insertions': 9, 'deletions': 1, 'filePath': 'docs/reference/search/request/collapse.asciidoc'}, {'lines': 9, 'insertions': 9, 'deletions': 0, 'filePath': 'core/src/main/java/org/elasticsearch/action/search/SearchTransportService.java'}, {'lines': 15, 'insertions': 0, 'deletions': 15, 'filePath': 'core/src/test/java/org/elasticsearch/search/SearchModuleTests.java'}, {'lines': 15, 'insertions': 0, 'deletions': 15, 'filePath': 'core/src/main/java/org/elasticsearch/action/search/SearchPhaseController.java'}, {'lines': 75, 'insertions': 0, 'deletions': 75, 'filePath': 'core/src/test/java/org/elasticsearch/search/ResponseListenerPluginIT.java'}, {'lines': 1, 'insertions': 1, 'deletions': 0, 'filePath': 'core/src/test/java/org/elasticsearch/search/collapse/CollapseBuilderTests.java'}, {'lines': 33, 'insertions': 31, 'deletions': 2, 'filePath': 'core/src/main/java/org/elasticsearch/search/collapse/CollapseBuilder.java'}, {'lines': 28, 'insertions': 0, 'deletions': 28, 'filePath': 'core/src/main/java/org/elasticsearch/search/SearchModule.java'}, {'lines': 113, 'insertions': 105, 'deletions': 8, 'filePath': 'core/src/main/java/org/elasticsearch/action/search/TransportSearchAction.java'}, {'lines': 4, 'insertions': 2, 'deletions': 2, 'filePath': 'core/src/main/java/org/elasticsearch/node/Node.java'}, {'lines': 58, 'insertions': 52, 'deletions': 6, 'filePath': 'rest-api-spec/src/main/resources/rest-api-spec/test/search/110_field_collapsing.yaml'}], 'spoonStatsSkippedReason': '', 'commitParents': ['f06a86447ce86ceeecec28cecb68411029deff6d'], 'commitUser': 'jimczi', 'commitDateTime': '2017-02-09 19:02:05', 'commitHash': '9868d3d123204a7a31574d857d473c140056e276', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.SearchModuleTests.testRegisterSearchResponseListener()', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'SearchModuleTests.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.SearchModule.registerSearchResponseListener(java.util.function.BiConsumer)', 'INS': 0, 'MOV': 0}, {'DEL': 4, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.SearchModule', 'INS': 0, 'MOV': 0}, {'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.SearchModule.getSearchResponseListeners()', 'INS': 0, 'MOV': 0}, {'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.SearchModule.registerSearchResponseListeners(org.elasticsearch.client.Client,java.util.List)', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'SearchModule.java'}, {'spoonMethods': [], 'spoonFilePath': 'AbstractSearchAsyncAction.java'}, {'spoonMethods': [{'DEL': 4, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.SearchPhaseController', 'INS': 0, 'MOV': 0}, {'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.SearchPhaseController.getSearchResponseListener()', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'SearchPhaseController.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 9, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder.equals(java.lang.Object)', 'INS': 3, 'MOV': 4}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder.innerToXContent(org.elasticsearch.common.xcontent.XContentBuilder)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder.setMaxConcurrentGroupRequests(int)', 'INS': 1, 'MOV': 0}, {'DEL': 1, 'TOT': 11, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder.hashCode()', 'INS': 6, 'MOV': 4}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder.writeTo(org.elasticsearch.common.io.stream.StreamOutput)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder.getMaxConcurrentGroupRequests()', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder', 'INS': 4, 'MOV': 0}], 'spoonFilePath': 'CollapseBuilder.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.ExpandCollapseSearchResponseListener', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'ExpandCollapseSearchResponseListener.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilderTests.randomCollapseBuilder()', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'CollapseBuilderTests.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.TransportSearchAction.buildExpandSearchSourceBuilder(org.elasticsearch.index.query.InnerHitBuilder)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.TransportSearchAction.expandCollapsedHits(org.elasticsearch.cluster.node.DiscoveryNode,org.elasticsearch.action.search.SearchTask,org.elasticsearch.action.search.SearchRequest,org.elasticsearch.action.search.SearchResponse,org.elasticsearch.action.ActionListener)', 'INS': 1, 'MOV': 0}, {'DEL': 3, 'TOT': 15, 'UPD': 4, 'spoonMethodName': 'org.elasticsearch.action.search.TransportSearchAction.executeSearch(org.elasticsearch.action.search.SearchTask,long,org.elasticsearch.action.search.SearchRequest,java.lang.String[],java.util.List,java.util.function.Function,org.elasticsearch.cluster.ClusterState,java.util.Map,org.elasticsearch.action.ActionListener)', 'INS': 2, 'MOV': 6}], 'spoonFilePath': 'TransportSearchAction.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.ResponseListenerPluginIT', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'ResponseListenerPluginIT.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.SearchTransportService.sendExecuteMultiSearch(org.elasticsearch.cluster.node.DiscoveryNode,org.elasticsearch.action.search.MultiSearchRequest,org.elasticsearch.action.search.SearchTask,org.elasticsearch.action.ActionListener)', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'SearchTransportService.java'}, {'spoonMethods': [{'DEL': 2, 'TOT': 2, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.node.Node', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'Node.java'}], 'nameRev': '9868d3d123204a7a31574d857d473c140056e276 tags/v5.3.0~167', 'commitGHEventType': 'referenced', 'commitMessage': 'Removes ExpandCollapseSearchResponseListener, search response listeners and blocking calls\n\nThis changes removes the SearchResponseListener that was used by the ExpandCollapseSearchResponseListener to expand collapsed hits.\nThe removal of SearchResponseListener is not a breaking change because it was never released.\nThis change also replace the blocking call in ExpandCollapseSearchResponseListener by a single asynchronous multi search request. The parallelism of the expand request can be set via CollapseBuilder#max_concurrent_group_searches\n\nCloses #23048\n', 'authoredDateTime': '2017-02-09 18:06:10'}, {'commitGitStats': [{'lines': 26, 'insertions': 7, 'deletions': 19, 'filePath': 'core/src/main/java/org/elasticsearch/action/search/AbstractSearchAsyncAction.java'}, {'lines': 117, 'insertions': 0, 'deletions': 117, 'filePath': 'core/src/main/java/org/elasticsearch/search/collapse/ExpandCollapseSearchResponseListener.java'}, {'lines': 10, 'insertions': 9, 'deletions': 1, 'filePath': 'docs/reference/search/request/collapse.asciidoc'}, {'lines': 9, 'insertions': 9, 'deletions': 0, 'filePath': 'core/src/main/java/org/elasticsearch/action/search/SearchTransportService.java'}, {'lines': 15, 'insertions': 0, 'deletions': 15, 'filePath': 'core/src/test/java/org/elasticsearch/search/SearchModuleTests.java'}, {'lines': 19, 'insertions': 2, 'deletions': 17, 'filePath': 'core/src/main/java/org/elasticsearch/action/search/SearchPhaseController.java'}, {'lines': 75, 'insertions': 0, 'deletions': 75, 'filePath': 'core/src/test/java/org/elasticsearch/search/ResponseListenerPluginIT.java'}, {'lines': 1, 'insertions': 1, 'deletions': 0, 'filePath': 'core/src/test/java/org/elasticsearch/search/collapse/CollapseBuilderTests.java'}, {'lines': 33, 'insertions': 31, 'deletions': 2, 'filePath': 'core/src/main/java/org/elasticsearch/search/collapse/CollapseBuilder.java'}, {'lines': 28, 'insertions': 0, 'deletions': 28, 'filePath': 'core/src/main/java/org/elasticsearch/search/SearchModule.java'}, {'lines': 111, 'insertions': 103, 'deletions': 8, 'filePath': 'core/src/main/java/org/elasticsearch/action/search/TransportSearchAction.java'}, {'lines': 4, 'insertions': 2, 'deletions': 2, 'filePath': 'core/src/main/java/org/elasticsearch/node/Node.java'}, {'lines': 58, 'insertions': 52, 'deletions': 6, 'filePath': 'rest-api-spec/src/main/resources/rest-api-spec/test/search/110_field_collapsing.yaml'}], 'spoonStatsSkippedReason': '', 'commitParents': ['33915aefd89c736df1c56251365fdf5658229998'], 'commitUser': 'jimczi', 'commitDateTime': '2017-02-09 18:06:10', 'commitHash': '94087b3274c1f4c98c100886e4ad9aefd32c4582', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.SearchModuleTests.testRegisterSearchResponseListener()', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'SearchModuleTests.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.SearchModule.registerSearchResponseListener(java.util.function.BiConsumer)', 'INS': 0, 'MOV': 0}, {'DEL': 4, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.SearchModule', 'INS': 0, 'MOV': 0}, {'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.SearchModule.getSearchResponseListeners()', 'INS': 0, 'MOV': 0}, {'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.SearchModule.registerSearchResponseListeners(org.elasticsearch.client.Client,java.util.List)', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'SearchModule.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 4, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.action.search.AbstractSearchAsyncAction.FetchPhase.sendResponse(org.elasticsearch.action.search.SearchPhaseController,org.apache.lucene.search.ScoreDoc[],java.lang.String,org.elasticsearch.action.search.SearchPhaseController$ReducedQueryPhase,org.elasticsearch.common.util.concurrent.AtomicArray).3.doRun()', 'INS': 0, 'MOV': 3}, {'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.AbstractSearchAsyncAction.FetchPhase.sendResponse(org.elasticsearch.action.search.SearchPhaseController,org.apache.lucene.search.ScoreDoc[],java.lang.String,org.elasticsearch.action.search.SearchPhaseController$ReducedQueryPhase,org.elasticsearch.common.util.concurrent.AtomicArray)', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'AbstractSearchAsyncAction.java'}, {'spoonMethods': [{'DEL': 4, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.SearchPhaseController', 'INS': 0, 'MOV': 0}, {'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.SearchPhaseController.getSearchResponseListener()', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'SearchPhaseController.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 9, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder.equals(java.lang.Object)', 'INS': 3, 'MOV': 4}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder.innerToXContent(org.elasticsearch.common.xcontent.XContentBuilder)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder.setMaxConcurrentGroupRequests(int)', 'INS': 1, 'MOV': 0}, {'DEL': 1, 'TOT': 11, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder.hashCode()', 'INS': 6, 'MOV': 4}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder.writeTo(org.elasticsearch.common.io.stream.StreamOutput)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder.getMaxConcurrentGroupRequests()', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilder', 'INS': 4, 'MOV': 0}], 'spoonFilePath': 'CollapseBuilder.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.ExpandCollapseSearchResponseListener', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'ExpandCollapseSearchResponseListener.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.collapse.CollapseBuilderTests.randomCollapseBuilder()', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'CollapseBuilderTests.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.TransportSearchAction.buildExpandSearchSourceBuilder(org.elasticsearch.index.query.InnerHitBuilder)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.TransportSearchAction.expandCollapsedHits(org.elasticsearch.cluster.node.DiscoveryNode,org.elasticsearch.action.search.SearchTask,org.elasticsearch.action.search.SearchRequest,org.elasticsearch.action.search.SearchResponse,org.elasticsearch.action.ActionListener)', 'INS': 1, 'MOV': 0}, {'DEL': 3, 'TOT': 15, 'UPD': 4, 'spoonMethodName': 'org.elasticsearch.action.search.TransportSearchAction.executeSearch(org.elasticsearch.action.search.SearchTask,long,org.elasticsearch.action.search.SearchRequest,java.lang.String[],java.util.List,java.util.function.Function,org.elasticsearch.cluster.ClusterState,java.util.Map,org.elasticsearch.action.ActionListener)', 'INS': 2, 'MOV': 6}], 'spoonFilePath': 'TransportSearchAction.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.ResponseListenerPluginIT', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'ResponseListenerPluginIT.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.SearchTransportService.sendExecuteMultiSearch(org.elasticsearch.cluster.node.DiscoveryNode,org.elasticsearch.action.search.MultiSearchRequest,org.elasticsearch.action.search.SearchTask,org.elasticsearch.action.ActionListener)', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'SearchTransportService.java'}, {'spoonMethods': [{'DEL': 2, 'TOT': 2, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.node.Node', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'Node.java'}], 'nameRev': '94087b3274c1f4c98c100886e4ad9aefd32c4582 tags/v6.0.0-alpha1~832', 'commitGHEventType': 'referenced', 'commitMessage': 'Removes ExpandCollapseSearchResponseListener, search response listeners and blocking calls\n\nThis changes removes the SearchResponseListener that was used by the ExpandCollapseSearchResponseListener to expand collapsed hits.\r\nThe removal of SearchResponseListener is not a breaking change because it was never released.\r\nThis change also replace the blocking call in ExpandCollapseSearchResponseListener by a single asynchronous multi search request. The parallelism of the expand request can be set via CollapseBuilder#max_concurrent_group_searches\r\n\r\nCloses #23048', 'authoredDateTime': '2017-02-09 18:06:10'}]",https://github.com/elastic/elasticsearch/issues/23048,1.0002777777777778,"[':Search/Search', '>bug', 'blocker', 'critical', 'v5.3.0', 'v5.4.0', 'v6.0.0-alpha1']",ExpandCollapseSearchResponseListener potentially executes blocking calls on a network thread,2.0,"['org.elasticsearch.search.SearchModule', 'org.elasticsearch.search.collapse.CollapseBuilder.setMaxConcurrentGroupRequests(int)', 'org.elasticsearch.action.search.SearchTransportService.sendExecuteMultiSearch(org.elasticsearch.cluster.node.DiscoveryNode,org.elasticsearch.action.search.MultiSearchRequest,org.elasticsearch.action.search.SearchTask,org.elasticsearch.action.ActionListener)', 'org.elasticsearch.search.collapse.CollapseBuilder.innerToXContent(org.elasticsearch.common.xcontent.XContentBuilder)', 'org.elasticsearch.action.search.SearchPhaseController', 'org.elasticsearch.search.collapse.CollapseBuilderTests.randomCollapseBuilder()', 'org.elasticsearch.search.collapse.ExpandCollapseSearchResponseListener', 'org.elasticsearch.action.search.TransportSearchAction.expandCollapsedHits(org.elasticsearch.cluster.node.DiscoveryNode,org.elasticsearch.action.search.SearchTask,org.elasticsearch.action.search.SearchRequest,org.elasticsearch.action.search.SearchResponse,org.elasticsearch.action.ActionListener)', 'org.elasticsearch.node.Node', 'org.elasticsearch.search.ResponseListenerPluginIT', 'org.elasticsearch.action.search.TransportSearchAction.executeSearch(org.elasticsearch.action.search.SearchTask,long,org.elasticsearch.action.search.SearchRequest,java.lang.String[],java.util.List,java.util.function.Function,org.elasticsearch.cluster.ClusterState,java.util.Map,org.elasticsearch.action.ActionListener)', 'org.elasticsearch.search.collapse.CollapseBuilder.hashCode()', 'org.elasticsearch.search.SearchModule.getSearchResponseListeners()', 'org.elasticsearch.search.SearchModule.registerSearchResponseListeners(org.elasticsearch.client.Client,java.util.List)', 'org.elasticsearch.action.search.AbstractSearchAsyncAction.FetchPhase.sendResponse(org.elasticsearch.action.search.SearchPhaseController,org.apache.lucene.search.ScoreDoc[],java.lang.String,org.elasticsearch.action.search.SearchPhaseController$ReducedQueryPhase,org.elasticsearch.common.util.concurrent.AtomicArray).3.doRun()', 'org.elasticsearch.search.collapse.CollapseBuilder.getMaxConcurrentGroupRequests()', 'org.elasticsearch.action.search.SearchPhaseController.getSearchResponseListener()', 'org.elasticsearch.search.collapse.CollapseBuilder.equals(java.lang.Object)', 'org.elasticsearch.search.SearchModuleTests.testRegisterSearchResponseListener()', 'org.elasticsearch.action.search.AbstractSearchAsyncAction.FetchPhase.sendResponse(org.elasticsearch.action.search.SearchPhaseController,org.apache.lucene.search.ScoreDoc[],java.lang.String,org.elasticsearch.action.search.SearchPhaseController$ReducedQueryPhase,org.elasticsearch.common.util.concurrent.AtomicArray)', 'org.elasticsearch.action.search.TransportSearchAction.buildExpandSearchSourceBuilder(org.elasticsearch.index.query.InnerHitBuilder)', 'org.elasticsearch.search.collapse.CollapseBuilder.writeTo(org.elasticsearch.common.io.stream.StreamOutput)', 'org.elasticsearch.search.SearchModule.registerSearchResponseListener(java.util.function.BiConsumer)', 'org.elasticsearch.search.collapse.CollapseBuilder']","['ae74ae3853b99850e149129f67a6c30242a2a8b9', 'd45761e4885400ab64653484f2330599e7c2d40a']",,"['docs/reference/search/request/collapse.asciidoc', 'core/src/main/java/org/elasticsearch/action/search', 'core/src/main/java/org/elasticsearch/search/collapse', 'core/src/main/java/org/elasticsearch/search', 'rest-api-spec/src/main/resources/rest-api-spec/test/search/110_field_collapsing.yaml', 'core/src/main/java/org/elasticsearch/node']",235.0,206.0,441.0,10.0,7.0,24.0,74.0,20.0,24.0,23.0,11.0,0.0,0.0,0.0,3.0,0.0,0.0,elasticsearch
20714,2016-10-26 05:51:13,imotov,"Cancelling search with scroll can leave some search contexts behind. See test failure in https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-unix-compatibility/os=ubuntu/160/console
",2016-11-11 05:13:36,"[{'commitGitStats': [{'lines': 53, 'insertions': 44, 'deletions': 9, 'filePath': 'core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/cancel/TransportCancelTasksAction.java'}, {'lines': 2, 'insertions': 0, 'deletions': 2, 'filePath': 'core/src/test/java/org/elasticsearch/search/SearchCancellationIT.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['0b46df685af0ee7ba9b7a93867cbd80182796fa9'], 'commitUser': 'imotov', 'commitDateTime': '2016-11-10 23:27:28', 'commitHash': 'f963a85a09bfcca59642c5e20f3f0f7abd775eef', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.SearchCancellationIT.testCancellationOfScrollSearches()', 'INS': 0, 'MOV': 0}, {'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.SearchCancellationIT.testCancellationOfScrollSearchesOnFollowupRequests()', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'SearchCancellationIT.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 3, 'UPD': 3, 'spoonMethodName': 'org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.setBanOnNodes(java.lang.String,org.elasticsearch.tasks.CancellableTask,java.util.Set,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanLock)', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.sendSetBanRequest(java.util.Set,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanParentTaskRequest,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanLock).1.handleResponse(org.elasticsearch.transport.TransportResponse$Empty)', 'INS': 0, 'MOV': 1}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.sendSetBanRequest(java.util.Set,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanParentTaskRequest,org.elasticsearch.action.ActionListener).2.handleException(org.elasticsearch.transport.TransportException)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.sendSetBanRequest(java.util.Set,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanParentTaskRequest,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanLock).1', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 2, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.sendSetBanRequest(java.util.Set,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanParentTaskRequest,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanLock).1.handleException(org.elasticsearch.transport.TransportException)', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.taskOperation(org.elasticsearch.action.admin.cluster.node.tasks.cancel.CancelTasksRequest,org.elasticsearch.tasks.CancellableTask,org.elasticsearch.action.ActionListener).1.processResponse()', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 5, 'UPD': 5, 'spoonMethodName': 'org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.sendSetBanRequest(java.util.Set,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanParentTaskRequest,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanLock)', 'INS': 0, 'MOV': 0}, {'DEL': 1, 'TOT': 13, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.taskOperation(org.elasticsearch.action.admin.cluster.node.tasks.cancel.CancelTasksRequest,org.elasticsearch.tasks.CancellableTask,org.elasticsearch.action.ActionListener)', 'INS': 4, 'MOV': 7}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.sendSetBanRequest(java.util.Set,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanParentTaskRequest,org.elasticsearch.action.ActionListener).2.handleResponse(org.elasticsearch.transport.TransportResponse$Empty)', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'TransportCancelTasksAction.java'}], 'nameRev': 'f963a85a09bfcca59642c5e20f3f0f7abd775eef tags/v5.2.0~681', 'commitGHEventType': 'referenced', 'commitMessage': 'Task cancellation command should wait for all child nodes to receive cancellation request before returning\n\nCurrently the task cancellation command returns as soon as the top-level parent child is marked as cancelled. This create race conditions in tests where child tasks on other nodes may continue to run for some time after the main task is cancelled. This commit fixes this situation making task cancellation command to wait until it got propagated to all nodes that have child tasks.\n\nCloses #21126\n', 'authoredDateTime': '2016-11-07 15:01:29'}]",https://github.com/elastic/elasticsearch/issues/21126,15.000277777777777,"[':Distributed/Task Management', ':Search/Search', '>bug']",Search context is not cleaned when scroll search is cancelled,1.0,"['org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.setBanOnNodes(java.lang.String,org.elasticsearch.tasks.CancellableTask,java.util.Set,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanLock)', 'org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.sendSetBanRequest(java.util.Set,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanParentTaskRequest,org.elasticsearch.action.ActionListener).2.handleResponse(org.elasticsearch.transport.TransportResponse$Empty)', 'org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.sendSetBanRequest(java.util.Set,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanParentTaskRequest,org.elasticsearch.action.ActionListener).2.handleException(org.elasticsearch.transport.TransportException)', 'org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.sendSetBanRequest(java.util.Set,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanParentTaskRequest,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanLock).1.handleResponse(org.elasticsearch.transport.TransportResponse$Empty)', 'org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.sendSetBanRequest(java.util.Set,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanParentTaskRequest,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanLock).1', 'org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.taskOperation(org.elasticsearch.action.admin.cluster.node.tasks.cancel.CancelTasksRequest,org.elasticsearch.tasks.CancellableTask,org.elasticsearch.action.ActionListener)', 'org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.taskOperation(org.elasticsearch.action.admin.cluster.node.tasks.cancel.CancelTasksRequest,org.elasticsearch.tasks.CancellableTask,org.elasticsearch.action.ActionListener).1.processResponse()', 'org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.sendSetBanRequest(java.util.Set,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanParentTaskRequest,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanLock)', 'org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.sendSetBanRequest(java.util.Set,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanParentTaskRequest,org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanLock).1.handleException(org.elasticsearch.transport.TransportException)', 'org.elasticsearch.search.SearchCancellationIT.testCancellationOfScrollSearches()', 'org.elasticsearch.search.SearchCancellationIT.testCancellationOfScrollSearchesOnFollowupRequests()']",['f963a85a09bfcca59642c5e20f3f0f7abd775eef'],,['core/src/main/java/org/elasticsearch/action/admin/cluster/node'],44.0,9.0,53.0,1.0,12.0,11.0,30.0,8.0,7.0,3.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,elasticsearch
20913,2016-05-03 19:08:26,jdconrad,"Unable to replicate, but I do not have a Windows machine for testing.

REPRODUCE WITH: gradle :core:integTest -Dtests.seed=4DA46F7D8DF8CF05 -Dtests.class=org.elasticsearch.search.aggregations.bucket.SignificantTermsSignificanceScoreIT -Dtests.method=""testScriptScore"" -Des.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=1024m -Dtests.jvm.argline=""-server -XX:+UseConcMarkSweepGC -XX:-UseCompressedOops -XX:+AggressiveOpts"" -Dtests.locale=sr-Latn-RS -Dtests.timezone=Asia/Chongqing

Build Failure: (http://build-us-00.elastic.co/job/es_core_master_window-2008/3699/testReport/junit/org.elasticsearch.search.aggregations.bucket/SignificantTermsSignificanceScoreIT/testScriptScore/)
",2016-10-27 13:30:45,"[{'commitGitStats': [{'lines': 71, 'insertions': 37, 'deletions': 34, 'filePath': 'core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java'}, {'lines': 4, 'insertions': 2, 'deletions': 2, 'filePath': 'core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregationBuilder.java'}, {'lines': 4, 'insertions': 2, 'deletions': 2, 'filePath': 'core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/InternalSignificantTerms.java'}, {'lines': 20, 'insertions': 16, 'deletions': 4, 'filePath': 'core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristic.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['f3e578f9428ba91dd44aedc86e1fe23510fb7446'], 'commitUser': 'markharwood', 'commitDateTime': '2016-10-27 13:56:48', 'commitHash': '9944a594b10d5cfa34fcf2fa8ed45ca6d52345fe', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 2, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic.initialize(org.elasticsearch.search.aggregations.InternalAggregation$ReduceContext)', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 2, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic.initialize(org.elasticsearch.search.internal.SearchContext)', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'SignificanceHeuristic.java'}, {'spoonMethods': [{'DEL': 2, 'TOT': 6, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.InternalSignificantTerms.doReduce(java.util.List,org.elasticsearch.search.aggregations.bucket.significant.ReduceContext)', 'INS': 2, 'MOV': 2}], 'spoonFilePath': 'InternalSignificantTerms.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 2, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.ExecutableScriptHeuristic', 'INS': 2, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.rewrite(org.elasticsearch.search.internal.SearchContext)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.ExecutableScriptHeuristic.getScore(long,long,long,long)', 'INS': 4, 'MOV': 0}, {'DEL': 1, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.initialize(org.elasticsearch.search.internal.SearchContext)', 'INS': 0, 'MOV': 3}, {'DEL': 2, 'TOT': 19, 'UPD': 4, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic', 'INS': 1, 'MOV': 12}, {'DEL': 1, 'TOT': 18, 'UPD': 4, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.initialize(org.elasticsearch.script.ExecutableScript)', 'INS': 0, 'MOV': 13}, {'DEL': 1, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.initialize(org.elasticsearch.search.aggregations.InternalAggregation$ReduceContext)', 'INS': 0, 'MOV': 3}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.rewrite(org.elasticsearch.search.aggregations.InternalAggregation$ReduceContext)', 'INS': 1, 'MOV': 0}, {'DEL': 1, 'TOT': 13, 'UPD': 5, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.getScore(long,long,long,long)', 'INS': 1, 'MOV': 6}], 'spoonFilePath': 'ScriptHeuristic.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 5, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregationBuilder.innerBuild(org.elasticsearch.search.aggregations.support.AggregationContext,org.elasticsearch.search.aggregations.support.ValuesSourceConfig,org.elasticsearch.search.aggregations.AggregatorFactory,org.elasticsearch.search.aggregations.AggregatorFactories.Builder)', 'INS': 2, 'MOV': 1}], 'spoonFilePath': 'SignificantTermsAggregationBuilder.java'}], 'nameRev': '9944a594b10d5cfa34fcf2fa8ed45ca6d52345fe tags/v6.0.0-alpha1~1830^2~190', 'commitGHEventType': 'closed', 'commitMessage': 'Aggregations fix: scripted heuristics for scoring significant_terms aggs were not thread safe when running local to the coordinating node. New code spawns an object for each shard search execution rather than sharing a common instance which is not thread safe.\nCloses #18120\n', 'authoredDateTime': '2016-10-25 15:00:54'}, {'commitGitStats': [{'lines': 72, 'insertions': 37, 'deletions': 35, 'filePath': 'core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java'}, {'lines': 4, 'insertions': 2, 'deletions': 2, 'filePath': 'core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregationBuilder.java'}, {'lines': 4, 'insertions': 2, 'deletions': 2, 'filePath': 'core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/InternalSignificantTerms.java'}, {'lines': 20, 'insertions': 16, 'deletions': 4, 'filePath': 'core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristic.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['515770b1bb2c6c2ca9247730be19ade8d394e7f0'], 'commitUser': 'markharwood', 'commitDateTime': '2016-10-27 14:44:17', 'commitHash': '5d75fcfcc5a8d8e42c2ad34ccd14538c2d636cd0', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 2, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic.initialize(org.elasticsearch.search.aggregations.InternalAggregation$ReduceContext)', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 2, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic.initialize(org.elasticsearch.search.internal.SearchContext)', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'SignificanceHeuristic.java'}, {'spoonMethods': [{'DEL': 2, 'TOT': 6, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.InternalSignificantTerms.doReduce(java.util.List,org.elasticsearch.search.aggregations.bucket.significant.ReduceContext)', 'INS': 2, 'MOV': 2}], 'spoonFilePath': 'InternalSignificantTerms.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 2, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.ExecutableScriptHeuristic', 'INS': 2, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.rewrite(org.elasticsearch.search.internal.SearchContext)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.ExecutableScriptHeuristic.getScore(long,long,long,long)', 'INS': 4, 'MOV': 0}, {'DEL': 1, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.initialize(org.elasticsearch.search.internal.SearchContext)', 'INS': 0, 'MOV': 3}, {'DEL': 2, 'TOT': 19, 'UPD': 4, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic', 'INS': 1, 'MOV': 12}, {'DEL': 1, 'TOT': 17, 'UPD': 5, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.initialize(org.elasticsearch.script.ExecutableScript)', 'INS': 0, 'MOV': 11}, {'DEL': 1, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.initialize(org.elasticsearch.search.aggregations.InternalAggregation$ReduceContext)', 'INS': 0, 'MOV': 3}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.rewrite(org.elasticsearch.search.aggregations.InternalAggregation$ReduceContext)', 'INS': 1, 'MOV': 0}, {'DEL': 1, 'TOT': 13, 'UPD': 5, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.getScore(long,long,long,long)', 'INS': 1, 'MOV': 6}], 'spoonFilePath': 'ScriptHeuristic.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 5, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregationBuilder.innerBuild(org.elasticsearch.search.aggregations.support.AggregationContext,org.elasticsearch.search.aggregations.support.ValuesSourceConfig,org.elasticsearch.search.aggregations.AggregatorFactory,org.elasticsearch.search.aggregations.AggregatorFactories.Builder)', 'INS': 2, 'MOV': 1}], 'spoonFilePath': 'SignificantTermsAggregationBuilder.java'}], 'nameRev': '5d75fcfcc5a8d8e42c2ad34ccd14538c2d636cd0 tags/v5.2.0~878', 'commitGHEventType': 'referenced', 'commitMessage': 'Aggregations fix: scripted heuristics for scoring significant_terms aggs were not thread safe when running local to the coordinating node. New code spawns an object for each shard search execution rather than sharing a common instance which is not thread safe.\nCloses #18120\n', 'authoredDateTime': '2016-10-25 15:00:54'}, {'commitGitStats': [{'lines': 72, 'insertions': 37, 'deletions': 35, 'filePath': 'core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java'}, {'lines': 4, 'insertions': 2, 'deletions': 2, 'filePath': 'core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregationBuilder.java'}, {'lines': 4, 'insertions': 2, 'deletions': 2, 'filePath': 'core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/InternalSignificantTerms.java'}, {'lines': 20, 'insertions': 16, 'deletions': 4, 'filePath': 'core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristic.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['1987a2bd93b66d17226883fabbd5adde66b31109'], 'commitUser': 'markharwood', 'commitDateTime': '2016-10-27 15:16:56', 'commitHash': '959fb3a9438c630cbdccdafe313ae3e22ca489b4', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 2, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic.initialize(org.elasticsearch.search.aggregations.InternalAggregation$ReduceContext)', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 2, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic.initialize(org.elasticsearch.search.internal.SearchContext)', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'SignificanceHeuristic.java'}, {'spoonMethods': [{'DEL': 2, 'TOT': 6, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.InternalSignificantTerms.doReduce(java.util.List,org.elasticsearch.search.aggregations.bucket.significant.ReduceContext)', 'INS': 2, 'MOV': 2}], 'spoonFilePath': 'InternalSignificantTerms.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 2, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.ExecutableScriptHeuristic', 'INS': 2, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.rewrite(org.elasticsearch.search.internal.SearchContext)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.ExecutableScriptHeuristic.getScore(long,long,long,long)', 'INS': 4, 'MOV': 0}, {'DEL': 1, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.initialize(org.elasticsearch.search.internal.SearchContext)', 'INS': 0, 'MOV': 3}, {'DEL': 2, 'TOT': 19, 'UPD': 4, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic', 'INS': 1, 'MOV': 12}, {'DEL': 1, 'TOT': 17, 'UPD': 5, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.initialize(org.elasticsearch.script.ExecutableScript)', 'INS': 0, 'MOV': 11}, {'DEL': 1, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.initialize(org.elasticsearch.search.aggregations.InternalAggregation$ReduceContext)', 'INS': 0, 'MOV': 3}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.rewrite(org.elasticsearch.search.aggregations.InternalAggregation$ReduceContext)', 'INS': 1, 'MOV': 0}, {'DEL': 1, 'TOT': 13, 'UPD': 5, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.getScore(long,long,long,long)', 'INS': 1, 'MOV': 6}], 'spoonFilePath': 'ScriptHeuristic.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 5, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregationBuilder.innerBuild(org.elasticsearch.search.aggregations.support.AggregationContext,org.elasticsearch.search.aggregations.support.ValuesSourceConfig,org.elasticsearch.search.aggregations.AggregatorFactory,org.elasticsearch.search.aggregations.AggregatorFactories.Builder)', 'INS': 2, 'MOV': 1}], 'spoonFilePath': 'SignificantTermsAggregationBuilder.java'}], 'nameRev': '959fb3a9438c630cbdccdafe313ae3e22ca489b4 tags/v5.0.1~126', 'commitGHEventType': 'referenced', 'commitMessage': 'Aggregations fix: scripted heuristics for scoring significant_terms aggs were not thread safe when running local to the coordinating node. New code spawns an object for each shard search execution rather than sharing a common instance which is not thread safe.\nCloses #18120\n', 'authoredDateTime': '2016-10-25 15:00:54'}]",https://github.com/elastic/elasticsearch/issues/18120,176.00027777777777,"['>bug', '>test', 'v5.1.1']",Build Failure: org.elasticsearch.search.aggregations.bucket.SignificantTermsSignificanceScoreIT.testScriptScore,1.0,"['org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.ExecutableScriptHeuristic', 'org.elasticsearch.search.aggregations.bucket.significant.InternalSignificantTerms.doReduce(java.util.List,org.elasticsearch.search.aggregations.bucket.significant.ReduceContext)', 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.initialize(org.elasticsearch.search.internal.SearchContext)', 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.rewrite(org.elasticsearch.search.internal.SearchContext)', 'org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregationBuilder.innerBuild(org.elasticsearch.search.aggregations.support.AggregationContext,org.elasticsearch.search.aggregations.support.ValuesSourceConfig,org.elasticsearch.search.aggregations.AggregatorFactory,org.elasticsearch.search.aggregations.AggregatorFactories.Builder)', 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic.initialize(org.elasticsearch.search.aggregations.InternalAggregation$ReduceContext)', 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic.initialize(org.elasticsearch.search.internal.SearchContext)', 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.initialize(org.elasticsearch.search.aggregations.InternalAggregation$ReduceContext)', 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic', 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.initialize(org.elasticsearch.script.ExecutableScript)', 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.getScore(long,long,long,long)', 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.rewrite(org.elasticsearch.search.aggregations.InternalAggregation$ReduceContext)', 'org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic.ExecutableScriptHeuristic.getScore(long,long,long,long)']",['9944a594b10d5cfa34fcf2fa8ed45ca6d52345fe'],,['core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant'],57.0,42.0,99.0,4.0,18.0,13.0,81.0,40.0,14.0,9.0,4.0,0.0,0.0,0.0,2.0,0.0,0.0,elasticsearch
21738,2014-09-06 07:01:01,s1monw,"Indexed scripts might need to get fetched via a GET call which is very cheap since those shards are local since they expand `[0-all]` but sometimes in the case of a node client holding no data we need to do a get call on the first get. Yet this get call seems to be executed on the transport thread and might deadlock since it needs that thread to process the get response. See stacktrace below... The problem here is that some of the actions in `SearchServiceTransportAction` don't use the `search` threadpool but use `SAME` instead which can cause this issue. We should use `SEARCH` instead for the most of the operations except of free context I guess.

```
2> ""elasticsearch[node_s2][local_transport][T#1]"" ID=1421 WAITING on org.elasticsearch.common.util.concurrent.BaseFuture$Sync@2b1fdd72
  2>    at sun.misc.Unsafe.park(Native Method)
  2>    - waiting on org.elasticsearch.common.util.concurrent.BaseFuture$Sync@2b1fdd72
  2>    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
  2>    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
  2>    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
  2>    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
  2>    at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:274)
  2>    at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:113)
  2>    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)
  2>    at org.elasticsearch.script.ScriptService.getScriptFromIndex(ScriptService.java:377)
  2>    at org.elasticsearch.script.ScriptService.compile(ScriptService.java:295)
  2>    at org.elasticsearch.script.ScriptService.executable(ScriptService.java:457)
  2>    at org.elasticsearch.search.aggregations.metrics.scripted.InternalScriptedMetric.reduce(InternalScriptedMetric.java:99)
  2>    at org.elasticsearch.search.aggregations.InternalAggregations.reduce(InternalAggregations.java:140)
  2>    at org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:374)
  2>    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.innerFinishHim(TransportSearchDfsQueryThenFetchAction.java:209)
  2>    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.finishHim(TransportSearchDfsQueryThenFetchAction.java:196)
  2>    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$2.onResult(TransportSearchDfsQueryThenFetchAction.java:172)
  2>    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$2.onResult(TransportSearchDfsQueryThenFetchAction.java:166)
  2>    at org.elasticsearch.search.action.SearchServiceTransportAction$18.handleResponse(SearchServiceTransportAction.java:440)
  2>    at org.elasticsearch.search.action.SearchServiceTransportAction$18.handleResponse(SearchServiceTransportAction.java:431)
  2>    at org.elasticsearch.transport.local.LocalTransport$3.run(LocalTransport.java:322)
  2>    at com.google.common.util.concurrent.MoreExecutors$DirectExecutorService.execute(MoreExecutors.java:299)
  2>    at org.elasticsearch.transport.local.LocalTransport.handleParsedResponse(LocalTransport.java:317)
  2>    at org.elasticsearch.test.transport.AssertingLocalTransport.handleParsedResponse(AssertingLocalTransport.java:59)
  2>    at org.elasticsearch.transport.local.LocalTransport.handleResponse(LocalTransport.java:313)
  2>    at org.elasticsearch.transport.local.LocalTransport.messageReceived(LocalTransport.java:238)
  2>    at org.elasticsearch.transport.local.LocalTransportChannel$1.run(LocalTransportChannel.java:78)
  2>    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  2>    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  2>    at java.lang.Thread.run(Thread.java:745)
  2>    Locked synchronizers:
  2>    - java.util.concurrent.ThreadPoolExecutor$Worker@2339bcc9
  2> 
```
",2014-09-08 09:38:36,"[{'commitGitStats': [{'lines': 3, 'insertions': 2, 'deletions': 1, 'filePath': 'src/test/java/org/elasticsearch/action/RejectionActionTests.java'}, {'lines': 45, 'insertions': 25, 'deletions': 20, 'filePath': 'src/main/java/org/elasticsearch/action/search/type/TransportSearchDfsQueryAndFetchAction.java'}, {'lines': 44, 'insertions': 24, 'deletions': 20, 'filePath': 'src/main/java/org/elasticsearch/action/search/type/TransportSearchQueryAndFetchAction.java'}, {'lines': 45, 'insertions': 28, 'deletions': 17, 'filePath': 'src/main/java/org/elasticsearch/action/search/type/TransportSearchDfsQueryThenFetchAction.java'}, {'lines': 47, 'insertions': 29, 'deletions': 18, 'filePath': 'src/main/java/org/elasticsearch/action/search/type/TransportSearchQueryThenFetchAction.java'}, {'lines': 7, 'insertions': 1, 'deletions': 6, 'filePath': 'src/main/java/org/elasticsearch/threadpool/ThreadPool.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['11a9997fccfbb64369f4767c85d283f573ad8367'], 'commitUser': 's1monw', 'commitDateTime': '2014-09-08 11:36:25', 'commitHash': 'cb9cf948dfaf8c91625dca9f436feacdc5c2deb9', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.threadpool.ThreadPool.stats()', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 4, 'UPD': 4, 'spoonMethodName': 'org.elasticsearch.threadpool.ThreadPool', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.threadpool.ThreadPool.updateSettings(org.elasticsearch.common.settings.Settings)', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 6, 'UPD': 6, 'spoonMethodName': 'org.elasticsearch.threadpool.ThreadPool.rebuild(java.lang.String,org.elasticsearch.threadpool.ThreadPool$ExecutorHolder,org.elasticsearch.common.settings.Settings,org.elasticsearch.common.settings.Settings)', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'ThreadPool.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 12, 'UPD': 4, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction.AsyncAction.innerFinishHim()', 'INS': 0, 'MOV': 8}, {'DEL': 1, 'TOT': 11, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction.AsyncAction.moveToSecondPhase()', 'INS': 1, 'MOV': 8}], 'spoonFilePath': 'TransportSearchQueryAndFetchAction.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 2, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.RejectionActionTests.simulateSearchRejectionLoad()', 'INS': 1, 'MOV': 1}], 'spoonFilePath': 'RejectionActionTests.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 8, 'UPD': 3, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction.AsyncAction.finishHim()', 'INS': 1, 'MOV': 3}, {'DEL': 1, 'TOT': 6, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction.AsyncAction.innerFinishHim()', 'INS': 0, 'MOV': 5}, {'DEL': 0, 'TOT': 2, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction.AsyncAction.finishHim().3.run()', 'INS': 2, 'MOV': 0}], 'spoonFilePath': 'TransportSearchDfsQueryThenFetchAction.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 7, 'UPD': 3, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchDfsQueryAndFetchAction.AsyncAction.finishHim()', 'INS': 1, 'MOV': 2}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchDfsQueryAndFetchAction.AsyncAction.finishHim().2.run()', 'INS': 1, 'MOV': 0}, {'DEL': 1, 'TOT': 8, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchDfsQueryAndFetchAction.AsyncAction.innerFinishHim()', 'INS': 0, 'MOV': 7}], 'spoonFilePath': 'TransportSearchDfsQueryAndFetchAction.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 6, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.AsyncAction.innerFinishHim()', 'INS': 0, 'MOV': 5}, {'DEL': 1, 'TOT': 8, 'UPD': 3, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.AsyncAction.finishHim()', 'INS': 1, 'MOV': 3}, {'DEL': 0, 'TOT': 3, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.AsyncAction.finishHim().2.run()', 'INS': 3, 'MOV': 0}], 'spoonFilePath': 'TransportSearchQueryThenFetchAction.java'}], 'nameRev': 'cb9cf948dfaf8c91625dca9f436feacdc5c2deb9 tags/v1.4.0.Beta1~220', 'commitGHEventType': 'referenced', 'commitMessage': ""[SEARCH] Execute search reduce phase on the search threadpool\n\nReduce Phases can be expensive and some of them like the aggregations\nreduce phase might even execute a one-off call via an internal client\nthat might cause a deadlock due to execution on the network thread\nthat is needed to handle the one-off call. This commit dispatches\nthe reduce phase to the search threadpool to ensure we don't wait\nfor the current thread to be available.\n\nCloses #7623\n"", 'authoredDateTime': '2014-09-08 11:32:55'}, {'commitGitStats': [{'lines': 3, 'insertions': 2, 'deletions': 1, 'filePath': 'src/test/java/org/elasticsearch/action/RejectionActionTests.java'}, {'lines': 45, 'insertions': 25, 'deletions': 20, 'filePath': 'src/main/java/org/elasticsearch/action/search/type/TransportSearchDfsQueryAndFetchAction.java'}, {'lines': 44, 'insertions': 24, 'deletions': 20, 'filePath': 'src/main/java/org/elasticsearch/action/search/type/TransportSearchQueryAndFetchAction.java'}, {'lines': 45, 'insertions': 28, 'deletions': 17, 'filePath': 'src/main/java/org/elasticsearch/action/search/type/TransportSearchDfsQueryThenFetchAction.java'}, {'lines': 47, 'insertions': 29, 'deletions': 18, 'filePath': 'src/main/java/org/elasticsearch/action/search/type/TransportSearchQueryThenFetchAction.java'}, {'lines': 7, 'insertions': 1, 'deletions': 6, 'filePath': 'src/main/java/org/elasticsearch/threadpool/ThreadPool.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['1400443c492ce6bb1f87f64b88022f6c9c8a494c'], 'commitUser': 's1monw', 'commitDateTime': '2014-09-08 11:36:39', 'commitHash': '958d1cdcfd58f8c5efbf2c06541661b2445af309', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.threadpool.ThreadPool.stats()', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 4, 'UPD': 4, 'spoonMethodName': 'org.elasticsearch.threadpool.ThreadPool', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.threadpool.ThreadPool.updateSettings(org.elasticsearch.common.settings.Settings)', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 6, 'UPD': 6, 'spoonMethodName': 'org.elasticsearch.threadpool.ThreadPool.rebuild(java.lang.String,org.elasticsearch.threadpool.ThreadPool$ExecutorHolder,org.elasticsearch.common.settings.Settings,org.elasticsearch.common.settings.Settings)', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'ThreadPool.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 12, 'UPD': 4, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction.AsyncAction.innerFinishHim()', 'INS': 0, 'MOV': 8}, {'DEL': 1, 'TOT': 11, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction.AsyncAction.moveToSecondPhase()', 'INS': 1, 'MOV': 8}], 'spoonFilePath': 'TransportSearchQueryAndFetchAction.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 2, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.RejectionActionTests.simulateSearchRejectionLoad()', 'INS': 1, 'MOV': 1}], 'spoonFilePath': 'RejectionActionTests.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 8, 'UPD': 3, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction.AsyncAction.finishHim()', 'INS': 1, 'MOV': 3}, {'DEL': 1, 'TOT': 6, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction.AsyncAction.innerFinishHim()', 'INS': 0, 'MOV': 5}, {'DEL': 0, 'TOT': 2, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction.AsyncAction.finishHim().3.run()', 'INS': 2, 'MOV': 0}], 'spoonFilePath': 'TransportSearchDfsQueryThenFetchAction.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 7, 'UPD': 3, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchDfsQueryAndFetchAction.AsyncAction.finishHim()', 'INS': 1, 'MOV': 2}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchDfsQueryAndFetchAction.AsyncAction.finishHim().2.run()', 'INS': 1, 'MOV': 0}, {'DEL': 1, 'TOT': 8, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchDfsQueryAndFetchAction.AsyncAction.innerFinishHim()', 'INS': 0, 'MOV': 7}], 'spoonFilePath': 'TransportSearchDfsQueryAndFetchAction.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 6, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.AsyncAction.innerFinishHim()', 'INS': 0, 'MOV': 5}, {'DEL': 1, 'TOT': 8, 'UPD': 3, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.AsyncAction.finishHim()', 'INS': 1, 'MOV': 3}, {'DEL': 0, 'TOT': 3, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.AsyncAction.finishHim().2.run()', 'INS': 3, 'MOV': 0}], 'spoonFilePath': 'TransportSearchQueryThenFetchAction.java'}], 'nameRev': '958d1cdcfd58f8c5efbf2c06541661b2445af309 tags/v1.3.3~57', 'commitGHEventType': 'referenced', 'commitMessage': ""[SEARCH] Execute search reduce phase on the search threadpool\n\nReduce Phases can be expensive and some of them like the aggregations\nreduce phase might even execute a one-off call via an internal client\nthat might cause a deadlock due to execution on the network thread\nthat is needed to handle the one-off call. This commit dispatches\nthe reduce phase to the search threadpool to ensure we don't wait\nfor the current thread to be available.\n\nCloses #7623\n"", 'authoredDateTime': '2014-09-08 11:32:55'}]",https://github.com/elastic/elasticsearch/issues/7623,2.000277777777778,"['>bug', 'v1.3.3', 'v1.4.0.Beta1', 'v2.0.0-beta1']",Indexed Scripts/Templates: Indexed Scripts used during reduce phase sometimes hang,1.0,"['org.elasticsearch.threadpool.ThreadPool.stats()', 'org.elasticsearch.action.search.type.TransportSearchDfsQueryAndFetchAction.AsyncAction.finishHim()', 'org.elasticsearch.threadpool.ThreadPool.updateSettings(org.elasticsearch.common.settings.Settings)', 'org.elasticsearch.threadpool.ThreadPool.rebuild(java.lang.String,org.elasticsearch.threadpool.ThreadPool$ExecutorHolder,org.elasticsearch.common.settings.Settings,org.elasticsearch.common.settings.Settings)', 'org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.AsyncAction.finishHim().2.run()', 'org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction.AsyncAction.finishHim()', 'org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction.AsyncAction.innerFinishHim()', 'org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction.AsyncAction.finishHim().3.run()', 'org.elasticsearch.action.search.type.TransportSearchDfsQueryAndFetchAction.AsyncAction.innerFinishHim()', 'org.elasticsearch.action.RejectionActionTests.simulateSearchRejectionLoad()', 'org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction.AsyncAction.innerFinishHim()', 'org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.AsyncAction.innerFinishHim()', 'org.elasticsearch.threadpool.ThreadPool', 'org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.AsyncAction.finishHim()', 'org.elasticsearch.action.search.type.TransportSearchDfsQueryAndFetchAction.AsyncAction.finishHim().2.run()', 'org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction.AsyncAction.moveToSecondPhase()']",['cb9cf948dfaf8c91625dca9f436feacdc5c2deb9'],,"['src/main/java/org/elasticsearch/action/search/type', 'src/main/java/org/elasticsearch/threadpool']",107.0,81.0,188.0,5.0,26.0,16.0,86.0,42.0,11.0,7.0,6.0,0.0,0.0,0.0,1.0,0.0,0.0,elasticsearch
21921,2014-04-10 15:26:26,s1monw,"the node level TTL Purger thread fires up bulk delete request that might trigger `auto_create_index` if the purger thread runs a bulk after the index has been deleted.
",2014-04-15 10:56:05,"[{'commitGitStats': [{'lines': 19, 'insertions': 16, 'deletions': 3, 'filePath': 'src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java'}, {'lines': 55, 'insertions': 53, 'deletions': 2, 'filePath': 'src/test/java/org/elasticsearch/percolator/TTLPercolatorTests.java'}, {'lines': 2, 'insertions': 1, 'deletions': 1, 'filePath': 'pom.xml'}, {'lines': 149, 'insertions': 108, 'deletions': 41, 'filePath': 'src/main/java/org/elasticsearch/indices/ttl/IndicesTTLService.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['296b87ee14f800fd39693c26d0ee3eaaff90ffaa'], 'commitUser': 's1monw', 'commitDateTime': '2014-04-15 12:43:39', 'commitHash': '5048838f1bbf4103fa913c3c7809fb960db693b7', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 5, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.doStop()', 'INS': 1, 'MOV': 2}, {'DEL': 4, 'TOT': 11, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.ApplySettings.onRefreshSettings(org.elasticsearch.common.settings.Settings)', 'INS': 6, 'MOV': 1}, {'DEL': 1, 'TOT': 9, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.PurgerThread', 'INS': 6, 'MOV': 0}, {'DEL': 2, 'TOT': 16, 'UPD': 7, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService', 'INS': 3, 'MOV': 4}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.processBulkIfNeeded(org.elasticsearch.indices.ttl.BulkRequest,boolean)', 'INS': 1, 'MOV': 0}, {'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.doStart()', 'INS': 0, 'MOV': 0}, {'DEL': 3, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.ExpiredDocsCollector', 'INS': 0, 'MOV': 1}, {'DEL': 1, 'TOT': 4, 'UPD': 3, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.processBulkIfNeeded(org.elasticsearch.action.bulk.BulkRequestBuilder,boolean)', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.processBulkIfNeeded(org.elasticsearch.action.bulk.BulkRequestBuilder,boolean).1', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.PurgerThread.resetInterval(org.elasticsearch.common.unit.TimeValue)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.processBulkIfNeeded(org.elasticsearch.action.bulk.BulkRequestBuilder,boolean).1.onResponse(org.elasticsearch.action.bulk.BulkResponse)', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.PurgerThread.getInterval()', 'INS': 1, 'MOV': 0}, {'DEL': 1, 'TOT': 10, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.PurgerThread.run()', 'INS': 2, 'MOV': 5}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.Notifier', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 2, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.PurgerThread.doStop()', 'INS': 0, 'MOV': 1}, {'DEL': 1, 'TOT': 4, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.purgeShards(java.util.List)', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'IndicesTTLService.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.bulk.TransportBulkAction.executeBulk(org.elasticsearch.action.bulk.BulkRequest,org.elasticsearch.action.ActionListener)', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'TransportBulkAction.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 3, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.percolator.TTLPercolatorTests.nodeSettings(int)', 'INS': 0, 'MOV': 1}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.percolator.TTLPercolatorTests.testEnsureTTLDoesNotCreateIndex()', 'INS': 1, 'MOV': 0}, {'DEL': 1, 'TOT': 4, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.percolator.TTLPercolatorTests.testPercolatingWithTimeToLive()', 'INS': 0, 'MOV': 1}], 'spoonFilePath': 'TTLPercolatorTests.java'}], 'nameRev': '5048838f1bbf4103fa913c3c7809fb960db693b7 tags/v1.2.0~366', 'commitGHEventType': 'referenced', 'commitMessage': 'Use TransportBulkAction for internal request from IndicesTTLService\n\nThis prevents executing bulks internal autocreate indices logic\nand ensures that this internal request never creates an index\nautomaticall.\n\nThis fixes a bug where the TTL purger thread ran after the actual\nindex it was purging was already closed / deleted and that re-created\nthat index.\n\nCloses #5766\n', 'authoredDateTime': '2014-04-10 17:27:05'}, {'commitGitStats': [{'lines': 1, 'insertions': 1, 'deletions': 0, 'filePath': 'src/test/java/org/elasticsearch/percolator/TTLPercolatorTests.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['e4b86e0fb011a0ab1a04ef6920fc494b728bdf15'], 'commitUser': 's1monw', 'commitDateTime': '2014-04-11 09:05:23', 'commitHash': '967301d1782f56eac9e8e48a94baa8cabf2cd5a6', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 3, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.percolator.TTLPercolatorTests.nodeSettings(int)', 'INS': 1, 'MOV': 1}], 'spoonFilePath': 'TTLPercolatorTests.java'}], 'nameRev': '967301d1782f56eac9e8e48a94baa8cabf2cd5a6 tags/v1.2.0~396', 'commitGHEventType': 'referenced', 'commitMessage': '[TEST] Prevent TTLPurger from recreating deleted index\n\nRelated to #5766\n', 'authoredDateTime': '2014-04-11 09:03:28'}, {'commitGitStats': [{'lines': 1, 'insertions': 1, 'deletions': 0, 'filePath': 'src/test/java/org/elasticsearch/percolator/TTLPercolatorTests.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['01794bf8ea9da1afc3b2deb01abe1754301f194f'], 'commitUser': 's1monw', 'commitDateTime': '2014-04-11 09:03:28', 'commitHash': '862611b792b837e9fc217787d58059960bf8d762', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 3, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.percolator.TTLPercolatorTests.nodeSettings(int)', 'INS': 1, 'MOV': 1}], 'spoonFilePath': 'TTLPercolatorTests.java'}], 'nameRev': '862611b792b837e9fc217787d58059960bf8d762 tags/v2.0.0-beta1~4886', 'commitGHEventType': 'referenced', 'commitMessage': '[TEST] Prevent TTLPurger from recreating deleted index\n\nRelated to #5766\n', 'authoredDateTime': '2014-04-11 09:03:28'}, {'commitGitStats': [{'lines': 16, 'insertions': 15, 'deletions': 1, 'filePath': 'src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java'}, {'lines': 58, 'insertions': 56, 'deletions': 2, 'filePath': 'src/test/java/org/elasticsearch/percolator/TTLPercolatorTests.java'}, {'lines': 2, 'insertions': 1, 'deletions': 1, 'filePath': 'pom.xml'}, {'lines': 149, 'insertions': 108, 'deletions': 41, 'filePath': 'src/main/java/org/elasticsearch/indices/ttl/IndicesTTLService.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['aee7c57838c2f4365081396a8584dea2632afdc1'], 'commitUser': 's1monw', 'commitDateTime': '2014-04-15 12:52:54', 'commitHash': '56e4b8bd974291ec882342df682840042b31ddbd', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 5, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.doStop()', 'INS': 1, 'MOV': 2}, {'DEL': 4, 'TOT': 11, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.ApplySettings.onRefreshSettings(org.elasticsearch.common.settings.Settings)', 'INS': 6, 'MOV': 1}, {'DEL': 1, 'TOT': 9, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.PurgerThread', 'INS': 6, 'MOV': 0}, {'DEL': 2, 'TOT': 15, 'UPD': 6, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService', 'INS': 3, 'MOV': 4}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.processBulkIfNeeded(org.elasticsearch.indices.ttl.BulkRequest,boolean)', 'INS': 1, 'MOV': 0}, {'DEL': 1, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.doStart()', 'INS': 0, 'MOV': 0}, {'DEL': 3, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.ExpiredDocsCollector', 'INS': 0, 'MOV': 1}, {'DEL': 1, 'TOT': 4, 'UPD': 3, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.processBulkIfNeeded(org.elasticsearch.action.bulk.BulkRequestBuilder,boolean)', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.processBulkIfNeeded(org.elasticsearch.action.bulk.BulkRequestBuilder,boolean).1', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.PurgerThread.resetInterval(org.elasticsearch.common.unit.TimeValue)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.processBulkIfNeeded(org.elasticsearch.action.bulk.BulkRequestBuilder,boolean).1.onResponse(org.elasticsearch.action.bulk.BulkResponse)', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.PurgerThread.getInterval()', 'INS': 1, 'MOV': 0}, {'DEL': 1, 'TOT': 10, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.PurgerThread.run()', 'INS': 2, 'MOV': 5}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.Notifier', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 2, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.PurgerThread.doStop()', 'INS': 0, 'MOV': 1}, {'DEL': 1, 'TOT': 4, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.purgeShards(java.util.List)', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'IndicesTTLService.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.bulk.TransportBulkAction.executeBulk(org.elasticsearch.action.bulk.BulkRequest,org.elasticsearch.action.ActionListener)', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'TransportBulkAction.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.percolator.TTLPercolatorTests.testEnsureTTLDoesNotCreateIndex()', 'INS': 1, 'MOV': 0}, {'DEL': 1, 'TOT': 5, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.percolator.TTLPercolatorTests.testPercolatingWithTimeToLive()', 'INS': 1, 'MOV': 1}], 'spoonFilePath': 'TTLPercolatorTests.java'}], 'nameRev': '56e4b8bd974291ec882342df682840042b31ddbd tags/v1.0.3~9', 'commitGHEventType': 'referenced', 'commitMessage': 'Use TransportBulkAction for internal request from IndicesTTLService\n\nThis prevents executing bulks internal autocreate indices logic\nand ensures that this internal request never creates an index\nautomaticall.\n\nThis fixes a bug where the TTL purger thread ran after the actual\nindex it was purging was already closed / deleted and that re-created\nthat index.\n\nCloses #5766\n', 'authoredDateTime': '2014-04-10 17:27:05'}, {'commitGitStats': [{'lines': 19, 'insertions': 16, 'deletions': 3, 'filePath': 'src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java'}, {'lines': 54, 'insertions': 53, 'deletions': 1, 'filePath': 'src/test/java/org/elasticsearch/percolator/TTLPercolatorTests.java'}, {'lines': 2, 'insertions': 1, 'deletions': 1, 'filePath': 'pom.xml'}, {'lines': 149, 'insertions': 108, 'deletions': 41, 'filePath': 'src/main/java/org/elasticsearch/indices/ttl/IndicesTTLService.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['e5be873f2360287d9128ff6d9ad9cba7534aeef8'], 'commitUser': 's1monw', 'commitDateTime': '2014-04-15 12:43:50', 'commitHash': '3c147e284227d3bf1f199301dfd2fa745bf19155', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 5, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.doStop()', 'INS': 1, 'MOV': 2}, {'DEL': 4, 'TOT': 11, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.ApplySettings.onRefreshSettings(org.elasticsearch.common.settings.Settings)', 'INS': 6, 'MOV': 1}, {'DEL': 1, 'TOT': 9, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.PurgerThread', 'INS': 6, 'MOV': 0}, {'DEL': 2, 'TOT': 16, 'UPD': 7, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService', 'INS': 3, 'MOV': 4}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.processBulkIfNeeded(org.elasticsearch.indices.ttl.BulkRequest,boolean)', 'INS': 1, 'MOV': 0}, {'DEL': 1, 'TOT': 2, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.doStart()', 'INS': 0, 'MOV': 1}, {'DEL': 3, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.ExpiredDocsCollector', 'INS': 0, 'MOV': 1}, {'DEL': 1, 'TOT': 4, 'UPD': 3, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.processBulkIfNeeded(org.elasticsearch.action.bulk.BulkRequestBuilder,boolean)', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.processBulkIfNeeded(org.elasticsearch.action.bulk.BulkRequestBuilder,boolean).1', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.PurgerThread.resetInterval(org.elasticsearch.common.unit.TimeValue)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.processBulkIfNeeded(org.elasticsearch.action.bulk.BulkRequestBuilder,boolean).1.onResponse(org.elasticsearch.action.bulk.BulkResponse)', 'INS': 0, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.PurgerThread.getInterval()', 'INS': 1, 'MOV': 0}, {'DEL': 1, 'TOT': 10, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.PurgerThread.run()', 'INS': 2, 'MOV': 5}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.Notifier', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 2, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.PurgerThread.doStop()', 'INS': 0, 'MOV': 1}, {'DEL': 1, 'TOT': 4, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.indices.ttl.IndicesTTLService.purgeShards(java.util.List)', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'IndicesTTLService.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.bulk.TransportBulkAction.executeBulk(org.elasticsearch.action.bulk.BulkRequest,org.elasticsearch.action.ActionListener)', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'TransportBulkAction.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.percolator.TTLPercolatorTests.testEnsureTTLDoesNotCreateIndex()', 'INS': 1, 'MOV': 0}, {'DEL': 1, 'TOT': 4, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.percolator.TTLPercolatorTests.testPercolatingWithTimeToLive()', 'INS': 0, 'MOV': 1}], 'spoonFilePath': 'TTLPercolatorTests.java'}], 'nameRev': '3c147e284227d3bf1f199301dfd2fa745bf19155 tags/v1.1.1~9', 'commitGHEventType': 'referenced', 'commitMessage': 'Use TransportBulkAction for internal request from IndicesTTLService\n\nThis prevents executing bulks internal autocreate indices logic\nand ensures that this internal request never creates an index\nautomaticall.\n\nThis fixes a bug where the TTL purger thread ran after the actual\nindex it was purging was already closed / deleted and that re-created\nthat index.\n\nCloses #5766\n', 'authoredDateTime': '2014-04-10 17:27:05'}]",https://github.com/elastic/elasticsearch/issues/5766,4.000277777777778,"['>bug', 'v1.0.3', 'v1.1.1', 'v1.2.0', 'v2.0.0-beta1']",TTL Purge Thread might bring back already deleted index,1.0,"['org.elasticsearch.indices.ttl.IndicesTTLService.processBulkIfNeeded(org.elasticsearch.action.bulk.BulkRequestBuilder,boolean).1.onResponse(org.elasticsearch.action.bulk.BulkResponse)', 'org.elasticsearch.indices.ttl.IndicesTTLService.ApplySettings.onRefreshSettings(org.elasticsearch.common.settings.Settings)', 'org.elasticsearch.percolator.TTLPercolatorTests.testEnsureTTLDoesNotCreateIndex()', 'org.elasticsearch.indices.ttl.IndicesTTLService', 'org.elasticsearch.indices.ttl.IndicesTTLService.processBulkIfNeeded(org.elasticsearch.indices.ttl.BulkRequest,boolean)', 'org.elasticsearch.indices.ttl.IndicesTTLService.doStart()', 'org.elasticsearch.percolator.TTLPercolatorTests.nodeSettings(int)', 'org.elasticsearch.indices.ttl.IndicesTTLService.ExpiredDocsCollector', 'org.elasticsearch.percolator.TTLPercolatorTests.testPercolatingWithTimeToLive()', 'org.elasticsearch.indices.ttl.IndicesTTLService.processBulkIfNeeded(org.elasticsearch.action.bulk.BulkRequestBuilder,boolean)', 'org.elasticsearch.indices.ttl.IndicesTTLService.processBulkIfNeeded(org.elasticsearch.action.bulk.BulkRequestBuilder,boolean).1', 'org.elasticsearch.indices.ttl.IndicesTTLService.PurgerThread.resetInterval(org.elasticsearch.common.unit.TimeValue)', 'org.elasticsearch.indices.ttl.IndicesTTLService.doStop()', 'org.elasticsearch.indices.ttl.IndicesTTLService.PurgerThread', 'org.elasticsearch.indices.ttl.IndicesTTLService.PurgerThread.getInterval()', 'org.elasticsearch.action.bulk.TransportBulkAction.executeBulk(org.elasticsearch.action.bulk.BulkRequest,org.elasticsearch.action.ActionListener)', 'org.elasticsearch.indices.ttl.IndicesTTLService.PurgerThread.run()', 'org.elasticsearch.indices.ttl.IndicesTTLService.Notifier', 'org.elasticsearch.indices.ttl.IndicesTTLService.PurgerThread.doStop()', 'org.elasticsearch.indices.ttl.IndicesTTLService.purgeShards(java.util.List)']","['967301d1782f56eac9e8e48a94baa8cabf2cd5a6', '5048838f1bbf4103fa913c3c7809fb960db693b7']",,"['src/main/java/org/elasticsearch/indices/ttl', 'src/main/java/org/elasticsearch/action/bulk']",124.0,44.0,168.0,2.0,25.0,20.0,84.0,17.0,26.0,16.0,3.0,0.0,0.0,0.0,3.0,0.0,0.0,elasticsearch
22094,2013-12-04 03:16:55,aganapat,"ES Version: 0.90.7, Java version: 1.7 update 45 64 bit Server VM.

I have a 7 node cluster with 5 master nodes and 2 client nodes. 
When I was shutting down all nodes to do a full cluster restart, one node did not die and looks there is a deadlock.

Stack Trace:

2013-12-03 22:07:50
Full thread dump Java HotSpot(TM) 64-Bit Server VM (24.45-b08 mixed mode):

""Attach Listener"" daemon prio=10 tid=0x00007f8ed4028000 nid=0x5d32 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Thread-1"" prio=10 tid=0x00007f8e88698000 nid=0x5c6e waiting on condition [0x00007f8e7e861000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1461)
    at org.elasticsearch.threadpool.ThreadPool.awaitTermination(ThreadPool.java:249)
    at org.elasticsearch.node.internal.InternalNode.close(InternalNode.java:342)
    at org.elasticsearch.bootstrap.Bootstrap$1.run(Bootstrap.java:73)

""SIGTERM handler"" daemon prio=10 tid=0x00007f8ed4042000 nid=0x5c6b in Object.wait() [0x00007f8ee7915000]
   java.lang.Thread.State: WAITING (on object monitor)
    at java.lang.Object.wait(Native Method)
    - waiting on <0x00000005fd3d76c8> (a org.elasticsearch.bootstrap.Bootstrap$1)
    at java.lang.Thread.join(Thread.java:1280)
    - locked <0x00000005fd3d76c8> (a org.elasticsearch.bootstrap.Bootstrap$1)
    at java.lang.Thread.join(Thread.java:1354)
    at java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:106)
    at java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:46)
    at java.lang.Shutdown.runHooks(Shutdown.java:123)
    at java.lang.Shutdown.sequence(Shutdown.java:167)
    at java.lang.Shutdown.exit(Shutdown.java:212)
    - locked <0x00000005fd340058> (a java.lang.Class for java.lang.Shutdown)
    at java.lang.Terminator$1.handle(Terminator.java:52)
    at sun.misc.Signal$1.run(Signal.java:212)
    at java.lang.Thread.run(Thread.java:744)

""elasticsearch[AG 8][search][T#7]"" daemon prio=10 tid=0x00007f8e84112800 nid=0x799a waiting on condition [0x00007f8ee7c62000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

""elasticsearch[AG 8][search][T#12]"" daemon prio=10 tid=0x00007f8e8c11f800 nid=0x7999 waiting on condition [0x00007f8ee7ca3000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

""elasticsearch[AG 8][search][T#3]"" daemon prio=10 tid=0x00007f8e8011e000 nid=0x7997 waiting on condition [0x00007f8ee7d25000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

""elasticsearch[AG 8][search][T#2]"" daemon prio=10 tid=0x00007f8e8c11d800 nid=0x7996 waiting on condition [0x00007f8ee7d66000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

""elasticsearch[AG 8][search][T#4]"" daemon prio=10 tid=0x00007f8e84111000 nid=0x7995 waiting on condition [0x00007f8ee7da7000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

""elasticsearch[AG 8][search][T#8]"" daemon prio=10 tid=0x0000000001fa0800 nid=0x7991 waiting for monitor entry [0x00007f8ee7f7c000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.terminated(EsThreadPoolExecutor.java:64)
    - waiting to lock <0x00000005fdbaaf50> (a java.lang.Object)
    - locked <0x00000005fae03ef0> (a org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor)
    at java.util.concurrent.ThreadPoolExecutor.tryTerminate(ThreadPoolExecutor.java:704)
    at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1006)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

""elasticsearch[AG 8][search][T#6]"" daemon prio=10 tid=0x0000000001f9f000 nid=0x7990 waiting on condition [0x00007f8ee7fbd000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

""elasticsearch[AG 8][search][T#1]"" daemon prio=10 tid=0x00007f8e8410f000 nid=0x798f waiting on condition [0x00007f8ee7ffe000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

""DestroyJavaVM"" prio=10 tid=0x00007f8f1000a800 nid=0x775c waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""elasticsearch[AG 8][clusterService#updateTask][T#1]"" daemon prio=10 tid=0x00007f8e84107800 nid=0x7798 waiting on condition [0x00007f8eee056000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.interruptIdleWorkers(ThreadPoolExecutor.java:781)
    at java.util.concurrent.ThreadPoolExecutor.tryTerminate(ThreadPoolExecutor.java:695)
    at java.util.concurrent.ThreadPoolExecutor.shutdown(ThreadPoolExecutor.java:1397)
    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.shutdown(EsThreadPoolExecutor.java:56)
    - locked <0x00000005fdbaaf50> (a java.lang.Object)
    at org.elasticsearch.threadpool.ThreadPool.updateSettings(ThreadPool.java:395)
    at org.elasticsearch.threadpool.ThreadPool$ApplySettings.onRefreshSettings(ThreadPool.java:656)
    at org.elasticsearch.node.settings.NodeSettingsService.clusterChanged(NodeSettingsService.java:84)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:417)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:135)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

""Service Thread"" daemon prio=10 tid=0x00007f8f10113800 nid=0x7769 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread1"" daemon prio=10 tid=0x00007f8f10111000 nid=0x7768 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread0"" daemon prio=10 tid=0x00007f8f1010e800 nid=0x7767 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Signal Dispatcher"" daemon prio=10 tid=0x00007f8f1010c800 nid=0x7766 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Surrogate Locker Thread (Concurrent GC)"" daemon prio=10 tid=0x00007f8f10102000 nid=0x7765 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Finalizer"" daemon prio=10 tid=0x00007f8f100eb800 nid=0x7764 in Object.wait() [0x00007f8f0c1bd000]
   java.lang.Thread.State: WAITING (on object monitor)
    at java.lang.Object.wait(Native Method)
    - waiting on <0x00000005fce11a08> (a java.lang.ref.ReferenceQueue$Lock)
    at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
    - locked <0x00000005fce11a08> (a java.lang.ref.ReferenceQueue$Lock)
    at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)
    at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:189)

""Reference Handler"" daemon prio=10 tid=0x00007f8f100e7800 nid=0x7763 in Object.wait() [0x00007f8f0c1fe000]
   java.lang.Thread.State: WAITING (on object monitor)
    at java.lang.Object.wait(Native Method)
    - waiting on <0x00000005fce13cb0> (a java.lang.ref.Reference$Lock)
    at java.lang.Object.wait(Object.java:503)
    at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:133)
    - locked <0x00000005fce13cb0> (a java.lang.ref.Reference$Lock)

""VM Thread"" prio=10 tid=0x00007f8f100e5000 nid=0x7762 runnable 

""Gang worker#0 (Parallel GC Threads)"" prio=10 tid=0x00007f8f1001c000 nid=0x775d runnable 

""Gang worker#1 (Parallel GC Threads)"" prio=10 tid=0x00007f8f1001e000 nid=0x775e runnable 

""Gang worker#2 (Parallel GC Threads)"" prio=10 tid=0x00007f8f1001f800 nid=0x775f runnable 

""Gang worker#3 (Parallel GC Threads)"" prio=10 tid=0x00007f8f10021800 nid=0x7760 runnable 

""Concurrent Mark-Sweep GC Thread"" prio=10 tid=0x00007f8f100a2000 nid=0x7761 runnable 
""VM Periodic Task Thread"" prio=10 tid=0x00007f8f1011e800 nid=0x776a waiting on condition 

JNI global references: 284
# Found one Java-level deadlock:

""Thread-1"":
  waiting for ownable synchronizer 0x00000005fdba9278, (a java.util.concurrent.locks.ReentrantLock$NonfairSync),
  which is held by ""elasticsearch[AG 8][search][T#8]""
""elasticsearch[AG 8][search][T#8]"":
  waiting to lock monitor 0x00007f8e980a33a8 (object 0x00000005fdbaaf50, a java.lang.Object),
  which is held by ""elasticsearch[AG 8][clusterService#updateTask][T#1]""
""elasticsearch[AG 8][clusterService#updateTask][T#1]"":
  waiting for ownable synchronizer 0x00000005fdba9278, (a java.util.concurrent.locks.ReentrantLock$NonfairSync),
  which is held by ""elasticsearch[AG 8][search][T#8]""
# Java stack information for the threads listed above:

""Thread-1"":
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1461)
    at org.elasticsearch.threadpool.ThreadPool.awaitTermination(ThreadPool.java:249)
    at org.elasticsearch.node.internal.InternalNode.close(InternalNode.java:342)
    at org.elasticsearch.bootstrap.Bootstrap$1.run(Bootstrap.java:73)
""elasticsearch[AG 8][search][T#8]"":
    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.terminated(EsThreadPoolExecutor.java:64)
    - waiting to lock <0x00000005fdbaaf50> (a java.lang.Object)
    - locked <0x00000005fae03ef0> (a org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor)
    at java.util.concurrent.ThreadPoolExecutor.tryTerminate(ThreadPoolExecutor.java:704)
    at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1006)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
""elasticsearch[AG 8][clusterService#updateTask][T#1]"":
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.interruptIdleWorkers(ThreadPoolExecutor.java:781)
    at java.util.concurrent.ThreadPoolExecutor.tryTerminate(ThreadPoolExecutor.java:695)
    at java.util.concurrent.ThreadPoolExecutor.shutdown(ThreadPoolExecutor.java:1397)
    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.shutdown(EsThreadPoolExecutor.java:56)
    - locked <0x00000005fdbaaf50> (a java.lang.Object)
    at org.elasticsearch.threadpool.ThreadPool.updateSettings(ThreadPool.java:395)
    at org.elasticsearch.threadpool.ThreadPool$ApplySettings.onRefreshSettings(ThreadPool.java:656)
    at org.elasticsearch.node.settings.NodeSettingsService.clusterChanged(NodeSettingsService.java:84)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:417)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:135)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

Found 1 deadlock.
",2013-12-15 03:13:44,"[{'commitGitStats': [{'lines': 2, 'insertions': 1, 'deletions': 1, 'filePath': 'src/main/java/org/elasticsearch/common/util/concurrent/EsThreadPoolExecutor.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['8f85d63b679b4996a2eeadba0af65caeb97e047d'], 'commitUser': 'imotov', 'commitDateTime': '2013-12-14 21:45:03', 'commitHash': 'd8ba92cfa80abd171c1c296b49e4d53ceb028681', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.shutdown(org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor$ShutdownListener)', 'INS': 0, 'MOV': 1}], 'spoonFilePath': 'EsThreadPoolExecutor.java'}], 'nameRev': 'd8ba92cfa80abd171c1c296b49e4d53ceb028681 tags/v2.0.0-beta1~6110', 'commitGHEventType': 'closed', 'commitMessage': 'Resolve potential deadlock state during EsThreadPoolExecutor shutdown\n\nFixes #4334\n\nThe deadlock occurs between monitor object of EsThreadPoolExecutor and mainLock of ThreadPoolExecutor. The shutdown method of EsThreadPoolExecutor obtains the lock on monitor first and waits for mainLock of ThreadPoolExecutor in ThreadPoolExecutor#shutdown for part of the processing, while EsThreadPoolExecutor#terminated is executed under mainLock and tries to obtain monitor to notify listeners.\n', 'authoredDateTime': '2013-12-13 18:56:32'}, {'commitGitStats': [{'lines': 2, 'insertions': 1, 'deletions': 1, 'filePath': 'src/main/java/org/elasticsearch/common/util/concurrent/EsThreadPoolExecutor.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['6a15eb380c6169397e8ff4ee6a998f4bda6754b6'], 'commitUser': 'imotov', 'commitDateTime': '2013-12-14 22:04:25', 'commitHash': 'f67fc930a58673efa4e973839d38c5effaba38df', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.shutdown(org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor$ShutdownListener)', 'INS': 0, 'MOV': 1}], 'spoonFilePath': 'EsThreadPoolExecutor.java'}], 'nameRev': 'f67fc930a58673efa4e973839d38c5effaba38df tags/v0.90.8~51', 'commitGHEventType': 'referenced', 'commitMessage': 'Resolve potential deadlock state during EsThreadPoolExecutor shutdown\n\nFixes #4334\n\nThe deadlock occurs between monitor object of EsThreadPoolExecutor and mainLock of ThreadPoolExecutor. The shutdown method of EsThreadPoolExecutor obtains the lock on monitor first and waits for mainLock of ThreadPoolExecutor in ThreadPoolExecutor#shutdown for part of the processing, while EsThreadPoolExecutor#terminated is executed under mainLock and tries to obtain monitor to notify listeners.\n', 'authoredDateTime': '2013-12-13 18:56:32'}]",https://github.com/elastic/elasticsearch/issues/4334,10.000277777777777,"['>bug', 'v0.90.8', 'v1.0.0.RC1']",Node deadlock on shutdown,1.0,['org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.shutdown(org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor$ShutdownListener)'],['d8ba92cfa80abd171c1c296b49e4d53ceb028681'],,['src/main/java/org/elasticsearch/common/util/concurrent'],1.0,1.0,2.0,1.0,0.0,1.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,elasticsearch
22372,2013-02-16 21:42:44,imotov,"If a master node operation, such as create index, is executed on a cluster with a global master block, it might be executed twice once block is removed.
",2013-02-17 01:30:39,"[{'commitGitStats': [], 'spoonStatsSkippedReason': '', 'commitParents': [], 'commitUser': 'imotov', 'commitDateTime': '', 'commitHash': '03a1437d6eb03b143c0b744cac15b936b3eff24d', 'commitSpoonAstDiffStats': [], 'nameRev': '', 'commitGHEventType': 'closed', 'commitMessage': '', 'authoredDateTime': ''}, {'commitGitStats': [{'lines': 10, 'insertions': 5, 'deletions': 5, 'filePath': 'src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['210cfaa9a4427a93455dca316e3136527fc89783'], 'commitUser': 'imotov', 'commitDateTime': '2013-02-16 16:45:14', 'commitHash': '9b80ee2bb9c9f09374d482868c77fcd9b8e8af2f', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 5, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.service.InternalClusterService.add(org.elasticsearch.common.unit.TimeValue,org.elasticsearch.cluster.service.TimeoutClusterStateListener)', 'INS': 1, 'MOV': 4}], 'spoonFilePath': 'InternalClusterService.java'}], 'nameRev': '9b80ee2bb9c9f09374d482868c77fcd9b8e8af2f tags/v0.20.6~28', 'commitGHEventType': 'referenced', 'commitMessage': 'Fix race condition in adding TimeoutClusterStateListener\n\nFixes #2658\n', 'authoredDateTime': '2013-02-16 16:44:33'}]",https://github.com/elastic/elasticsearch/issues/2658,0.0002777777777777778,"['>bug', 'v0.20.6', 'v0.90.0.Beta1']",Master node operations might be executed twice,1.0,"['org.elasticsearch.cluster.service.InternalClusterService.add(org.elasticsearch.common.unit.TimeValue,org.elasticsearch.cluster.service.TimeoutClusterStateListener)']",['9b80ee2bb9c9f09374d482868c77fcd9b8e8af2f'],,['src/main/java/org/elasticsearch/cluster/service'],5.0,5.0,10.0,1.0,0.0,1.0,5.0,4.0,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,elasticsearch
22592,2011-09-01 21:18:43,amckinley,"I tried to delete an index, and less than a second later, another machine attempted to perform an index. That recreated the index before the deletion propagated cleanly, leaving the cluster inconsistent. Example log output: https://gist.github.com/1187311
",2011-09-02 06:36:41,"[{'commitGitStats': [{'lines': 26, 'insertions': 22, 'deletions': 4, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java'}, {'lines': 61, 'insertions': 61, 'deletions': 0, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaDataService.java'}, {'lines': 26, 'insertions': 22, 'deletions': 4, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java'}, {'lines': 2, 'insertions': 2, 'deletions': 0, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/cluster/ClusterModule.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['a8baec696080751b3f4e6a79a6be103704cd9e9e'], 'commitUser': 'kimchy', 'commitDateTime': '2011-09-02 09:36:25', 'commitHash': '8facdb2e3dc49935a4c4b93967acd33882b113d3', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.CreateIndexListener.onFailure(java.lang.Throwable)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 3, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.metadata.MetaDataCreateIndexService', 'INS': 3, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.CreateIndexListener.onResponse(org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$Response)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.CreateIndexListener', 'INS': 4, 'MOV': 0}, {'DEL': 0, 'TOT': 3, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.createIndex(org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$Request,org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$Listener)', 'INS': 2, 'MOV': 0}], 'spoonFilePath': 'MetaDataCreateIndexService.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.metadata.MetaDataService', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'MetaDataService.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService.DeleteIndexListener', 'INS': 4, 'MOV': 0}, {'DEL': 0, 'TOT': 3, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService', 'INS': 3, 'MOV': 0}, {'DEL': 1, 'TOT': 2, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService.deleteIndex(org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService$Request,org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService$Listener).1.execute(org.elasticsearch.cluster.ClusterState)', 'INS': 0, 'MOV': 1}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService.DeleteIndexListener.onFailure(java.lang.Throwable)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService.DeleteIndexListener.onResponse(org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService$Response)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 3, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService.deleteIndex(org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService$Request,org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService$Listener)', 'INS': 3, 'MOV': 0}], 'spoonFilePath': 'MetaDataDeleteIndexService.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.ClusterModule.configure()', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'ClusterModule.java'}], 'nameRev': '8facdb2e3dc49935a4c4b93967acd33882b113d3 tags/v0.18.0~175', 'commitGHEventType': 'closed', 'commitMessage': 'Rapidly concurrent deleting/creating an index leaves index inconsistent, closes #1296.\n', 'authoredDateTime': '2011-09-02 09:36:25'}]",https://github.com/elastic/elasticsearch/issues/1296,0.0002777777777777778,"['>bug', 'v0.17.7', 'v0.18.0']",Rapidly concurrent deleting/creating an index leaves index inconsistent,1.0,"['org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService.DeleteIndexListener', 'org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.CreateIndexListener', 'org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService.DeleteIndexListener.onResponse(org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService$Response)', 'org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService.deleteIndex(org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService$Request,org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService$Listener)', 'org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService.deleteIndex(org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService$Request,org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService$Listener).1.execute(org.elasticsearch.cluster.ClusterState)', 'org.elasticsearch.cluster.metadata.MetaDataCreateIndexService', 'org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.createIndex(org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$Request,org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$Listener)', 'org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService', 'org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.CreateIndexListener.onResponse(org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$Response)', 'org.elasticsearch.cluster.ClusterModule.configure()', 'org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService.DeleteIndexListener.onFailure(java.lang.Throwable)', 'org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.CreateIndexListener.onFailure(java.lang.Throwable)', 'org.elasticsearch.cluster.metadata.MetaDataService']",['8facdb2e3dc49935a4c4b93967acd33882b113d3'],,"['modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata', 'modules/elasticsearch/src/main/java/org/elasticsearch/cluster']",107.0,8.0,115.0,4.0,1.0,13.0,28.0,1.0,25.0,1.0,4.0,0.0,0.0,0.0,0.0,0.0,0.0,elasticsearch
22623,2011-07-24 12:39:31,kimchy,"Concurrency bug when trying to aggregate results from different endpoints can cause response corruption, or other type of failures like error notices that a message can't be sent twice for the same http request, or search context missing failures.
",2011-07-24 12:40:15,"[{'commitGitStats': [{'lines': 19, 'insertions': 12, 'deletions': 7, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/action/support/broadcast/TransportBroadcastOperationAction.java'}, {'lines': 24, 'insertions': 24, 'deletions': 0, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/PlainShardsIterator.java'}, {'lines': 14, 'insertions': 7, 'deletions': 7, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java'}, {'lines': 16, 'insertions': 16, 'deletions': 0, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/ShardsIterator.java'}, {'lines': 7, 'insertions': 7, 'deletions': 0, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/status/TransportIndicesStatusAction.java'}, {'lines': 7, 'insertions': 7, 'deletions': 0, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/segments/TransportIndicesSegmentsAction.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['5e78f14f044fe0e0d875bb942d36610b27edf6b6'], 'commitUser': 'kimchy', 'commitDateTime': '2011-07-24 15:40:01', 'commitHash': 'b31f68a0ebdbaa61ff92293dc1914ba7bbad1765', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 1, 'TOT': 2, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchTypeAction.BaseAsyncAction.start().2.run()', 'INS': 0, 'MOV': 1}, {'DEL': 2, 'TOT': 7, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchTypeAction.BaseAsyncAction.start()', 'INS': 0, 'MOV': 3}, {'DEL': 1, 'TOT': 4, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.action.search.type.TransportSearchTypeAction.BaseAsyncAction.start().1.run()', 'INS': 0, 'MOV': 2}], 'spoonFilePath': 'TransportSearchTypeAction.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.routing.firstActiveOrNull()', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.routing.firstAssignedOrNull()', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'ShardsIterator.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.routing.PlainShardsIterator.firstAssignedOrNull()', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.routing.PlainShardsIterator.firstActiveOrNull()', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'PlainShardsIterator.java'}, {'spoonMethods': [{'DEL': 1, 'TOT': 4, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.AsyncBroadcastAction.start().1.run()', 'INS': 0, 'MOV': 2}, {'DEL': 2, 'TOT': 7, 'UPD': 2, 'spoonMethodName': 'org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.AsyncBroadcastAction.start()', 'INS': 0, 'MOV': 3}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.firstShardOrNull(org.elasticsearch.cluster.routing.ShardIterator)', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'TransportBroadcastOperationAction.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.admin.indices.segments.TransportIndicesSegmentsAction.firstShardOrNull(org.elasticsearch.cluster.routing.ShardIterator)', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'TransportIndicesSegmentsAction.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.firstShardOrNull(org.elasticsearch.cluster.routing.ShardIterator)', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'TransportIndicesStatusAction.java'}], 'nameRev': 'b31f68a0ebdbaa61ff92293dc1914ba7bbad1765 tags/v0.18.0~362', 'commitGHEventType': 'closed', 'commitMessage': 'Search / Broadcast concurrency bug can result in response corruption / errors, closes #1152.\n', 'authoredDateTime': '2011-07-24 15:40:01'}]",https://github.com/elastic/elasticsearch/issues/1152,0.0002777777777777778,"['>bug', 'v0.16.5', 'v0.17.2', 'v0.18.0']",Search / Broadcast concurrency bug can result in response corruption / errors,1.0,"['org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.AsyncBroadcastAction.start()', 'org.elasticsearch.cluster.routing.firstAssignedOrNull()', 'org.elasticsearch.cluster.routing.PlainShardsIterator.firstActiveOrNull()', 'org.elasticsearch.action.search.type.TransportSearchTypeAction.BaseAsyncAction.start()', 'org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.firstShardOrNull(org.elasticsearch.cluster.routing.ShardIterator)', 'org.elasticsearch.action.search.type.TransportSearchTypeAction.BaseAsyncAction.start().1.run()', 'org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.AsyncBroadcastAction.start().1.run()', 'org.elasticsearch.cluster.routing.firstActiveOrNull()', 'org.elasticsearch.action.search.type.TransportSearchTypeAction.BaseAsyncAction.start().2.run()', 'org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.firstShardOrNull(org.elasticsearch.cluster.routing.ShardIterator)', 'org.elasticsearch.action.admin.indices.segments.TransportIndicesSegmentsAction.firstShardOrNull(org.elasticsearch.cluster.routing.ShardIterator)', 'org.elasticsearch.cluster.routing.PlainShardsIterator.firstAssignedOrNull()']",['b31f68a0ebdbaa61ff92293dc1914ba7bbad1765'],,"['modules/elasticsearch/src/main/java/org/elasticsearch/action/support/broadcast', 'modules/elasticsearch/src/main/java/org/elasticsearch/action/search/type', 'modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices', 'modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing']",73.0,14.0,87.0,6.0,6.0,12.0,31.0,11.0,7.0,7.0,6.0,0.0,0.0,0.0,0.0,0.0,0.0,elasticsearch
22626,2011-07-21 22:00:29,kimchy,"In 0.16, only create it under a global mutex lock. In 0.17, lets try and be smarted and detect that its stuck....
",2011-07-21 22:01:50,"[{'commitGitStats': [{'lines': 3, 'insertions': 2, 'deletions': 1, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/index/store/fs/NioFsStore.java'}, {'lines': 3, 'insertions': 2, 'deletions': 1, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/index/store/fs/MmapFsStore.java'}, {'lines': 4, 'insertions': 2, 'deletions': 2, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobStore.java'}, {'lines': 15, 'insertions': 14, 'deletions': 1, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/common/io/FileSystemUtils.java'}, {'lines': 3, 'insertions': 2, 'deletions': 1, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/monitor/dump/SimpleDumpGenerator.java'}, {'lines': 4, 'insertions': 2, 'deletions': 2, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/plugins/PluginManager.java'}, {'lines': 2, 'insertions': 1, 'deletions': 1, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/gateway/local/LocalGateway.java'}, {'lines': 3, 'insertions': 2, 'deletions': 1, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/index/store/fs/SimpleFsStore.java'}, {'lines': 5, 'insertions': 3, 'deletions': 2, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/index/translog/fs/FsTranslog.java'}, {'lines': 3, 'insertions': 2, 'deletions': 1, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/env/NodeEnvironment.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['79fbd6d954df48a50c2e80a4105a328e9959aecb'], 'commitUser': 'kimchy', 'commitDateTime': '2011-07-22 01:01:40', 'commitHash': 'ceb697364c5f91dbb9f266f46558172115145aad', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.index.store.fs.NioFsStore', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'NioFsStore.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.common.io.FileSystemUtils', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.common.io.FileSystemUtils.mkdirs(java.io.File)', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'FileSystemUtils.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.env.NodeEnvironment', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'NodeEnvironment.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.index.store.fs.SimpleFsStore', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'SimpleFsStore.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.common.blobstore.fs.FsBlobStore.buildAndCreate(org.elasticsearch.common.blobstore.BlobPath)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.common.blobstore.fs.FsBlobStore', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'FsBlobStore.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.index.store.fs.MmapFsStore', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'MmapFsStore.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.plugins.PluginManager.main(java.lang.String[])', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.plugins.PluginManager.downloadPlugin(java.lang.String)', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'PluginManager.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.gateway.local.LocalGateway.lazyInitialize()', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'LocalGateway.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 2, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.index.translog.fs.FsTranslog', 'INS': 2, 'MOV': 0}], 'spoonFilePath': 'FsTranslog.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.monitor.dump.SimpleDumpGenerator.generateDump(java.lang.String,java.util.Map,java.lang.String[])', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'SimpleDumpGenerator.java'}], 'nameRev': 'ceb697364c5f91dbb9f266f46558172115145aad tags/v0.16.5~3', 'commitGHEventType': 'closed', 'commitMessage': 'File#mkdirs gets stuck, might be concurrency issue, closes #1147.\n', 'authoredDateTime': '2011-07-22 01:01:40'}]",https://github.com/elastic/elasticsearch/issues/1147,0.0002777777777777778,"['>bug', 'v0.16.5', 'v0.17.2', 'v0.18.0']","File#mkdirs gets stuck, might be concurrency issue",1.0,"['org.elasticsearch.index.store.fs.NioFsStore', 'org.elasticsearch.index.store.fs.SimpleFsStore', 'org.elasticsearch.common.blobstore.fs.FsBlobStore.buildAndCreate(org.elasticsearch.common.blobstore.BlobPath)', 'org.elasticsearch.index.translog.fs.FsTranslog', 'org.elasticsearch.monitor.dump.SimpleDumpGenerator.generateDump(java.lang.String,java.util.Map,java.lang.String[])', 'org.elasticsearch.plugins.PluginManager.main(java.lang.String[])', 'org.elasticsearch.common.blobstore.fs.FsBlobStore', 'org.elasticsearch.index.store.fs.MmapFsStore', 'org.elasticsearch.env.NodeEnvironment', 'org.elasticsearch.common.io.FileSystemUtils', 'org.elasticsearch.plugins.PluginManager.downloadPlugin(java.lang.String)', 'org.elasticsearch.common.io.FileSystemUtils.mkdirs(java.io.File)', 'org.elasticsearch.gateway.local.LocalGateway.lazyInitialize()']",['ceb697364c5f91dbb9f266f46558172115145aad'],,"['modules/elasticsearch/src/main/java/org/elasticsearch/index/store/fs', 'modules/elasticsearch/src/main/java/org/elasticsearch/gateway/local', 'modules/elasticsearch/src/main/java/org/elasticsearch/plugins', 'modules/elasticsearch/src/main/java/org/elasticsearch/env', 'modules/elasticsearch/src/main/java/org/elasticsearch/common/blobstore/fs', 'modules/elasticsearch/src/main/java/org/elasticsearch/common/io', 'modules/elasticsearch/src/main/java/org/elasticsearch/index/translog/fs', 'modules/elasticsearch/src/main/java/org/elasticsearch/monitor/dump']",32.0,13.0,45.0,10.0,0.0,13.0,14.0,0.0,14.0,0.0,10.0,0.0,0.0,0.0,0.0,0.0,0.0,elasticsearch
22662,2011-05-16 13:12:11,clintongormley,"Hiya - there is a bug with `auto_expand_replicas: [0-all]` in v 0.16.1 which causes loss of all data in that index.

To replicate:
- start two nodes
- run the script below
- count for index `bar` : 3
- kill the node that holds the primary shard for index `bar`
- count for index `bar`: 0

If you change auto expand to `[1-all]` then data is not lost.

```
curl -XDELETE 'http://127.0.0.1:9200/bar,foo/?pretty=1'

curl -XPUT 'http://127.0.0.1:9200/foo/?pretty=1'  -d '
{
   ""settings"" : {
      ""number_of_replicas"" : 0,
      ""number_of_shards"" : 1
   }
}
'

curl -XPUT 'http://127.0.0.1:9200/bar/?pretty=1'  -d '
{
   ""settings"" : {
      ""index"" : {
         ""number_of_replicas"" : 0,
         ""number_of_shards"" : 1
      }
   }
}
'


curl -XGET 'http://127.0.0.1:9200/_cluster/health/bar?pretty=1&wait_for_status=green' 


curl -XPOST 'http://127.0.0.1:9200/_bulk?pretty=1'  -d '
{""index"" : {""_index"" : ""bar"", ""_type"" : ""name""}}
{""tokens"" : [""stuart"", ""watt""], ""context"" : ""/2850246/all"", ""rank"" : 1}
{""index"" : {""_index"" : ""bar"", ""_type"" : ""name""}}
{""tokens"" : [""stuart"", ""watt""], ""context"" : ""/2850246/jpnw/all"", ""rank"" : 1}
{""index"" : {""_index"" : ""bar"", ""_type"" : ""name""}}
{""tokens"" : [""stuart"", ""watt""], ""context"" : ""/2850246/jpnw_pres/all"", ""rank"" : 1}
{""index"" : {""_index"" : ""bar"", ""_type"" : ""name""}}
'

curl -XPOST 'http://127.0.0.1:9200/bar/_refresh?pretty=1' 

curl -XPUT 'http://127.0.0.1:9200/bar/_settings?pretty=1'  -d '
{
   ""index"" : {
      ""auto_expand_replicas"" : ""0-all""
   }
}
'

curl -XGET 'http://127.0.0.1:9200/_cluster/health/bar?pretty=1&wait_for_status=green' 


curl -XGET 'http://127.0.0.1:9200/bar/_count?pretty=1'  -d '
{
   ""match_all"" : {}
}
'
```
",2011-05-16 22:41:18,"[{'commitGitStats': [{'lines': 5, 'insertions': 5, 'deletions': 0, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/cluster/ClusterService.java'}, {'lines': 10, 'insertions': 9, 'deletions': 1, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java'}, {'lines': 25, 'insertions': 25, 'deletions': 0, 'filePath': '.idea/projectCodeStyle.xml'}, {'lines': 56, 'insertions': 30, 'deletions': 26, 'filePath': 'modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java'}], 'spoonStatsSkippedReason': '', 'commitParents': ['c9aca9c6ded5c955c67ef26fc2e1f8297d4fba97'], 'commitUser': 'kimchy', 'commitDateTime': '2011-05-17 01:41:05', 'commitHash': '518488b0b299ba26d3ad9503c5ef88ff287709a2', 'commitSpoonAstDiffStats': [{'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.addPriority(org.elasticsearch.cluster.ClusterStateListener)', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'ClusterService.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.cluster.routing.RoutingService.doStart()', 'INS': 0, 'MOV': 0}, {'DEL': 1, 'TOT': 4, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.routing.RoutingService.RoutingTableUpdater', 'INS': 1, 'MOV': 2}, {'DEL': 0, 'TOT': 3, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.cluster.routing.RoutingService.RoutingTableUpdater.run()', 'INS': 0, 'MOV': 2}, {'DEL': 0, 'TOT': 1, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.cluster.routing.RoutingService.RoutingTableUpdater.run().1.execute(org.elasticsearch.cluster.ClusterState$ClusterState)', 'INS': 0, 'MOV': 0}, {'DEL': 1, 'TOT': 2, 'UPD': 1, 'spoonMethodName': 'org.elasticsearch.cluster.routing.RoutingService.clusterChanged(org.elasticsearch.cluster.routing.ClusterChangedEvent)', 'INS': 0, 'MOV': 0}], 'spoonFilePath': 'RoutingService.java'}, {'spoonMethods': [{'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.service.InternalClusterService.addPriority(org.elasticsearch.cluster.service.ClusterStateListener)', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.service.InternalClusterService', 'INS': 1, 'MOV': 0}, {'DEL': 0, 'TOT': 1, 'UPD': 0, 'spoonMethodName': 'org.elasticsearch.cluster.service.InternalClusterService.submitStateUpdateTask(java.lang.String,org.elasticsearch.cluster.service.ClusterStateUpdateTask).2.run()', 'INS': 1, 'MOV': 0}], 'spoonFilePath': 'InternalClusterService.java'}], 'nameRev': '518488b0b299ba26d3ad9503c5ef88ff287709a2 tags/v0.17.0~262', 'commitGHEventType': 'closed', 'commitMessage': 'auto_expand_replicas: [0-auto] can cause data loss when nodes are removed, closes #934.\n\nThis is caused because of a race condition between when to handle the removed node and move a replica to a primary mode, and when to remove the replica because of the 0-auto setting.\n', 'authoredDateTime': '2011-05-17 01:41:05'}]",https://github.com/elastic/elasticsearch/issues/934,0.0002777777777777778,"['>bug', 'v0.16.2', 'v0.17.0']",auto_expand_replicas: [0-all] can cause data loss when nodes are removed,1.0,"['org.elasticsearch.cluster.routing.RoutingService.doStart()', 'org.elasticsearch.cluster.routing.RoutingService.RoutingTableUpdater', 'org.elasticsearch.cluster.service.InternalClusterService.submitStateUpdateTask(java.lang.String,org.elasticsearch.cluster.service.ClusterStateUpdateTask).2.run()', 'org.elasticsearch.cluster.routing.RoutingService.RoutingTableUpdater.run()', 'org.elasticsearch.cluster.service.InternalClusterService.addPriority(org.elasticsearch.cluster.service.ClusterStateListener)', 'org.elasticsearch.cluster.service.InternalClusterService', 'org.elasticsearch.cluster.routing.RoutingService.RoutingTableUpdater.run().1.execute(org.elasticsearch.cluster.ClusterState$ClusterState)', 'org.elasticsearch.cluster.routing.RoutingService.clusterChanged(org.elasticsearch.cluster.routing.ClusterChangedEvent)', 'org.elasticsearch.cluster.addPriority(org.elasticsearch.cluster.ClusterStateListener)']",['518488b0b299ba26d3ad9503c5ef88ff287709a2'],,"['modules/elasticsearch/src/main/java/org/elasticsearch/cluster', 'modules/elasticsearch/src/main/java/org/elasticsearch/cluster/service', 'modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing']",44.0,27.0,71.0,3.0,4.0,9.0,15.0,4.0,5.0,2.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,elasticsearch
23571,2013-05-22 06:29:36,dalaro,"TitanBlueprintsGraph (TBG) implements the Blueprints interface TransactionalGraph.  TBG's implementation internally creates a single ThreadLocal transaction for each caller and also stores all of these transaction references as keys in a WeakHashMap.  TBG's shutdown() method iterates over the WeakHashMap and autocommits every open transaction.

The test method BerkeleyJEBlueprintsTest#testTransactionalGraphTestSuite would sometimes fail like this:

```
Encountered error in testCompetingThreads
java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:601)
    at com.thinkaurelius.titan.blueprints.TitanBlueprintsTest.doTestSuite(TitanBlueprintsTest.java:117)
    at com.thinkaurelius.titan.blueprints.TitanBlueprintsTest.testTransactionalGraphTestSuite(TitanBlueprintsTest.java:63)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:601)
    at junit.framework.TestCase.runTest(TestCase.java:168)
    at junit.framework.TestCase.runBare(TestCase.java:134)
    at junit.framework.TestResult$1.protect(TestResult.java:110)
    at junit.framework.TestResult.runProtected(TestResult.java:128)
    at junit.framework.TestResult.run(TestResult.java:113)
    at junit.framework.TestCase.run(TestCase.java:124)
    at junit.framework.TestSuite.runTest(TestSuite.java:232)
    at junit.framework.TestSuite.run(TestSuite.java:227)
    at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)
    at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
    at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: java.lang.IllegalStateException: There is 1 existing transaction opened against the Environment.
Aborting open transactions ...
aborting <Transaction id=""269"">
    at com.sleepycat.je.Environment.close(Environment.java:384)
    at com.thinkaurelius.titan.diskstorage.berkeleyje.BerkeleyJEStoreManager.close(BerkeleyJEStoreManager.java:152)
    at com.thinkaurelius.titan.diskstorage.keycolumnvalue.keyvalue.OrderedKeyValueStoreManagerAdapter.close(OrderedKeyValueStoreManagerAdapter.java:52)
    at com.thinkaurelius.titan.diskstorage.Backend.close(Backend.java:346)
    at com.thinkaurelius.titan.graphdb.database.StandardTitanGraph.shutdown(StandardTitanGraph.java:92)
    at com.tinkerpop.blueprints.TransactionalGraphTestSuite.testCompetingThreads(TransactionalGraphTestSuite.java:491)
    at com.thinkaurelius.titan.blueprints.TransactionalTitanGraphTestSuite.testCompetingThreads(TransactionalTitanGraphTestSuite.java:33)
    ... 25 more
```

The count of open transactions and BDB transaction id are always 1 and 269, respectively.

I found this failure accidentally while attempting to move Titan from Tinkerpop 2.3.0 to 2.4.0-SNAPSHOT to support Metrics integration.  The failure appears to have nothing to do with new Tinkerpop code or Metrics.

The failure is setup in the last three lines of `TransactionalGraphTestSuite#testCompetingThreads()`:

```
edgeCount(graph, edges.get());
vertexCount(graph, vertices.get());
graph.shutdown();
```

`edgeCount()` ends up calling TBG's `getAutoStartTx()` method.  This creates a new transaction and stores its reference in both TBG's WeakHashMap and the main thread's ThreadLocal storage.  `graph.shutdown()` iterates over TBG's WeakHashMap to look for open transactions to autocommit.  Sometimes the size of the map (as measured by a counter incremented during iteration) is zero, and other times one.  When it's of size one, `shutdown()` successfully autocommits the transaction and the test passes.  When it's empty, TBG's reference to the still-open transaction created for edge counting has been lost, and we get the exception stacktrace from BDB shown above.  I've verified this with conditional breakpoints.

I'm not convinced that I have the root cause figured out.  I have three observations so far.
1. WeakHashMap is not synchronized according to [its javadoc](http://docs.oracle.com/javase/6/docs/api/java/util/WeakHashMap.html).  TBG allows multiple threads to simultaneously write to a single WeakHashMap without synchronization.  This might be sufficient to cause the failure.  When I run the test on a loop, the first failure usually emerges within ten iterations; when I changed WeakHashMap to ConcurrentHashMap, it ran for 551 successful consecutive iterations before I killed the loop.  I'm trying Collections.synchronizedMap(new WeakHashMap()) now.
2. ThreadLocalMap (an inner static class of ThreadLocal that tracks all its data) uses WeakReference.  For a while, I thought this failure might have something to do with WeakReferences: maybe we erroneously kept only WeakReferences to our transactions and wound up unintentionally losing them to GC?  However, I currently think this is not the case.  TBG puts every transaction in ThreadLocal storage before putting it in the WeakHashMap as a key.  I think this is sufficient to guarantee that every transaction in the WeakHashMap visible from a live thread cannot be GC'ed.  Here's the strong-reachability path from each live thread to its ThreadLocal transaction:
   - `Thread` has a field `threadLocals` (type `ThreadLocal.ThreadLocalMap`)
   - `ThreadLocalMap` has a field `table`  (type `ThreadLocalMap.Entry[]`)
   - `Entry` has a field `value` (type `Object`) holding the transaction
   
   If that's all correct, then I think I can rule out WeakReference semantics and premature GC as the root cause.  But this code is unfamiliar and I could easily have bungled my analysis.
3. A workaround for this problem is to always call TBG's `commit()` or `rollback()` method after calling some method that could have opened a ThreadLocal transaction.  This problem can only arise if a thread relies on `shutdown()` to autocommit an open transaction.
",2013-05-22 22:31:15,"[{'commitUser': 'dalaro', 'commitDateTime': '2013-05-22 18:20:12', 'commitHash': '1bea1e5a9d3de1b8eb128dd05c237b4a1d01201a', 'commitParents': ['97f3c69b06ed6cf67dab963c4157eb9a0a77d50c'], 'commitGHEventType': 'referenced', 'nameRev': '1bea1e5a9d3de1b8eb128dd05c237b4a1d01201a tags/0.3.2~28', 'commitGitStats': [{'insertions': 31, 'lines': 36, 'filePath': 'titan-core/src/main/java/com/thinkaurelius/titan/graphdb/blueprints/TitanBlueprintsGraph.java', 'deletions': 5}], 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 1, 'TOT': 1, 'DEL': 0, 'INS': 0, 'MOV': 0, 'spoonMethodName': 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.1'}, {'UPD': 1, 'TOT': 2, 'DEL': 0, 'INS': 1, 'MOV': 0, 'spoonMethodName': 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.rollback()'}, {'UPD': 1, 'TOT': 1, 'DEL': 0, 'INS': 0, 'MOV': 0, 'spoonMethodName': 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.shutdown()'}, {'UPD': 1, 'TOT': 1, 'DEL': 0, 'INS': 0, 'MOV': 0, 'spoonMethodName': 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.1.initialValue()'}, {'UPD': 1, 'TOT': 1, 'DEL': 0, 'INS': 0, 'MOV': 0, 'spoonMethodName': 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.newThreadBoundTransaction()'}, {'UPD': 1, 'TOT': 1, 'DEL': 0, 'INS': 0, 'MOV': 0, 'spoonMethodName': 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.makeType()'}, {'UPD': 2, 'TOT': 3, 'DEL': 0, 'INS': 1, 'MOV': 0, 'spoonMethodName': 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph'}, {'UPD': 1, 'TOT': 2, 'DEL': 0, 'INS': 1, 'MOV': 0, 'spoonMethodName': 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.commit()'}, {'UPD': 2, 'TOT': 3, 'DEL': 0, 'INS': 1, 'MOV': 0, 'spoonMethodName': 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.getAutoStartTx()'}, {'UPD': 1, 'TOT': 1, 'DEL': 0, 'INS': 0, 'MOV': 0, 'spoonMethodName': 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.query()'}, {'UPD': 1, 'TOT': 1, 'DEL': 0, 'INS': 0, 'MOV': 0, 'spoonMethodName': 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.getCurrentThreadTx()'}, {'UPD': 1, 'TOT': 1, 'DEL': 0, 'INS': 0, 'MOV': 0, 'spoonMethodName': 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.getType(java.lang.String)'}], 'spoonFilePath': 'TitanBlueprintsGraph.java'}], 'commitMessage': 'Change WeakHashMap to ConcurrentHashMap\n\nThe WeakHashMap in TitanBlueprintsGraph can be accessed concurrently\nby multiple threads.  This access pattern is tested in\nTransactionalGraphTestSuite#testCompetingThreads().  The javadoc for\nWeakHashMap says the collection is unsynchronized.  When running the\ntest on my box, these unsynchronized concurrent accesses seem to the\ntype of failure seen in #284.\n\nI changed the WeakHashMap to a ConcurrentHashMap.\nCollections.synchronizedMap(new WeakHashMap()) would also work, but I\nsuspect that we want concurrency more than we want the ability to\nautomatically forget and GC uncommitted/unrollbacked transactions.\n', 'spoonStatsSkippedReason': '', 'authoredDateTime': '2013-05-22 18:20:12'}]",https://github.com/thinkaurelius/titan/issues/284,0.0002777777777777778,['bug'],TitanBlueprintsGraph's WeakHashMap loses transactions,1.0,"['com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.1', 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.rollback()', 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.shutdown()', 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.1.initialValue()', 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.newThreadBoundTransaction()', 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.makeType()', 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph', 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.commit()', 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.getAutoStartTx()', 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.query()', 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.getCurrentThreadTx()', 'com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.getType(java.lang.String)']",['1bea1e5a9d3de1b8eb128dd05c237b4a1d01201a'],,['titan-core/src/main/java/com/thinkaurelius/titan/graphdb/blueprints'],31.0,5.0,36.0,1.0,14.0,12.0,18.0,0.0,4.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,titan
24232,2016-07-04 14:28:37,volth,"## Expected behavior and actual behavior

Concurrent INSERTs and DELETEs cause deadlock.
![img](http://i.imgur.com/SzjrjA4.png)

Stacktrace (taken with jvisualvm)

```
Found one Java-level deadlock:
=============================
""Thread-7"":
  waiting for ownable synchronizer 0x00000000d59c41d0, (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync),
  which is held by ""Thread-6""
""Thread-6"":
  waiting for ownable synchronizer 0x00000000d59c41a0, (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync),
  which is held by ""Thread-7""

Java stack information for the threads listed above:
===================================================
""Thread-7"":
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x00000000d59c41d0> (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
    at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:943)
    at com.orientechnologies.common.concur.lock.OOneEntryPerKeyLockManager.acquireLock(OOneEntryPerKeyLockManager.java:173)
    at com.orientechnologies.common.concur.lock.OOneEntryPerKeyLockManager.acquireLock(OOneEntryPerKeyLockManager.java:100)
    at com.orientechnologies.orient.core.storage.impl.local.paginated.atomicoperations.OAtomicOperationsManager.acquireExclusiveLockTillOperationComplete(OAtomicOperationsManager.java:464)
    at com.orientechnologies.orient.core.storage.impl.local.paginated.atomicoperations.OAtomicOperationsManager.startAtomicOperation(OAtomicOperationsManager.java:175)
    at com.orientechnologies.orient.core.storage.impl.local.paginated.atomicoperations.OAtomicOperationsManager.startAtomicOperation(OAtomicOperationsManager.java:140)
    at com.orientechnologies.orient.core.storage.impl.local.paginated.base.ODurableComponent.startAtomicOperation(ODurableComponent.java:123)
    at com.orientechnologies.orient.core.storage.impl.local.paginated.OClusterPositionMap.remove(OClusterPositionMap.java:411)
    at com.orientechnologies.orient.core.storage.impl.local.paginated.OPaginatedCluster.deleteRecord(OPaginatedCluster.java:806)
    at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.doDeleteRecord(OAbstractPaginatedStorage.java:3205)
    at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.deleteRecord(OAbstractPaginatedStorage.java:1151)
    at com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx.executeDeleteRecord(ODatabaseDocumentTx.java:2139)
    at com.orientechnologies.orient.core.tx.OTransactionNoTx.deleteRecord(OTransactionNoTx.java:274)
    at com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx.delete(ODatabaseDocumentTx.java:2563)
    at com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx.delete(ODatabaseDocumentTx.java:101)
    at com.orientechnologies.orient.core.record.ORecordAbstract.delete(ORecordAbstract.java:296)
    at com.orientechnologies.orient.core.sql.OCommandExecutorSQLDelete.result(OCommandExecutorSQLDelete.java:325)
    at com.orientechnologies.orient.core.sql.OCommandExecutorSQLResultsetAbstract.pushResult(OCommandExecutorSQLResultsetAbstract.java:278)
    at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.addResult(OCommandExecutorSQLSelect.java:734)
    at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.handleResult(OCommandExecutorSQLSelect.java:666)
    at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.executeSearchRecord(OCommandExecutorSQLSelect.java:615)
    at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.serialIterator(OCommandExecutorSQLSelect.java:1638)
    at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.fetchFromTarget(OCommandExecutorSQLSelect.java:1553)
    at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.executeSearch(OCommandExecutorSQLSelect.java:510)
    at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.execute(OCommandExecutorSQLSelect.java:473)
    at com.orientechnologies.orient.core.sql.OCommandExecutorSQLDelegate.execute(OCommandExecutorSQLDelegate.java:72)
    at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.executeCommand(OAbstractPaginatedStorage.java:2577)
    at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.command(OAbstractPaginatedStorage.java:2523)
    at com.orientechnologies.orient.core.sql.query.OSQLQuery.run(OSQLQuery.java:78)
    at com.orientechnologies.orient.core.sql.query.OSQLAsynchQuery.run(OSQLAsynchQuery.java:74)
    at com.orientechnologies.orient.core.query.OQueryAbstract.execute(OQueryAbstract.java:33)
    at com.orientechnologies.orient.core.sql.OCommandExecutorSQLDelete.execute(OCommandExecutorSQLDelete.java:205)
    at com.orientechnologies.orient.core.sql.OCommandExecutorSQLDelegate.execute(OCommandExecutorSQLDelegate.java:72)
    at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.executeCommand(OAbstractPaginatedStorage.java:2577)
    at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.command(OAbstractPaginatedStorage.java:2523)
    at com.orientechnologies.orient.core.command.OCommandRequestTextAbstract.execute(OCommandRequestTextAbstract.java:69)
    at Hello$$anon$2.run(Hello.scala:752)
""Thread-6"":
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x00000000d59c41a0> (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
    at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:943)
    at com.orientechnologies.common.concur.lock.OOneEntryPerKeyLockManager.acquireLock(OOneEntryPerKeyLockManager.java:173)
    at com.orientechnologies.common.concur.lock.OOneEntryPerKeyLockManager.acquireLock(OOneEntryPerKeyLockManager.java:100)
    at com.orientechnologies.orient.core.storage.impl.local.paginated.atomicoperations.OAtomicOperationsManager.acquireExclusiveLockTillOperationComplete(OAtomicOperationsManager.java:464)
    at com.orientechnologies.orient.core.storage.impl.local.paginated.atomicoperations.OAtomicOperationsManager.startAtomicOperation(OAtomicOperationsManager.java:175)
    at com.orientechnologies.orient.core.storage.impl.local.paginated.atomicoperations.OAtomicOperationsManager.startAtomicOperation(OAtomicOperationsManager.java:140)
    at com.orientechnologies.orient.core.storage.impl.local.paginated.base.ODurableComponent.startAtomicOperation(ODurableComponent.java:123)
    at com.orientechnologies.orient.core.storage.impl.local.paginated.OPaginatedCluster.createRecord(OPaginatedCluster.java:441)
    at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.doCreateRecord(OAbstractPaginatedStorage.java:3023)
    at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.commitEntry(OAbstractPaginatedStorage.java:3584)
    at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.commit(OAbstractPaginatedStorage.java:1344)
    at com.orientechnologies.orient.core.tx.OTransactionOptimistic.doCommit(OTransactionOptimistic.java:555)
    at com.orientechnologies.orient.core.tx.OTransactionOptimistic.commit(OTransactionOptimistic.java:109)
    at com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx.commit(ODatabaseDocumentTx.java:2665)
    at com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx.commit(ODatabaseDocumentTx.java:2634)
    at Hello$$anon$1.run(Hello.scala:728)

Found 1 deadlock.

```

Expected no deadlock.
## Steps to reproduce the problem

Run the code and wait a little (usually less than a minute)

```
      locally {
        val db = new ODatabaseDocumentTx(""plocal:databases/TestDb"")
        FileUtils.deleteDirectory(new java.io.File(""databases/TestDb""))
        db.create()
        db.close()

        db.open(""admin"", ""admin"")
        try {
          println(Try(db.command[OCommandRequest](new OCommandSQL(""CREATE CLASS Ping"")).execute[Object]()))
        } finally {
          db.close()
        }
      }

      new Thread {
        override def run {
          while(true) {
            // insert a simple record
            val db = new ODatabaseDocumentTx(""plocal:databases/TestDb"")
            db.open(""admin"", ""admin"")
            try {
              db.begin(OTransaction.TXTYPE.OPTIMISTIC)
              try {
                val d = new ODocument(""Ping"")
                d.field(""time"", System.currentTimeMillis)
                d.save()
                db.commit()
                print(GREEN+"".""+RESET)
              } catch {
                case e: Throwable =>
                  e.printStackTrace
                  db.rollback()
              }
            } finally {
              db.close()
            }
            Thread sleep 5
          }
        }
      }.start()

      new Thread {
        override def run {
          while(true) {
            // delete records older than 5 sec
            val db = new ODatabaseDocumentTx(""plocal:databases/TestDb"")
            db.open(""admin"", ""admin"")
            try {
              val now = System.currentTimeMillis
              println(s""${RED}deleting:${RESET}"")
              val rc = db .command[OCommandRequest](new OCommandSQL(""delete from Ping where time < ?"")) .execute[Int]((now - 5000).asInstanceOf[AnyRef])
              println(s""${RED}deleted old msgs ${BOLD}${rc}${RESET+RED} in ${BOLD}${System.currentTimeMillis - now}${RESET+RED} ms${RESET}"")
            } finally {
              db.close()
            }
            Thread sleep 5000
          }
        }
      }.start()
```
## Important Questions
### Runninng Mode
- [ x] Embedded, using PLOCAL access mode
- [  ] Embedded, using MEMORY access mode
- [ x] Remote

I have the issue with embedded PLOCAL, remote PLOCAL and remote MEMORY (single-server setup, no distribution). In the remote case, the deadlock occurs in the OrientDB server process.
### Misc
- [  ] I have a distributed setup with multiple servers. How many? 
- [  ] I'm using the Enterprise Edition
### OrientDB Version
- [  ] v2.0.x - Please specify last number: 
- [  ] v2.1.x - Please specify last number: 
- [x ] v2.2.x - Please specify last number: 2.2.3 community edition
### Operating System
- [  ] Linux
- [  ] MacOSX
- [ x] Windows (Windows Server 2008, Java 8)
- [  ] Other Unix
- [  ] Other, name? 
### Java Version
- [  ] 6
- [ x] 7
- [ x] 8
",2016-07-05 14:26:28,"[{'commitUser': 'laa', 'commitDateTime': '2016-07-05 13:22:31', 'commitHash': '6dcfa5394024fd1349fc2e22c504541ef8feeed0', 'commitParents': ['eb2afefba4b9f0e7537c8b230ab7a721ba56e04f'], 'commitGHEventType': 'referenced', 'nameRev': '6dcfa5394024fd1349fc2e22c504541ef8feeed0 tags/2.2.4~9^2~8', 'commitGitStats': [{'insertions': 1, 'lines': 2, 'filePath': 'core/src/main/java/com/orientechnologies/orient/core/storage/impl/local/paginated/atomicoperations/OAtomicOperationsManager.java', 'deletions': 1}], 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 1, 'TOT': 1, 'DEL': 0, 'INS': 0, 'MOV': 0, 'spoonMethodName': 'com.orientechnologies.orient.core.storage.impl.local.paginated.atomicoperations.OAtomicOperationsManager.startAtomicOperation(com.orientechnologies.orient.core.storage.impl.local.paginated.base.ODurableComponent,boolean)'}], 'spoonFilePath': 'OAtomicOperationsManager.java'}], 'commitMessage': 'Issue #6380 was fixed.\n', 'spoonStatsSkippedReason': '', 'authoredDateTime': '2016-07-05 13:22:31'}, {'commitUser': 'laa', 'commitDateTime': '2016-07-05 16:24:19', 'commitHash': '624db2b2afcb3b155029c0c6fb52fe7b692abe0f', 'commitParents': ['cbf5dc4d4ec8d760c7c04ed66475d316945336d6'], 'commitGHEventType': 'referenced', 'nameRev': '624db2b2afcb3b155029c0c6fb52fe7b692abe0f tags/2.2.4~9^2~6', 'commitGitStats': [{'insertions': 60, 'lines': 106, 'filePath': 'core/src/main/java/com/orientechnologies/orient/core/storage/impl/local/paginated/OPaginatedCluster.java', 'deletions': 46}], 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 8, 'DEL': 0, 'INS': 7, 'MOV': 1, 'spoonMethodName': 'com.orientechnologies.orient.core.storage.impl.local.paginated.OPaginatedCluster.allocatePosition(byte)'}], 'spoonFilePath': 'OPaginatedCluster.java'}], 'commitMessage': 'Issue #6380 Atomic operation lock order was restored for preallocate record.\n', 'spoonStatsSkippedReason': '', 'authoredDateTime': '2016-07-05 16:24:19'}, {'commitUser': 'laa', 'commitDateTime': '2016-07-05 16:10:00', 'commitHash': 'ebd292bbe0bb62a619e61d5d977ace13e95a40ab', 'commitParents': ['0cc7b3977ec6c6d6e4504f6a6283d2e1df71b953'], 'commitGHEventType': 'referenced', 'nameRev': 'ebd292bbe0bb62a619e61d5d977ace13e95a40ab remotes/origin/autosharding~74^2~9', 'commitGitStats': [{'insertions': 2, 'lines': 3, 'filePath': 'core/src/main/java/com/orientechnologies/orient/core/storage/impl/local/paginated/OPaginatedCluster.java', 'deletions': 1}], 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 2, 'DEL': 0, 'INS': 1, 'MOV': 1, 'spoonMethodName': 'com.orientechnologies.orient.core.storage.impl.local.paginated.OPaginatedCluster.allocatePosition(byte)'}], 'spoonFilePath': 'OPaginatedCluster.java'}], 'commitMessage': 'Issue #6380 Atomic operation lock order was restored for preallocate record.\n', 'spoonStatsSkippedReason': '', 'authoredDateTime': '2016-07-05 16:10:00'}, {'commitUser': 'laa', 'commitDateTime': '2016-07-05 16:03:29', 'commitHash': '8ff190404e9db66d5b41a16ed6856c72121d08c4', 'commitParents': ['2d2d037119772cf6a978c9961a7ab8fb99fa22f8'], 'commitGHEventType': 'referenced', 'nameRev': '8ff190404e9db66d5b41a16ed6856c72121d08c4 remotes/origin/autosharding~74^2~11', 'commitGitStats': [{'insertions': 58, 'lines': 104, 'filePath': 'core/src/main/java/com/orientechnologies/orient/core/storage/impl/local/paginated/OPaginatedCluster.java', 'deletions': 46}], 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 0, 'TOT': 7, 'DEL': 0, 'INS': 6, 'MOV': 1, 'spoonMethodName': 'com.orientechnologies.orient.core.storage.impl.local.paginated.OPaginatedCluster.allocatePosition(byte)'}], 'spoonFilePath': 'OPaginatedCluster.java'}], 'commitMessage': 'Issue #6380 Atomic operation lock order was restored for preallocate record.\n', 'spoonStatsSkippedReason': '', 'authoredDateTime': '2016-07-05 16:03:29'}, {'commitUser': 'laa', 'commitDateTime': '2016-07-05 12:55:58', 'commitHash': '1fa3222aeef5718cbd65fdd709590882d1e26d16', 'commitParents': ['034ff7b1735b7cd12d511b4ff6833b6ab9d4421d'], 'commitGHEventType': 'referenced', 'nameRev': '1fa3222aeef5718cbd65fdd709590882d1e26d16 remotes/origin/autosharding~74^2~14', 'commitGitStats': [{'insertions': 1, 'lines': 2, 'filePath': 'core/src/main/java/com/orientechnologies/orient/core/storage/impl/local/paginated/atomicoperations/OAtomicOperationsManager.java', 'deletions': 1}], 'commitSpoonAstDiffStats': [{'spoonMethods': [{'UPD': 1, 'TOT': 1, 'DEL': 0, 'INS': 0, 'MOV': 0, 'spoonMethodName': 'com.orientechnologies.orient.core.storage.impl.local.paginated.atomicoperations.OAtomicOperationsManager.startAtomicOperation(com.orientechnologies.orient.core.storage.impl.local.paginated.base.ODurableComponent,boolean)'}], 'spoonFilePath': 'OAtomicOperationsManager.java'}], 'commitMessage': 'Issue #6380 was fixed.\n', 'spoonStatsSkippedReason': '', 'authoredDateTime': '2016-07-05 12:55:58'}, {'commitUser': 'laa', 'commitDateTime': '2016-07-05 16:26:15', 'commitHash': 'b6d2c4b1130dab906e777be5399102d7ddbf8f98', 'commitParents': ['624db2b2afcb3b155029c0c6fb52fe7b692abe0f'], 'commitGHEventType': 'referenced', 'nameRev': 'b6d2c4b1130dab906e777be5399102d7ddbf8f98 tags/2.2.4~9^2~5', 'commitGitStats': [{'insertions': 0, 'lines': 1, 'filePath': 'core/src/main/java/com/orientechnologies/orient/core/storage/impl/local/paginated/OPaginatedCluster.java', 'deletions': 1}], 'commitSpoonAstDiffStats': [{'spoonMethods': [], 'spoonFilePath': 'OPaginatedCluster.java'}], 'commitMessage': 'Issue #6380 Atomic operation lock order was restored for preallocate record.\n', 'spoonStatsSkippedReason': '', 'authoredDateTime': '2016-07-05 16:26:15'}]",https://github.com/orientechnologies/orientdb/issues/6380,0.0002777777777777778,['bug'],Concurrent INSERTs and DELETEs cause deadlock.,2.0,"['com.orientechnologies.orient.core.storage.impl.local.paginated.atomicoperations.OAtomicOperationsManager.startAtomicOperation(com.orientechnologies.orient.core.storage.impl.local.paginated.base.ODurableComponent,boolean)', 'com.orientechnologies.orient.core.storage.impl.local.paginated.OPaginatedCluster.allocatePosition(byte)']","['624db2b2afcb3b155029c0c6fb52fe7b692abe0f', '6dcfa5394024fd1349fc2e22c504541ef8feeed0']",,['core/src/main/java/com/orientechnologies/orient/core/storage/impl'],61.0,47.0,108.0,2.0,1.0,2.0,9.0,1.0,7.0,0.0,2.0,0.0,0.0,0.0,4.0,0.0,0.0,orientdb
26465,2018-04-27 10:54:45,uruuru,"In the context of KLighD and incremental update we currently face a concurrency issue. 

First some context: 
There are two `#layout(...)` methods within the `DiagramLayoutEngine`: [The first one](https://github.com/eclipse/elk/blob/master/plugins/org.eclipse.elk.core.service/src/org/eclipse/elk/core/service/DiagramLayoutEngine.java#L356) divides the layout task into three steps (buildLayoutGraph, layout, and applyLayout), where the first and the third step are executed on the UI thread. [The second one](https://github.com/eclipse/elk/blob/master/plugins/org.eclipse.elk.core.service/src/org/eclipse/elk/core/service/DiagramLayoutEngine.java#L486) executes the overall process in one go and on the calling thread. 

We were wondering as to why this distinction is made. One thing we could imagine is that GMF-like editors require that the access to the view model is performed on the UI thread. 

Now to problem: KLighD internally maintains a _view model_ for which a layout graph is created and laid out. It always uses the _first_ `layout(..)` method to perform the layout. Now, since the second step (of the three steps mentioned above) is executed on a different thread, the incremental update may alter the view model during the layout process. One may thing that this is not a problem since layout is performed on the previously created layout graph (which was done on the UI thread). However, before the actual layout algorithm is called, the `addDiagramConfig(...)` method 
 is invoked in [line 393](https://github.com/eclipse/elk/blob/master/plugins/org.eclipse.elk.core.service/src/org/eclipse/elk/core/service/DiagramLayoutEngine.java#L393). In KLighD's case this calls the `KLighdLayoutConfigurationStore`, which in turn accesses the original view model. 

During this it can happen that the incremental update modifies the view model and the configuration store tries to access invalid elements, which eventually yields NPEs and the likes. 

Note that the issue doesn't occur with the second `layout` method. 

An easy way to solve the issue seems to be to move the `addDiagramConfig` call into the first step, which would then _build and configure the layout graph_. 
@spoenemann any thoughts from your side? ",2018-05-02 09:47:27,"[{'commitHash': '50e8adef6120fc1d68bd71a51650a34da23b33a9', 'commitGHEventType': 'referenced', 'commitUser': 'uruuru', 'commitParents': ['b7699e5d3d1bedeb31c110b96be560d6ff9cae5b'], 'nameRev': '50e8adef6120fc1d68bd71a51650a34da23b33a9 tags/v0.4.0~16', 'commitMessage': ""core.service: #325 moved 'addDiagramConfig' to avoid concurrency issues\n\n - see the ticket for a more detailed explanation "", 'commitDateTime': '2018-05-02 11:40:49', 'authoredDateTime': '2018-05-02 11:40:49', 'commitGitStats': [{'filePath': 'plugins/org.eclipse.elk.core.service/src/org/eclipse/elk/core/service/DiagramLayoutEngine.java', 'insertions': 6, 'deletions': 2, 'lines': 8}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'DiagramLayoutEngine.java', 'spoonMethods': [{'spoonMethodName': 'org.eclipse.elk.core.service.DiagramLayoutEngine.layout(org.eclipse.ui.IWorkbenchPart,java.lang.Object,org.eclipse.elk.core.util.IElkCancelIndicator,org.eclipse.elk.core.service.DiagramLayoutEngine$Parameters).1.preUIexec()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.eclipse.elk.core.service.DiagramLayoutEngine.layout(org.eclipse.ui.IWorkbenchPart,java.lang.Object,org.eclipse.elk.core.util.IElkCancelIndicator,org.eclipse.elk.core.service.DiagramLayoutEngine$Parameters).1.execute(org.eclipse.elk.core.util.IElkProgressMonitor)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/eclipse/elk/issues/325,4.000277777777778,['bug'],DiagramLayoutEngine#layout(...) and concurrency,1.0,"['org.eclipse.elk.core.service.DiagramLayoutEngine.layout(org.eclipse.ui.IWorkbenchPart,java.lang.Object,org.eclipse.elk.core.util.IElkCancelIndicator,org.eclipse.elk.core.service.DiagramLayoutEngine$Parameters).1.preUIexec()', 'org.eclipse.elk.core.service.DiagramLayoutEngine.layout(org.eclipse.ui.IWorkbenchPart,java.lang.Object,org.eclipse.elk.core.util.IElkCancelIndicator,org.eclipse.elk.core.service.DiagramLayoutEngine$Parameters).1.execute(org.eclipse.elk.core.util.IElkProgressMonitor)']",['50e8adef6120fc1d68bd71a51650a34da23b33a9'],,['plugins/org.eclipse.elk.core.service/src/org/eclipse/elk/core/service'],6.0,2.0,8.0,1.0,0.0,2.0,2.0,1.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,elk
26671,2018-05-05 15:54:23,adamretter,Not all methods in IndexManager are correctly synchronized for updating/reading state.,2018-05-13 08:32:43,"[{'commitHash': '95083b69394652699c7ed73c86404853d7e11915', 'commitGHEventType': 'closed', 'commitUser': 'dizzzz', 'commitParents': ['9603c5c611095cb732f1cfe07f47294d42a12b72'], 'nameRev': '95083b69394652699c7ed73c86404853d7e11915 tags/eXist-4.2.0~24^2', 'commitMessage': '[bugfix] Make IndexManager thread safe\nCloses https://github.com/eXist-db/exist/issues/1856\n', 'commitDateTime': '2018-05-06 00:53:56', 'authoredDateTime': '2018-05-06 00:52:46', 'commitGitStats': [{'filePath': 'src/org/exist/indexing/IndexManager.java', 'insertions': 15, 'deletions': 4, 'lines': 19}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'IndexManager.java', 'spoonMethods': [{'spoonMethodName': 'org.exist.indexing.IndexManager', 'TOT': 6, 'UPD': 2, 'INS': 3, 'MOV': 1, 'DEL': 0}, {'spoonMethodName': 'org.exist.indexing.IndexManager.configurationChanged()', 'TOT': 3, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'org.exist.indexing.IndexManager.getConfigurationTimestamp()', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/eXist-db/exist/issues/1856,7.000277777777778,['bug'],IndexManager has insufficient synchronization,1.0,"['org.exist.indexing.IndexManager', 'org.exist.indexing.IndexManager.configurationChanged()', 'org.exist.indexing.IndexManager.getConfigurationTimestamp()']",['95083b69394652699c7ed73c86404853d7e11915'],,['src/org/exist/indexing'],15.0,4.0,19.0,1.0,2.0,3.0,11.0,3.0,5.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,exist
26734,2017-04-17 17:30:21,adamretter,READ_LOCK is acquired here: https://github.com/eXist-db/exist/blob/develop/extensions/security/openid/src/org/exist/security/realm/openid/OpenIDUtility.java#L107,2017-04-19 08:24:07,"[{'commitHash': '60fdec29d7f88627a263bc786c20e67c5de1bd0e', 'commitGHEventType': 'closed', 'commitUser': 'shabanovd', 'commitParents': ['e642b066384989c359b58da6e8a4322df9a3b843'], 'nameRev': '60fdec29d7f88627a263bc786c20e67c5de1bd0e tags/eXist-3.2.0~11^2', 'commitMessage': '[bugfix] OpenIDUtility must release the READ_LOCK that it acquired\nCloses https://github.com/eXist-db/exist/issues/1415\n', 'commitDateTime': '2017-04-18 17:14:00', 'authoredDateTime': '2017-04-18 17:14:00', 'commitGitStats': [{'filePath': 'extensions/security/openid/src/org/exist/security/realm/openid/OpenIDUtility.java', 'insertions': 4, 'deletions': 0, 'lines': 4}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'OpenIDUtility.java', 'spoonMethods': [{'spoonMethodName': 'org.exist.security.realm.openid.OpenIDUtility.registerUser(org.exist.security.Subject)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/eXist-db/exist/issues/1415,1.0002777777777778,['bug'],org.exist.security.realm.openid.OpenIDUtility never releases READ_LOCK,1.0,['org.exist.security.realm.openid.OpenIDUtility.registerUser(org.exist.security.Subject)'],['60fdec29d7f88627a263bc786c20e67c5de1bd0e'],,['extensions/security/openid/src/org/exist/security/realm/openid'],4.0,0.0,4.0,1.0,0.0,1.0,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,exist
26752,2017-02-24 22:59:02,adamretter,"See https://github.com/eXist-db/exist/blob/develop/extensions/webdav/src/org/exist/webdav/ExistDocument.java#L549

For performance reasons, Lock should be READ_LOCK when mode == Mode.COPY.

Suggest refactor to:

final LockMode srcMode = (mode == Mode.COPY ? LockMode.READ_LOCK : LockMode.WRITE_LOCK);

...

srcCollection = broker.openCollection(srcCollectionUri, srcMode);

...

if (srcCollection != null) {
    srcCollection.release(srcMode);
}",2017-03-01 16:35:15,"[{'commitHash': '38f3a8626a28b6d929c2744f0d9c79955299ca37', 'commitGHEventType': 'closed', 'commitUser': 'shabanovd', 'commitParents': ['d29c8d5d4d6a27056ec59e42f63c947434d9f866'], 'nameRev': '38f3a8626a28b6d929c2744f0d9c79955299ca37 tags/eXist-3.1.0~14^2~1', 'commitMessage': '[bugfix] webdav.ExistDocument#resourceCopyMove should take only a READ_LOCK on the source Collection\n\nCloses https://github.com/eXist-db/exist/issues/1293\n', 'commitDateTime': '2017-02-26 10:07:52', 'authoredDateTime': '2017-02-25 22:20:39', 'commitGitStats': [{'filePath': 'extensions/webdav/src/org/exist/webdav/ExistDocument.java', 'insertions': 4, 'deletions': 3, 'lines': 7}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'ExistDocument.java', 'spoonMethods': [{'spoonMethodName': 'org.exist.webdav.ExistDocument.resourceCopyMove(org.exist.xmldb.XmldbURI,java.lang.String,org.exist.webdav.Mode)', 'TOT': 6, 'UPD': 1, 'INS': 3, 'MOV': 0, 'DEL': 2}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/eXist-db/exist/issues/1293,4.000277777777778,"['bug', 'enhancement']",webdav.ExistDocument.resourceCopyMove takes wrong Lock for copy,1.0,"['org.exist.webdav.ExistDocument.resourceCopyMove(org.exist.xmldb.XmldbURI,java.lang.String,org.exist.webdav.Mode)']",['38f3a8626a28b6d929c2744f0d9c79955299ca37'],,['extensions/webdav/src/org/exist/webdav'],4.0,3.0,7.0,1.0,1.0,1.0,6.0,0.0,3.0,2.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,exist
27346,2017-02-13 12:43:10,hangduykhiem,"### Description

```
E/AndroidRuntime: FATAL EXCEPTION: main
    Process: com.facebook.samples.comparison, PID: 4529
    java.util.ConcurrentModificationException
       at java.util.HashMap$HashIterator.nextEntry(HashMap.java:851)
       at java.util.HashMap$ValueIterator.next(HashMap.java:879)
       at com.android.volley.toolbox.ImageLoader$4.run(ImageLoader.java:464)
       at android.os.Handler.handleCallback(Handler.java:751)
       at android.os.Handler.dispatchMessage(Handler.java:95)
       at android.os.Looper.loop(Looper.java:154)
       at android.app.ActivityThread.main(ActivityThread.java:6119)
       at java.lang.reflect.Method.invoke(Native Method)
       at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:886)
       at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:776)
```

### Reproduction

Select Volley and Network in the Comparison demo application 

### Additional Information

* Fresco version: 1.0.1
* Platform version: 5.1 - Xiaomi Redmi Note 3, 7.0 - Android Emulator
",2017-05-23 16:39:03,"[{'commitHash': 'ad5793109bc6ab6aa6fa8df50e7e491e41a732b5', 'commitGHEventType': 'referenced', 'commitUser': 'facebook-github-bot', 'commitParents': ['c75654ca2f743f8c9c8690bcb6c322f304cfcc84'], 'nameRev': 'ad5793109bc6ab6aa6fa8df50e7e491e41a732b5 tags/v1.4.0~101', 'commitMessage': ""#1664: Prevent ConcurrentModificationException in Volley\n\nSummary:\nIssue:\nhttps://github.com/facebook/fresco/issues/1664\n\nIt's because that _Volley_ notifies `listeners` inside a `for-loop` uses `mBatchedResponses.values()` :\n```\nfor (BatchedImageRequest bir : mBatchedResponses.values()) {\n  ...\n  mListener.onResponse(...)\n}\n```\nAfter some methods calling inside the `onResponse`, finally, the `mBatchedResponses.remove` will be called.\n\nThe stacktrace will look like:\n```\ninoke mBatchedResponses.remove(mCacheKey)\nat com.android.volley.toolbox.ImageLoader$ImageContainer.cancelRequest(ImageLoader.java:365)\nat com.facebook.drawee.backends.volley.VolleyDataSource.close(VolleyDataSource.java:66)\nat com.facebook.datasource.BaseDataSubscriber.onNewResult(BaseDataSubscriber.java:51)\nat com.facebook.datasource.AbstractDataSource$1.run(AbstractDataSource.java:181)\nat com.facebook.common.executors.UiThreadImmediateExecutorService.execute(UiThreadImmediateExecutorService.java:40)\nat com.facebook.datasource.AbstractDataSource.notifyDataSubscriber\nat com.facebook.datasource.AbstractDataSource.notifyDataSubscribers(AbstractDataSource.java:163)\nat com.facebook.datasource.AbstractDataSource.setResult(AbstractDataSource.java:215)\nat com.facebook.drawee.backends.volley.VolleyDataSource$1.onResponse\nat com.android.volley.toolbox.ImageLoader$4.run(ImageLoader.java:474)\n```\nThanks for submitting a PR! Please read these instructions carefully:\n\n- [x] Explain the **motivation** for making this change.\n- [x] Provide a **test plan** demonstrating that the code is solid.\n- [x] Match the **code formatting** of the rest of the codebase.\n- [x] Target the `master` branch\n\nFix issue: https://github.com/facebook/fresco/issues/1664\n```\njava.util.ConcurrentModificationException\n  at java.util.HashMap$HashIterator.nextEntry(HashMap.java:851)\n  at java.util.HashMap$ValueIterator.next(HashMap.java:879)\n  at com.android.volley.toolbox.ImageLoader$4.run(ImageLoader.java:464)\n  at android.os.Handler.handleCallback(Handler.java:751)\n```\n\n![5 -19-2017 20-38-34](https://cloud.githubusercontent.com/assets/520786/26248050/424afdea-3cd3-11e7-815b-517a1e468069.gif)\n\nSign the [CLA][2], if you haven't already.\n\nSmall pull requests are much easier to review and more likely to get merged. Make sure the PR does only one thing, otherwise please split it.\n\nMake sure all **tests pass** on [Circle CI][4]. PRs that break tests are unlikely to be merged.\n\nFor more info, see the [Contributing guide][4].\n\n[1]: https://medium.com/martinkonicek/what-is-a-test-plan-8bfc840ec171#.y9lcuqqi9\n[2]: https://code.facebook.com/cla\n[3]: http://circleci.com/gh/facebook/fresco\n[4]: https://github.com/facebook/fresco/blob/master/CONTRIBUTING.md\nCloses https://github.com/facebook/fresco/pull/1775\n\nReviewed By: lambdapioneer\n\nDifferential Revision: D5095502\n\nPulled By: kirwan\n\nfbshipit-source-id: 1531afa7852313436c1cdc52ebc3507d67d35c86\n"", 'commitDateTime': '2017-05-22 09:47:38', 'authoredDateTime': '2017-05-22 09:37:11', 'commitGitStats': [{'filePath': 'drawee-backends/drawee-volley/src/main/java/com/facebook/drawee/backends/volley/VolleyDataSource.java', 'insertions': 12, 'deletions': 1, 'lines': 13}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'VolleyDataSource.java', 'spoonMethods': [{'spoonMethodName': 'com.facebook.drawee.backends.volley.VolleyDataSource.close()', 'TOT': 5, 'UPD': 2, 'INS': 1, 'MOV': 2, 'DEL': 0}, {'spoonMethodName': 'com.facebook.drawee.backends.volley.VolleyDataSource', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/facebook/fresco/issues/1664,99.00027777777778,"['bug', 'starter-task']",Comparison demo crash when select Volley + network,1.0,"['com.facebook.drawee.backends.volley.VolleyDataSource', 'com.facebook.drawee.backends.volley.VolleyDataSource.close()']",['ad5793109bc6ab6aa6fa8df50e7e491e41a732b5'],,['drawee-backends/drawee-volley/src/main/java/com/facebook/drawee/backends/volley'],12.0,1.0,13.0,1.0,2.0,2.0,6.0,2.0,2.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,fresco
27366,2016-10-18 15:43:50,ffelini,"Hello. I'm getting a strange crash when loading a simple local gif.

10-18 18:00:57.522 8284-8284/com.goodrun.android.debug D/AndroidRuntime: Shutting down VM

```
FATAL EXCEPTION: main
 Process: com.goodrun.android.debug, PID: 8284
 java.lang.IndexOutOfBoundsException: Invalid index 1, size is 1
     at java.util.ArrayList.throwIndexOutOfBoundsException(ArrayList.java:255)
     at java.util.ArrayList.get(ArrayList.java:308)
     at com.facebook.drawee.controller.ForwardingControllerListener.onFinalImageSet(ForwardingControllerListener.java:91)
     at com.facebook.drawee.controller.AbstractDraweeController.onNewResultInternal(AbstractDraweeController.java:524)
     at com.facebook.drawee.controller.AbstractDraweeController.access$000(AbstractDraweeController.java:47)
     at com.facebook.drawee.controller.AbstractDraweeController$1.onNewResultImpl(AbstractDraweeController.java:469)
     at com.facebook.datasource.BaseDataSubscriber.onNewResult(BaseDataSubscriber.java:48)
     at com.facebook.datasource.AbstractDataSource$1.run(AbstractDataSource.java:181)
     at android.os.Handler.handleCallback(Handler.java:739)
     at android.os.Handler.dispatchMessage(Handler.java:95)
     at android.os.Looper.loop(Looper.java:148)
     at android.app.ActivityThread.main(ActivityThread.java:5417)
     at java.lang.reflect.Method.invoke(Native Method)
     at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:726)
     at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:616)
```",2016-12-14 15:12:47,"[{'commitHash': '0a140fc368830c5c978eb3c1b18e3c18b5b650b8', 'commitGHEventType': 'referenced', 'commitUser': 'facebook-github-bot', 'commitParents': ['11d01363746baf593d0e123d758a1f5c82a6db14'], 'nameRev': '0a140fc368830c5c978eb3c1b18e3c18b5b650b8 tags/v1.0.0~150', 'commitMessage': 'Mitigate crash in ForwardControllerListener caused by race condition\n\nSummary:\nIn #1504 a race condition is described that leads to a `OutOfBounds` in `ForwardingControllerListener`. It happens when a `ControllerListener` is removed while an event is propagated in the `for` loop of any of those methods.\n\nThis diffs mitigates the crash by moving the `get(i)` part inside `try-catch` block. By this we still have visibility when it happens, but it does not crash the app.\n\nReviewed By: oprisnik\n\nDifferential Revision: D4037029\n\nfbshipit-source-id: 6d110c2d2d2f32820579d4c998d4fcc33a41745b\n', 'commitDateTime': '2016-10-18 12:00:22', 'authoredDateTime': '2016-10-18 11:52:25', 'commitGitStats': [{'filePath': 'drawee/src/main/java/com/facebook/drawee/controller/ForwardingControllerListener.java', 'insertions': 6, 'deletions': 6, 'lines': 12}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'ForwardingControllerListener.java', 'spoonMethods': [{'spoonMethodName': 'com.facebook.drawee.controller.ForwardingControllerListener.onSubmit(java.lang.String,java.lang.Object)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 0}, {'spoonMethodName': 'com.facebook.drawee.controller.ForwardingControllerListener.onFinalImageSet(java.lang.String,java.lang.Object,android.graphics.drawable.Animatable)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 0}, {'spoonMethodName': 'com.facebook.drawee.controller.ForwardingControllerListener.onIntermediateImageSet(java.lang.String,java.lang.Object)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 0}, {'spoonMethodName': 'com.facebook.drawee.controller.ForwardingControllerListener.onIntermediateImageFailed(java.lang.String,java.lang.Throwable)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 0}, {'spoonMethodName': 'com.facebook.drawee.controller.ForwardingControllerListener.onFailure(java.lang.String,java.lang.Throwable)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 0}, {'spoonMethodName': 'com.facebook.drawee.controller.ForwardingControllerListener.onRelease(java.lang.String)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/facebook/fresco/issues/1504,56.000277777777775,['bug'],Race-condition leads to OutOfBounds in ForwardingControllerListener,1.0,"['com.facebook.drawee.controller.ForwardingControllerListener.onSubmit(java.lang.String,java.lang.Object)', 'com.facebook.drawee.controller.ForwardingControllerListener.onRelease(java.lang.String)', 'com.facebook.drawee.controller.ForwardingControllerListener.onFinalImageSet(java.lang.String,java.lang.Object,android.graphics.drawable.Animatable)', 'com.facebook.drawee.controller.ForwardingControllerListener.onFailure(java.lang.String,java.lang.Throwable)', 'com.facebook.drawee.controller.ForwardingControllerListener.onIntermediateImageFailed(java.lang.String,java.lang.Throwable)', 'com.facebook.drawee.controller.ForwardingControllerListener.onIntermediateImageSet(java.lang.String,java.lang.Object)']",['0a140fc368830c5c978eb3c1b18e3c18b5b650b8'],,['drawee/src/main/java/com/facebook/drawee/controller'],6.0,6.0,12.0,1.0,0.0,6.0,6.0,6.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,fresco
29127,2016-06-15 12:19:18,karussell,"I've stumbled over this problem only in the tests so far and it happens very rarely (maybe only on slow virtual computers?):

```
java.lang.AssertionError: Properties for fastest_mtb should NOT contain error CH preparation incomplete [5]
    at com.graphhopper.GraphHopperTest.testMultipleCHPreparationsInParallel(GraphHopperTest.java:894)
```
",2016-09-15 11:06:56,"[{'commitHash': '933de657986080e857436b10e22f5df3e8436790', 'commitGHEventType': 'closed', 'commitUser': 'karussell', 'commitParents': ['5b2662ec6e22af337518872d292bbfb69c67f20c'], 'nameRev': '933de657986080e857436b10e22f5df3e8436790 tags/0.8.0~44', 'commitMessage': 'make StorableProperties thread safe, fixes #743\n', 'commitDateTime': '2016-09-15 12:57:56', 'authoredDateTime': '2016-09-15 12:57:56', 'commitGitStats': [{'filePath': 'core/src/main/java/com/graphhopper/storage/StorableProperties.java', 'insertions': 21, 'deletions': 23, 'lines': 44}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'StorableProperties.java', 'spoonMethods': [{'spoonMethodName': 'com.graphhopper.storage.StorableProperties.copyTo(com.graphhopper.storage.StorableProperties)', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'com.graphhopper.storage.StorableProperties.putAll(java.util.Map)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.graphhopper.storage.StorableProperties.loadExisting()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.graphhopper.storage.StorableProperties.flush()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.graphhopper.storage.StorableProperties.remove(java.lang.String)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.graphhopper.storage.StorableProperties.put(java.lang.String,java.lang.String)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.graphhopper.storage.StorableProperties.put(java.lang.String,java.lang.Object)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.graphhopper.storage.StorableProperties.get(java.lang.String)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.graphhopper.storage.StorableProperties.close()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.graphhopper.storage.StorableProperties.isClosed()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.graphhopper.storage.StorableProperties.create(long)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.graphhopper.storage.StorableProperties.getCapacity()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.graphhopper.storage.StorableProperties.putCurrentVersions()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.graphhopper.storage.StorableProperties.versionsToString()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.graphhopper.storage.StorableProperties.checkVersions(boolean)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.graphhopper.storage.StorableProperties.toString()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/graphhopper/graphhopper/issues/743,91.00027777777778,['bug'],CH Preparation sometimes not complete for tests,1.0,"['com.graphhopper.storage.StorableProperties.create(long)', 'com.graphhopper.storage.StorableProperties.versionsToString()', 'com.graphhopper.storage.StorableProperties.checkVersions(boolean)', 'com.graphhopper.storage.StorableProperties.toString()', 'com.graphhopper.storage.StorableProperties.remove(java.lang.String)', 'com.graphhopper.storage.StorableProperties.putAll(java.util.Map)', 'com.graphhopper.storage.StorableProperties.flush()', 'com.graphhopper.storage.StorableProperties.isClosed()', 'com.graphhopper.storage.StorableProperties.put(java.lang.String,java.lang.Object)', 'com.graphhopper.storage.StorableProperties.copyTo(com.graphhopper.storage.StorableProperties)', 'com.graphhopper.storage.StorableProperties.putCurrentVersions()', 'com.graphhopper.storage.StorableProperties.get(java.lang.String)', 'com.graphhopper.storage.StorableProperties.put(java.lang.String,java.lang.String)', 'com.graphhopper.storage.StorableProperties.getCapacity()', 'com.graphhopper.storage.StorableProperties.close()', 'com.graphhopper.storage.StorableProperties.loadExisting()']",['933de657986080e857436b10e22f5df3e8436790'],,['core/src/main/java/com/graphhopper/storage'],21.0,23.0,44.0,1.0,0.0,16.0,17.0,1.0,15.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,graphhopper
29203,2020-02-28 15:16:22,karussell,"Error:
```
2020-02-28 15:25:52.374 [dw-4182 - GET /route?turn_costs=true&type=json&locale=en_US&weighting=fastest&<redacted>&vehicle=car] ERROR i.d.j.errors.LoggingExceptionMapper - Error handling a request: fec09cafb194a822
java.lang.RuntimeException: Time was negative -9223372036852055154 for index 0. Please report as bug and include:<redacted> (Hints:{turn_costs=true, instructions=true, edge_based=true, way_point_max_distance=1.0, calc_points=true, type=json, locale=en_US, weighting=fastest, vehicle=car})
        at com.graphhopper.routing.template.ViaRoutingTemplate.calcPaths(ViaRoutingTemplate.java:182)
        at com.graphhopper.GraphHopper.calcPaths(GraphHopper.java:1087)
        at com.graphhopper.GraphHopper.route(GraphHopper.java:962)
        at com.graphhopper.resources.RouteResource.doGet(RouteResource.java:143)
```

Unfortunately I cannot reproduce this problem on the server using the still running same version and the exact same request parameters (14 via points in the Netherlands, point 3 == point 4)

There are more of these bugs in the last days but none of them is reproducible. The smallest one still had 2 via points. But it only occurred for the edged based CH it seems.",2020-02-29 20:07:54,"[{'commitHash': '8d1570839c9547ab11f2e4c5fd068f29473a3631', 'commitGHEventType': 'referenced', 'commitUser': 'karussell', 'commitParents': ['7d1f8c855945f93a38f587a1bdbb4b1982e138f7'], 'nameRev': '8d1570839c9547ab11f2e4c5fd068f29473a3631 tags/1.0-pre26~4', 'commitMessage': 'make readFlags private, remove unnecessary int set and remove QueryGraph.getOriginalEdgeFromVirtNode, #1938\n', 'commitDateTime': '2020-02-29 15:23:08', 'authoredDateTime': '2020-02-29 15:22:57', 'commitGitStats': [{'filePath': 'core/src/main/java/com/graphhopper/routing/querygraph/QueryGraph.java', 'insertions': 0, 'deletions': 4, 'lines': 4}, {'filePath': 'core/src/main/java/com/graphhopper/storage/TurnCostStorage.java', 'insertions': 1, 'deletions': 3, 'lines': 4}, {'filePath': 'reader-osm/src/test/java/com/graphhopper/reader/osm/OSMReaderTest.java', 'insertions': 61, 'deletions': 57, 'lines': 118}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'QueryGraph.java', 'spoonMethods': [{'spoonMethodName': 'com.graphhopper.routing.querygraph.QueryGraph.getOriginalEdgeFromVirtNode(int)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}]}, {'spoonFilePath': 'TurnCostStorage.java', 'spoonMethods': [{'spoonMethodName': 'com.graphhopper.storage.TurnCostStorage.readFlags(com.graphhopper.storage.IntsRef,int,int,int)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.graphhopper.storage.TurnCostStorage.get(com.graphhopper.routing.profiles.DecimalEncodedValue,com.graphhopper.storage.IntsRef,int,int,int)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}]}, {'spoonFilePath': 'OSMReaderTest.java', 'spoonMethods': [{'spoonMethodName': 'com.graphhopper.reader.osm.OSMReaderTest.testTurnFlagCombination()', 'TOT': 24, 'UPD': 9, 'INS': 6, 'MOV': 6, 'DEL': 3}, {'spoonMethodName': 'com.graphhopper.reader.osm.OSMReaderTest.testConditionalTurnRestriction()', 'TOT': 110, 'UPD': 22, 'INS': 0, 'MOV': 88, 'DEL': 0}, {'spoonMethodName': 'com.graphhopper.reader.osm.OSMReaderTest.testMultipleTurnRestrictions()', 'TOT': 40, 'UPD': 8, 'INS': 0, 'MOV': 32, 'DEL': 0}, {'spoonMethodName': 'com.graphhopper.reader.osm.OSMReaderTest.testTurnRestrictions()', 'TOT': 78, 'UPD': 14, 'INS': 2, 'MOV': 61, 'DEL': 1}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': 'a14e7a96c7117b2b9397500168c4552ae04bb3cb', 'commitGHEventType': 'closed', 'commitUser': 'karussell', 'commitParents': ['9129481617879637aac338b6f530f9ac2268ec6f'], 'nameRev': 'a14e7a96c7117b2b9397500168c4552ae04bb3cb tags/1.0-pre26~2', 'commitMessage': 'make DefaultTurnCostProvider thread-safe, fixes #1938\n', 'commitDateTime': '2020-02-29 20:53:50', 'authoredDateTime': '2020-02-29 20:53:12', 'commitGitStats': [{'filePath': 'core/src/main/java/com/graphhopper/routing/weighting/DefaultTurnCostProvider.java', 'insertions': 1, 'deletions': 2, 'lines': 3}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'DefaultTurnCostProvider.java', 'spoonMethods': [{'spoonMethodName': 'com.graphhopper.routing.weighting.DefaultTurnCostProvider', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'com.graphhopper.routing.weighting.DefaultTurnCostProvider.calcTurnWeight(int,int,int)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/graphhopper/graphhopper/issues/1938,1.0002777777777778,['critical bug'],RuntimeException: Time was negative,2.0,"['com.graphhopper.routing.querygraph.QueryGraph.getOriginalEdgeFromVirtNode(int)', 'com.graphhopper.routing.weighting.DefaultTurnCostProvider', 'com.graphhopper.routing.weighting.DefaultTurnCostProvider.calcTurnWeight(int,int,int)', 'com.graphhopper.storage.TurnCostStorage.readFlags(com.graphhopper.storage.IntsRef,int,int,int)', 'com.graphhopper.storage.TurnCostStorage.get(com.graphhopper.routing.profiles.DecimalEncodedValue,com.graphhopper.storage.IntsRef,int,int,int)']","['8d1570839c9547ab11f2e4c5fd068f29473a3631', 'a14e7a96c7117b2b9397500168c4552ae04bb3cb']",,"['core/src/main/java/com/graphhopper/storage', 'core/src/main/java/com/graphhopper/routing/weighting', 'core/src/main/java/com/graphhopper/routing/querygraph']",2.0,9.0,11.0,3.0,1.0,5.0,6.0,1.0,0.0,4.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,graphhopper
29295,2020-01-14 11:45:22,avgustinmm,"### What version of gRPC-Java are you using?
1.26.0
### What is your environment?
Linux, Alpine
openjdk version ""1.8.0_171""
OpenJDK Runtime Environment (IcedTea 3.8.0) (Alpine 8.171.11-r0)
OpenJDK 64-Bit Server VM (build 25.171-b11, mixed mode)
Netty - 4.1.44.Final
Vertx & Vertx-gRPC - 3.8.4
### What did you expect to see?
Proper start of gRPC Server
### What did you see instead?
Start sometimes hangs with deadlock
### Steps to reproduce the bug
[int_threaddump.txt](https://github.com/grpc/grpc-java/files/4058905/int_threaddump.txt)

I suppose it's a race condition related to synchronization in gRPC (ServerImpl), await in NetServer.start and vertx/nettty event loops (probably single threaded). Probably it could happened at any time if someone start gRPC server and concurrently open new client connection to that server.
In my case I stopped and started the gGPR server but I'm not sure if it is somehow related.

### Analysis
What I see in the thread dump is the following 2 threads that stays in that state, seems, forever:
>
""vert.x-eventloop-thread-0"" #39 prio=10 os_prio=0 tid=0x000055711e379000 nid=0x2d waiting for monitor entry [0x00007fb72abc8000]
java.lang.Thread.State: BLOCKED (on object monitor)
  at io.grpc.internal.ServerImpl$ServerListenerImpl.transportCreated(ServerImpl.java:379)
  \- waiting to lock <0x00000000c559f1a0> (a java.lang.Object)
  at io.grpc.netty.NettyServer$1.initChannel(NettyServer.java:224)
  \- locked <0x00000000c559bfd8> (a io.grpc.netty.NettyServer)
  at io.netty.channel.ChannelInitializer.initChannel(ChannelInitializer.java:129)

""vert.x-worker-thread-12"" #214 prio=10 os_prio=0 tid=0x000055711f2d1800 nid=0x418 in Object.wait() [0x00007fb720656000]
java.lang.Thread.State: WAITING (on object monitor)
  at java.lang.Object.wait(Native Method)
  at java.lang.Object.wait(Object.java:502)
  at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:252)
  \- locked <0x00000000c589baa0> (a io.netty.util.concurrent.PromiseTask)
  at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:35)
  at io.grpc.netty.NettyServer.start(NettyServer.java:269)
  at io.grpc.internal.ServerImpl.start(ServerImpl.java:184)
  \- locked <0x00000000c559f1a0> (a java.lang.Object)
  at io.grpc.internal.ServerImpl.start(ServerImpl.java:90)
>
From what I see in these thread dumps and the code I think that this could be the problem (deadlock):
1. Vertx grpc starts server (ServerImpl.start) in vertx blocking thread
2. ServerImpl synchronize on lock and then try (keeping lock) to start server (NetServer.start)
3. NetServer.start opens a channel, binds to it, and since that moment it, I assume, may receive connections from remote clients
4. It seems, at this time a remote client opens connection to this server (already bound)
5. Then in channel's event loop (probably single threaded) is received initChannel which try to get ServerImpl.lock in ServerListenerImpl.transportCreated (coudln't because got by ServerImpl.start)
6. NetServer.start then schedules runnable in channel's event loop and blocks with channelzFuture.await() 
7. Now, channelzFuture.await() waits for a runnable to be executed in channel's event loop (probably single threaded)
8. At this point channelzFuture.await keeps ServerImpl.lock lock, while the ServerListenerImpl.transportCreated occupies/blocks (this is what I suppose) the single threaded channel's event loop thus making impossible to process further

I'm attaching file with thread dumps of the whole JVM",2020-01-24 22:59:16,"[{'commitHash': 'b8474d61c966f60cc0bad4e5abaca351e90afc9d', 'commitGHEventType': 'referenced', 'commitUser': 'dapengzhang0', 'commitParents': ['cb4a7fb2de6d2e5aa58e2db097d7fd0b69afef27'], 'nameRev': 'b8474d61c966f60cc0bad4e5abaca351e90afc9d tags/v1.28.0~81', 'commitMessage': 'netty: fix a race for channelz at server transport creation\n\nA race condition was reported by user in #6601:\r\n\r\n`ServerImpl.start()` calls `NettyServer.start()` while holding `ServerImpl.lock`. `NettyServer.start()` awaits a submitted runnable in eventloop. However, this pending runnable may never be executed because the eventloop might be executing some other task, like `ServerListenerImpl.transportCreated()`, that is trying to acquire `ServerImpl.lock` causing a deadlock.\r\n\r\nThis PR resolves the particular issue reported in #6601 for server with a single port, but `NettyServer` (https://github.com/grpc/grpc-java/blob/v1.26.0/netty/src/main/java/io/grpc/netty/NettyServer.java#L251) and `ServerImpl` (https://github.com/grpc/grpc-java/blob/v1.26.0/core/src/main/java/io/grpc/internal/ServerImpl.java#L184) in general still have the same potential risk of deadlock, which need further fix. \r\n', 'commitDateTime': '2020-01-16 11:53:37', 'authoredDateTime': '2020-01-16 11:53:37', 'commitGitStats': [{'filePath': 'netty/src/main/java/io/grpc/netty/NettyServer.java', 'insertions': 8, 'deletions': 15, 'lines': 23}, {'filePath': 'netty/src/test/java/io/grpc/netty/NettyServerTest.java', 'insertions': 32, 'deletions': 14, 'lines': 46}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'NettyServer.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.netty.NettyServer', 'TOT': 4, 'UPD': 3, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'io.grpc.netty.NettyServer.getListenSocketStats()', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'io.grpc.netty.NettyServer.start(io.grpc.internal.ServerListener).2.run()', 'TOT': 6, 'UPD': 0, 'INS': 1, 'MOV': 2, 'DEL': 3}, {'spoonMethodName': 'io.grpc.netty.NettyServer.start(io.grpc.internal.ServerListener)', 'TOT': 5, 'UPD': 0, 'INS': 1, 'MOV': 2, 'DEL': 2}, {'spoonMethodName': 'io.grpc.netty.NettyServer.shutdown().3.operationComplete(io.netty.channel.ChannelFuture)', 'TOT': 5, 'UPD': 0, 'INS': 2, 'MOV': 2, 'DEL': 1}]}, {'spoonFilePath': 'NettyServerTest.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.netty.NettyServerTest.startStop()', 'TOT': 8, 'UPD': 2, 'INS': 3, 'MOV': 1, 'DEL': 2}, {'spoonMethodName': 'io.grpc.netty.NettyServerTest.getPort_notStarted()', 'TOT': 17, 'UPD': 2, 'INS': 4, 'MOV': 8, 'DEL': 3}, {'spoonMethodName': 'io.grpc.netty.NettyServerTest.childChannelOptions()', 'TOT': 8, 'UPD': 2, 'INS': 3, 'MOV': 1, 'DEL': 2}, {'spoonMethodName': 'io.grpc.netty.NettyServerTest.channelzListenSocket()', 'TOT': 9, 'UPD': 2, 'INS': 4, 'MOV': 1, 'DEL': 2}, {'spoonMethodName': 'io.grpc.netty.NettyServerTest', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.grpc.netty.NettyServerTest.tearDown()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '8c346d00e573c4247427e64be0ce47fe9f75ac84', 'commitGHEventType': 'referenced', 'commitUser': 'dapengzhang0', 'commitParents': ['5acb70e8b4815368b47ed46c583af531af7af158'], 'nameRev': '8c346d00e573c4247427e64be0ce47fe9f75ac84 tags/v1.27.0~2', 'commitMessage': 'netty: fix a race for channelz at server transport creation\n\nA race condition was reported by user in #6601:\r\n\r\n`ServerImpl.start()` calls `NettyServer.start()` while holding `ServerImpl.lock`. `NettyServer.start()` awaits a submitted runnable in eventloop. However, this pending runnable may never be executed because the eventloop might be executing some other task, like `ServerListenerImpl.transportCreated()`, that is trying to acquire `ServerImpl.lock` causing a deadlock.\r\n\r\nThis PR resolves the particular issue reported in #6601 for server with a single port, but `NettyServer` (https://github.com/grpc/grpc-java/blob/v1.26.0/netty/src/main/java/io/grpc/netty/NettyServer.java#L251) and `ServerImpl` (https://github.com/grpc/grpc-java/blob/v1.26.0/core/src/main/java/io/grpc/internal/ServerImpl.java#L184) in general still have the same potential risk of deadlock, which need further fix. \r\n', 'commitDateTime': '2020-01-21 16:56:02', 'authoredDateTime': '2020-01-16 11:53:37', 'commitGitStats': [{'filePath': 'netty/src/main/java/io/grpc/netty/NettyServer.java', 'insertions': 8, 'deletions': 15, 'lines': 23}, {'filePath': 'netty/src/test/java/io/grpc/netty/NettyServerTest.java', 'insertions': 32, 'deletions': 14, 'lines': 46}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'NettyServer.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.netty.NettyServer', 'TOT': 4, 'UPD': 3, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'io.grpc.netty.NettyServer.getListenSocketStats()', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'io.grpc.netty.NettyServer.start(io.grpc.internal.ServerListener).2.run()', 'TOT': 6, 'UPD': 0, 'INS': 1, 'MOV': 2, 'DEL': 3}, {'spoonMethodName': 'io.grpc.netty.NettyServer.start(io.grpc.internal.ServerListener)', 'TOT': 5, 'UPD': 0, 'INS': 1, 'MOV': 2, 'DEL': 2}, {'spoonMethodName': 'io.grpc.netty.NettyServer.shutdown().3.operationComplete(io.netty.channel.ChannelFuture)', 'TOT': 5, 'UPD': 0, 'INS': 2, 'MOV': 2, 'DEL': 1}]}, {'spoonFilePath': 'NettyServerTest.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.netty.NettyServerTest.startStop()', 'TOT': 8, 'UPD': 2, 'INS': 3, 'MOV': 1, 'DEL': 2}, {'spoonMethodName': 'io.grpc.netty.NettyServerTest.getPort_notStarted()', 'TOT': 17, 'UPD': 2, 'INS': 4, 'MOV': 8, 'DEL': 3}, {'spoonMethodName': 'io.grpc.netty.NettyServerTest.childChannelOptions()', 'TOT': 8, 'UPD': 2, 'INS': 3, 'MOV': 1, 'DEL': 2}, {'spoonMethodName': 'io.grpc.netty.NettyServerTest.channelzListenSocket()', 'TOT': 9, 'UPD': 2, 'INS': 4, 'MOV': 1, 'DEL': 2}, {'spoonMethodName': 'io.grpc.netty.NettyServerTest', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.grpc.netty.NettyServerTest.tearDown()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/grpc/grpc-java/issues/6601,10.000277777777777,['bug'],Deadlock on start gRPC server,1.0,"['io.grpc.netty.NettyServer.shutdown().3.operationComplete(io.netty.channel.ChannelFuture)', 'io.grpc.netty.NettyServer.start(io.grpc.internal.ServerListener)', 'io.grpc.netty.NettyServer.start(io.grpc.internal.ServerListener).2.run()', 'io.grpc.netty.NettyServer', 'io.grpc.netty.NettyServer.getListenSocketStats()']",['b8474d61c966f60cc0bad4e5abaca351e90afc9d'],,['netty/src/main/java/io/grpc/netty'],8.0,15.0,23.0,1.0,3.0,5.0,22.0,7.0,4.0,8.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,grpc-java
29324,2019-05-13 14:06:23,rsgowman,"Please answer these questions before submitting your issue.

### What version of gRPC are you using?
1.16.1 (but also appears to be present in master)


### What did you expect to see?
No NullPointerException in DelayedClientTransport.java

### Description
We (Firestore) have had a few reports of a NPE within DelayedClientTransport.java. (https://github.com/firebase/firebase-android-sdk/issues/428).

Looking at the stacktrace, it seems as though `picker` is null here: https://github.com/grpc/grpc-java/blob/57043233bf5aecce92f0c6629b6ac46d9393ce8c/core/src/main/java/io/grpc/internal/DelayedClientTransport.java#L149

Examining that function further, it seems as though picker is set here and here:
https://github.com/grpc/grpc-java/blob/57043233bf5aecce92f0c6629b6ac46d9393ce8c/core/src/main/java/io/grpc/internal/DelayedClientTransport.java#L139-L142
https://github.com/grpc/grpc-java/blob/57043233bf5aecce92f0c6629b6ac46d9393ce8c/core/src/main/java/io/grpc/internal/DelayedClientTransport.java#L165

In the first case, there's a null check that protects it, but there isn't in the second case. Should there be?

(All links to the 1.16.1 tag, but the code looks effectively the same in master wrt this issue.)

",2019-05-15 23:56:47,"[{'commitHash': 'f3bf250a46571db3321fc3078b55e74280bdfd87', 'commitGHEventType': 'referenced', 'commitUser': 'ejona86', 'commitParents': ['8fdd2072eeaf393f76b40668a60b353abde8758f'], 'nameRev': 'f3bf250a46571db3321fc3078b55e74280bdfd87 tags/v1.22.0~100', 'commitMessage': 'core: Handle IDLE MODE race in DelayedClientTransport\n\nWe check for idle mode the first time we try newStream(), but failed to when\nnewStream races with reprocess(). This would normally be a very rare race,\nexcept when you consider that AbstractChannelBuilder will call\nmanagedChannel.enterIdle() when the network changes.\n\nFixes #5729\n', 'commitDateTime': '2019-05-15 16:56:46', 'authoredDateTime': '2019-05-15 16:28:07', 'commitGitStats': [{'filePath': 'core/src/main/java/io/grpc/internal/DelayedClientTransport.java', 'insertions': 10, 'deletions': 17, 'lines': 27}, {'filePath': 'core/src/test/java/io/grpc/internal/DelayedClientTransportTest.java', 'insertions': 20, 'deletions': 0, 'lines': 20}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'DelayedClientTransport.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.internal.DelayedClientTransport.newStream(io.grpc.MethodDescriptor,io.grpc.Metadata,io.grpc.CallOptions)', 'TOT': 6, 'UPD': 0, 'INS': 2, 'MOV': 3, 'DEL': 1}]}, {'spoonFilePath': 'DelayedClientTransportTest.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.internal.DelayedClientTransportTest.newStream_racesWithReprocessIdleMode()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '8225aa5218ae3d3f1d7f17744e8681f69c73e84e', 'commitGHEventType': 'referenced', 'commitUser': 'ejona86', 'commitParents': ['7ba3a589b5f6ec965342f32935f6c5bcb1ddc090'], 'nameRev': '8225aa5218ae3d3f1d7f17744e8681f69c73e84e tags/v1.21.0~5', 'commitMessage': 'core: Handle IDLE MODE race in DelayedClientTransport\n\nWe check for idle mode the first time we try newStream(), but failed to when\nnewStream races with reprocess(). This would normally be a very rare race,\nexcept when you consider that AbstractChannelBuilder will call\nmanagedChannel.enterIdle() when the network changes.\n\nFixes #5729\n', 'commitDateTime': '2019-05-16 14:30:10', 'authoredDateTime': '2019-05-15 16:28:07', 'commitGitStats': [{'filePath': 'core/src/main/java/io/grpc/internal/DelayedClientTransport.java', 'insertions': 10, 'deletions': 17, 'lines': 27}, {'filePath': 'core/src/test/java/io/grpc/internal/DelayedClientTransportTest.java', 'insertions': 20, 'deletions': 0, 'lines': 20}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'DelayedClientTransport.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.internal.DelayedClientTransport.newStream(io.grpc.MethodDescriptor,io.grpc.Metadata,io.grpc.CallOptions)', 'TOT': 6, 'UPD': 0, 'INS': 2, 'MOV': 3, 'DEL': 1}]}, {'spoonFilePath': 'DelayedClientTransportTest.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.internal.DelayedClientTransportTest.newStream_racesWithReprocessIdleMode()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/grpc/grpc-java/issues/5729,2.000277777777778,['bug'],NPE in DelayedClientTransport,1.0,"['io.grpc.internal.DelayedClientTransport.newStream(io.grpc.MethodDescriptor,io.grpc.Metadata,io.grpc.CallOptions)']",['f3bf250a46571db3321fc3078b55e74280bdfd87'],,['core/src/main/java/io/grpc/internal'],10.0,17.0,27.0,1.0,0.0,1.0,6.0,3.0,2.0,1.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,grpc-java
29454,2016-01-20 21:42:20,ejona86,"We aren't reporting a helpful error in very simple connectivity cases, like the server port is closed. Here is what it looks like when using the interop client against a non-existent server: 

```
$ ./build/install/grpc-interop-testing/bin/test-client
Running test empty_unary
...
Exception in thread ""main"" io.grpc.StatusRuntimeException: UNAVAILABLE
        at io.grpc.Status.asRuntimeException(Status.java:431)
        at io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:157)
        at io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:106)
        at io.grpc.testing.integration.TestServiceGrpc$TestServiceBlockingStub.emptyCall(TestServiceGrpc.java:212)
        at io.grpc.testing.integration.AbstractTransportTest.emptyUnary(AbstractTransportTest.java:164)
        at io.grpc.testing.integration.TestServiceClient.runTest(TestServiceClient.java:220)
        at io.grpc.testing.integration.TestServiceClient.run(TestServiceClient.java:209)
        at io.grpc.testing.integration.TestServiceClient.main(TestServiceClient.java:80)
Caused by: java.nio.channels.ClosedChannelException
```

It seems the problem may also impact other, much harder to diagnose, issues like client certificate validation (like in #1327).
",2016-09-12 17:17:34,"[{'commitHash': '1170afd16844bfe8654dd8634ff026d02e77baa0', 'commitGHEventType': 'closed', 'commitUser': 'ejona86', 'commitParents': ['72f1e88d78ffd598021930652142165fd4b5b25f'], 'nameRev': '1170afd16844bfe8654dd8634ff026d02e77baa0 tags/v0.14.0~129', 'commitMessage': ""Add transport test for Netty\n\nNetty client shutdown would race with the negotiation handling and\ncircumvent AbstractBufferingHandler. Use a new command in order to\nleave channel.close() available for abrupt killing of the connection\nwhen connecting.\n\nping_afterTermination was previously racey that made it succeed. After\nfixing the test, Netty would consistently fail to call callback. After\nfixing Netty to fail the callback it was not using the right status\nbecause when Netty's channel is closed none of our handlers are run.\nThis reliably fails the future with ClosedChannelException, which is\nuseless, so now we special-case that exception and fill in the reason\nfor shutdown.\n\nTo prevent accidentally reporting Status.OK, the transports no longer\nuse OK when calling transportShutdown. The OK status was already no\nlonger being consumed, since keying off whether transportReady was\ncalled is more helpful.\n\nThis fixes #1330\n"", 'commitDateTime': '2016-03-01 17:43:30', 'authoredDateTime': '2016-02-08 11:17:42', 'commitGitStats': [{'filePath': 'core/src/main/java/io/grpc/inprocess/InProcessTransport.java', 'insertions': 1, 'deletions': 1, 'lines': 2}, {'filePath': 'core/src/main/java/io/grpc/internal/DelayedClientTransport.java', 'insertions': 1, 'deletions': 1, 'lines': 2}, {'filePath': 'core/src/main/java/io/grpc/internal/ManagedClientTransport.java', 'insertions': 0, 'deletions': 1, 'lines': 1}, {'filePath': 'netty/src/main/java/io/grpc/netty/GracefulCloseCommand.java', 'insertions': 35, 'deletions': 0, 'lines': 35}, {'filePath': 'netty/src/main/java/io/grpc/netty/NettyClientHandler.java', 'insertions': 27, 'deletions': 11, 'lines': 38}, {'filePath': 'netty/src/main/java/io/grpc/netty/NettyClientStream.java', 'insertions': 8, 'deletions': 2, 'lines': 10}, {'filePath': 'netty/src/main/java/io/grpc/netty/NettyClientTransport.java', 'insertions': 49, 'deletions': 8, 'lines': 57}, {'filePath': 'netty/src/main/java/io/grpc/netty/Utils.java', 'insertions': 7, 'deletions': 0, 'lines': 7}, {'filePath': 'netty/src/test/java/io/grpc/netty/NettyClientStreamTest.java', 'insertions': 16, 'deletions': 2, 'lines': 18}, {'filePath': 'netty/src/test/java/io/grpc/netty/NettyTransportTest.java', 'insertions': 89, 'deletions': 0, 'lines': 89}, {'filePath': 'testing/src/main/java/io/grpc/internal/testing/AbstractTransportTest.java', 'insertions': 23, 'deletions': 1, 'lines': 24}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'InProcessTransport.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.inprocess.InProcessTransport.shutdown()', 'TOT': 3, 'UPD': 1, 'INS': 0, 'MOV': 1, 'DEL': 1}]}, {'spoonFilePath': 'DelayedClientTransport.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.internal.DelayedClientTransport.shutdown()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'ManagedClientTransport.java', 'spoonMethods': []}, {'spoonFilePath': 'GracefulCloseCommand.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.netty.GracefulCloseCommand', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'NettyClientHandler.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.netty.NettyClientHandler.sendPingFrame(io.netty.channel.ChannelHandlerContext,io.grpc.netty.SendPingCommand,io.netty.channel.ChannelPromise)', 'TOT': 8, 'UPD': 2, 'INS': 3, 'MOV': 2, 'DEL': 1}, {'spoonMethodName': 'io.grpc.netty.NettyClientHandler.close(io.netty.channel.ChannelHandlerContext,io.netty.channel.ChannelPromise)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.grpc.netty.NettyClientHandler.write(io.netty.channel.ChannelHandlerContext,java.lang.Object,io.netty.channel.ChannelPromise)', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 0}, {'spoonMethodName': 'io.grpc.netty.NettyClientHandler.createStream(io.grpc.netty.CreateStreamCommand,io.netty.channel.ChannelPromise)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 0}]}, {'spoonFilePath': 'NettyClientStream.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.netty.NettyClientStream.start(io.grpc.internal.ClientStreamListener).1.operationComplete(io.netty.channel.ChannelFuture)', 'TOT': 4, 'UPD': 1, 'INS': 1, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'io.grpc.netty.NettyClientStream', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.grpc.netty.NettyClientStream.statusFromFailedFuture(io.netty.channel.ChannelFuture)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'NettyClientTransport.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.netty.NettyClientTransport.start(io.grpc.netty.Listener).2', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.grpc.netty.NettyClientTransport.start(io.grpc.netty.Listener).3', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.grpc.netty.NettyClientTransport.shutdown()', 'TOT': 4, 'UPD': 3, 'INS': 0, 'MOV': 1, 'DEL': 0}, {'spoonMethodName': 'io.grpc.netty.NettyClientTransport.start(io.grpc.netty.Listener).1', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.grpc.netty.NettyClientTransport.start(io.grpc.netty.Listener).3.operationComplete(io.netty.channel.ChannelFuture)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.grpc.netty.NettyClientTransport.start(io.grpc.netty.Listener).2.operationComplete(io.netty.channel.ChannelFuture)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.grpc.netty.NettyClientTransport.newStream(io.grpc.MethodDescriptor,io.grpc.Metadata)', 'TOT': 7, 'UPD': 0, 'INS': 1, 'MOV': 5, 'DEL': 1}, {'spoonMethodName': 'io.grpc.netty.NettyClientTransport.notifyShutdown(io.grpc.Status)', 'TOT': 6, 'UPD': 0, 'INS': 2, 'MOV': 3, 'DEL': 1}, {'spoonMethodName': 'io.grpc.netty.NettyClientTransport', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.grpc.netty.NettyClientTransport.statusFromFailedFuture(io.netty.channel.ChannelFuture)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.grpc.netty.NettyClientTransport.ping(io.grpc.netty.PingCallback,java.util.concurrent.Executor)', 'TOT': 5, 'UPD': 0, 'INS': 4, 'MOV': 1, 'DEL': 0}]}, {'spoonFilePath': 'Utils.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.netty.Utils.statusFromThrowable(java.lang.Throwable)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'NettyClientStreamTest.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.netty.NettyClientStreamTest.setHttp2StreamShouldNotifyReady()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.grpc.netty.NettyClientStreamTest.createStream()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.grpc.netty.NettyClientStreamTest.NettyClientStreamImpl', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'NettyTransportTest.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.netty.NettyTransportTest', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'AbstractTransportTest.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.internal.testing.AbstractTransportTest.newStream_afterTermination()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.grpc.internal.testing.AbstractTransportTest.clientStartStop()', 'TOT': 3, 'UPD': 0, 'INS': 2, 'MOV': 1, 'DEL': 0}, {'spoonMethodName': 'io.grpc.internal.testing.AbstractTransportTest.ping_duringShutdown()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.grpc.internal.testing.AbstractTransportTest.ping_afterTermination()', 'TOT': 2, 'UPD': 0, 'INS': 2, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.grpc.internal.testing.AbstractTransportTest.newStream_duringShutdown()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '7504b79f0083db84cd1b2bd141603e20d70d2a16', 'commitGHEventType': 'referenced', 'commitUser': 'buchgr', 'commitParents': ['d9001ca4488ba48cf8f732c5fbe59386f55833dc'], 'nameRev': '7504b79f0083db84cd1b2bd141603e20d70d2a16 tags/v1.0.0-pre1~22', 'commitMessage': 'netty: add logging to find cause of ClosedChannelException.\n\n- See #1330 for details.\r\n- Also, Netty 4.1.2.Final comes with additional information where\r\na (usually statically cached) ClosedChannelException was triggered [1]\r\n\r\n[1] https://github.com/netty/netty@e845670#diff-8c9a7d1d9b0fdb6c5aef2794d05a9f88', 'commitDateTime': '2016-07-09 14:50:39', 'authoredDateTime': '2016-07-09 14:50:39', 'commitGitStats': [{'filePath': 'testing/src/main/java/io/grpc/internal/testing/AbstractTransportTest.java', 'insertions': 14, 'deletions': 3, 'lines': 17}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AbstractTransportTest.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.internal.testing.AbstractTransportTest.assertCodeEquals(io.grpc.Status,io.grpc.Status)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.grpc.internal.testing.AbstractTransportTest.ping_afterTermination()', 'TOT': 3, 'UPD': 0, 'INS': 3, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.grpc.internal.testing.AbstractTransportTest.assertCodeEquals(java.lang.String,io.grpc.Status,io.grpc.Status)', 'TOT': 2, 'UPD': 0, 'INS': 2, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '4aadf550ee6b4f6c3a7f63bb7a295cbdfb9b12d9', 'commitGHEventType': 'referenced', 'commitUser': 'buchgr', 'commitParents': ['8c18a0d35589f21678f614361a9ec1ba82794e13'], 'nameRev': '4aadf550ee6b4f6c3a7f63bb7a295cbdfb9b12d9 tags/v1.1.0~205', 'commitMessage': ""netty: Fix receipt of ClosedChannelException instead of actual error. Fixes #1330.\n\n\r\nOur API allows pings to be send even after the transport has been shutdown. We currently\r\ndon't handle the case, where the Netty channel has been closed but the NettyClientHandler\r\nhas not yet been removed from the pipeline, correctly. That is, we need to query the shutdown\r\nstatus whenever we receive a ClosedChannelException.\r\n\r\nAlso, some cleanup."", 'commitDateTime': '2016-09-12 19:17:34', 'authoredDateTime': '2016-09-12 19:17:34', 'commitGitStats': [{'filePath': 'core/src/main/java/io/grpc/internal/Http2Ping.java', 'insertions': 1, 'deletions': 1, 'lines': 2}, {'filePath': 'netty/src/main/java/io/grpc/netty/NettyClientHandler.java', 'insertions': 10, 'deletions': 1, 'lines': 11}, {'filePath': 'netty/src/main/java/io/grpc/netty/NettyClientTransport.java', 'insertions': 8, 'deletions': 6, 'lines': 14}, {'filePath': 'netty/src/main/java/io/grpc/netty/Utils.java', 'insertions': 1, 'deletions': 4, 'lines': 5}, {'filePath': 'testing/src/main/java/io/grpc/internal/testing/AbstractTransportTest.java', 'insertions': 1, 'deletions': 8, 'lines': 9}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'Http2Ping.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.internal.Http2Ping.failed(java.lang.Throwable)', 'TOT': 5, 'UPD': 0, 'INS': 1, 'MOV': 3, 'DEL': 1}]}, {'spoonFilePath': 'NettyClientHandler.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.netty.NettyClientHandler.sendPingFrame(io.netty.channel.ChannelHandlerContext,io.grpc.netty.SendPingCommand,io.netty.channel.ChannelPromise).4.operationComplete(io.netty.channel.ChannelFuture)', 'TOT': 4, 'UPD': 0, 'INS': 3, 'MOV': 1, 'DEL': 0}]}, {'spoonFilePath': 'NettyClientTransport.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.netty.NettyClientTransport.statusFromFailedFuture(io.netty.channel.ChannelFuture)', 'TOT': 4, 'UPD': 0, 'INS': 0, 'MOV': 3, 'DEL': 1}]}, {'spoonFilePath': 'Utils.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.netty.Utils.statusFromThrowable(java.lang.Throwable)', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}]}, {'spoonFilePath': 'AbstractTransportTest.java', 'spoonMethods': [{'spoonMethodName': 'io.grpc.internal.testing.AbstractTransportTest.ping_afterTermination()', 'TOT': 3, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 3}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/grpc/grpc-java/issues/1330,235.00027777777777,"['P2', 'bug', 'netty']",Receiving ClosedChannelException instead of actual error,2.0,"['io.grpc.netty.NettyClientTransport.statusFromFailedFuture(io.netty.channel.ChannelFuture)', 'io.grpc.netty.Utils.statusFromThrowable(java.lang.Throwable)', 'io.grpc.netty.NettyClientTransport.start(io.grpc.netty.Listener).3', 'io.grpc.internal.Http2Ping.failed(java.lang.Throwable)', 'io.grpc.netty.NettyClientTransport.start(io.grpc.netty.Listener).3.operationComplete(io.netty.channel.ChannelFuture)', 'io.grpc.netty.NettyClientTransport.ping(io.grpc.netty.PingCallback,java.util.concurrent.Executor)', 'io.grpc.netty.NettyClientHandler.write(io.netty.channel.ChannelHandlerContext,java.lang.Object,io.netty.channel.ChannelPromise)', 'io.grpc.netty.NettyClientHandler.sendPingFrame(io.netty.channel.ChannelHandlerContext,io.grpc.netty.SendPingCommand,io.netty.channel.ChannelPromise)', 'io.grpc.netty.NettyClientHandler.close(io.netty.channel.ChannelHandlerContext,io.netty.channel.ChannelPromise)', 'io.grpc.netty.NettyClientTransport.shutdown()', 'io.grpc.netty.NettyClientTransport', 'io.grpc.netty.NettyClientTransport.newStream(io.grpc.MethodDescriptor,io.grpc.Metadata)', 'io.grpc.netty.NettyClientTransport.start(io.grpc.netty.Listener).2.operationComplete(io.netty.channel.ChannelFuture)', 'io.grpc.netty.NettyClientStream.start(io.grpc.internal.ClientStreamListener).1.operationComplete(io.netty.channel.ChannelFuture)', 'io.grpc.netty.NettyClientStream', 'io.grpc.netty.NettyClientTransport.start(io.grpc.netty.Listener).1', 'io.grpc.netty.NettyClientHandler.sendPingFrame(io.netty.channel.ChannelHandlerContext,io.grpc.netty.SendPingCommand,io.netty.channel.ChannelPromise).4.operationComplete(io.netty.channel.ChannelFuture)', 'io.grpc.netty.NettyClientTransport.start(io.grpc.netty.Listener).2', 'io.grpc.inprocess.InProcessTransport.shutdown()', 'io.grpc.netty.NettyClientStream.statusFromFailedFuture(io.netty.channel.ChannelFuture)', 'io.grpc.netty.NettyClientHandler.createStream(io.grpc.netty.CreateStreamCommand,io.netty.channel.ChannelPromise)', 'io.grpc.internal.DelayedClientTransport.shutdown()', 'io.grpc.netty.GracefulCloseCommand', 'io.grpc.netty.NettyClientTransport.notifyShutdown(io.grpc.Status)']","['1170afd16844bfe8654dd8634ff026d02e77baa0', '7504b79f0083db84cd1b2bd141603e20d70d2a16', '4aadf550ee6b4f6c3a7f63bb7a295cbdfb9b12d9']",,"['netty/src/main/java/io/grpc/netty', 'core/src/main/java/io/grpc/internal', 'core/src/main/java/io/grpc/inprocess']",148.0,36.0,184.0,9.0,13.0,24.0,68.0,24.0,23.0,8.0,8.0,0.0,0.0,0.0,0.0,0.0,0.0,grpc-java
29800,2015-06-09 13:52:31,stevo58008,"(Source attached below which demonstrates the issue).
I have an implementation of an AbstractScheduledService which overrides the scheduler() method and creates and returns a CustomScheduler.  This CustomScheduler's getNextSchedule() method sleeps for a period of time (which makes the deadlock more predicable) and then either returns a Schedule, or if the Service’s state is STOPPING it throws an exception.  
The problem arises when it throws an exception.  After throwing the exception the service never actually terminates, and the shutDown() method is never called.  After a brief investigation, it looks as though there is a deadlock between notifyFailed() and stopAsync() in AbstractService class.  notifyFailed() (called from the catch block of reschedule() method in one of AbstractScheduledService inner classes) tries to enter the monitor field but is blocked as the monitor is currently held by stopAsync(), and stopAsync() hasn’t released the monitor yet as it is blocked when calling this.doStop() (in case 3 of switch stmt).  The doStop() called is the impl in AbstractScheduledService, and this gets blocked on the first line waiting for this.runningTask.cancel(false) to return.

I haven’t had time to download the source code and build things myself to investigate it further, but hopefully there is enough here to go on.  Below is some source code which demonstrates the problem, and the log output showing that the service’s shutDown() method is not called and the service and JVM hasn’t terminated yet.

``` java
public class Main {
    public static void main(String[] args) throws Exception {
        Service service = new ScheduledServiceWithCustomBlockingScheduler(5000);
        new ServiceStateMonitor().monitorServiceStateWithServiceListener(service);
        System.out.println(""starting service"");
        service.startAsync();
        Thread.sleep(6000); // wait just enough time for 2nd call of getNextSchedule() to start
        System.out.println(""Stopping service"");
        service.stopAsync();
        System.out.println(""Bye"");
      }
}
```

``` java
public class ScheduledServiceWithCustomBlockingScheduler extends AbstractScheduledService {
    private long blockTime;

    public ScheduledServiceWithCustomBlockingScheduler(final long blockTime) {
        this.blockTime = blockTime;
    }

    @Override
    protected void runOneIteration() throws Exception {
        System.out.println(""runOneIteration()"");
    }

    @Override
    protected Scheduler scheduler() {
        return new CustomScheduler() {
            @Override
            protected Schedule getNextSchedule() throws Exception {
                System.out.println(""getNextSchedule()..."");
                Thread.sleep(blockTime);
                if (state() == State.STOPPING) {
                    System.out.println(""getNextSchedule() - not running anymore so don't return scheduler"");
                    throw new Exception(""not running anymore so don't return scheduler"");
                }
                System.out.println(""...getNextSchedule()"");
                return new Schedule(0, TimeUnit.NANOSECONDS);
            }
        };
    }

    @Override
    protected void startUp() throws Exception {
        System.out.println(""startUp()"");
    }

    @Override
    protected void shutDown() throws Exception {
        System.out.println(""shutDown()"");
    }
}
```

``` java
public class ServiceStateMonitor {
    public void monitorServiceStateWithServiceListener(final Service serviceToMonitor) {
        serviceToMonitor.addListener(new PrintOutNewStateServiceListener(), MoreExecutors.directExecutor());
    }

    private class PrintOutNewStateServiceListener extends Service.Listener {
        @Override
        public void starting() {
            System.out.println(""SERVICE LISTENER : Starting"");
        }

        @Override
        public void running() {
            System.out.println(""SERVICE LISTENER : Running"");
        }

        @Override
        public void stopping(final Service.State from) {
            System.out.println(""SERVICE LISTENER : Stopping"");
        }

        @Override
        public void terminated(final Service.State from) {
            System.out.println(""SERVICE LISTENER : Terminated"");
        }

        @Override
        public void failed(final Service.State from, final Throwable failure) {
            System.out.println(""SERVICE LISTENER : Failed"");
        }
    }
}
```

The Standard Ouput from running the main()

```
starting service
SERVICE LISTENER : Starting
startUp()
getNextSchedule()...
...getNextSchedule()
SERVICE LISTENER : Running
runOneIteration()
getNextSchedule()...
Stopping service
getNextSchedule() - not running anymore so don't return scheduler
```

notice that there is no ""shutDown()"" or ""Bye"" output line.
",2015-06-16 19:44:59,"[{'commitHash': '6315a434ab54c804c0a9d89434f14803ace52b64', 'commitGHEventType': 'referenced', 'commitUser': 'lukesandberg', 'commitParents': ['a6ce7b511bff96f39c56e598143557dcff08db86'], 'nameRev': '6315a434ab54c804c0a9d89434f14803ace52b64 tags/v19.0-rc1~45', 'commitMessage': ""Fix a lock ordering deadlock in CustomScheduler reported in https://github.com/google/guava/issues/2072\n\nThe issue is that CustomScheduler uses a lock to atomically reschedule after each iteration, unfortunately this lock has inconsistent ordering with respect to the lock used by AbstractService.  In particular for a RUNNING service the following can ocurr\n\nT1:\nservice.stopAsync()\n  -> AbstractService.stopAsync() [ ACQUIRE AbstractService.monitor]\n     -> AbstractService.doStop()\n        -> CustomScheduler.cancel() [ ACQUIRE CustomScheduler.lock]\n\nT2:\nCustomScheduler.reschedule() [ ACQUIRE CustomScheduler.lock]\n   (getNextSchedule throws an exception)\n  -> AbstractService.notifyFailed [ACQUIRE AbstractService.monitor]\n\nBoom deadlock!\n\nIt looks like i introduced this deadlock in https://github.com/google/guava/commit/2c0727049d7e7cbbe1febf5bcf6b7a4b1860dea5 though it is possible it wasn't an issue at the time since the AbstractService implementation was different\n\nThe fix was to simply move the call to notifyFailed outside the lock.\n\nI have similar concerns about the other lock in this class, but i believe it actually is safe (though we should probably remove it anyway, as per an ancient TODO)\n-------------\nCreated by MOE: http://code.google.com/p/moe-java\nMOE_MIGRATED_REVID=96113737\n"", 'commitDateTime': '2015-06-16 13:44:08', 'authoredDateTime': '2015-06-16 09:20:29', 'commitGitStats': [{'filePath': 'guava-tests/test/com/google/common/util/concurrent/AbstractScheduledServiceTest.java', 'insertions': 26, 'deletions': 0, 'lines': 26}, {'filePath': 'guava/src/com/google/common/util/concurrent/AbstractScheduledService.java', 'insertions': 18, 'deletions': 3, 'lines': 21}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AbstractScheduledServiceTest.java', 'spoonMethods': [{'spoonMethodName': 'com.google.common.util.concurrent.AbstractScheduledServiceTest.SchedulerTest.testBig()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.google.common.util.concurrent.AbstractScheduledServiceTest.SchedulerTest.testBig().7', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.google.common.util.concurrent.AbstractScheduledServiceTest.SchedulerTest.testBig().7.scheduler()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.google.common.util.concurrent.AbstractScheduledServiceTest.SchedulerTest.testCustomScheduler_deadlock()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'AbstractScheduledService.java', 'spoonMethods': [{'spoonMethodName': 'com.google.common.util.concurrent.AbstractScheduledService.CustomScheduler.ReschedulableCallable.reschedule()', 'TOT': 9, 'UPD': 0, 'INS': 5, 'MOV': 2, 'DEL': 2}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/google/guava/issues/2072,7.000277777777778,"['package=concurrent', 'status=fixed', 'type=defect']",Deadlock when stopping an AbstractScheduledService and Exception thrown from CustomScheduler.getNextSchedule(),1.0,['com.google.common.util.concurrent.AbstractScheduledService.CustomScheduler.ReschedulableCallable.reschedule()'],['6315a434ab54c804c0a9d89434f14803ace52b64'],,['guava/src/com/google/common/util/concurrent'],18.0,3.0,21.0,1.0,0.0,1.0,9.0,2.0,5.0,2.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,guava
29804,2015-02-19 15:54:48,kronar,"ImmutableList contains following code since version 15.0

 private static final ImmutableList<Object> EMPTY =
      new RegularImmutableList<Object>(ObjectArrays.EMPTY_ARRAY);
",2015-06-23 04:42:58,"[{'commitHash': 'f3099c7e01df0f240615eea996ec0c973a47aa11', 'commitGHEventType': 'referenced', 'commitUser': 'cpovirk', 'commitParents': ['1839378a0dc48379f018597adb0cbcadc3225d34'], 'nameRev': 'f3099c7e01df0f240615eea996ec0c973a47aa11 tags/v19.0-rc1~210', 'commitMessage': 'Move EMPTY to RegularImmutableList.\nThis was requested in https://github.com/google/guava/issues/1977 to avoid circular static-init dependencies.\nI fear that we have other such problems. We should consider adding a test for them.\n-------------\nCreated by MOE: http://code.google.com/p/moe-java\nMOE_MIGRATED_REVID=88131198\n', 'commitDateTime': '2015-03-16 14:35:04', 'authoredDateTime': '2015-03-09 10:49:03', 'commitGitStats': [{'filePath': 'guava/src/com/google/common/collect/ImmutableList.java', 'insertions': 3, 'deletions': 6, 'lines': 9}, {'filePath': 'guava/src/com/google/common/collect/RegularImmutableList.java', 'insertions': 4, 'deletions': 1, 'lines': 5}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'ImmutableList.java', 'spoonMethods': [{'spoonMethodName': 'com.google.common.collect.ImmutableList', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}]}, {'spoonFilePath': 'RegularImmutableList.java', 'spoonMethods': [{'spoonMethodName': 'com.google.common.collect.RegularImmutableList', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '83923892da4dc396e497c956f00cb4c577d941ec', 'commitGHEventType': 'referenced', 'commitUser': 'cpovirk', 'commitParents': ['91b5db6570568265aec0a4636023c0e0925661ba'], 'nameRev': '83923892da4dc396e497c956f00cb4c577d941ec tags/v20.0-rc1~738', 'commitMessage': ""Remove more possible static-init deadlocks (https://github.com/google/guava/issues/1977).\n\n- ImmutableSortedMultiset: I couldn't figure out a way for this to actually deadlock, but I figure it's best for it to follow the pattern of the others.\n- ImmutableSortedSet: This might be able to deadlock if one thread creates a RegularImmutableMap (which then calls into RegularImmutableSortedSet and then ImmutableSet) while another initializes ImmutableSortedSet (which then calls into RegularImmutableSortedSet).\n- ImmutableTable: This might be able to deadlock if one thread GWT-deserializes a SparseImmutableTable (which then calls into ImmutableTable) while another initializes ImmutableTable (which calls into SparseImmutableTable).\n-------------\nCreated by MOE: https://github.com/google/moe\nMOE_MIGRATED_REVID=104995935\n"", 'commitDateTime': '2015-10-08 17:35:29', 'authoredDateTime': '2015-10-08 14:03:55', 'commitGitStats': [{'filePath': 'guava/src/com/google/common/collect/ImmutableSortedMultiset.java', 'insertions': 3, 'deletions': 8, 'lines': 11}, {'filePath': 'guava/src/com/google/common/collect/ImmutableSortedSet.java', 'insertions': 3, 'deletions': 9, 'lines': 12}, {'filePath': 'guava/src/com/google/common/collect/ImmutableTable.java', 'insertions': 1, 'deletions': 7, 'lines': 8}, {'filePath': 'guava/src/com/google/common/collect/RegularImmutableSortedMultiset.java', 'insertions': 3, 'deletions': 0, 'lines': 3}, {'filePath': 'guava/src/com/google/common/collect/RegularImmutableSortedSet.java', 'insertions': 2, 'deletions': 0, 'lines': 2}, {'filePath': 'guava/src/com/google/common/collect/SparseImmutableTable.java', 'insertions': 3, 'deletions': 0, 'lines': 3}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'ImmutableSortedMultiset.java', 'spoonMethods': [{'spoonMethodName': 'com.google.common.collect.ImmutableSortedMultiset', 'TOT': 3, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 2}, {'spoonMethodName': 'com.google.common.collect.ImmutableSortedMultiset.emptyMultiset(java.util.Comparator)', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'com.google.common.collect.ImmutableSortedMultiset.of()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'ImmutableSortedSet.java', 'spoonMethods': [{'spoonMethodName': 'com.google.common.collect.ImmutableSortedSet', 'TOT': 3, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 2}, {'spoonMethodName': 'com.google.common.collect.ImmutableSortedSet.emptySet(java.util.Comparator)', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'com.google.common.collect.ImmutableSortedSet.of()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'ImmutableTable.java', 'spoonMethods': [{'spoonMethodName': 'com.google.common.collect.ImmutableTable', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'com.google.common.collect.ImmutableTable.of()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'RegularImmutableSortedMultiset.java', 'spoonMethods': [{'spoonMethodName': 'com.google.common.collect.RegularImmutableSortedMultiset', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'RegularImmutableSortedSet.java', 'spoonMethods': [{'spoonMethodName': 'com.google.common.collect.RegularImmutableSortedSet', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'SparseImmutableTable.java', 'spoonMethods': [{'spoonMethodName': 'com.google.common.collect.SparseImmutableTable', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/google/guava/issues/1977,123.00027777777778,"['package=general', 'status=fixed', 'type=defect']",Possible deadlock in ImmutableList ,2.0,"['com.google.common.collect.RegularImmutableList', 'com.google.common.collect.ImmutableSortedSet.emptySet(java.util.Comparator)', 'com.google.common.collect.ImmutableSortedSet', 'com.google.common.collect.ImmutableSortedMultiset.of()', 'com.google.common.collect.RegularImmutableSortedMultiset', 'com.google.common.collect.ImmutableTable', 'com.google.common.collect.ImmutableTable.of()', 'com.google.common.collect.ImmutableList', 'com.google.common.collect.RegularImmutableSortedSet', 'com.google.common.collect.ImmutableSortedMultiset', 'com.google.common.collect.SparseImmutableTable', 'com.google.common.collect.ImmutableSortedMultiset.emptyMultiset(java.util.Comparator)', 'com.google.common.collect.ImmutableSortedSet.of()']","['f3099c7e01df0f240615eea996ec0c973a47aa11', '83923892da4dc396e497c956f00cb4c577d941ec']",,['guava/src/com/google/common/collect'],22.0,31.0,53.0,8.0,0.0,13.0,19.0,2.0,9.0,8.0,8.0,0.0,0.0,0.0,0.0,0.0,0.0,guava
29865,2014-10-31 17:51:04,gissuebot,"_[Original issue](https://code.google.com/p/guava-libraries/issues/detail?id=1558) created by **Maaartinus** on 2013-10-19 at 04:37 PM_

---

In case the iterable shrinks in the meantime, the result of the class-accepting version of `Iterables.toArray`&nbsp;may contain nulls at its end.

The documentation says ""return a newly-allocated array into which all the elements of the iterable have been copied"", which actually doesn't prevent this. However, nobody expects `Iterables.toArray(a_not_null_allowing_set, some_class)`&nbsp;to contain several nulls.

In case you don't want to fix it, the documentation should state this very explicitly. Note that the other overload works fine.

The fix is trivial, just pass an empty array in place of the presized one. The only cost is the allocation of the empty array. A test is attached.
",2015-10-09 18:46:38,"[{'commitHash': '0cdfef602bc04513efe9a7a4c2821d71f9dcf00d', 'commitGHEventType': 'referenced', 'commitUser': 'lowasser', 'commitParents': ['80b78c8ba68e32963e1684787a10b7f78c91e81a'], 'nameRev': '0cdfef602bc04513efe9a7a4c2821d71f9dcf00d tags/v20.0-rc1~734', 'commitMessage': 'Rewrite Iterables.toArray to work correctly in the presence of concurrent modification (https://github.com/google/guava/issues/1558).\n-------------\nCreated by MOE: https://github.com/google/moe\nMOE_MIGRATED_REVID=105071914\n', 'commitDateTime': '2015-10-12 09:39:24', 'authoredDateTime': '2015-10-09 11:06:04', 'commitGitStats': [{'filePath': 'guava-gwt/src-super/com/google/common/collect/super/com/google/common/collect/Iterables.java', 'insertions': 3, 'deletions': 3, 'lines': 6}, {'filePath': 'guava/src/com/google/common/collect/Iterables.java', 'insertions': 4, 'deletions': 6, 'lines': 10}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'Iterables.java', 'spoonMethods': [{'spoonMethodName': 'com.google.common.collect.Iterables.toCollection(java.lang.Iterable)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.google.common.collect.Iterables.toArray(java.lang.Iterable,java.lang.Object[])', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.google.common.collect.Iterables.toArray(java.lang.Iterable)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.google.common.collect.Iterables.toArray(java.lang.Iterable,java.lang.Class)', 'TOT': 7, 'UPD': 0, 'INS': 1, 'MOV': 3, 'DEL': 3}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/google/guava/issues/1558,343.0002777777778,"['package=collect', 'status=fixed', 'type=defect']",Iterables.toArray broken in case of concurrent modifications,1.0,"['com.google.common.collect.Iterables.toCollection(java.lang.Iterable)', 'com.google.common.collect.Iterables.toArray(java.lang.Iterable)', 'com.google.common.collect.Iterables.toArray(java.lang.Iterable,java.lang.Object[])', 'com.google.common.collect.Iterables.toArray(java.lang.Iterable,java.lang.Class)']",['0cdfef602bc04513efe9a7a4c2821d71f9dcf00d'],,"['guava/src/com/google/common/collect', 'guava-gwt/src-super/com/google/common/collect/super/com/google/common']",7.0,9.0,16.0,2.0,3.0,4.0,10.0,3.0,1.0,3.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,guava
32520,2013-09-12 13:53:02,enesakar,"See MapStoreTest.testMapGetAll()
",2013-09-19 08:42:04,"[{'commitHash': 'b8c8465886bf54fa993ff57abe5d6cb4a794d219', 'commitGHEventType': 'closed', 'commitUser': 'enesakar', 'commitParents': ['6fa8fa0639e5b44d288480f8991b18a2b31c1c8f'], 'nameRev': 'b8c8465886bf54fa993ff57abe5d6cb4a794d219 tags/v3.1~32^2^2~2', 'commitMessage': 'fixes #887\nI have cleared all the code about synchronizing mapstore loadAll among cluster members.\nIt was too complicated, and have concurrency problems, may easily cause deadlocks.\nSo currently all nodes tries to loadAll and operations do not wait load all.\nIf this improvement needed, it should be carefully redesigned.\n', 'commitDateTime': '2013-09-15 01:04:16', 'authoredDateTime': '2013-09-15 01:04:16', 'commitGitStats': [{'filePath': 'hazelcast/src/main/java/com/hazelcast/map/MapContainer.java', 'insertions': 3, 'deletions': 52, 'lines': 55}, {'filePath': 'hazelcast/src/main/java/com/hazelcast/map/operation/AbstractMapOperation.java', 'insertions': 0, 'deletions': 3, 'lines': 3}, {'filePath': 'hazelcast/src/main/java/com/hazelcast/map/operation/KeyBasedMapOperation.java', 'insertions': 0, 'deletions': 3, 'lines': 3}, {'filePath': 'hazelcast/src/main/java/com/hazelcast/map/operation/MapInitialLoadOperation.java', 'insertions': 0, 'deletions': 36, 'lines': 36}, {'filePath': 'hazelcast/src/main/java/com/hazelcast/map/operation/MapIsReadyOperation.java', 'insertions': 0, 'deletions': 42, 'lines': 42}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'MapContainer.java', 'spoonMethods': [{'spoonMethodName': 'com.hazelcast.map.MapContainer.loadMapFromStore(boolean)', 'TOT': 3, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 2}, {'spoonMethodName': 'com.hazelcast.map.MapContainer', 'TOT': 13, 'UPD': 0, 'INS': 2, 'MOV': 5, 'DEL': 6}, {'spoonMethodName': 'com.hazelcast.map.MapContainer.isMapReady()', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'com.hazelcast.map.MapContainer.MapLoadAllTask', 'TOT': 3, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 3}, {'spoonMethodName': 'com.hazelcast.map.MapContainer.MapLoadAllTask.run()', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}]}, {'spoonFilePath': 'AbstractMapOperation.java', 'spoonMethods': [{'spoonMethodName': 'com.hazelcast.map.operation.AbstractMapOperation.beforeRun()', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}]}, {'spoonFilePath': 'KeyBasedMapOperation.java', 'spoonMethods': [{'spoonMethodName': 'com.hazelcast.map.operation.KeyBasedMapOperation.beforeRun()', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}]}, {'spoonFilePath': 'MapInitialLoadOperation.java', 'spoonMethods': [{'spoonMethodName': 'com.hazelcast.map.operation.MapInitialLoadOperation', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}]}, {'spoonFilePath': 'MapIsReadyOperation.java', 'spoonMethods': [{'spoonMethodName': 'com.hazelcast.map.operation.MapIsReadyOperation', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/hazelcast/hazelcast/issues/887,6.000277777777778,"['Source: Internal', 'Type: Defect']","Map getAll is blocked with ""Map is not ready exception""",1.0,"['com.hazelcast.map.MapContainer', 'com.hazelcast.map.MapContainer.MapLoadAllTask.run()', 'com.hazelcast.map.MapContainer.loadMapFromStore(boolean)', 'com.hazelcast.map.operation.MapIsReadyOperation', 'com.hazelcast.map.operation.MapInitialLoadOperation', 'com.hazelcast.map.MapContainer.MapLoadAllTask', 'com.hazelcast.map.operation.KeyBasedMapOperation.beforeRun()', 'com.hazelcast.map.operation.AbstractMapOperation.beforeRun()', 'com.hazelcast.map.MapContainer.isMapReady()']",['b8c8465886bf54fa993ff57abe5d6cb4a794d219'],,"['hazelcast/src/main/java/com/hazelcast/map', 'hazelcast/src/main/java/com/hazelcast/map/operation']",3.0,136.0,139.0,5.0,1.0,9.0,25.0,5.0,2.0,17.0,5.0,0.0,0.0,0.0,0.0,0.0,0.0,hazelcast
32719,2012-05-17 08:04:18,jshiell,"We found the following exception in our cluster logs this morning. This looks to be a concurrency edge case - getLockCount is relying on a volatile variable being non-null.

This was found on Hazelcast 2.1 with the Sun JDK 1.6.0_31 i386 on CentOS 5.8 x86_64.

```
java.lang.NullPointerException
    at com.hazelcast.impl.AbstractRecord.getLockCount(AbstractRecord.java:417)
    at com.hazelcast.impl.AbstractRecord.isEvictable(AbstractRecord.java:153)
    at com.hazelcast.impl.CMap.startCleanup(CMap.java:1332)
    at com.hazelcast.impl.ConcurrentMapManager$4.run(ConcurrentMapManager.java:431)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
```
",2012-05-17 10:01:50,"[{'commitHash': 'e559141e0d4b36ed3d9cb110c874c16ea1318059', 'commitGHEventType': 'closed', 'commitUser': 'mdogan', 'commitParents': ['c1e69178b903bc9ed70a093dbffb4d1e48de2bdf'], 'nameRev': 'e559141e0d4b36ed3d9cb110c874c16ea1318059 tags/v2.0.4~8', 'commitMessage': 'Fixes #166.\n', 'commitDateTime': '2012-05-17 12:05:55', 'authoredDateTime': '2012-05-17 12:05:55', 'commitGitStats': [{'filePath': 'hazelcast/src/main/java/com/hazelcast/impl/AbstractRecord.java', 'insertions': 13, 'deletions': 5, 'lines': 18}, {'filePath': 'hazelcast/src/main/java/com/hazelcast/impl/CMap.java', 'insertions': 1, 'deletions': 1, 'lines': 2}, {'filePath': 'hazelcast/src/main/java/com/hazelcast/impl/ConcurrentMapManager.java', 'insertions': 2, 'deletions': 2, 'lines': 4}, {'filePath': 'hazelcast/src/main/java/com/hazelcast/impl/DefaultRecord.java', 'insertions': 3, 'deletions': 2, 'lines': 5}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AbstractRecord.java', 'spoonMethods': [{'spoonMethodName': 'com.hazelcast.impl.AbstractRecord.unlock(int,com.hazelcast.nio.Address)', 'TOT': 5, 'UPD': 0, 'INS': 3, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'com.hazelcast.impl.AbstractRecord.testLock(int,com.hazelcast.nio.Address)', 'TOT': 5, 'UPD': 0, 'INS': 3, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'com.hazelcast.impl.AbstractRecord.isLocked()', 'TOT': 5, 'UPD': 0, 'INS': 3, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'com.hazelcast.impl.AbstractRecord.getLockCount()', 'TOT': 6, 'UPD': 0, 'INS': 3, 'MOV': 1, 'DEL': 2}, {'spoonMethodName': 'com.hazelcast.impl.AbstractRecord.getLockAddress()', 'TOT': 6, 'UPD': 0, 'INS': 3, 'MOV': 3, 'DEL': 0}]}, {'spoonFilePath': 'CMap.java', 'spoonMethods': [{'spoonMethodName': 'com.hazelcast.impl.CMap.doBackup(com.hazelcast.impl.Request)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'ConcurrentMapManager.java', 'spoonMethods': [{'spoonMethodName': 'com.hazelcast.impl.ConcurrentMapManager.ForceUnlockOperationHandler.doOperation(com.hazelcast.impl.Request)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.hazelcast.impl.ConcurrentMapManager.destroyEndpointThreads(com.hazelcast.nio.Address,java.util.Set)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'DefaultRecord.java', 'spoonMethods': [{'spoonMethodName': 'com.hazelcast.impl.DefaultRecord.copy()', 'TOT': 5, 'UPD': 0, 'INS': 3, 'MOV': 0, 'DEL': 2}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '214a5b3580268df158d983c9d8b8c2ed5efa6c69', 'commitGHEventType': 'referenced', 'commitUser': 'mdogan', 'commitParents': ['e559141e0d4b36ed3d9cb110c874c16ea1318059'], 'nameRev': '214a5b3580268df158d983c9d8b8c2ed5efa6c69 tags/v2.0.4~7', 'commitMessage': 'Test for issue #166.\n', 'commitDateTime': '2012-05-17 13:00:38', 'authoredDateTime': '2012-05-17 13:00:38', 'commitGitStats': [{'filePath': 'hazelcast/src/test/java/com/hazelcast/impl/ClusterLockTest.java', 'insertions': 83, 'deletions': 0, 'lines': 83}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'ClusterLockTest.java', 'spoonMethods': [{'spoonMethodName': 'com.hazelcast.impl.ClusterLockTest.testAbstractRecordLockConcurrentAccess()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '842a7ce5fb35c2718e26f53d6752b075c17ac30d', 'commitGHEventType': 'referenced', 'commitUser': 'mdogan', 'commitParents': ['620aa966bff28bad9a22094b25d9c7e86831d860'], 'nameRev': '842a7ce5fb35c2718e26f53d6752b075c17ac30d tags/v2.1.1~9', 'commitMessage': 'Test for issue #166.\n', 'commitDateTime': '2012-05-17 12:58:50', 'authoredDateTime': '2012-05-17 12:58:50', 'commitGitStats': [{'filePath': 'hazelcast/src/test/java/com/hazelcast/impl/ClusterLockTest.java', 'insertions': 83, 'deletions': 0, 'lines': 83}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'ClusterLockTest.java', 'spoonMethods': [{'spoonMethodName': 'com.hazelcast.impl.ClusterLockTest.testAbstractRecordLockConcurrentAccess()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '0c89672210a77fd412d5b2cdcf82e1ff22788f06', 'commitGHEventType': 'referenced', 'commitUser': 'mdogan', 'commitParents': ['cbdb7358d3d1df246bbed9eed0e485d1bd7ad131'], 'nameRev': '0c89672210a77fd412d5b2cdcf82e1ff22788f06 tags/v2.2~54^2~20', 'commitMessage': 'Test for issue #166.\n', 'commitDateTime': '2012-05-17 13:00:15', 'authoredDateTime': '2012-05-17 13:00:15', 'commitGitStats': [{'filePath': 'hazelcast/src/test/java/com/hazelcast/impl/ClusterLockTest.java', 'insertions': 83, 'deletions': 0, 'lines': 83}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'ClusterLockTest.java', 'spoonMethods': [{'spoonMethodName': 'com.hazelcast.impl.ClusterLockTest.testAbstractRecordLockConcurrentAccess()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/hazelcast/hazelcast/issues/166,0.0002777777777777778,['Type: Defect'],NullPointerException in AbstractRecord.getLockCount,1.0,"['com.hazelcast.impl.DefaultRecord.copy()', 'com.hazelcast.impl.AbstractRecord.getLockAddress()', 'com.hazelcast.impl.AbstractRecord.testLock(int,com.hazelcast.nio.Address)', 'com.hazelcast.impl.CMap.doBackup(com.hazelcast.impl.Request)', 'com.hazelcast.impl.AbstractRecord.isLocked()', 'com.hazelcast.impl.ConcurrentMapManager.destroyEndpointThreads(com.hazelcast.nio.Address,java.util.Set)', 'com.hazelcast.impl.ConcurrentMapManager.ForceUnlockOperationHandler.doOperation(com.hazelcast.impl.Request)', 'com.hazelcast.impl.AbstractRecord.getLockCount()', 'com.hazelcast.impl.AbstractRecord.unlock(int,com.hazelcast.nio.Address)']","['e559141e0d4b36ed3d9cb110c874c16ea1318059', '214a5b3580268df158d983c9d8b8c2ed5efa6c69']",,['hazelcast/src/main/java/com/hazelcast/impl'],19.0,10.0,29.0,4.0,3.0,9.0,35.0,7.0,18.0,7.0,4.0,0.0,0.0,0.0,2.0,0.0,0.0,hazelcast
32806,2020-02-06 08:12:51,calohmn,"`HonoConnection.shutdown()` blocks the current thread for 5s waiting for the close frame from the remote peer.
If the current thread is the thread of the vertx context that the HonoConnection operates on, the response from the remote peer can't be processed while the thread is blocked, causing for the 5s timeout to occur.

Same applies to the `disconnect` method.",2020-02-06 11:47:02,"[{'commitHash': 'a661f8d928a3e695e294de5e30560de79f069759', 'commitGHEventType': 'referenced', 'commitUser': 'calohmn', 'commitParents': ['a7da2777eeb8eb0e48c055d3eefd36fc5607056c'], 'nameRev': 'a661f8d928a3e695e294de5e30560de79f069759 tags/1.1.0~17', 'commitMessage': '[#1750] Fix shutdown/disconnect if used on event loop thread.\n\nThe timeout used in closeConnection() and in shutdown(), disconnect()\nhas also been reduced to half the connect timeout.\n\nSigned-off-by: Carsten Lohmann <carsten.lohmann@bosch.io>\n', 'commitDateTime': '2020-02-06 12:47:00', 'authoredDateTime': '2020-02-06 12:06:24', 'commitGitStats': [{'filePath': 'client/src/main/java/org/eclipse/hono/client/HonoConnection.java', 'insertions': 7, 'deletions': 2, 'lines': 9}, {'filePath': 'client/src/main/java/org/eclipse/hono/client/impl/HonoConnectionImpl.java', 'insertions': 44, 'deletions': 20, 'lines': 64}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'HonoConnection.java', 'spoonMethods': []}, {'spoonFilePath': 'HonoConnectionImpl.java', 'spoonMethods': [{'spoonMethodName': 'org.eclipse.hono.client.impl.HonoConnectionImpl.disconnect()', 'TOT': 15, 'UPD': 6, 'INS': 3, 'MOV': 3, 'DEL': 3}, {'spoonMethodName': 'org.eclipse.hono.client.impl.HonoConnectionImpl.shutdown()', 'TOT': 20, 'UPD': 3, 'INS': 6, 'MOV': 8, 'DEL': 3}, {'spoonMethodName': 'org.eclipse.hono.client.impl.HonoConnectionImpl.closeConnection(io.vertx.core.Handler)', 'TOT': 5, 'UPD': 0, 'INS': 3, 'MOV': 0, 'DEL': 2}, {'spoonMethodName': 'org.eclipse.hono.client.impl.HonoConnectionImpl.getCloseConnectionTimeout()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/eclipse/hono/issues/1750,0.0002777777777777778,"['Client', 'bug']",HonoConnection.shutdown() blocks for 5s if called on event loop thread,1.0,"['org.eclipse.hono.client.impl.HonoConnectionImpl.disconnect()', 'org.eclipse.hono.client.impl.HonoConnectionImpl.shutdown()', 'org.eclipse.hono.client.impl.HonoConnectionImpl.getCloseConnectionTimeout()', 'org.eclipse.hono.client.impl.HonoConnectionImpl.closeConnection(io.vertx.core.Handler)']",['a661f8d928a3e695e294de5e30560de79f069759'],,"['client/src/main/java/org/eclipse/hono/client', 'client/src/main/java/org/eclipse/hono/client/impl']",51.0,22.0,73.0,2.0,9.0,4.0,41.0,11.0,13.0,8.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,hono
32936,2017-06-23 09:10:01,calohmn,"In our Hono setup we have reached a state where the Protocol Adapters can't re-create a message sender.

REST Adapter log excerpt:
```
05:03:24.641 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.AbstractSender - sender [telemetry/DEFAULT_TENANT] closed: Error{condition=hono:no-downstream-consumer, description='no downstream consumer available for data', info=null}
05:03:27.319 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - creating new message sender for telemetry/DEFAULT_TENANT
05:03:27.319 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - reusing existing registration client for [DEFAULT_TENANT]
05:03:27.347 [vert.x-eventloop-thread-0] DEBUG o.e.h.c.i.AbstractRequestResponseClient - received response [correlation ID: reg-client-23, status: 200]
05:03:27.841 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - already trying to create a message sender for telemetry/DEFAULT_TENANT
05:03:27.841 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - reusing existing registration client for [DEFAULT_TENANT]
05:03:27.841 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.h.AbstractVertxBasedHttpProtocolAdapter - Service unavailable: null
05:03:27.869 [vert.x-eventloop-thread-0] DEBUG o.e.h.c.i.AbstractRequestResponseClient - received response [correlation ID: reg-client-24, status: 200]
05:03:37.118 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - already trying to create a message sender for telemetry/DEFAULT_TENANT
05:03:37.118 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - reusing existing registration client for [DEFAULT_TENANT]
05:03:37.118 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.h.AbstractVertxBasedHttpProtocolAdapter - Service unavailable: null
05:03:37.147 [vert.x-eventloop-thread-0] DEBUG o.e.h.c.i.AbstractRequestResponseClient - received response [correlation ID: reg-client-25, status: 200]
05:03:47.206 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - already trying to create a message sender for telemetry/DEFAULT_TENANT
05:03:47.206 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - reusing existing registration client for [DEFAULT_TENANT]
05:03:47.206 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.h.AbstractVertxBasedHttpProtocolAdapter - Service unavailable: null
05:03:47.235 [vert.x-eventloop-thread-0] DEBUG o.e.h.c.i.AbstractRequestResponseClient - received response [correlation ID: reg-client-26, status: 200]
05:03:51.731 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - already trying to create a message sender for telemetry/DEFAULT_TENANT
05:03:51.731 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - reusing existing registration client for [DEFAULT_TENANT]
05:03:51.731 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.h.AbstractVertxBasedHttpProtocolAdapter - Service unavailable: null
05:03:51.759 [vert.x-eventloop-thread-0] DEBUG o.e.h.c.i.AbstractRequestResponseClient - received response [correlation ID: reg-client-27, status: 200]
05:03:56.738 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - already trying to create a message sender for telemetry/DEFAULT_TENANT
05:03:56.738 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - reusing existing registration client for [DEFAULT_TENANT]
05:03:56.738 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.h.AbstractVertxBasedHttpProtocolAdapter - Service unavailable: null
05:03:56.766 [vert.x-eventloop-thread-0] DEBUG o.e.h.c.i.AbstractRequestResponseClient - received response [correlation ID: reg-client-28, status: 200]
05:03:57.297 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - already trying to create a message sender for telemetry/DEFAULT_TENANT
05:03:57.297 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - reusing existing registration client for [DEFAULT_TENANT]
05:03:57.297 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.h.AbstractVertxBasedHttpProtocolAdapter - Service unavailable: null
05:03:57.325 [vert.x-eventloop-thread-0] DEBUG o.e.h.c.i.AbstractRequestResponseClient - received response [correlation ID: reg-client-29, status: 200]
05:04:07.401 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - already trying to create a message sender for telemetry/DEFAULT_TENANT
05:04:07.401 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - reusing existing registration client for [DEFAULT_TENANT]
05:04:07.401 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.h.AbstractVertxBasedHttpProtocolAdapter - Service unavailable: null
05:04:07.429 [vert.x-eventloop-thread-0] DEBUG o.e.h.c.i.AbstractRequestResponseClient - received response [correlation ID: reg-client-30, status: 200]
05:04:07.673 [vert.x-eventloop-thread-0] INFO o.e.hono.client.impl.HonoClientImpl - Hono server [hono.hono:5671] closed connection with error condition: Error{condition=amqp:resource-limit-exceeded, description='local-idle-timeout expired', info=null}
05:04:07.673 [vert.x-eventloop-thread-0] INFO o.e.hono.client.impl.HonoClientImpl - lost connection to Hono server [hono.hono:5671]
05:04:07.674 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.h.AbstractVertxBasedHttpProtocolAdapter - Service unavailable: null
05:04:07.674 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - scheduling re-connect attempt ...
05:04:07.674 [vert.x-eventloop-thread-0] WARN o.e.hono.client.impl.HonoClientImpl - cannot handle failure of unknown connection
05:04:08.175 [vert.x-eventloop-thread-0] INFO o.e.hono.client.impl.HonoClientImpl - attempting to re-connect to Hono server [hono.hono:5671]
05:04:08.175 [vert.x-eventloop-thread-0] INFO o.e.h.c.ConnectionFactoryImpl - connecting to AMQP 1.0 container [amqps://hono.hono:5671]
05:04:08.301 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - already trying to create a message sender for telemetry/DEFAULT_TENANT
05:04:08.301 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - reusing existing registration client for [DEFAULT_TENANT]
05:04:08.301 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.h.AbstractVertxBasedHttpProtocolAdapter - Service unavailable: null
05:04:08.329 [vert.x-eventloop-thread-0] DEBUG o.e.h.c.i.AbstractRequestResponseClient - received response [correlation ID: reg-client-31, status: 200]
05:04:08.345 [vert.x-eventloop-thread-0] INFO o.e.h.c.ConnectionFactoryImpl - connected to AMQP 1.0 container [amqps://hono.hono:5671], opening connection ...
05:04:08.346 [vert.x-eventloop-thread-0] INFO o.e.h.c.ConnectionFactoryImpl - connection to container [Hono-0.0.0.0:5671] at [amqps://hono.hono:5671] open
05:04:09.072 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - already trying to create a message sender for telemetry/DEFAULT_TENANT
05:04:09.072 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - reusing existing registration client for [DEFAULT_TENANT]
05:04:09.072 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.h.AbstractVertxBasedHttpProtocolAdapter - Service unavailable: null
```

MQTT Adapter log:
```
05:03:24.642 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.AbstractSender - sender [telemetry/DEFAULT_TENANT] closed: Error{condition=hono:no-downstream-consumer, description='no downstream consumer available for data', info=null}
05:03:26.402 [vert.x-eventloop-thread-0] INFO o.e.h.a.m.VertxBasedMqttProtocolAdapter - Connection request from client esp8266.600194195a1b
05:03:26.445 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - reusing existing registration client for [DEFAULT_TENANT]
05:03:26.445 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - creating new message sender for telemetry/DEFAULT_TENANT
05:03:26.474 [vert.x-eventloop-thread-0] DEBUG o.e.h.c.i.AbstractRequestResponseClient - received response [correlation ID: reg-client-23, status: 200]
05:03:26.846 [vert.x-eventloop-thread-0] INFO o.e.h.a.m.VertxBasedMqttProtocolAdapter - Connection request from client esp8266.5ccf7f9509f1
05:03:26.897 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - reusing existing registration client for [DEFAULT_TENANT]
05:03:26.897 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - already trying to create a message sender for telemetry/DEFAULT_TENANT
05:03:26.897 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.m.VertxBasedMqttProtocolAdapter - cannot process message [client ID: esp8266.5ccf7f9509f1, topic: telemetry/DEFAULT_TENANT/esp8266.5ccf7f9509f1, QoS: AT_MOST_ONCE]: sender link not established yet
05:03:26.897 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.m.VertxBasedMqttProtocolAdapter - closing connection with client [client ID: esp8266.5ccf7f9509f1]
05:03:26.926 [vert.x-eventloop-thread-0] DEBUG o.e.h.c.i.AbstractRequestResponseClient - received response [correlation ID: reg-client-24, status: 200]
05:03:36.408 [vert.x-eventloop-thread-0] INFO o.e.h.a.m.VertxBasedMqttProtocolAdapter - Connection request from client esp8266.600194195a1b
05:03:36.452 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - reusing existing registration client for [DEFAULT_TENANT]
05:03:36.452 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - already trying to create a message sender for telemetry/DEFAULT_TENANT
05:03:36.452 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.m.VertxBasedMqttProtocolAdapter - cannot process message [client ID: esp8266.600194195a1b, topic: telemetry/DEFAULT_TENANT/esp8266.600194195a1b, QoS: AT_MOST_ONCE]: sender link not established yet
05:03:36.452 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.m.VertxBasedMqttProtocolAdapter - closing connection with client [client ID: esp8266.600194195a1b]
05:03:36.481 [vert.x-eventloop-thread-0] DEBUG o.e.h.c.i.AbstractRequestResponseClient - received response [correlation ID: reg-client-25, status: 200]
05:03:46.500 [vert.x-eventloop-thread-0] INFO o.e.h.a.m.VertxBasedMqttProtocolAdapter - Connection request from client esp8266.600194195a1b
05:03:46.543 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - reusing existing registration client for [DEFAULT_TENANT]
05:03:46.543 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - already trying to create a message sender for telemetry/DEFAULT_TENANT
05:03:46.543 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.m.VertxBasedMqttProtocolAdapter - cannot process message [client ID: esp8266.600194195a1b, topic: telemetry/DEFAULT_TENANT/esp8266.600194195a1b, QoS: AT_MOST_ONCE]: sender link not established yet
05:03:46.543 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.m.VertxBasedMqttProtocolAdapter - closing connection with client [client ID: esp8266.600194195a1b]
05:03:46.572 [vert.x-eventloop-thread-0] DEBUG o.e.h.c.i.AbstractRequestResponseClient - received response [correlation ID: reg-client-26, status: 200]
05:03:51.736 [vert.x-eventloop-thread-0] INFO o.e.h.a.m.VertxBasedMqttProtocolAdapter - Connection request from client HealthCheck_c8ca8231-a348-4d58-b135-1c24883611d8_mqtt
05:03:51.739 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - reusing existing registration client for [DEFAULT_TENANT]
05:03:51.739 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - already trying to create a message sender for telemetry/DEFAULT_TENANT
05:03:51.739 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.m.VertxBasedMqttProtocolAdapter - cannot process message [client ID: HealthCheck_c8ca8231-a348-4d58-b135-1c24883611d8_mqtt, topic: telemetry/DEFAULT_TENANT/HealthCheck_c8ca8231-a348-4d58-b135-1c24883611d8_mqtt, QoS: AT_MOST_ONCE]: sender link not established yet
05:03:51.739 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.m.VertxBasedMqttProtocolAdapter - closing connection with client [client ID: HealthCheck_c8ca8231-a348-4d58-b135-1c24883611d8_mqtt]
05:03:51.767 [vert.x-eventloop-thread-0] DEBUG o.e.h.c.i.AbstractRequestResponseClient - received response [correlation ID: reg-client-27, status: 200]
05:03:56.585 [vert.x-eventloop-thread-0] INFO o.e.h.a.m.VertxBasedMqttProtocolAdapter - Connection request from client esp8266.600194195a1b
05:03:56.629 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - reusing existing registration client for [DEFAULT_TENANT]
05:03:56.629 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - already trying to create a message sender for telemetry/DEFAULT_TENANT
05:03:56.629 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.m.VertxBasedMqttProtocolAdapter - cannot process message [client ID: esp8266.600194195a1b, topic: telemetry/DEFAULT_TENANT/esp8266.600194195a1b, QoS: AT_MOST_ONCE]: sender link not established yet
05:03:56.629 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.m.VertxBasedMqttProtocolAdapter - closing connection with client [client ID: esp8266.600194195a1b]
05:03:56.661 [vert.x-eventloop-thread-0] DEBUG o.e.h.c.i.AbstractRequestResponseClient - received response [correlation ID: reg-client-28, status: 200]
05:04:06.689 [vert.x-eventloop-thread-0] INFO o.e.h.a.m.VertxBasedMqttProtocolAdapter - Connection request from client esp8266.600194195a1b
05:04:06.732 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - reusing existing registration client for [DEFAULT_TENANT]
05:04:06.733 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - already trying to create a message sender for telemetry/DEFAULT_TENANT
05:04:06.733 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.m.VertxBasedMqttProtocolAdapter - cannot process message [client ID: esp8266.600194195a1b, topic: telemetry/DEFAULT_TENANT/esp8266.600194195a1b, QoS: AT_MOST_ONCE]: sender link not established yet
05:04:06.733 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.m.VertxBasedMqttProtocolAdapter - closing connection with client [client ID: esp8266.600194195a1b]
05:04:06.762 [vert.x-eventloop-thread-0] DEBUG o.e.h.c.i.AbstractRequestResponseClient - received response [correlation ID: reg-client-29, status: 200]
05:04:08.305 [vert.x-eventloop-thread-0] INFO o.e.h.a.m.VertxBasedMqttProtocolAdapter - Connection request from client HealthCheck_1467bc4a-e738-45ab-9390-cd590e9532bc_mqtt
05:04:08.308 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - reusing existing registration client for [DEFAULT_TENANT]
05:04:08.308 [vert.x-eventloop-thread-0] DEBUG o.e.hono.client.impl.HonoClientImpl - already trying to create a message sender for telemetry/DEFAULT_TENANT
05:04:08.308 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.m.VertxBasedMqttProtocolAdapter - cannot process message [client ID: HealthCheck_1467bc4a-e738-45ab-9390-cd590e9532bc_mqtt, topic: telemetry/DEFAULT_TENANT/HealthCheck_1467bc4a-e738-45ab-9390-cd590e9532bc_mqtt, QoS: AT_MOST_ONCE]: sender link not established yet
05:04:08.308 [vert.x-eventloop-thread-0] DEBUG o.e.h.a.m.VertxBasedMqttProtocolAdapter - closing connection with client [client ID: HealthCheck_1467bc4a-e738-45ab-9390-cd590e9532bc_mqtt]
05:04:08.337 [vert.x-eventloop-thread-0] DEBUG o.e.h.c.i.AbstractRequestResponseClient - received response [correlation ID: reg-client-30, status: 200]
```

Notice the recurring output of

> already trying to create a message sender for telemetry/DEFAULT_TENANT",2017-06-27 08:56:22,"[{'commitHash': 'c1550b3e8b39f061226873d180eb14ec52bb8e20', 'commitGHEventType': 'referenced', 'commitUser': 'sophokles73', 'commitParents': ['e317e2298dc6aff6349ec64275087d3fea94d4b5'], 'nameRev': 'c1550b3e8b39f061226873d180eb14ec52bb8e20 tags/0.5-M6~41', 'commitMessage': '[#231] Remove sender creation locks when connection fails.\n\nThe locks indicating an ongoing attempt to open a sender link are now\nremoved if the connection to the server fails during such an attempt.\n\nThis way, the next attempt to open a sender (once the connection has\nbeen re-established) does not fail due to an attempt already being in\nprogress.\n\nSigned-off-by: Kai Hudalla <kai.hudalla@bosch-si.com>', 'commitDateTime': '2017-06-23 14:54:56', 'authoredDateTime': '2017-06-23 14:54:56', 'commitGitStats': [{'filePath': 'client/src/main/java/org/eclipse/hono/client/impl/HonoClientImpl.java', 'insertions': 10, 'deletions': 8, 'lines': 18}, {'filePath': 'client/src/test/java/org/eclipse/hono/client/impl/HonoClientImplTest.java', 'insertions': 41, 'deletions': 0, 'lines': 41}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'HonoClientImpl.java', 'spoonMethods': [{'spoonMethodName': 'org.eclipse.hono.client.impl.HonoClientImpl.getConnectionStatus()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.eclipse.hono.client.impl.HonoClientImpl.onRemoteClose(io.vertx.core.AsyncResult)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.eclipse.hono.client.impl.HonoClientImpl.onRemoteDisconnect(io.vertx.proton.ProtonConnection,io.vertx.core.Handler)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.eclipse.hono.client.impl.HonoClientImpl.checkConnection()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.eclipse.hono.client.impl.HonoClientImpl.createRegistrationClient(java.lang.String,io.vertx.core.Handler)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.eclipse.hono.client.impl.HonoClientImpl.createCredentialsClient(java.lang.String,io.vertx.core.Handler)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.eclipse.hono.client.impl.HonoClientImpl.connect(io.vertx.proton.ProtonClientOptions,io.vertx.core.Handler,io.vertx.core.Handler)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.eclipse.hono.client.impl.HonoClientImpl.reconnect(io.vertx.core.Handler)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.eclipse.hono.client.impl.HonoClientImpl.getOrCreateSender(java.lang.String,java.util.function.Consumer,io.vertx.core.Handler)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'HonoClientImplTest.java', 'spoonMethods': [{'spoonMethodName': 'org.eclipse.hono.client.impl.HonoClientImplTest.testDownstreamDisconnectClearsSenderCreationLocks(io.vertx.ext.unit.TestContext)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/eclipse/hono/issues/231,3.000277777777778,['bug'],Protocol Adapters can reach state where re-creation of message sender fails,1.0,"['org.eclipse.hono.client.impl.HonoClientImpl.createCredentialsClient(java.lang.String,io.vertx.core.Handler)', 'org.eclipse.hono.client.impl.HonoClientImpl.onRemoteClose(io.vertx.core.AsyncResult)', 'org.eclipse.hono.client.impl.HonoClientImpl.connect(io.vertx.proton.ProtonClientOptions,io.vertx.core.Handler,io.vertx.core.Handler)', 'org.eclipse.hono.client.impl.HonoClientImpl.getConnectionStatus()', 'org.eclipse.hono.client.impl.HonoClientImpl.getOrCreateSender(java.lang.String,java.util.function.Consumer,io.vertx.core.Handler)', 'org.eclipse.hono.client.impl.HonoClientImpl.onRemoteDisconnect(io.vertx.proton.ProtonConnection,io.vertx.core.Handler)', 'org.eclipse.hono.client.impl.HonoClientImpl.reconnect(io.vertx.core.Handler)', 'org.eclipse.hono.client.impl.HonoClientImpl.createRegistrationClient(java.lang.String,io.vertx.core.Handler)', 'org.eclipse.hono.client.impl.HonoClientImpl.checkConnection()']",['c1550b3e8b39f061226873d180eb14ec52bb8e20'],,['client/src/main/java/org/eclipse/hono/client/impl'],10.0,8.0,18.0,1.0,8.0,9.0,9.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,hono
32942,2017-04-03 09:13:26,calohmn,"Sometimes, the HonoClientImplTest fails with this error:

> Running org.eclipse.hono.client.impl.HonoClientImplTest
> 10:08:29.303 [main] DEBUG o.e.hono.client.impl.HonoClientImpl - creating new message sender for telemetry/tenant
> 10:08:29.313 [main] DEBUG o.e.hono.client.impl.HonoClientImpl - already trying to create a message sender for telemetry/tenant
> Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.619 sec <<< FAILURE! - in org.eclipse.hono.client.impl.HonoClientImplTest
> testGetOrCreateTelemetrySenderFailsIfInvokedConcurrently(org.eclipse.hono.client.impl.HonoClientImplTest)  Time elapsed: 0.567 sec  <<< FAILURE!
> org.mockito.exceptions.verification.WantedButNotInvoked: 
> 
> Wanted but not invoked:
> protonConnection.createSender(
>     ""telemetry/tenant""
> );
> -> at org.eclipse.hono.client.impl.HonoClientImplTest.testGetOrCreateTelemetrySenderFailsIfInvokedConcurrently(HonoClientImplTest.java:91)
> 
> However, there were other interactions with this mock:
> protonConnection.isDisconnected();
> -> at org.eclipse.hono.client.impl.HonoClientImpl.checkConnection(HonoClientImpl.java:361)
> 
> 
> 	at org.eclipse.hono.client.impl.HonoClientImplTest.testGetOrCreateTelemetrySenderFailsIfInvokedConcurrently(HonoClientImplTest.java:91)

See here for example:
https://travis-ci.org/eclipse/hono/builds/217209692",2017-04-04 11:18:19,"[{'commitHash': 'cf5efbfd6ff1b14d723e50fcc2294a7ecd1adc83', 'commitGHEventType': 'referenced', 'commitUser': 'sophokles73', 'commitParents': ['68e9831900429b64e3ac451fbb68a2b6fab8581c'], 'nameRev': 'cf5efbfd6ff1b14d723e50fcc2294a7ecd1adc83 tags/0.5-M5~16', 'commitMessage': '[#145] Fix race condition in unit test.\n\nSigned-off-by: Kai Hudalla <kai.hudalla@bosch-si.com>', 'commitDateTime': '2017-04-03 17:26:06', 'authoredDateTime': '2017-04-03 17:26:06', 'commitGitStats': [{'filePath': 'client/src/main/java/org/eclipse/hono/client/impl/HonoClientImpl.java', 'insertions': 4, 'deletions': 7, 'lines': 11}, {'filePath': 'client/src/test/java/org/eclipse/hono/client/impl/HonoClientImplTest.java', 'insertions': 15, 'deletions': 18, 'lines': 33}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'HonoClientImpl.java', 'spoonMethods': [{'spoonMethodName': 'org.eclipse.hono.client.impl.HonoClientImpl.getOrCreateSender(java.lang.String,java.util.function.Consumer,io.vertx.core.Handler)', 'TOT': 6, 'UPD': 2, 'INS': 0, 'MOV': 0, 'DEL': 4}, {'spoonMethodName': 'org.eclipse.hono.client.impl.HonoClientImpl.getOrCreateTelemetrySender(java.lang.String,java.lang.String,io.vertx.core.Handler)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'org.eclipse.hono.client.impl.HonoClientImpl.getOrCreateEventSender(java.lang.String,java.lang.String,io.vertx.core.Handler)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}]}, {'spoonFilePath': 'HonoClientImplTest.java', 'spoonMethods': [{'spoonMethodName': 'org.eclipse.hono.client.impl.HonoClientImplTest.testGetOrCreateTelemetrySenderFailsIfInvokedConcurrently(io.vertx.ext.unit.TestContext)', 'TOT': 34, 'UPD': 13, 'INS': 2, 'MOV': 9, 'DEL': 10}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/eclipse/hono/issues/145,1.0002777777777778,['bug'],Intermittent failures of HonoClientImplTest,1.0,"['org.eclipse.hono.client.impl.HonoClientImpl.getOrCreateTelemetrySender(java.lang.String,java.lang.String,io.vertx.core.Handler)', 'org.eclipse.hono.client.impl.HonoClientImpl.getOrCreateEventSender(java.lang.String,java.lang.String,io.vertx.core.Handler)', 'org.eclipse.hono.client.impl.HonoClientImpl.getOrCreateSender(java.lang.String,java.util.function.Consumer,io.vertx.core.Handler)']",['cf5efbfd6ff1b14d723e50fcc2294a7ecd1adc83'],,['client/src/main/java/org/eclipse/hono/client/impl'],4.0,7.0,11.0,1.0,2.0,3.0,8.0,0.0,0.0,6.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,hono
32945,2017-03-07 15:56:31,sophokles73,"When the MQTT adapter is starting up, it throws a NoClassDefFound exception because it requires `io.vertx.core.net.impl.NetServerBase` which is provided by vertx-core-3.3.4.Beta1 only. However, we are using vertx-core-3.3.3 ...

Here's the stack tracec:

15:43:34.851 [vert.x-eventloop-thread-2] ERROR o.e.h.a.m.Application$$EnhancerBySpringCGLIB$$5f2015e6 - could not start 'MQTT' adapter
java.lang.NoClassDefFoundError: io/vertx/core/net/impl/NetServerBase
	at java.lang.ClassLoader.defineClass1(Native Method) ~[na:1.8.0_102]
	at java.lang.ClassLoader.defineClass(ClassLoader.java:763) ~[na:1.8.0_102]
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) ~[na:1.8.0_102]
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:467) ~[na:1.8.0_102]
	at java.net.URLClassLoader.access$100(URLClassLoader.java:73) ~[na:1.8.0_102]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:368) ~[na:1.8.0_102]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:362) ~[na:1.8.0_102]
	at java.security.AccessController.doPrivileged(Native Method) ~[na:1.8.0_102]
	at java.net.URLClassLoader.findClass(URLClassLoader.java:361) ~[na:1.8.0_102]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[na:1.8.0_102]
	at org.springframework.boot.loader.LaunchedURLClassLoader.loadClass(LaunchedURLClassLoader.java:89) ~[hono-adapter-mqtt-vertx-0.5-M5-SNAPSHOT.jar:na]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[na:1.8.0_102]
	at io.vertx.mqtt.MqttServer.create(MqttServer.java:45) ~[vertx-mqtt-server-3.4.0.Beta1.jar!/:na]
	at org.eclipse.hono.adapter.mqtt.VertxBasedMqttProtocolAdapter.bindMqttServer(VertxBasedMqttProtocolAdapter.java:88) ~[classes!/:na]
	at org.eclipse.hono.adapter.mqtt.VertxBasedMqttProtocolAdapter.start(VertxBasedMqttProtocolAdapter.java:130) ~[classes!/:na]
	at io.vertx.core.impl.DeploymentManager.lambda$doDeploy$8(DeploymentManager.java:434) ~[vertx-core-3.3.3.jar!/:na]
	at io.vertx.core.impl.ContextImpl.lambda$wrapTask$2(ContextImpl.java:316) ~[vertx-core-3.3.3.jar!/:na]
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163) ~[netty-common-4.1.8.Final.jar!/:4.1.8.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403) ~[netty-common-4.1.8.Final.jar!/:4.1.8.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:445) ~[netty-transport-4.1.8.Final.jar!/:4.1.8.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) ~[netty-common-4.1.8.Final.jar!/:4.1.8.Final]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_102]
Caused by: java.lang.ClassNotFoundException: io.vertx.core.net.impl.NetServerBase
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[na:1.8.0_102]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[na:1.8.0_102]
	at org.springframework.boot.loader.LaunchedURLClassLoader.loadClass(LaunchedURLClassLoader.java:89) ~[hono-adapter-mqtt-vertx-0.5-M5-SNAPSHOT.jar:na]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[na:1.8.0_102]
	... 22 common frames omitted
",2017-03-10 08:45:18,"[{'commitHash': '4305b364b0ffe5f79108e16bb40856ffa39c70be', 'commitGHEventType': 'referenced', 'commitUser': 'sophokles73', 'commitParents': ['af2149863f5a746cc7282bb43741bd954b74df11'], 'nameRev': '4305b364b0ffe5f79108e16bb40856ffa39c70be tags/0.5-M5~54', 'commitMessage': '[#110] Set scope of adapter and HonoClient to ""prototype"".\n\nWithout the ""prototype"" scope only one instance of the adapter is\ncreated and deployed multiple times. Same holds true for the HonoClient.\nWithout the ""prototype"" scope only one client is used and accessed\nconcurrently by multiple adapter instances (for which the HonoClient has\nnot been designed).', 'commitDateTime': '2017-03-10 10:08:49', 'authoredDateTime': '2017-03-10 10:08:49', 'commitGitStats': [{'filePath': 'adapters/mqtt-vertx/src/main/java/org/eclipse/hono/adapter/mqtt/Config.java', 'insertions': 2, 'deletions': 0, 'lines': 2}, {'filePath': 'adapters/mqtt-vertx/src/main/java/org/eclipse/hono/adapter/mqtt/VertxBasedMqttProtocolAdapter.java', 'insertions': 2, 'deletions': 0, 'lines': 2}, {'filePath': 'adapters/rest-vertx/src/main/java/org/eclipse/hono/adapter/rest/Config.java', 'insertions': 2, 'deletions': 0, 'lines': 2}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'Config.java', 'spoonMethods': [{'spoonMethodName': 'org.eclipse.hono.adapter.rest.Config.honoClient()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'VertxBasedMqttProtocolAdapter.java', 'spoonMethods': [{'spoonMethodName': 'org.eclipse.hono.adapter.mqtt.VertxBasedMqttProtocolAdapter', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/eclipse/hono/issues/110,2.000277777777778,"['MQTT Adapter', 'bug']",MQTT adapter doesn't start anymore,1.0,"['org.eclipse.hono.adapter.rest.Config.honoClient()', 'org.eclipse.hono.adapter.mqtt.VertxBasedMqttProtocolAdapter']",['4305b364b0ffe5f79108e16bb40856ffa39c70be'],,"['adapters/mqtt-vertx/src/main/java/org/eclipse/hono/adapter/mqtt', 'adapters/rest-vertx/src/main/java/org/eclipse/hono/adapter/rest']",6.0,0.0,6.0,3.0,0.0,2.0,2.0,0.0,2.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,hono
33000,2017-09-26 05:52:34,buffcode,"### Prerequisites

<!-- Put an X between the brackets on this line if you have done all of the following: -->

* [X] Plugin is in the latest version
* [X]  Issue was not reported yet
* [X] Stack trace (if provided) contains `mobi.hsz.idea.gitignore` package name


### Description

IDE Fatal error ""Accessing 'IgnoreFilesIndex' during processing 'FilenameIndex'. Nested different indices processing may cause deadlock"" on startup.

```
Accessing 'IgnoreFilesIndex' during processing 'FilenameIndex'. Nested different indices processing may cause deadlock
java.lang.Throwable
	at com.intellij.openapi.diagnostic.Logger.error(Logger.java:127)
	at com.intellij.util.indexing.IndexAccessValidator.checkAccessingIndexDuringOtherIndexProcessing(IndexAccessValidator.java:37)
	at com.intellij.util.indexing.FileBasedIndexImpl.a(FileBasedIndexImpl.java:829)
	at com.intellij.util.indexing.FileBasedIndexImpl.a(FileBasedIndexImpl.java:905)
	at com.intellij.util.indexing.FileBasedIndexImpl.getValues(FileBasedIndexImpl.java:771)
	at mobi.hsz.idea.gitignore.indexing.IgnoreFilesIndex.getEntries(IgnoreFilesIndex.java:233)
	at mobi.hsz.idea.gitignore.indexing.IgnoreFilesIndex.getFiles(IgnoreFilesIndex.java:249)
	at mobi.hsz.idea.gitignore.lang.kind.GitLanguage.getOuterFiles(GitLanguage.java:119)
	at mobi.hsz.idea.gitignore.lang.IgnoreLanguage.getOuterFiles(IgnoreLanguage.java:221)
	at mobi.hsz.idea.gitignore.indexing.ExternalIndexableSetContributor.getAdditionalFiles(ExternalIndexableSetContributor.java:77)
	at mobi.hsz.idea.gitignore.indexing.ExternalIndexableSetContributor.getAdditionalProjectRootsToIndex(ExternalIndexableSetContributor.java:102)
	at com.intellij.util.indexing.IndexableSetContributor.getProjectRootsToIndex(IndexableSetContributor.java:38)
	at com.intellij.util.indexing.FileBasedIndexImpl.a(FileBasedIndexImpl.java:2258)
	at com.intellij.util.indexing.FileBasedIndexImpl.iterateIndexableFiles(FileBasedIndexImpl.java:2222)
	at com.intellij.util.indexing.FileBasedIndexImpl.projectIndexableFiles(FileBasedIndexImpl.java:1012)
	at com.intellij.util.indexing.FileBasedIndexImpl.a(FileBasedIndexImpl.java:883)
	at com.intellij.util.indexing.FileBasedIndexImpl.a(FileBasedIndexImpl.java:834)
	at com.intellij.util.indexing.FileBasedIndexImpl.a(FileBasedIndexImpl.java:905)
	at com.intellij.util.indexing.FileBasedIndexImpl.getContainingFiles(FileBasedIndexImpl.java:787)
	at com.intellij.psi.search.FilenameIndex.getVirtualFilesByName(FilenameIndex.java:92)
	at com.intellij.docker.DockerDeploymentRuntimeProvider$2.compute(DockerDeploymentRuntimeProvider.java:63)
	at com.intellij.docker.DockerDeploymentRuntimeProvider$2.compute(DockerDeploymentRuntimeProvider.java:61)
	at com.intellij.openapi.application.impl.ApplicationImpl.runReadAction(ApplicationImpl.java:859)
	at com.intellij.docker.DockerDeploymentRuntimeProvider.getDockerFiles(DockerDeploymentRuntimeProvider.java:61)
	at com.intellij.docker.DockerFileDetector.initNotifiers(DockerFileDetector.java:86)
	at com.intellij.docker.DockerFileDetector.projectOpened(DockerFileDetector.java:60)
	at com.intellij.openapi.project.impl.ProjectImpl.c(ProjectImpl.java:383)
	at com.intellij.openapi.project.impl.ProjectImpl.access$200(ProjectImpl.java:68)
	at com.intellij.openapi.project.impl.ProjectImpl$MyProjectManagerListener.projectOpened(ProjectImpl.java:418)
	at com.intellij.openapi.project.impl.ProjectManagerImpl$1.projectOpened(ProjectManagerImpl.java:91)
	at com.intellij.openapi.project.impl.ProjectManagerImpl.h(ProjectManagerImpl.java:655)
	at com.intellij.openapi.project.impl.ProjectManagerImpl.i(ProjectManagerImpl.java:329)
	at com.intellij.openapi.application.TransactionGuardImpl$3.run(TransactionGuardImpl.java:168)
	at com.intellij.openapi.application.TransactionGuardImpl.a(TransactionGuardImpl.java:88)
	at com.intellij.openapi.application.TransactionGuardImpl.access$300(TransactionGuardImpl.java:40)
	at com.intellij.openapi.application.TransactionGuardImpl$2.run(TransactionGuardImpl.java:113)
	at com.intellij.openapi.application.impl.LaterInvocator$FlushQueue.a(LaterInvocator.java:326)
	at com.intellij.openapi.application.impl.LaterInvocator$FlushQueue.run(LaterInvocator.java:310)
	at java.awt.event.InvocationEvent.dispatch(InvocationEvent.java:311)
	at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:756)
	at java.awt.EventQueue.access$500(EventQueue.java:97)
	at java.awt.EventQueue$3.run(EventQueue.java:709)
	at java.awt.EventQueue$3.run(EventQueue.java:703)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:80)
	at java.awt.EventQueue.dispatchEvent(EventQueue.java:726)
	at com.intellij.ide.IdeEventQueue.c(IdeEventQueue.java:795)
	at com.intellij.ide.IdeEventQueue._dispatchEvent(IdeEventQueue.java:631)
	at com.intellij.ide.IdeEventQueue.dispatchEvent(IdeEventQueue.java:387)
	at com.intellij.ide.IdeEventQueue.pumpEventsForHierarchy(IdeEventQueue.java:882)
	at com.intellij.openapi.progress.util.ProgressWindow.startBlocking(ProgressWindow.java:207)
	at com.intellij.openapi.progress.util.ProgressWindow.startBlocking(ProgressWindow.java:194)
	at com.intellij.openapi.application.impl.ApplicationImpl.runProcessWithProgressSynchronously(ApplicationImpl.java:562)
	at com.intellij.openapi.progress.impl.CoreProgressManager.runProcessWithProgressSynchronously(CoreProgressManager.java:396)
	at com.intellij.openapi.progress.impl.ProgressManagerImpl.runProcessWithProgressSynchronously(ProgressManagerImpl.java:76)
	at com.intellij.openapi.progress.impl.CoreProgressManager.runProcessWithProgressSynchronously(CoreProgressManager.java:259)
	at com.intellij.openapi.progress.impl.CoreProgressManager.runProcessWithProgressSynchronously(CoreProgressManager.java:211)
	at com.intellij.openapi.project.impl.ProjectManagerImpl.openProject(ProjectManagerImpl.java:357)
	at com.intellij.platform.PlatformProjectOpenProcessor.doOpenProject(PlatformProjectOpenProcessor.java:217)
	at com.intellij.ide.RecentDirectoryProjectsManager.doOpenProject(RecentDirectoryProjectsManager.java:55)
	at com.intellij.ide.ReopenProjectAction.actionPerformed(ReopenProjectAction.java:68)
	at com.intellij.openapi.wm.impl.welcomeScreen.RecentProjectPanel$2.onClick(RecentProjectPanel.java:126)
	at com.intellij.ui.ClickListener$1.mouseReleased(ClickListener.java:73)
	at java.awt.AWTEventMulticaster.mouseReleased(AWTEventMulticaster.java:290)
	at java.awt.AWTEventMulticaster.mouseReleased(AWTEventMulticaster.java:289)
	at java.awt.AWTEventMulticaster.mouseReleased(AWTEventMulticaster.java:289)
	at java.awt.Component.processMouseEvent(Component.java:6533)
	at javax.swing.JComponent.processMouseEvent(JComponent.java:3324)
	at com.intellij.openapi.wm.impl.welcomeScreen.RecentProjectPanel$MyList.processMouseEvent(RecentProjectPanel.java:347)
	at java.awt.Component.processEvent(Component.java:6298)
	at java.awt.Container.processEvent(Container.java:2236)
	at java.awt.Component.dispatchEventImpl(Component.java:4889)
	at java.awt.Container.dispatchEventImpl(Container.java:2294)
	at java.awt.Component.dispatchEvent(Component.java:4711)
	at java.awt.LightweightDispatcher.retargetMouseEvent(Container.java:4888)
	at java.awt.LightweightDispatcher.processMouseEvent(Container.java:4525)
	at java.awt.LightweightDispatcher.dispatchEvent(Container.java:4466)
	at java.awt.Container.dispatchEventImpl(Container.java:2280)
	at java.awt.Window.dispatchEventImpl(Window.java:2746)
	at java.awt.Component.dispatchEvent(Component.java:4711)
	at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:758)
	at java.awt.EventQueue.access$500(EventQueue.java:97)
	at java.awt.EventQueue$3.run(EventQueue.java:709)
	at java.awt.EventQueue$3.run(EventQueue.java:703)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:80)
	at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:90)
	at java.awt.EventQueue$4.run(EventQueue.java:731)
	at java.awt.EventQueue$4.run(EventQueue.java:729)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:80)
	at java.awt.EventQueue.dispatchEvent(EventQueue.java:728)
	at com.intellij.ide.IdeEventQueue.c(IdeEventQueue.java:795)
	at com.intellij.ide.IdeEventQueue._dispatchEvent(IdeEventQueue.java:627)
	at com.intellij.ide.IdeEventQueue.dispatchEvent(IdeEventQueue.java:387)
	at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:201)
	at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:116)
	at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:105)
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:101)
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:93)
	at java.awt.EventDispatchThread.run(EventDispatchThread.java:82)
```


### Steps to Reproduce

Cannot reproduce currently, did not happen before. Triggered on PHPStorm startup during project load.

**Expected behavior:** no errors :)

**Actual behavior:** IDE Fatal error

**Reproduces how often:** unknown


### Versions

**Plugin:** 2.2.1


**IDE:** 
```
 PhpStorm 2016.2.2
Build #PS-162.2380.11, built on October 24, 2016
JRE: 1.8.0_112-b15 amd64
JVM: Java HotSpot(TM) 64-Bit Server VM by Oracle Corporation
```

**OS:** 

```
Windows 10 Pro x64
```",2017-11-21 20:05:06,"[{'commitHash': '1ffcd72fdfec23ed983b6aa2087eb8d8cad30114', 'commitGHEventType': 'referenced', 'commitUser': 'hsz', 'commitParents': ['8b14db6fe75806dbb37cc72c50ae1cfa09123c27'], 'nameRev': '1ffcd72fdfec23ed983b6aa2087eb8d8cad30114 tags/v2.3.1~6', 'commitMessage': ""#480 IDE Fatal Error: Accessing 'IgnoreFilesIndex' during processing 'FilenameIndex'\n"", 'commitDateTime': '2017-11-16 23:31:08', 'authoredDateTime': '2017-11-16 23:31:08', 'commitGitStats': [{'filePath': 'gradle.properties', 'insertions': 2, 'deletions': 2, 'lines': 4}, {'filePath': 'src/mobi/hsz/idea/gitignore/indexing/ExternalIndexableSetContributor.java', 'insertions': 1, 'deletions': 1, 'lines': 2}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'ExternalIndexableSetContributor.java', 'spoonMethods': [{'spoonMethodName': 'mobi.hsz.idea.gitignore.indexing.ExternalIndexableSetContributor.getAdditionalFiles(com.intellij.openapi.project.Project)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': 'bf9924731768cdd64a50a54abaa5b797a1690b11', 'commitGHEventType': 'referenced', 'commitUser': 'hsz', 'commitParents': ['00c83436f339eca1d7f58d5d3a7e804ab603b355'], 'nameRev': 'bf9924731768cdd64a50a54abaa5b797a1690b11 tags/v2.3.2~1', 'commitMessage': '#480 ConcurrentModificationException in IgnoreSettings.notifyOnChange\n', 'commitDateTime': '2017-11-17 11:38:20', 'authoredDateTime': '2017-11-17 11:38:20', 'commitGitStats': [{'filePath': 'src/mobi/hsz/idea/gitignore/settings/IgnoreSettings.java', 'insertions': 2, 'deletions': 1, 'lines': 3}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'IgnoreSettings.java', 'spoonMethods': [{'spoonMethodName': 'mobi.hsz.idea.gitignore.settings.IgnoreSettings', 'TOT': 2, 'UPD': 2, 'INS': 0, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/JetBrains/idea-gitignore/issues/480,56.000277777777775,"['bug', 'ready for release']",IDE Fatal Error: Accessing 'IgnoreFilesIndex' during processing 'FilenameIndex',2.0,"['mobi.hsz.idea.gitignore.indexing.ExternalIndexableSetContributor.getAdditionalFiles(com.intellij.openapi.project.Project)', 'mobi.hsz.idea.gitignore.settings.IgnoreSettings']","['1ffcd72fdfec23ed983b6aa2087eb8d8cad30114', 'bf9924731768cdd64a50a54abaa5b797a1690b11']",,"['src/mobi/hsz/idea/gitignore/indexing', 'src/mobi/hsz/idea/gitignore/settings', 'gradle.properties']",5.0,4.0,9.0,3.0,2.0,2.0,3.0,0.0,1.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,idea-gitignore
33052,2017-08-09 10:28:11,vbauer,"I've just caught the following exception:
```
null
java.util.ConcurrentModificationException
	at gnu.trove.THashIterator.nextIndex(THashIterator.java:83)
	at gnu.trove.TIterator.moveToNextIndex(TIterator.java:88)
	at gnu.trove.THashIterator.next(THashIterator.java:67)
	at com.intellij.util.containers.RefHashMap$EntrySet$1.hasNext(RefHashMap.java:308)
	at com.intellij.util.containers.RefHashMap$EntrySet.isEmpty(RefHashMap.java:340)
	at com.intellij.util.containers.RefHashMap.isEmpty(RefHashMap.java:205)
	at com.intellij.util.containers.WeakHashMap.isEmpty(WeakHashMap.java:36)
	at mobi.hsz.idea.gitignore.IgnoreManager.isFileTracked(IgnoreManager.java:369)
	at mobi.hsz.idea.gitignore.projectView.IgnoreViewNodeDecorator.decorate(IgnoreViewNodeDecorator.java:89)
	at com.intellij.ide.projectView.impl.nodes.AbstractPsiBasedNode.a(AbstractPsiBasedNode.java:169)
	at com.intellij.openapi.application.impl.ApplicationImpl.runReadAction(ApplicationImpl.java:934)
	at com.intellij.ide.projectView.impl.nodes.AbstractPsiBasedNode.update(AbstractPsiBasedNode.java:139)
	at com.intellij.ide.util.treeView.PresentableNodeDescriptor.getUpdatedPresentation(PresentableNodeDescriptor.java:88)
	at com.intellij.ide.util.treeView.PresentableNodeDescriptor.update(PresentableNodeDescriptor.java:41)
	at com.intellij.ide.util.treeView.AbstractTreeBuilder.updateNodeDescriptor(AbstractTreeBuilder.java:577)
	at com.intellij.ide.util.treeView.AbstractTreeUi$22.perform(AbstractTreeUi.java:935)
	at com.intellij.ide.util.treeView.TreeRunnable.run(TreeRunnable.java:36)
	at com.intellij.ide.util.treeView.AbstractTreeUi.execute(AbstractTreeUi.java:1856)
	at com.intellij.ide.util.treeView.AbstractTreeUi.update(AbstractTreeUi.java:931)
	at com.intellij.ide.util.treeView.AbstractTreeUi.update(AbstractTreeUi.java:883)
	at com.intellij.ide.util.treeView.AbstractTreeUi.access$1600(AbstractTreeUi.java:64)
	at com.intellij.ide.util.treeView.AbstractTreeUi$24$1.run(AbstractTreeUi.java:1092)
	at com.intellij.ide.util.treeView.AbstractTreeUi.execute(AbstractTreeUi.java:1835)
	at com.intellij.ide.util.treeView.AbstractTreeUi.access$5200(AbstractTreeUi.java:64)
	at com.intellij.ide.util.treeView.AbstractTreeUi$33.perform(AbstractTreeUi.java:1801)
	at com.intellij.ide.util.treeView.TreeRunnable.run(TreeRunnable.java:36)
	at com.intellij.ide.util.treeView.AbstractTreeUi.executeYieldingRequest(AbstractTreeUi.java:2090)
	at com.intellij.ide.util.treeView.AbstractTreeUi.access$5800(AbstractTreeUi.java:64)
	at com.intellij.ide.util.treeView.AbstractTreeUi$36$1.perform(AbstractTreeUi.java:2002)
	at com.intellij.ide.util.treeView.TreeRunnable.run(TreeRunnable.java:36)
	at com.intellij.ide.util.treeView.AbstractTreeBuilder.runOnYeildingDone(AbstractTreeBuilder.java:433)
	at com.intellij.ide.util.treeView.AbstractTreeUi.runOnYieldingDone(AbstractTreeUi.java:2185)
	at com.intellij.ide.util.treeView.AbstractTreeUi$36.perform(AbstractTreeUi.java:1997)
	at com.intellij.ide.util.treeView.TreeRunnable.run(TreeRunnable.java:36)
	at com.intellij.ide.util.treeView.AbstractTreeBuilder.lambda$new$0(AbstractTreeBuilder.java:51)
	at com.intellij.util.containers.TransferToEDTQueue.processNext(TransferToEDTQueue.java:102)
	at com.intellij.util.containers.TransferToEDTQueue.access$300(TransferToEDTQueue.java:37)
	at com.intellij.util.containers.TransferToEDTQueue$1.run(TransferToEDTQueue.java:57)
	at java.awt.event.InvocationEvent.dispatch(InvocationEvent.java:311)
	at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:762)
	at java.awt.EventQueue.access$500(EventQueue.java:98)
	at java.awt.EventQueue$3.run(EventQueue.java:715)
	at java.awt.EventQueue$3.run(EventQueue.java:709)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:80)
	at java.awt.EventQueue.dispatchEvent(EventQueue.java:732)
	at com.intellij.ide.IdeEventQueue.e(IdeEventQueue.java:821)
	at com.intellij.ide.IdeEventQueue._dispatchEvent(IdeEventQueue.java:649)
	at com.intellij.ide.IdeEventQueue.dispatchEvent(IdeEventQueue.java:365)
	at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:201)
	at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:116)
	at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:105)
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:101)
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:93)
	at java.awt.EventDispatchThread.run(EventDispatchThread.java:82)
```

IntelliJ IDEA 2017.2.1
Build #IU-172.3544.35, built on July 31, 2017

JRE: 1.8.0_152-release-915-b6 x86_64
JVM: OpenJDK 64-Bit Server VM by JetBrains s.r.o
Mac OS X 10.12.6",2017-08-22 20:16:37,"[{'commitHash': '805987a2443ab73f560ff9b4cc60206fa87c7695', 'commitGHEventType': 'referenced', 'commitUser': 'hsz', 'commitParents': ['3a6e9d7c335d92e304de972834a75b42ef24ab21'], 'nameRev': '805987a2443ab73f560ff9b4cc60206fa87c7695 tags/v2.1.0-RC.1~14', 'commitMessage': '#413 - ConcurrentModificationException in THashIterator\n', 'commitDateTime': '2017-08-19 22:28:23', 'authoredDateTime': '2017-08-19 22:28:23', 'commitGitStats': [{'filePath': 'src/mobi/hsz/idea/gitignore/IgnoreManager.java', 'insertions': 6, 'deletions': 6, 'lines': 12}, {'filePath': 'src/mobi/hsz/idea/gitignore/TrackedIgnoredFilesComponent.java', 'insertions': 2, 'deletions': 2, 'lines': 4}, {'filePath': 'src/mobi/hsz/idea/gitignore/actions/HandleTrackedIgnoredFilesAction.java', 'insertions': 5, 'deletions': 3, 'lines': 8}, {'filePath': 'src/mobi/hsz/idea/gitignore/ui/untrackFiles/UntrackFilesDialog.java', 'insertions': 3, 'deletions': 3, 'lines': 6}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'IgnoreManager.java', 'spoonMethods': [{'spoonMethodName': 'mobi.hsz.idea.gitignore.IgnoreManager', 'TOT': 3, 'UPD': 1, 'INS': 1, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'mobi.hsz.idea.gitignore.IgnoreManager.getConfirmedIgnoredFiles()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'mobi.hsz.idea.gitignore.IgnoreManager.RefreshTrackedIgnoredRunnable.run(boolean)', 'TOT': 3, 'UPD': 1, 'INS': 1, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'mobi.hsz.idea.gitignore.IgnoreManager.handleFiles(com.intellij.util.containers.WeakHashMap)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'TrackedIgnoredFilesComponent.java', 'spoonMethods': [{'spoonMethodName': 'mobi.hsz.idea.gitignore.TrackedIgnoredFilesComponent.handleFiles(com.intellij.util.containers.WeakHashMap)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'HandleTrackedIgnoredFilesAction.java', 'spoonMethods': [{'spoonMethodName': 'mobi.hsz.idea.gitignore.actions.HandleTrackedIgnoredFilesAction.getTrackedIgnoredFiles(com.intellij.openapi.actionSystem.AnActionEvent)', 'TOT': 3, 'UPD': 1, 'INS': 1, 'MOV': 0, 'DEL': 1}]}, {'spoonFilePath': 'UntrackFilesDialog.java', 'spoonMethods': [{'spoonMethodName': 'mobi.hsz.idea.gitignore.ui.untrackFiles.UntrackFilesDialog', 'TOT': 2, 'UPD': 2, 'INS': 0, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': 'a5b78353366e9db1e24da253235ea63590e8c616', 'commitGHEventType': 'referenced', 'commitUser': 'hsz', 'commitParents': ['805987a2443ab73f560ff9b4cc60206fa87c7695'], 'nameRev': 'a5b78353366e9db1e24da253235ea63590e8c616 tags/v2.1.0-RC.1~13', 'commitMessage': '#413 - changelog\n', 'commitDateTime': '2017-08-19 22:36:18', 'authoredDateTime': '2017-08-19 22:36:18', 'commitGitStats': [{'filePath': 'CHANGELOG.md', 'insertions': 12, 'deletions': 0, 'lines': 12}, {'filePath': 'README.md', 'insertions': 3, 'deletions': 19, 'lines': 22}, {'filePath': 'gradle.properties', 'insertions': 1, 'deletions': 1, 'lines': 2}, {'filePath': 'resources/META-INF/plugin.xml', 'insertions': 3, 'deletions': 18, 'lines': 21}, {'filePath': 'resources/messages/IgnoreBundle.properties', 'insertions': 1, 'deletions': 17, 'lines': 18}], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}]",https://github.com/JetBrains/idea-gitignore/issues/413,13.000277777777777,"['bug', 'ready for release']",ConcurrentModificationException in THashIterator,2.0,"['mobi.hsz.idea.gitignore.IgnoreManager.RefreshTrackedIgnoredRunnable.run(boolean)', 'mobi.hsz.idea.gitignore.ui.untrackFiles.UntrackFilesDialog', 'mobi.hsz.idea.gitignore.IgnoreManager.getConfirmedIgnoredFiles()', 'mobi.hsz.idea.gitignore.IgnoreManager.handleFiles(com.intellij.util.containers.WeakHashMap)', 'mobi.hsz.idea.gitignore.actions.HandleTrackedIgnoredFilesAction.getTrackedIgnoredFiles(com.intellij.openapi.actionSystem.AnActionEvent)', 'mobi.hsz.idea.gitignore.TrackedIgnoredFilesComponent.handleFiles(com.intellij.util.containers.WeakHashMap)', 'mobi.hsz.idea.gitignore.IgnoreManager']","['805987a2443ab73f560ff9b4cc60206fa87c7695', 'a5b78353366e9db1e24da253235ea63590e8c616']",,"['src/mobi/hsz/idea/gitignore', 'resources/messages/IgnoreBundle.properties', 'src/mobi/hsz/idea/gitignore/actions', 'gradle.properties', 'src/mobi/hsz/idea/gitignore/ui/untrackFiles']",18.0,32.0,50.0,6.0,8.0,7.0,14.0,0.0,3.0,3.0,4.0,0.0,0.0,0.0,0.0,0.0,0.0,idea-gitignore
33158,2016-05-03 18:51:30,briangordon,"Hi- 

I just got this stack trace from Intellij 16.1.1:

```
null
java.util.ConcurrentModificationException
    at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901)
    at java.util.ArrayList$Itr.next(ArrayList.java:851)
    at com.intellij.ide.bookmarks.BookmarkManager$2.documentCreated(BookmarkManager.java:100)
    at com.intellij.psi.impl.PsiDocumentManagerBase.fireDocumentCreated(PsiDocumentManagerBase.java:600)
    at com.intellij.psi.impl.PsiDocumentManagerImpl$1.fileContentLoaded(PsiDocumentManagerImpl.java:80)
    at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at com.intellij.util.messages.impl.MessageBusConnectionImpl.deliverMessage(MessageBusConnectionImpl.java:117)
    at com.intellij.util.messages.impl.MessageBusImpl.doPumpMessages(MessageBusImpl.java:372)
    at com.intellij.util.messages.impl.MessageBusImpl.pumpMessages(MessageBusImpl.java:359)
    at com.intellij.util.messages.impl.MessageBusImpl.sendMessage(MessageBusImpl.java:338)
    at com.intellij.util.messages.impl.MessageBusImpl.access$200(MessageBusImpl.java:42)
    at com.intellij.util.messages.impl.MessageBusImpl$2.invoke(MessageBusImpl.java:226)
    at com.sun.proxy.$Proxy21.fileContentLoaded(Unknown Source)
    at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at com.intellij.openapi.fileEditor.impl.FileDocumentManagerImpl.a(FileDocumentManagerImpl.java:136)
    at com.intellij.openapi.fileEditor.impl.FileDocumentManagerImpl.access$000(FileDocumentManagerImpl.java:86)
    at com.intellij.openapi.fileEditor.impl.FileDocumentManagerImpl$1.invoke(FileDocumentManagerImpl.java:114)
    at com.sun.proxy.$Proxy21.fileContentLoaded(Unknown Source)
    at com.intellij.openapi.fileEditor.impl.FileDocumentManagerImpl.getDocument(FileDocumentManagerImpl.java:215)
    at com.intellij.psi.SingleRootFileViewProvider.getDocument(SingleRootFileViewProvider.java:437)
    at com.intellij.psi.SingleRootFileViewProvider$VirtualFileContent.getText(SingleRootFileViewProvider.java:594)
    at com.intellij.psi.SingleRootFileViewProvider.getContents(SingleRootFileViewProvider.java:417)
    at com.intellij.psi.impl.source.PsiFileImpl.getText(PsiFileImpl.java:398)
    at mobi.hsz.idea.gitignore.IgnoreManager$5$1.run(IgnoreManager.java:437)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

I'm using .ignore 1.3.3.
",2016-05-04 08:34:33,"[{'commitHash': '9d5ac551f06a4c7e7caf751d752584edeb2c24e0', 'commitGHEventType': 'closed', 'commitUser': 'hsz', 'commitParents': ['b719ca6b42ecdcfd6c0df060e968c08378f72be0'], 'nameRev': '9d5ac551f06a4c7e7caf751d752584edeb2c24e0 tags/v1.4.1~24', 'commitMessage': 'Fixed #221 - ConcurrentModificationException in IgnoreManager\n', 'commitDateTime': '2016-05-04 10:31:19', 'authoredDateTime': '2016-05-04 10:31:19', 'commitGitStats': [{'filePath': 'src/mobi/hsz/idea/gitignore/IgnoreManager.java', 'insertions': 8, 'deletions': 4, 'lines': 12}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'IgnoreManager.java', 'spoonMethods': [{'spoonMethodName': 'mobi.hsz.idea.gitignore.IgnoreManager.retrieve().5.run().1.run()', 'TOT': 4, 'UPD': 0, 'INS': 1, 'MOV': 3, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/JetBrains/idea-gitignore/issues/221,0.0002777777777777778,['bug'],ConcurrentModificationException,1.0,['mobi.hsz.idea.gitignore.IgnoreManager.retrieve().5.run().1.run()'],['9d5ac551f06a4c7e7caf751d752584edeb2c24e0'],,['src/mobi/hsz/idea/gitignore'],8.0,4.0,12.0,1.0,0.0,1.0,4.0,3.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,idea-gitignore
33219,2015-03-02 10:25:35,distinctgrey,"Upgraded from v0.9 to 1.0 a few moments ago.
After restarting (and every subsequent restart) Webstorm (v9.0.3), I receive a ""ConcurrentModificationException: null"" in the Event Log.

Error details:

```
Exception in plugin .ignore

null
java.util.ConcurrentModificationException
    at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
    at java.util.HashMap$KeyIterator.next(HashMap.java:828)
    at mobi.hsz.idea.gitignore.IgnoreManager$IgnoreCacheMap.isFileIgnored(IgnoreManager.java:345)
    at mobi.hsz.idea.gitignore.IgnoreManager.isFileIgnored(IgnoreManager.java:213)
    at mobi.hsz.idea.gitignore.vcs.IgnoreFileStatusProvider.getFileStatus(IgnoreFileStatusProvider.java:68)
    at com.intellij.openapi.vcs.impl.FileStatusManagerImpl.calcStatus(FileStatusManagerImpl.java:133)
    at com.intellij.openapi.vcs.impl.FileStatusManagerImpl.getStatus(FileStatusManagerImpl.java:268)
    at com.intellij.ide.projectView.impl.nodes.AbstractPsiBasedNode.getFileStatus(AbstractPsiBasedNode.java:115)
    at com.intellij.ide.util.treeView.AbstractTreeNode.setForcedForeground(AbstractTreeNode.java:85)
    at com.intellij.ide.util.treeView.AbstractTreeNode.postprocess(AbstractTreeNode.java:77)
    at com.intellij.ide.util.treeView.PresentableNodeDescriptor.getUpdatedPresentation(PresentableNodeDescriptor.java:91)
    at com.intellij.ide.util.treeView.PresentableNodeDescriptor.update(PresentableNodeDescriptor.java:41)
    at com.intellij.ide.util.treeView.AbstractTreeBuilder.updateNodeDescriptor(AbstractTreeBuilder.java:594)
    at com.intellij.ide.util.treeView.AbstractTreeUi$22.run(AbstractTreeUi.java:944)
    at com.intellij.ide.util.treeView.AbstractTreeUi.execute(AbstractTreeUi.java:1830)
    at com.intellij.ide.util.treeView.AbstractTreeUi.update(AbstractTreeUi.java:940)
    at com.intellij.ide.util.treeView.AbstractTreeUi.update(AbstractTreeUi.java:892)
    at com.intellij.ide.util.treeView.AbstractTreeUi.collectNodesToInsert(AbstractTreeUi.java:2256)
    at com.intellij.ide.util.treeView.AbstractTreeUi.access$4500(AbstractTreeUi.java:62)
    at com.intellij.ide.util.treeView.AbstractTreeUi$30.run(AbstractTreeUi.java:1399)
    at com.intellij.openapi.util.ExecutionCallback.setExecuted(ExecutionCallback.java:59)
    at com.intellij.openapi.util.ActionCallback.setDone(ActionCallback.java:69)
    at com.intellij.openapi.util.ActionCallback$3.run(ActionCallback.java:233)
    at com.intellij.openapi.util.ExecutionCallback.doWhenExecuted(ExecutionCallback.java:101)
    at com.intellij.openapi.util.ActionCallback.doWhenDone(ActionCallback.java:108)
    at com.intellij.openapi.util.ActionCallback.notify(ActionCallback.java:153)
    at com.intellij.ide.util.treeView.AbstractTreeUi$34.run(AbstractTreeUi.java:1783)
    at com.intellij.ide.util.treeView.AbstractTreeUi.executeYieldingRequest(AbstractTreeUi.java:2091)
    at com.intellij.ide.util.treeView.AbstractTreeUi.access$5900(AbstractTreeUi.java:62)
    at com.intellij.ide.util.treeView.AbstractTreeUi$38$1.run(AbstractTreeUi.java:1983)
    at com.intellij.ide.util.treeView.AbstractTreeBuilder.runOnYeildingDone(AbstractTreeBuilder.java:439)
    at com.intellij.ide.util.treeView.AbstractTreeUi.runOnYieldingDone(AbstractTreeUi.java:2186)
    at com.intellij.ide.util.treeView.AbstractTreeUi$38.run(AbstractTreeUi.java:1978)
    at com.intellij.ide.util.treeView.AbstractTreeBuilder$1.process(AbstractTreeBuilder.java:53)
    at com.intellij.ide.util.treeView.AbstractTreeBuilder$1.process(AbstractTreeBuilder.java:50)
    at com.intellij.util.containers.TransferToEDTQueue.processNext(TransferToEDTQueue.java:97)
    at com.intellij.util.containers.TransferToEDTQueue.access$300(TransferToEDTQueue.java:35)
    at com.intellij.util.containers.TransferToEDTQueue$1.run(TransferToEDTQueue.java:56)
    at java.awt.event.InvocationEvent.dispatch(InvocationEvent.java:209)
    at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:715)
    at java.awt.EventQueue.access$400(EventQueue.java:82)
    at java.awt.EventQueue$2.run(EventQueue.java:676)
    at java.awt.EventQueue$2.run(EventQueue.java:674)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.security.AccessControlContext$1.doIntersectionPrivilege(AccessControlContext.java:86)
    at java.awt.EventQueue.dispatchEvent(EventQueue.java:685)
    at com.intellij.ide.IdeEventQueue.e(IdeEventQueue.java:748)
    at com.intellij.ide.IdeEventQueue._dispatchEvent(IdeEventQueue.java:577)
    at com.intellij.ide.IdeEventQueue.dispatchEvent(IdeEventQueue.java:384)
    at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:296)
    at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:211)
    at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:201)
    at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:196)
    at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:188)
    at java.awt.EventDispatchThread.run(EventDispatchThread.java:122)
```
",2015-03-04 10:21:10,"[{'commitHash': '5c2c75249f05a0b636f2a55296860d450d48beda', 'commitGHEventType': 'referenced', 'commitUser': 'hsz', 'commitParents': ['ea401b4acdb35a6b458184f93ac6f84276348c04'], 'nameRev': '5c2c75249f05a0b636f2a55296860d450d48beda tags/v1.0.1~11', 'commitMessage': 'Fix for ConcurrentModificationException in IgnoreManager (#84)\n', 'commitDateTime': '2015-03-02 11:36:20', 'authoredDateTime': '2015-03-02 11:36:20', 'commitGitStats': [{'filePath': 'src/mobi/hsz/idea/gitignore/IgnoreManager.java', 'insertions': 5, 'deletions': 5, 'lines': 10}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'IgnoreManager.java', 'spoonMethods': [{'spoonMethodName': 'mobi.hsz.idea.gitignore.IgnoreManager.IgnoreCacheMap', 'TOT': 3, 'UPD': 3, 'INS': 0, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/JetBrains/idea-gitignore/issues/84,1.0002777777777778,['bug'],ConcurrentModificationException in v1.0,1.0,['mobi.hsz.idea.gitignore.IgnoreManager.IgnoreCacheMap'],['5c2c75249f05a0b636f2a55296860d450d48beda'],,['src/mobi/hsz/idea/gitignore'],5.0,5.0,10.0,1.0,3.0,1.0,3.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,idea-gitignore
33700,2019-08-27 11:04:12,sergey-wowwow,"Hi @skylot,

I've noticed that a lot of apps are not decompiled completely. I caught one of them with a deadlock 

jadx-cli args
```
--no-replace-consts --show-bad-code --threads-count 8 --no-inline-anonymous --no-imports --deobf --deobf-min 2 --deobf-rewrite-cfg
```

Output of ``` jstack ```
```
Full thread dump Java HotSpot(TM) 64-Bit Server VM (12.0.2+10 mixed mode, sharing):

Threads class SMR info:
_java_thread_list=0x00007ffec3f45cc0, length=21, elements={
0x00007ffec200b000, 0x00007ffec60b1800, 0x00007ffec2004000, 0x00007ffec5834000,
0x00007ffec5833000, 0x00007ffec5837000, 0x00007ffec5008800, 0x00007ffec5860000,
0x00007ffec508f800, 0x00007ffec2923000, 0x00007ffec662e000, 0x00007ffec7755000,
0x00007ffec30af000, 0x00007ffec30b2000, 0x00007ffec4115000, 0x00007ffec32f3800,
0x00007ffec4116000, 0x00007ffec32fe800, 0x00007ffec5a9a000, 0x00007ffec742f000,
0x00007ffec8341800
}

""main"" #1 prio=5 os_prio=31 cpu=6028.67ms elapsed=150.42s tid=0x00007ffec200b000 nid=0x2203 waiting on condition  [0x000070000eb68000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at jdk.internal.misc.Unsafe.park(java.base@12.0.2/Native Method)
	- parking to wait for  <0x000000046b2754d0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(java.base@12.0.2/LockSupport.java:235)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(java.base@12.0.2/AbstractQueuedSynchronizer.java:2123)
	at java.util.concurrent.ThreadPoolExecutor.awaitTermination(java.base@12.0.2/ThreadPoolExecutor.java:1454)
	at jadx.api.JadxDecompiler.save(JadxDecompiler.java:143)
	at jadx.api.JadxDecompiler.save(JadxDecompiler.java:128)
	at jadx.cli.JadxCLI.processAndSave(JadxCLI.java:39)
	at jadx.cli.JadxCLI.main(JadxCLI.java:19)

   Locked ownable synchronizers:
	- None

""Reference Handler"" #2 daemon prio=10 os_prio=31 cpu=1.32ms elapsed=150.40s tid=0x00007ffec60b1800 nid=0x4403 waiting on condition  [0x000070001007d000]
   java.lang.Thread.State: RUNNABLE
	at java.lang.ref.Reference.waitForReferencePendingList(java.base@12.0.2/Native Method)
	at java.lang.ref.Reference.processPendingReferences(java.base@12.0.2/Reference.java:241)
	at java.lang.ref.Reference$ReferenceHandler.run(java.base@12.0.2/Reference.java:213)

   Locked ownable synchronizers:
	- None

""Finalizer"" #3 daemon prio=8 os_prio=31 cpu=0.20ms elapsed=150.40s tid=0x00007ffec2004000 nid=0x3703 in Object.wait()  [0x0000700010f80000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(java.base@12.0.2/Native Method)
	- waiting on <0x000000044479a3d0> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(java.base@12.0.2/ReferenceQueue.java:155)
	- locked <0x000000044479a3d0> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(java.base@12.0.2/ReferenceQueue.java:176)
	at java.lang.ref.Finalizer$FinalizerThread.run(java.base@12.0.2/Finalizer.java:170)

   Locked ownable synchronizers:
	- None

""Signal Dispatcher"" #4 daemon prio=9 os_prio=31 cpu=0.26ms elapsed=150.40s tid=0x00007ffec5834000 nid=0x3d03 runnable  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread0"" #5 daemon prio=9 os_prio=31 cpu=9832.54ms elapsed=150.40s tid=0x00007ffec5833000 nid=0xa903 waiting on condition  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
   No compile task

   Locked ownable synchronizers:
	- None

""C1 CompilerThread0"" #8 daemon prio=9 os_prio=31 cpu=3159.32ms elapsed=150.40s tid=0x00007ffec5837000 nid=0x5603 waiting on condition  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
   No compile task

   Locked ownable synchronizers:
	- None

""Sweeper thread"" #9 daemon prio=9 os_prio=31 cpu=82.48ms elapsed=150.40s tid=0x00007ffec5008800 nid=0xa603 runnable  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""Service Thread"" #10 daemon prio=9 os_prio=31 cpu=4.99ms elapsed=150.38s tid=0x00007ffec5860000 nid=0x5903 runnable  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""Common-Cleaner"" #11 daemon prio=8 os_prio=31 cpu=1.34ms elapsed=150.38s tid=0x00007ffec508f800 nid=0x5c03 in Object.wait()  [0x0000700014e95000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(java.base@12.0.2/Native Method)
	- waiting on <no object reference available>
	at java.lang.ref.ReferenceQueue.remove(java.base@12.0.2/ReferenceQueue.java:155)
	- locked <0x000000044479ad88> (a java.lang.ref.ReferenceQueue$Lock)
	at jdk.internal.ref.CleanerImpl.run(java.base@12.0.2/CleanerImpl.java:148)
	at java.lang.Thread.run(java.base@12.0.2/Thread.java:835)
	at jdk.internal.misc.InnocuousThread.run(java.base@12.0.2/InnocuousThread.java:134)

   Locked ownable synchronizers:
	- None

""pool-1-thread-1"" #12 prio=5 os_prio=31 cpu=2235.01ms elapsed=143.88s tid=0x00007ffec2923000 nid=0x8f03 waiting for monitor entry  [0x0000700016ecb000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at jadx.core.ProcessClass.process(ProcessClass.java:35)
	- waiting to lock <0x0000000444a58f28> (a jadx.core.dex.info.ClassInfo)
	at jadx.core.ProcessClass$$Lambda$153/0x000000080127a840.accept(Unknown Source)
	at java.util.ArrayList.forEach(java.base@12.0.2/ArrayList.java:1540)
	at jadx.core.ProcessClass.generateCode(ProcessClass.java:61)
	at jadx.core.dex.nodes.ClassNode.decompile(ClassNode.java:262)
	- locked <0x00000004423df9f0> (a jadx.core.dex.nodes.ClassNode)
	at jadx.api.JavaClass.getCodeInfo(JavaClass.java:53)
	at jadx.api.JadxDecompiler.lambda$appendSourcesSave$0(JadxDecompiler.java:201)
	at jadx.api.JadxDecompiler$$Lambda$74/0x00000008011f0040.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@12.0.2/ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@12.0.2/ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(java.base@12.0.2/Thread.java:835)

   Locked ownable synchronizers:
	- <0x000000046b276298> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""pool-1-thread-2"" #13 prio=5 os_prio=31 cpu=4026.55ms elapsed=143.88s tid=0x00007ffec662e000 nid=0x8c03 waiting for monitor entry  [0x0000700017dce000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at jadx.core.ProcessClass.process(ProcessClass.java:35)
	- waiting to lock <0x0000000444a53c20> (a jadx.core.dex.info.ClassInfo)
	at jadx.core.ProcessClass$$Lambda$153/0x000000080127a840.accept(Unknown Source)
	at java.util.ArrayList.forEach(java.base@12.0.2/ArrayList.java:1540)
	at jadx.core.ProcessClass.generateCode(ProcessClass.java:61)
	at jadx.core.dex.nodes.ClassNode.decompile(ClassNode.java:262)
	- locked <0x00000004423d76b8> (a jadx.core.dex.nodes.ClassNode)
	at jadx.api.JavaClass.getCodeInfo(JavaClass.java:53)
	at jadx.api.JadxDecompiler.lambda$appendSourcesSave$0(JadxDecompiler.java:201)
	at jadx.api.JadxDecompiler$$Lambda$74/0x00000008011f0040.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@12.0.2/ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@12.0.2/ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(java.base@12.0.2/Thread.java:835)

   Locked ownable synchronizers:
	- <0x000000046b2eadd8> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""pool-1-thread-3"" #14 prio=5 os_prio=31 cpu=2233.49ms elapsed=143.88s tid=0x00007ffec7755000 nid=0x7003 waiting for monitor entry  [0x0000700018cd0000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at jadx.core.ProcessClass.process(ProcessClass.java:35)
	- waiting to lock <0x0000000444a58f28> (a jadx.core.dex.info.ClassInfo)
	at jadx.core.dex.nodes.ClassNode.loadAndProcess(ClassNode.java:251)
	at jadx.core.dex.nodes.RootNode.getClassGenerics(RootNode.java:279)
	at jadx.core.utils.TypeUtils.replaceClassGenerics(TypeUtils.java:35)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.invokeListener(TypeUpdate.java:296)
	at jadx.core.dex.visitors.typeinference.TypeUpdate$$Lambda$45/0x00000008011adc40.update(Unknown Source)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.runListeners(TypeUpdate.java:190)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.requestUpdate(TypeUpdate.java:171)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.requestUpdateForSsaVar(TypeUpdate.java:148)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.updateTypeForSsaVar(TypeUpdate.java:136)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.updateTypeChecked(TypeUpdate.java:116)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.apply(TypeUpdate.java:73)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.apply(TypeUpdate.java:50)
	at jadx.core.dex.visitors.typeinference.TypeInferenceVisitor.applyImmutableType(TypeInferenceVisitor.java:155)
	at jadx.core.dex.visitors.typeinference.TypeInferenceVisitor.setImmutableType(TypeInferenceVisitor.java:136)
	at jadx.core.dex.visitors.typeinference.TypeInferenceVisitor$$Lambda$126/0x000000080126b840.accept(Unknown Source)
	at java.util.ArrayList.forEach(java.base@12.0.2/ArrayList.java:1540)
	at jadx.core.dex.visitors.typeinference.TypeInferenceVisitor.runTypePropagation(TypeInferenceVisitor.java:102)
	at jadx.core.dex.visitors.typeinference.TypeInferenceVisitor.visit(TypeInferenceVisitor.java:71)
	at jadx.core.dex.visitors.DepthTraversal.visit(DepthTraversal.java:30)
	at jadx.core.dex.visitors.DepthTraversal.lambda$visit$1(DepthTraversal.java:15)
	at jadx.core.dex.visitors.DepthTraversal$$Lambda$90/0x0000000801246840.accept(Unknown Source)
	at java.util.ArrayList.forEach(java.base@12.0.2/ArrayList.java:1540)
	at jadx.core.dex.visitors.DepthTraversal.visit(DepthTraversal.java:15)
	at jadx.core.dex.visitors.DepthTraversal.lambda$visit$0(DepthTraversal.java:14)
	at jadx.core.dex.visitors.DepthTraversal$$Lambda$89/0x0000000801247440.accept(Unknown Source)
	at java.util.ArrayList.forEach(java.base@12.0.2/ArrayList.java:1540)
	at jadx.core.dex.visitors.DepthTraversal.visit(DepthTraversal.java:14)
	at jadx.core.ProcessClass.process(ProcessClass.java:43)
	- locked <0x0000000444a53c20> (a jadx.core.dex.info.ClassInfo)
	at jadx.core.ProcessClass$$Lambda$153/0x000000080127a840.accept(Unknown Source)
	at java.util.ArrayList.forEach(java.base@12.0.2/ArrayList.java:1540)
	at jadx.core.ProcessClass.generateCode(ProcessClass.java:61)
	at jadx.core.dex.nodes.ClassNode.decompile(ClassNode.java:262)
	- locked <0x00000004422eb2f0> (a jadx.core.dex.nodes.ClassNode)
	at jadx.api.JavaClass.getCodeInfo(JavaClass.java:53)
	at jadx.api.JadxDecompiler.lambda$appendSourcesSave$0(JadxDecompiler.java:201)
	at jadx.api.JadxDecompiler$$Lambda$74/0x00000008011f0040.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@12.0.2/ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@12.0.2/ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(java.base@12.0.2/Thread.java:835)

   Locked ownable synchronizers:
	- <0x000000046b3604f8> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""pool-1-thread-4"" #15 prio=5 os_prio=31 cpu=2264.72ms elapsed=143.88s tid=0x00007ffec30af000 nid=0x8703 waiting for monitor entry  [0x0000700019bd4000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at jadx.core.ProcessClass.process(ProcessClass.java:35)
	- waiting to lock <0x0000000444a58f28> (a jadx.core.dex.info.ClassInfo)
	at jadx.core.ProcessClass$$Lambda$153/0x000000080127a840.accept(Unknown Source)
	at java.util.ArrayList.forEach(java.base@12.0.2/ArrayList.java:1540)
	at jadx.core.ProcessClass.generateCode(ProcessClass.java:61)
	at jadx.core.dex.nodes.ClassNode.decompile(ClassNode.java:262)
	- locked <0x00000004423f94e8> (a jadx.core.dex.nodes.ClassNode)
	at jadx.api.JavaClass.getCodeInfo(JavaClass.java:53)
	at jadx.api.JadxDecompiler.lambda$appendSourcesSave$0(JadxDecompiler.java:201)
	at jadx.api.JadxDecompiler$$Lambda$74/0x00000008011f0040.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@12.0.2/ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@12.0.2/ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(java.base@12.0.2/Thread.java:835)

   Locked ownable synchronizers:
	- <0x000000046b3d5550> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""pool-1-thread-5"" #16 prio=5 os_prio=31 cpu=2017.32ms elapsed=143.88s tid=0x00007ffec30b2000 nid=0x7203 waiting for monitor entry  [0x000070001aad7000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at jadx.core.ProcessClass.process(ProcessClass.java:35)
	- waiting to lock <0x0000000444a53c20> (a jadx.core.dex.info.ClassInfo)
	at jadx.core.ProcessClass$$Lambda$153/0x000000080127a840.accept(Unknown Source)
	at java.util.ArrayList.forEach(java.base@12.0.2/ArrayList.java:1540)
	at jadx.core.ProcessClass.generateCode(ProcessClass.java:61)
	at jadx.core.dex.nodes.ClassNode.decompile(ClassNode.java:262)
	- locked <0x00000004423a1f28> (a jadx.core.dex.nodes.ClassNode)
	at jadx.api.JavaClass.getCodeInfo(JavaClass.java:53)
	at jadx.api.JadxDecompiler.lambda$appendSourcesSave$0(JadxDecompiler.java:201)
	at jadx.api.JadxDecompiler$$Lambda$74/0x00000008011f0040.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@12.0.2/ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@12.0.2/ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(java.base@12.0.2/Thread.java:835)

   Locked ownable synchronizers:
	- <0x000000046b000360> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""pool-1-thread-6"" #17 prio=5 os_prio=31 cpu=2136.99ms elapsed=143.88s tid=0x00007ffec4115000 nid=0x7303 waiting for monitor entry  [0x000070001b9da000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at jadx.core.ProcessClass.process(ProcessClass.java:35)
	- waiting to lock <0x0000000444a53c20> (a jadx.core.dex.info.ClassInfo)
	at jadx.core.ProcessClass.generateCode(ProcessClass.java:60)
	at jadx.core.dex.nodes.ClassNode.decompile(ClassNode.java:262)
	- locked <0x000000044243a208> (a jadx.core.dex.nodes.ClassNode)
	at jadx.api.JavaClass.getCodeInfo(JavaClass.java:53)
	at jadx.api.JadxDecompiler.lambda$appendSourcesSave$0(JadxDecompiler.java:201)
	at jadx.api.JadxDecompiler$$Lambda$74/0x00000008011f0040.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@12.0.2/ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@12.0.2/ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(java.base@12.0.2/Thread.java:835)

   Locked ownable synchronizers:
	- <0x000000046b0eac38> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""pool-1-thread-7"" #18 prio=5 os_prio=31 cpu=2134.24ms elapsed=143.88s tid=0x00007ffec32f3800 nid=0x7403 waiting for monitor entry  [0x000070001c8db000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at jadx.core.ProcessClass.process(ProcessClass.java:35)
	- waiting to lock <0x0000000444a53c20> (a jadx.core.dex.info.ClassInfo)
	at jadx.core.ProcessClass.process(ProcessClass.java:27)
	at jadx.core.dex.nodes.ClassNode.loadAndProcess(ClassNode.java:251)
	at jadx.core.dex.nodes.RootNode.getClassGenerics(RootNode.java:279)
	at jadx.core.utils.TypeUtils.replaceClassGenerics(TypeUtils.java:35)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.invokeListener(TypeUpdate.java:296)
	at jadx.core.dex.visitors.typeinference.TypeUpdate$$Lambda$45/0x00000008011adc40.update(Unknown Source)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.runListeners(TypeUpdate.java:190)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.requestUpdate(TypeUpdate.java:171)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.requestUpdateForSsaVar(TypeUpdate.java:148)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.updateTypeForSsaVar(TypeUpdate.java:136)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.updateTypeChecked(TypeUpdate.java:116)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.moveListener(TypeUpdate.java:324)
	at jadx.core.dex.visitors.typeinference.TypeUpdate$$Lambda$36/0x00000008011af840.update(Unknown Source)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.runListeners(TypeUpdate.java:190)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.requestUpdate(TypeUpdate.java:171)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.requestUpdateForSsaVar(TypeUpdate.java:148)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.updateTypeForSsaVar(TypeUpdate.java:136)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.updateTypeChecked(TypeUpdate.java:116)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.moveListener(TypeUpdate.java:324)
	at jadx.core.dex.visitors.typeinference.TypeUpdate$$Lambda$36/0x00000008011af840.update(Unknown Source)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.runListeners(TypeUpdate.java:190)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.requestUpdate(TypeUpdate.java:171)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.requestUpdateForSsaVar(TypeUpdate.java:142)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.updateTypeForSsaVar(TypeUpdate.java:136)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.updateTypeChecked(TypeUpdate.java:116)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.moveListener(TypeUpdate.java:324)
	at jadx.core.dex.visitors.typeinference.TypeUpdate$$Lambda$36/0x00000008011af840.update(Unknown Source)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.runListeners(TypeUpdate.java:190)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.requestUpdate(TypeUpdate.java:171)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.requestUpdateForSsaVar(TypeUpdate.java:148)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.updateTypeForSsaVar(TypeUpdate.java:136)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.updateTypeChecked(TypeUpdate.java:116)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.apply(TypeUpdate.java:73)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.apply(TypeUpdate.java:50)
	at jadx.core.dex.visitors.typeinference.TypeInferenceVisitor.applyImmutableType(TypeInferenceVisitor.java:155)
	at jadx.core.dex.visitors.typeinference.TypeInferenceVisitor.setImmutableType(TypeInferenceVisitor.java:136)
	at jadx.core.dex.visitors.typeinference.TypeInferenceVisitor$$Lambda$126/0x000000080126b840.accept(Unknown Source)
	at java.util.ArrayList.forEach(java.base@12.0.2/ArrayList.java:1540)
	at jadx.core.dex.visitors.typeinference.TypeInferenceVisitor.runTypePropagation(TypeInferenceVisitor.java:102)
	at jadx.core.dex.visitors.typeinference.TypeInferenceVisitor.visit(TypeInferenceVisitor.java:71)
	at jadx.core.dex.visitors.DepthTraversal.visit(DepthTraversal.java:30)
	at jadx.core.dex.visitors.DepthTraversal.lambda$visit$1(DepthTraversal.java:15)
	at jadx.core.dex.visitors.DepthTraversal$$Lambda$90/0x0000000801246840.accept(Unknown Source)
	at java.util.ArrayList.forEach(java.base@12.0.2/ArrayList.java:1540)
	at jadx.core.dex.visitors.DepthTraversal.visit(DepthTraversal.java:15)
	at jadx.core.ProcessClass.process(ProcessClass.java:43)
	- locked <0x0000000444a58f28> (a jadx.core.dex.info.ClassInfo)
	at jadx.core.ProcessClass.generateCode(ProcessClass.java:60)
	at jadx.core.dex.nodes.ClassNode.decompile(ClassNode.java:262)
	- locked <0x00000004423a5b08> (a jadx.core.dex.nodes.ClassNode)
	at jadx.api.JavaClass.getCodeInfo(JavaClass.java:53)
	at jadx.api.JadxDecompiler.lambda$appendSourcesSave$0(JadxDecompiler.java:201)
	at jadx.api.JadxDecompiler$$Lambda$74/0x00000008011f0040.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@12.0.2/ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@12.0.2/ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(java.base@12.0.2/Thread.java:835)

   Locked ownable synchronizers:
	- <0x000000046b075828> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""pool-1-thread-8"" #19 prio=5 os_prio=31 cpu=2154.06ms elapsed=143.87s tid=0x00007ffec4116000 nid=0x8003 waiting for monitor entry  [0x000070001d7e0000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at jadx.core.ProcessClass.process(ProcessClass.java:35)
	- waiting to lock <0x0000000444a58f28> (a jadx.core.dex.info.ClassInfo)
	at jadx.core.ProcessClass$$Lambda$153/0x000000080127a840.accept(Unknown Source)
	at java.util.ArrayList.forEach(java.base@12.0.2/ArrayList.java:1540)
	at jadx.core.ProcessClass.generateCode(ProcessClass.java:61)
	at jadx.core.dex.nodes.ClassNode.decompile(ClassNode.java:262)
	- locked <0x0000000446a769b8> (a jadx.core.dex.nodes.ClassNode)
	at jadx.api.JavaClass.getCodeInfo(JavaClass.java:53)
	at jadx.api.JadxDecompiler.lambda$appendSourcesSave$0(JadxDecompiler.java:201)
	at jadx.api.JadxDecompiler$$Lambda$74/0x00000008011f0040.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@12.0.2/ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@12.0.2/ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(java.base@12.0.2/Thread.java:835)

   Locked ownable synchronizers:
	- <0x000000046b2eb100> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""Java2D Disposer"" #21 daemon prio=10 os_prio=31 cpu=1.86ms elapsed=141.24s tid=0x00007ffec32fe800 nid=0x7907 in Object.wait()  [0x000070001e6e3000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(java.base@12.0.2/Native Method)
	- waiting on <0x000000046b3d56f8> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(java.base@12.0.2/ReferenceQueue.java:155)
	- locked <0x000000046b3d56f8> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(java.base@12.0.2/ReferenceQueue.java:176)
	at sun.java2d.Disposer.run(java.desktop@12.0.2/Disposer.java:144)
	at java.lang.Thread.run(java.base@12.0.2/Thread.java:835)

   Locked ownable synchronizers:
	- None

""AppKit Thread"" #22 daemon prio=5 os_prio=31 cpu=264.56ms elapsed=141.21s tid=0x00007ffec5a9a000 nid=0x307 runnable  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""Java2D Queue Flusher"" #24 daemon prio=10 os_prio=31 cpu=83.59ms elapsed=140.95s tid=0x00007ffec742f000 nid=0xf507 in Object.wait()  [0x00007000207fb000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(java.base@12.0.2/Native Method)
	- waiting on <no object reference available>
	at sun.java2d.opengl.OGLRenderQueue$QueueFlusher.run(java.desktop@12.0.2/OGLRenderQueue.java:205)
	- locked <0x000000046b075b48> (a sun.java2d.opengl.OGLRenderQueue$QueueFlusher)
	at java.lang.Thread.run(java.base@12.0.2/Thread.java:835)

   Locked ownable synchronizers:
	- None

""Attach Listener"" #54 daemon prio=9 os_prio=31 cpu=0.61ms elapsed=0.57s tid=0x00007ffec8341800 nid=0xc063 waiting on condition  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""VM Thread"" os_prio=31 cpu=583.42ms elapsed=150.41s tid=0x00007ffec5832000 nid=0x4503 runnable  

""GC Thread#0"" os_prio=31 cpu=357.17ms elapsed=150.42s tid=0x00007ffec581e800 nid=0x4f03 runnable  

""GC Thread#1"" os_prio=31 cpu=372.99ms elapsed=149.43s tid=0x00007ffec60a1800 nid=0x5f07 runnable  

""GC Thread#2"" os_prio=31 cpu=364.40ms elapsed=149.43s tid=0x00007ffec6158000 nid=0x5e07 runnable  

""GC Thread#3"" os_prio=31 cpu=363.83ms elapsed=149.43s tid=0x00007ffec6159000 nid=0x6207 runnable  

""GC Thread#4"" os_prio=31 cpu=378.84ms elapsed=149.43s tid=0x00007ffec2018000 nid=0xa003 runnable  

""GC Thread#5"" os_prio=31 cpu=374.66ms elapsed=149.43s tid=0x00007ffec6152000 nid=0x6403 runnable  

""GC Thread#6"" os_prio=31 cpu=245.64ms elapsed=148.57s tid=0x00007ffec290d000 nid=0x6e03 runnable  

""GC Thread#7"" os_prio=31 cpu=247.52ms elapsed=148.57s tid=0x00007ffec304e000 nid=0x9103 runnable  

""G1 Main Marker"" os_prio=31 cpu=0.56ms elapsed=150.42s tid=0x00007ffec500a000 nid=0x4d03 runnable  

""G1 Conc#0"" os_prio=31 cpu=230.08ms elapsed=150.42s tid=0x00007ffec581f800 nid=0x4a03 runnable  

""G1 Conc#1"" os_prio=31 cpu=235.44ms elapsed=148.63s tid=0x00007ffec49d7800 nid=0x6d03 runnable  

""G1 Refine#0"" os_prio=31 cpu=285.65ms elapsed=150.41s tid=0x00007ffec60a0000 nid=0x3003 runnable  

""G1 Refine#1"" os_prio=31 cpu=116.28ms elapsed=148.85s tid=0x00007ffec3099000 nid=0x9b03 runnable  

""G1 Refine#2"" os_prio=31 cpu=81.27ms elapsed=148.85s tid=0x00007ffec5861800 nid=0x9a03 runnable  

""G1 Refine#3"" os_prio=31 cpu=56.36ms elapsed=148.85s tid=0x00007ffec309a000 nid=0x9803 runnable  

""G1 Refine#4"" os_prio=31 cpu=50.29ms elapsed=148.85s tid=0x00007ffec291f000 nid=0x9603 runnable  

""G1 Refine#5"" os_prio=31 cpu=27.60ms elapsed=148.85s tid=0x00007ffec40c3800 nid=0x6a03 runnable  

""G1 Refine#6"" os_prio=31 cpu=26.09ms elapsed=148.85s tid=0x00007ffec309a800 nid=0x9403 runnable  

""G1 Refine#7"" os_prio=31 cpu=27.13ms elapsed=148.84s tid=0x00007ffec309b800 nid=0x6b03 runnable  

""G1 Young RemSet Sampling"" os_prio=31 cpu=129.26ms elapsed=150.41s tid=0x00007ffec60a1000 nid=0x4803 runnable  
""VM Periodic Task Thread"" os_prio=31 cpu=118.62ms elapsed=150.38s tid=0x00007ffec508e800 nid=0xa303 waiting on condition  

JNI global refs: 77, weak refs: 2


Found one Java-level deadlock:
=============================
""pool-1-thread-1"":
  waiting to lock monitor 0x000000014600b200 (object 0x0000000444a58f28, a jadx.core.dex.info.ClassInfo),
  which is held by ""pool-1-thread-7""
""pool-1-thread-7"":
  waiting to lock monitor 0x0000000145e9ee00 (object 0x0000000444a53c20, a jadx.core.dex.info.ClassInfo),
  which is held by ""pool-1-thread-3""
""pool-1-thread-3"":
  waiting to lock monitor 0x000000014600b200 (object 0x0000000444a58f28, a jadx.core.dex.info.ClassInfo),
  which is held by ""pool-1-thread-7""

Java stack information for the threads listed above:
===================================================
""pool-1-thread-1"":
	at jadx.core.ProcessClass.process(ProcessClass.java:35)
	- waiting to lock <0x0000000444a58f28> (a jadx.core.dex.info.ClassInfo)
	at jadx.core.ProcessClass$$Lambda$153/0x000000080127a840.accept(Unknown Source)
	at java.util.ArrayList.forEach(java.base@12.0.2/ArrayList.java:1540)
	at jadx.core.ProcessClass.generateCode(ProcessClass.java:61)
	at jadx.core.dex.nodes.ClassNode.decompile(ClassNode.java:262)
	- locked <0x00000004423df9f0> (a jadx.core.dex.nodes.ClassNode)
	at jadx.api.JavaClass.getCodeInfo(JavaClass.java:53)
	at jadx.api.JadxDecompiler.lambda$appendSourcesSave$0(JadxDecompiler.java:201)
	at jadx.api.JadxDecompiler$$Lambda$74/0x00000008011f0040.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@12.0.2/ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@12.0.2/ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(java.base@12.0.2/Thread.java:835)
""pool-1-thread-7"":
	at jadx.core.ProcessClass.process(ProcessClass.java:35)
	- waiting to lock <0x0000000444a53c20> (a jadx.core.dex.info.ClassInfo)
	at jadx.core.ProcessClass.process(ProcessClass.java:27)
	at jadx.core.dex.nodes.ClassNode.loadAndProcess(ClassNode.java:251)
	at jadx.core.dex.nodes.RootNode.getClassGenerics(RootNode.java:279)
	at jadx.core.utils.TypeUtils.replaceClassGenerics(TypeUtils.java:35)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.invokeListener(TypeUpdate.java:296)
	at jadx.core.dex.visitors.typeinference.TypeUpdate$$Lambda$45/0x00000008011adc40.update(Unknown Source)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.runListeners(TypeUpdate.java:190)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.requestUpdate(TypeUpdate.java:171)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.requestUpdateForSsaVar(TypeUpdate.java:148)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.updateTypeForSsaVar(TypeUpdate.java:136)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.updateTypeChecked(TypeUpdate.java:116)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.moveListener(TypeUpdate.java:324)
	at jadx.core.dex.visitors.typeinference.TypeUpdate$$Lambda$36/0x00000008011af840.update(Unknown Source)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.runListeners(TypeUpdate.java:190)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.requestUpdate(TypeUpdate.java:171)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.requestUpdateForSsaVar(TypeUpdate.java:148)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.updateTypeForSsaVar(TypeUpdate.java:136)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.updateTypeChecked(TypeUpdate.java:116)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.moveListener(TypeUpdate.java:324)
	at jadx.core.dex.visitors.typeinference.TypeUpdate$$Lambda$36/0x00000008011af840.update(Unknown Source)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.runListeners(TypeUpdate.java:190)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.requestUpdate(TypeUpdate.java:171)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.requestUpdateForSsaVar(TypeUpdate.java:142)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.updateTypeForSsaVar(TypeUpdate.java:136)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.updateTypeChecked(TypeUpdate.java:116)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.moveListener(TypeUpdate.java:324)
	at jadx.core.dex.visitors.typeinference.TypeUpdate$$Lambda$36/0x00000008011af840.update(Unknown Source)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.runListeners(TypeUpdate.java:190)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.requestUpdate(TypeUpdate.java:171)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.requestUpdateForSsaVar(TypeUpdate.java:148)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.updateTypeForSsaVar(TypeUpdate.java:136)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.updateTypeChecked(TypeUpdate.java:116)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.apply(TypeUpdate.java:73)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.apply(TypeUpdate.java:50)
	at jadx.core.dex.visitors.typeinference.TypeInferenceVisitor.applyImmutableType(TypeInferenceVisitor.java:155)
	at jadx.core.dex.visitors.typeinference.TypeInferenceVisitor.setImmutableType(TypeInferenceVisitor.java:136)
	at jadx.core.dex.visitors.typeinference.TypeInferenceVisitor$$Lambda$126/0x000000080126b840.accept(Unknown Source)
	at java.util.ArrayList.forEach(java.base@12.0.2/ArrayList.java:1540)
	at jadx.core.dex.visitors.typeinference.TypeInferenceVisitor.runTypePropagation(TypeInferenceVisitor.java:102)
	at jadx.core.dex.visitors.typeinference.TypeInferenceVisitor.visit(TypeInferenceVisitor.java:71)
	at jadx.core.dex.visitors.DepthTraversal.visit(DepthTraversal.java:30)
	at jadx.core.dex.visitors.DepthTraversal.lambda$visit$1(DepthTraversal.java:15)
	at jadx.core.dex.visitors.DepthTraversal$$Lambda$90/0x0000000801246840.accept(Unknown Source)
	at java.util.ArrayList.forEach(java.base@12.0.2/ArrayList.java:1540)
	at jadx.core.dex.visitors.DepthTraversal.visit(DepthTraversal.java:15)
	at jadx.core.ProcessClass.process(ProcessClass.java:43)
	- locked <0x0000000444a58f28> (a jadx.core.dex.info.ClassInfo)
	at jadx.core.ProcessClass.generateCode(ProcessClass.java:60)
	at jadx.core.dex.nodes.ClassNode.decompile(ClassNode.java:262)
	- locked <0x00000004423a5b08> (a jadx.core.dex.nodes.ClassNode)
	at jadx.api.JavaClass.getCodeInfo(JavaClass.java:53)
	at jadx.api.JadxDecompiler.lambda$appendSourcesSave$0(JadxDecompiler.java:201)
	at jadx.api.JadxDecompiler$$Lambda$74/0x00000008011f0040.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@12.0.2/ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@12.0.2/ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(java.base@12.0.2/Thread.java:835)
""pool-1-thread-3"":
	at jadx.core.ProcessClass.process(ProcessClass.java:35)
	- waiting to lock <0x0000000444a58f28> (a jadx.core.dex.info.ClassInfo)
	at jadx.core.dex.nodes.ClassNode.loadAndProcess(ClassNode.java:251)
	at jadx.core.dex.nodes.RootNode.getClassGenerics(RootNode.java:279)
	at jadx.core.utils.TypeUtils.replaceClassGenerics(TypeUtils.java:35)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.invokeListener(TypeUpdate.java:296)
	at jadx.core.dex.visitors.typeinference.TypeUpdate$$Lambda$45/0x00000008011adc40.update(Unknown Source)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.runListeners(TypeUpdate.java:190)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.requestUpdate(TypeUpdate.java:171)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.requestUpdateForSsaVar(TypeUpdate.java:148)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.updateTypeForSsaVar(TypeUpdate.java:136)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.updateTypeChecked(TypeUpdate.java:116)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.apply(TypeUpdate.java:73)
	at jadx.core.dex.visitors.typeinference.TypeUpdate.apply(TypeUpdate.java:50)
	at jadx.core.dex.visitors.typeinference.TypeInferenceVisitor.applyImmutableType(TypeInferenceVisitor.java:155)
	at jadx.core.dex.visitors.typeinference.TypeInferenceVisitor.setImmutableType(TypeInferenceVisitor.java:136)
	at jadx.core.dex.visitors.typeinference.TypeInferenceVisitor$$Lambda$126/0x000000080126b840.accept(Unknown Source)
	at java.util.ArrayList.forEach(java.base@12.0.2/ArrayList.java:1540)
	at jadx.core.dex.visitors.typeinference.TypeInferenceVisitor.runTypePropagation(TypeInferenceVisitor.java:102)
	at jadx.core.dex.visitors.typeinference.TypeInferenceVisitor.visit(TypeInferenceVisitor.java:71)
	at jadx.core.dex.visitors.DepthTraversal.visit(DepthTraversal.java:30)
	at jadx.core.dex.visitors.DepthTraversal.lambda$visit$1(DepthTraversal.java:15)
	at jadx.core.dex.visitors.DepthTraversal$$Lambda$90/0x0000000801246840.accept(Unknown Source)
	at java.util.ArrayList.forEach(java.base@12.0.2/ArrayList.java:1540)
	at jadx.core.dex.visitors.DepthTraversal.visit(DepthTraversal.java:15)
	at jadx.core.dex.visitors.DepthTraversal.lambda$visit$0(DepthTraversal.java:14)
	at jadx.core.dex.visitors.DepthTraversal$$Lambda$89/0x0000000801247440.accept(Unknown Source)
	at java.util.ArrayList.forEach(java.base@12.0.2/ArrayList.java:1540)
	at jadx.core.dex.visitors.DepthTraversal.visit(DepthTraversal.java:14)
	at jadx.core.ProcessClass.process(ProcessClass.java:43)
	- locked <0x0000000444a53c20> (a jadx.core.dex.info.ClassInfo)
	at jadx.core.ProcessClass$$Lambda$153/0x000000080127a840.accept(Unknown Source)
	at java.util.ArrayList.forEach(java.base@12.0.2/ArrayList.java:1540)
	at jadx.core.ProcessClass.generateCode(ProcessClass.java:61)
	at jadx.core.dex.nodes.ClassNode.decompile(ClassNode.java:262)
	- locked <0x00000004422eb2f0> (a jadx.core.dex.nodes.ClassNode)
	at jadx.api.JavaClass.getCodeInfo(JavaClass.java:53)
	at jadx.api.JadxDecompiler.lambda$appendSourcesSave$0(JadxDecompiler.java:201)
	at jadx.api.JadxDecompiler$$Lambda$74/0x00000008011f0040.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@12.0.2/ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@12.0.2/ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(java.base@12.0.2/Thread.java:835)

Found 1 deadlock.
```

APK: https://drive.google.com/file/d/18wPwqqSeeppz5LDybFX3CLvAZJ3sB7JJ/view?usp=sharing",2019-08-27 17:31:40,"[{'commitHash': 'db892adf343c03cfc19325df014732dae2d14c63', 'commitGHEventType': 'referenced', 'commitUser': 'skylot', 'commitParents': ['1cbaad3ec982032de96a4a7f34033634cf2b7ca0'], 'nameRev': 'db892adf343c03cfc19325df014732dae2d14c63 tags/v1.1.0~23', 'commitMessage': ""fix: don't run class process from visitors to avoid deadlock (#743)\n"", 'commitDateTime': '2019-08-27 17:24:18', 'authoredDateTime': '2019-08-27 16:34:43', 'commitGitStats': [{'filePath': 'jadx-core/src/main/java/jadx/core/ProcessClass.java', 'insertions': 1, 'deletions': 3, 'lines': 4}, {'filePath': 'jadx-core/src/main/java/jadx/core/codegen/InsnGen.java', 'insertions': 5, 'deletions': 18, 'lines': 23}, {'filePath': 'jadx-core/src/main/java/jadx/core/dex/nodes/ClassNode.java', 'insertions': 23, 'deletions': 12, 'lines': 35}, {'filePath': 'jadx-core/src/main/java/jadx/core/dex/nodes/MethodNode.java', 'insertions': 8, 'deletions': 8, 'lines': 16}, {'filePath': 'jadx-core/src/main/java/jadx/core/dex/nodes/ProcessState.java', 'insertions': 9, 'deletions': 1, 'lines': 10}, {'filePath': 'jadx-core/src/main/java/jadx/core/dex/nodes/RootNode.java', 'insertions': 0, 'deletions': 1, 'lines': 1}, {'filePath': 'jadx-core/src/main/java/jadx/core/dex/visitors/ModVisitor.java', 'insertions': 0, 'deletions': 1, 'lines': 1}, {'filePath': 'jadx-core/src/test/java/jadx/tests/api/IntegrationTest.java', 'insertions': 2, 'deletions': 1, 'lines': 3}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'ProcessClass.java', 'spoonMethods': [{'spoonMethodName': 'jadx.core.ProcessClass.process(jadx.core.dex.nodes.ClassNode)', 'TOT': 3, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 1}]}, {'spoonFilePath': 'InsnGen.java', 'spoonMethods': [{'spoonMethodName': 'jadx.core.codegen.InsnGen.makeConstructor(jadx.core.dex.instructions.mods.ConstructorInsn,jadx.core.codegen.CodeWriter)', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'jadx.core.codegen.InsnGen.processOverloadedArg(jadx.core.codegen.CodeWriter,jadx.core.dex.nodes.InsnNode,jadx.core.dex.nodes.MethodNode,jadx.core.dex.instructions.args.InsnArg,int)', 'TOT': 4, 'UPD': 0, 'INS': 0, 'MOV': 2, 'DEL': 2}]}, {'spoonFilePath': 'ClassNode.java', 'spoonMethods': [{'spoonMethodName': 'jadx.core.dex.nodes.ClassNode.loadAndProcess()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'jadx.core.dex.nodes.ClassNode.initAccessFlags(com.android.dex.ClassDef)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'jadx.core.dex.nodes.ClassNode', 'TOT': 6, 'UPD': 0, 'INS': 2, 'MOV': 4, 'DEL': 0}]}, {'spoonFilePath': 'MethodNode.java', 'spoonMethods': [{'spoonMethodName': 'jadx.core.dex.nodes.MethodNode.load()', 'TOT': 2, 'UPD': 2, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'jadx.core.dex.nodes.MethodNode.initMethodTypes()', 'TOT': 3, 'UPD': 1, 'INS': 0, 'MOV': 2, 'DEL': 0}, {'spoonMethodName': 'jadx.core.dex.nodes.MethodNode.getArgTypes()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'ProcessState.java', 'spoonMethods': [{'spoonMethodName': 'jadx.core.dex.nodes.ProcessState.isLoaded()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'jadx.core.dex.nodes.ProcessState.isProcessed()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'RootNode.java', 'spoonMethods': [{'spoonMethodName': 'jadx.core.dex.nodes.RootNode.getClassGenerics(jadx.core.dex.instructions.args.ArgType)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}]}, {'spoonFilePath': 'ModVisitor.java', 'spoonMethods': [{'spoonMethodName': 'jadx.core.dex.visitors.ModVisitor.processAnonymousConstructor(jadx.core.dex.nodes.MethodNode,jadx.core.dex.instructions.mods.ConstructorInsn)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}]}, {'spoonFilePath': 'IntegrationTest.java', 'spoonMethods': [{'spoonMethodName': 'jadx.tests.api.IntegrationTest.decompileWithoutUnload(jadx.api.JadxDecompiler,jadx.core.dex.nodes.ClassNode)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/skylot/jadx/issues/743,0.0002777777777777778,"['Core', 'bug']",[core] Deadlock,1.0,"['jadx.core.dex.nodes.MethodNode.load()', 'jadx.core.ProcessClass.process(jadx.core.dex.nodes.ClassNode)', 'jadx.core.dex.nodes.MethodNode.initMethodTypes()', 'jadx.core.dex.nodes.ProcessState.isProcessed()', 'jadx.core.codegen.InsnGen.makeConstructor(jadx.core.dex.instructions.mods.ConstructorInsn,jadx.core.codegen.CodeWriter)', 'jadx.core.dex.nodes.ClassNode.loadAndProcess()', 'jadx.core.dex.nodes.MethodNode.getArgTypes()', 'jadx.core.dex.visitors.ModVisitor.processAnonymousConstructor(jadx.core.dex.nodes.MethodNode,jadx.core.dex.instructions.mods.ConstructorInsn)', 'jadx.core.dex.nodes.RootNode.getClassGenerics(jadx.core.dex.instructions.args.ArgType)', 'jadx.core.dex.nodes.ClassNode', 'jadx.core.dex.nodes.ClassNode.initAccessFlags(com.android.dex.ClassDef)', 'jadx.core.dex.nodes.ProcessState.isLoaded()', 'jadx.core.codegen.InsnGen.processOverloadedArg(jadx.core.codegen.CodeWriter,jadx.core.dex.nodes.InsnNode,jadx.core.dex.nodes.MethodNode,jadx.core.dex.instructions.args.InsnArg,int)']",['db892adf343c03cfc19325df014732dae2d14c63'],,"['jadx-core/src/main/java/jadx/core/dex/nodes', 'jadx-core/src/main/java/jadx/core/dex/visitors', 'jadx-core/src/main/java/jadx/core/codegen', 'jadx-core/src/main/java/jadx/core']",46.0,44.0,90.0,7.0,4.0,13.0,27.0,9.0,8.0,6.0,7.0,0.0,0.0,0.0,0.0,0.0,0.0,jadx
33706,2019-07-29 10:00:28,sergey-wowwow,"**Checks before report**
- [+] check [latest unstable build](https://bintray.com/skylot/jadx/unstable/_latestVersion#files) (maybe issue already fixed)
- [+] search existing issues by exception message
- [+] check [Troubleshooting Q&A](https://github.com/skylot/jadx/wiki/Troubleshooting-Q&A) section on wiki

**Describe error**
Hi, I've noticed that an app is never decompiled. I run jadx with the following arguments
```
--no-replace-consts --show-bad-code --threads-count 8 --no-inline-anonymous --no-imports --deobf --deobf-min 2 --deobf-rewrite-cfg
```
jstack output
```
2019-07-29 12:53:48
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.201-b09 mixed mode):

""Java2D Disposer"" #16 daemon prio=10 os_prio=2 tid=0x0000000045548800 nid=0xd34 in Object.wait() [0x000000004e0af000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x0000000408b02e00> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
        - locked <0x0000000408b02e00> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
        at sun.java2d.Disposer.run(Disposer.java:148)
        at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
        - None

""pool-1-thread-4"" #14 prio=5 os_prio=0 tid=0x0000000045942000 nid=0xec4 waiting for monitor entry [0x000000004d1ae000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at jadx.core.ProcessClass.process(ProcessClass.java:46)
        - waiting to lock <0x0000000407e3b578> (a jadx.core.dex.info.ClassInfo)
        at jadx.core.ProcessClass.process(ProcessClass.java:25)
        at jadx.core.dex.nodes.ClassNode.loadAndProcess(ClassNode.java:248)
        at jadx.core.codegen.InsnGen.processOverloadedArg(InsnGen.java:794)
        at jadx.core.codegen.InsnGen.generateMethodArguments(InsnGen.java:765)
        at jadx.core.codegen.InsnGen.makeInvoke(InsnGen.java:709)
        at jadx.core.codegen.InsnGen.makeInsnBody(InsnGen.java:353)
        at jadx.core.codegen.InsnGen.makeInsn(InsnGen.java:235)
        at jadx.core.codegen.InsnGen.makeInsn(InsnGen.java:206)
        at jadx.core.dex.visitors.regions.CheckRegions.getBlockInsnStr(CheckRegions.java:93)
        at jadx.core.dex.visitors.regions.CheckRegions.visit(CheckRegions.java:64)
        at jadx.core.dex.visitors.DepthTraversal.visit(DepthTraversal.java:30)
        at jadx.core.dex.visitors.DepthTraversal.lambda$visit$1(DepthTraversal.java:15)
        at jadx.core.dex.visitors.DepthTraversal$$Lambda$31/253399104.accept(Unknown Source)
        at java.util.ArrayList.forEach(ArrayList.java:1257)
        at jadx.core.dex.visitors.DepthTraversal.visit(DepthTraversal.java:15)
        at jadx.core.dex.visitors.DepthTraversal.lambda$visit$0(DepthTraversal.java:14)
        at jadx.core.dex.visitors.DepthTraversal$$Lambda$30/993888079.accept(Unknown Source)
        at java.util.ArrayList.forEach(ArrayList.java:1257)
        at jadx.core.dex.visitors.DepthTraversal.visit(DepthTraversal.java:14)
        at jadx.core.ProcessClass.process(ProcessClass.java:54)
        - locked <0x0000000407f76398> (a jadx.core.dex.info.ClassInfo)
        at jadx.core.ProcessClass.generateCode(ProcessClass.java:30)
        at jadx.core.dex.nodes.ClassNode.decompile(ClassNode.java:255)
        at jadx.api.JavaClass.decompile(JavaClass.java:61)
        - locked <0x000000040a3135d0> (a jadx.api.JavaClass)
        at jadx.api.JadxDecompiler.lambda$appendSourcesSave$0(JadxDecompiler.java:211)
        at jadx.api.JadxDecompiler$$Lambda$24/914942811.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
        - <0x0000000408d053d0> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""pool-1-thread-3"" #13 prio=5 os_prio=0 tid=0x0000000045941800 nid=0xe14 waiting for monitor entry [0x000000004c2ae000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at jadx.core.ProcessClass.process(ProcessClass.java:46)
        - waiting to lock <0x0000000407f76398> (a jadx.core.dex.info.ClassInfo)
        at jadx.core.ProcessClass.processDependencies(ProcessClass.java:78)
        at jadx.core.ProcessClass.process(ProcessClass.java:59)
        - locked <0x0000000407e3b578> (a jadx.core.dex.info.ClassInfo)
        at jadx.core.ProcessClass.generateCode(ProcessClass.java:30)
        at jadx.core.dex.nodes.ClassNode.decompile(ClassNode.java:255)
        at jadx.api.JavaClass.decompile(JavaClass.java:61)
        - locked <0x000000040a313580> (a jadx.api.JavaClass)
        at jadx.api.JadxDecompiler.lambda$appendSourcesSave$0(JadxDecompiler.java:211)
        at jadx.api.JadxDecompiler$$Lambda$24/914942811.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
        - <0x0000000408c05780> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""pool-1-thread-2"" #12 prio=5 os_prio=0 tid=0x0000000045940800 nid=0x4f4 waiting for monitor entry [0x000000004b3ae000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at jadx.core.ProcessClass.process(ProcessClass.java:46)
        - waiting to lock <0x0000000407f76398> (a jadx.core.dex.info.ClassInfo)
        at jadx.core.ProcessClass.processDependencies(ProcessClass.java:78)
        at jadx.core.ProcessClass.process(ProcessClass.java:59)
        - locked <0x00000004082e4090> (a jadx.core.dex.info.ClassInfo)
        at jadx.core.ProcessClass.generateCode(ProcessClass.java:30)
        at jadx.core.dex.nodes.ClassNode.decompile(ClassNode.java:255)
        at jadx.api.JavaClass.decompile(JavaClass.java:61)
        - locked <0x000000040a3135f8> (a jadx.api.JavaClass)
        at jadx.api.JadxDecompiler.lambda$appendSourcesSave$0(JadxDecompiler.java:211)
        at jadx.api.JadxDecompiler$$Lambda$24/914942811.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
        - <0x0000000408b02fe0> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""pool-1-thread-1"" #11 prio=5 os_prio=0 tid=0x00000000484bb000 nid=0x7dc waiting for monitor entry [0x000000004a4ae000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at jadx.core.ProcessClass.process(ProcessClass.java:46)
        - waiting to lock <0x0000000407f76398> (a jadx.core.dex.info.ClassInfo)
        at jadx.core.ProcessClass.processDependencies(ProcessClass.java:78)
        at jadx.core.ProcessClass.process(ProcessClass.java:59)
        - locked <0x0000000407fd04d8> (a jadx.core.dex.info.ClassInfo)
        at jadx.core.ProcessClass.generateCode(ProcessClass.java:30)
        at jadx.core.dex.nodes.ClassNode.decompile(ClassNode.java:255)
        at jadx.api.JavaClass.decompile(JavaClass.java:61)
        - locked <0x000000040a312a18> (a jadx.api.JavaClass)
        at jadx.api.JadxDecompiler.lambda$appendSourcesSave$0(JadxDecompiler.java:211)
        at jadx.api.JadxDecompiler$$Lambda$24/914942811.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
        - <0x0000000408d055b8> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""Service Thread"" #10 daemon prio=9 os_prio=0 tid=0x000000003ccdd000 nid=0x130c runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
        - None

""C1 CompilerThread2"" #9 daemon prio=9 os_prio=2 tid=0x000000003cc5a000 nid=0x81c waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
        - None

""C2 CompilerThread1"" #8 daemon prio=9 os_prio=2 tid=0x000000003cc56000 nid=0x12c8 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
        - None

""C2 CompilerThread0"" #7 daemon prio=9 os_prio=2 tid=0x000000003cc54000 nid=0xf64 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
        - None

""Attach Listener"" #6 daemon prio=5 os_prio=2 tid=0x000000003cc50000 nid=0xe90 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
        - None

""Signal Dispatcher"" #5 daemon prio=9 os_prio=2 tid=0x000000003cc4d000 nid=0x1460 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
        - None

""Surrogate Locker Thread (Concurrent GC)"" #4 daemon prio=9 os_prio=0 tid=0x000000003cc4b800 nid=0x1010 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
        - None

""Finalizer"" #3 daemon prio=8 os_prio=1 tid=0x000000003cc2e800 nid=0xee0 in Object.wait() [0x000000003edaf000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x0000000401806718> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
        - locked <0x0000000401806718> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
        at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)

   Locked ownable synchronizers:
        - None

""Reference Handler"" #2 daemon prio=10 os_prio=2 tid=0x000000003cc21000 nid=0x280 in Object.wait() [0x000000003deaf000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x0000000401705c60> (a java.lang.ref.Reference$Lock)
        at java.lang.Object.wait(Object.java:502)
        at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
        - locked <0x0000000401705c60> (a java.lang.ref.Reference$Lock)
        at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)

   Locked ownable synchronizers:
        - None

""main"" #1 prio=5 os_prio=0 tid=0x0000000002d53800 nid=0x1434 waiting on condition [0x0000000002d4f000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000408d05600> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1475)
        at jadx.api.JadxDecompiler.save(JadxDecompiler.java:153)
        at jadx.api.JadxDecompiler.save(JadxDecompiler.java:138)
        at jadx.cli.JadxCLI.processAndSave(JadxCLI.java:37)
        at jadx.cli.JadxCLI.main(JadxCLI.java:18)

   Locked ownable synchronizers:
        - None

""VM Thread"" os_prio=2 tid=0x000000003cbfc800 nid=0xea4 runnable

""Gang worker#0 (Parallel GC Threads)"" os_prio=2 tid=0x0000000002d6c000 nid=0xf18 runnable

""Gang worker#1 (Parallel GC Threads)"" os_prio=2 tid=0x0000000002d6d800 nid=0xda0 runnable

""Gang worker#2 (Parallel GC Threads)"" os_prio=2 tid=0x0000000002d6f000 nid=0x15c4 runnable

""Gang worker#3 (Parallel GC Threads)"" os_prio=2 tid=0x0000000002d71000 nid=0x12b8 runnable

""G1 Main Concurrent Mark GC Thread"" os_prio=2 tid=0x0000000002e1f800 nid=0x174c runnable

""Gang worker#0 (G1 Parallel Marking Threads)"" os_prio=2 tid=0x0000000002e20800 nid=0xe84 runnable

""G1 Concurrent Refinement Thread#0"" os_prio=2 tid=0x0000000002d7c800 nid=0xe24 runnable

""G1 Concurrent Refinement Thread#1"" os_prio=2 tid=0x0000000002d7a800 nid=0xbf4 runnable

""G1 Concurrent Refinement Thread#2"" os_prio=2 tid=0x0000000002d79000 nid=0xf00 runnable

""G1 Concurrent Refinement Thread#3"" os_prio=2 tid=0x0000000002d76000 nid=0x1214 runnable

""G1 Concurrent Refinement Thread#4"" os_prio=2 tid=0x0000000002d75000 nid=0x1300 runnable

""VM Periodic Task Thread"" os_prio=2 tid=0x000000003ccf6800 nid=0x16d8 waiting on condition

JNI global references: 517


Found one Java-level deadlock:
=============================
""pool-1-thread-4"":
  waiting to lock monitor 0x0000000047032988 (object 0x0000000407e3b578, a jadx.core.dex.info.ClassInfo),
  which is held by ""pool-1-thread-3""
""pool-1-thread-3"":
  waiting to lock monitor 0x0000000048aeb628 (object 0x0000000407f76398, a jadx.core.dex.info.ClassInfo),
  which is held by ""pool-1-thread-4""

Java stack information for the threads listed above:
===================================================
""pool-1-thread-4"":
        at jadx.core.ProcessClass.process(ProcessClass.java:46)
        - waiting to lock <0x0000000407e3b578> (a jadx.core.dex.info.ClassInfo)
        at jadx.core.ProcessClass.process(ProcessClass.java:25)
        at jadx.core.dex.nodes.ClassNode.loadAndProcess(ClassNode.java:248)
        at jadx.core.codegen.InsnGen.processOverloadedArg(InsnGen.java:794)
        at jadx.core.codegen.InsnGen.generateMethodArguments(InsnGen.java:765)
        at jadx.core.codegen.InsnGen.makeInvoke(InsnGen.java:709)
        at jadx.core.codegen.InsnGen.makeInsnBody(InsnGen.java:353)
        at jadx.core.codegen.InsnGen.makeInsn(InsnGen.java:235)
        at jadx.core.codegen.InsnGen.makeInsn(InsnGen.java:206)
        at jadx.core.dex.visitors.regions.CheckRegions.getBlockInsnStr(CheckRegions.java:93)
        at jadx.core.dex.visitors.regions.CheckRegions.visit(CheckRegions.java:64)
        at jadx.core.dex.visitors.DepthTraversal.visit(DepthTraversal.java:30)
        at jadx.core.dex.visitors.DepthTraversal.lambda$visit$1(DepthTraversal.java:15)
        at jadx.core.dex.visitors.DepthTraversal$$Lambda$31/253399104.accept(Unknown Source)
        at java.util.ArrayList.forEach(ArrayList.java:1257)
        at jadx.core.dex.visitors.DepthTraversal.visit(DepthTraversal.java:15)
        at jadx.core.dex.visitors.DepthTraversal.lambda$visit$0(DepthTraversal.java:14)
        at jadx.core.dex.visitors.DepthTraversal$$Lambda$30/993888079.accept(Unknown Source)
        at java.util.ArrayList.forEach(ArrayList.java:1257)
        at jadx.core.dex.visitors.DepthTraversal.visit(DepthTraversal.java:14)
        at jadx.core.ProcessClass.process(ProcessClass.java:54)
        - locked <0x0000000407f76398> (a jadx.core.dex.info.ClassInfo)
        at jadx.core.ProcessClass.generateCode(ProcessClass.java:30)
        at jadx.core.dex.nodes.ClassNode.decompile(ClassNode.java:255)
        at jadx.api.JavaClass.decompile(JavaClass.java:61)
        - locked <0x000000040a3135d0> (a jadx.api.JavaClass)
        at jadx.api.JadxDecompiler.lambda$appendSourcesSave$0(JadxDecompiler.java:211)
        at jadx.api.JadxDecompiler$$Lambda$24/914942811.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
""pool-1-thread-3"":
        at jadx.core.ProcessClass.process(ProcessClass.java:46)
        - waiting to lock <0x0000000407f76398> (a jadx.core.dex.info.ClassInfo)
        at jadx.core.ProcessClass.processDependencies(ProcessClass.java:78)
        at jadx.core.ProcessClass.process(ProcessClass.java:59)
        - locked <0x0000000407e3b578> (a jadx.core.dex.info.ClassInfo)
        at jadx.core.ProcessClass.generateCode(ProcessClass.java:30)
        at jadx.core.dex.nodes.ClassNode.decompile(ClassNode.java:255)
        at jadx.api.JavaClass.decompile(JavaClass.java:61)
        - locked <0x000000040a313580> (a jadx.api.JavaClass)
        at jadx.api.JadxDecompiler.lambda$appendSourcesSave$0(JadxDecompiler.java:211)
        at jadx.api.JadxDecompiler$$Lambda$24/914942811.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Found 1 deadlock.
```

APK: https://drive.google.com/file/d/16KjFNIb20qGCWREbQ64maNHlrsKurWkK/view?usp=sharing",2019-07-31 10:28:25,"[{'commitHash': '068234f0cab197a476cba5b30592aa4d88c50249', 'commitGHEventType': 'referenced', 'commitUser': 'skylot', 'commitParents': ['ccb8ed1394e57966aab79f1adc177ccf00b59cc9'], 'nameRev': '068234f0cab197a476cba5b30592aa4d88c50249 tags/v1.1.0~43', 'commitMessage': 'fix: remove synchronization lock for code generation (#726)\n', 'commitDateTime': '2019-07-29 14:55:50', 'authoredDateTime': '2019-07-29 14:55:50', 'commitGitStats': [{'filePath': 'jadx-core/src/main/java/jadx/api/JavaClass.java', 'insertions': 13, 'deletions': 6, 'lines': 19}, {'filePath': 'jadx-core/src/main/java/jadx/core/ProcessClass.java', 'insertions': 18, 'deletions': 32, 'lines': 50}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'JavaClass.java', 'spoonMethods': [{'spoonMethodName': 'jadx.api.JavaClass.load()', 'TOT': 2, 'UPD': 2, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'jadx.api.JavaClass.getInnerClasses()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'jadx.api.JavaClass.getFields()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'jadx.api.JavaClass.getMethods()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'jadx.api.JavaClass.decompile()', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'jadx.api.JavaClass', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'jadx.api.JavaClass.unload()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'ProcessClass.java', 'spoonMethods': [{'spoonMethodName': 'jadx.core.ProcessClass.generateCode(jadx.core.dex.nodes.ClassNode)', 'TOT': 12, 'UPD': 3, 'INS': 2, 'MOV': 6, 'DEL': 1}, {'spoonMethodName': 'jadx.core.ProcessClass.process(jadx.core.dex.nodes.ClassNode)', 'TOT': 6, 'UPD': 1, 'INS': 2, 'MOV': 1, 'DEL': 2}, {'spoonMethodName': 'jadx.core.ProcessClass.process(jadx.core.dex.nodes.ClassNode,boolean)', 'TOT': 9, 'UPD': 1, 'INS': 0, 'MOV': 6, 'DEL': 2}, {'spoonMethodName': 'jadx.core.ProcessClass.getSyncObj(jadx.core.dex.nodes.ClassNode)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'jadx.core.ProcessClass.processDependencies(jadx.core.dex.nodes.ClassNode)', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/skylot/jadx/issues/726,2.000277777777778,"['Core', 'bug']",[core] Thread lock,1.0,"['jadx.api.JavaClass.getInnerClasses()', 'jadx.api.JavaClass', 'jadx.api.JavaClass.load()', 'jadx.core.ProcessClass.process(jadx.core.dex.nodes.ClassNode)', 'jadx.api.JavaClass.decompile()', 'jadx.api.JavaClass.getMethods()', 'jadx.core.ProcessClass.getSyncObj(jadx.core.dex.nodes.ClassNode)', 'jadx.core.ProcessClass.generateCode(jadx.core.dex.nodes.ClassNode)', 'jadx.api.JavaClass.unload()', 'jadx.core.ProcessClass.processDependencies(jadx.core.dex.nodes.ClassNode)', 'jadx.core.ProcessClass.process(jadx.core.dex.nodes.ClassNode,boolean)', 'jadx.api.JavaClass.getFields()']",['068234f0cab197a476cba5b30592aa4d88c50249'],,"['jadx-core/src/main/java/jadx/core', 'jadx-core/src/main/java/jadx/api']",31.0,38.0,69.0,2.0,10.0,12.0,38.0,14.0,6.0,8.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,jadx
33827,2017-05-19 11:31:35,anonym24,"`mFrameRecorder.stop();`

```
05-19 14:31:06.807 22871-24324/com.example.ffmpegvideorecord A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x54 in tid 24324 (Thread-32239)
05-19 14:31:06.860 6507-6507/? A/DEBUG: pid: 22871, tid: 24324, name: Thread-32239  >>> com.example.driverassistance <<<
05-19 14:31:06.880 6507-6507/? A/DEBUG:     #00 pc 00678808  /data/app/com.example.driverassistance-1/lib/arm/libavcodec.so (avcodec_encode_audio2+32)
05-19 14:31:06.880 6507-6507/? A/DEBUG:     #01 pc 00068f94  /data/app/com.example.driverassistance-1/lib/arm/libjniavcodec.so (Java_org_bytedeco_javacpp_avcodec_avcodec_1encode_1audio2__Lorg_bytedeco_javacpp_avcodec_00024AVCodecContext_2Lorg_bytedeco_javacpp_avcodec_00024AVPacket_2Lorg_bytedeco_javacpp_avutil_00024AVFrame_2_3I+296)
05-19 14:31:06.880 6507-6507/? A/DEBUG:     #02 pc 00006a5f  /data/data/com.example.driverassistance/cache/slice-ffmpeg-3.2.1-1.3_e3128a5b50aeace251d3aa308a4b04261bbb0d49-classes.dex (offset 0x50000)

```",2017-07-27 04:29:10,"[{'commitHash': '32fbadde6c56096580be3d4e7d171ac6c94995d7', 'commitGHEventType': 'referenced', 'commitUser': 'saudet', 'commitParents': ['5e3bc7227b6af7f584eba5fac671319e4c98f94a'], 'nameRev': '32fbadde6c56096580be3d4e7d171ac6c94995d7 tags/1.3.3~9', 'commitMessage': ' * Synchronize on `FFmpegFrameRecorder.stop()` to avoid potential race conditions (issue #700)\n', 'commitDateTime': '2017-06-02 17:50:55', 'authoredDateTime': '2017-06-02 17:50:55', 'commitGitStats': [{'filePath': 'CHANGELOG.md', 'insertions': 1, 'deletions': 0, 'lines': 1}, {'filePath': 'src/main/java/org/bytedeco/javacv/FFmpegFrameRecorder.java', 'insertions': 12, 'deletions': 10, 'lines': 22}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'FFmpegFrameRecorder.java', 'spoonMethods': [{'spoonMethodName': 'org.bytedeco.javacv.FFmpegFrameRecorder.stop()', 'TOT': 5, 'UPD': 0, 'INS': 1, 'MOV': 4, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/bytedeco/javacv/issues/700,68.00027777777778,['bug'], FFmpegFrameRecorder.stop() sometimes causes app crashing,1.0,['org.bytedeco.javacv.FFmpegFrameRecorder.stop()'],['32fbadde6c56096580be3d4e7d171ac6c94995d7'],,['src/main/java/org/bytedeco/javacv'],12.0,10.0,22.0,1.0,0.0,1.0,5.0,4.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,javacv
33982,2013-07-20 11:58:30,jodastephen,"Fix cache synchronization in BuddhistChronology and GJChronology.
",2013-07-20 11:59:15,"[{'commitHash': '0f274a6e0450cfd23bae02f80d283dd0e66432a2', 'commitGHEventType': 'closed', 'commitUser': 'jodastephen', 'commitParents': ['fe915d2cb3cfac7d8f2d2f95970d41a65012e055'], 'nameRev': '0f274a6e0450cfd23bae02f80d283dd0e66432a2 tags/v2.3~19', 'commitMessage': 'Fix chronology cache synchronization\n\nFixes #44\n', 'commitDateTime': '2013-07-20 12:59:10', 'authoredDateTime': '2013-07-20 12:59:10', 'commitGitStats': [{'filePath': 'src/main/java/org/joda/time/chrono/BuddhistChronology.java', 'insertions': 11, 'deletions': 8, 'lines': 19}, {'filePath': 'src/main/java/org/joda/time/chrono/GJChronology.java', 'insertions': 27, 'deletions': 29, 'lines': 56}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'BuddhistChronology.java', 'spoonMethods': [{'spoonMethodName': 'org.joda.time.chrono.BuddhistChronology.getInstance(org.joda.time.DateTimeZone)', 'TOT': 3, 'UPD': 0, 'INS': 1, 'MOV': 2, 'DEL': 0}]}, {'spoonFilePath': 'GJChronology.java', 'spoonMethods': [{'spoonMethodName': 'org.joda.time.chrono.GJChronology.getInstance(org.joda.time.DateTimeZone,org.joda.time.ReadableInstant,int)', 'TOT': 5, 'UPD': 0, 'INS': 1, 'MOV': 4, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/JodaOrg/joda-time/issues/44,0.0002777777777777778,"['Bug', 'Fixed']",Fix chronology cache synchronization,1.0,"['org.joda.time.chrono.BuddhistChronology.getInstance(org.joda.time.DateTimeZone)', 'org.joda.time.chrono.GJChronology.getInstance(org.joda.time.DateTimeZone,org.joda.time.ReadableInstant,int)']",['0f274a6e0450cfd23bae02f80d283dd0e66432a2'],,['src/main/java/org/joda/time/chrono'],38.0,37.0,75.0,2.0,0.0,2.0,8.0,6.0,2.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,joda-time
33989,2013-04-19 08:28:47,lucclaes,"When a DateTimeZone is build with duplicate-named 'recurring saving time' in a first thread, all goes Ok: a warning message is generated and an identifier is automatically generated in PrecalculatedZone.create(). When a second thread does the same, an NPE is generated in ZoneInfoCompiler.verbose().

The cause is that the cVerbose ThreadLocal is incorrectly initialized in ZoneInfoCompiler:

``` java
   static {
        cVerbose.set(Boolean.FALSE);
    }
```

...will initialize cVerbose only for the first thread and not for the subsequent ones. The NPE is caused by the autoboxing in:

``` java
   public static boolean verbose() {
        return cVerbose.get();
    }
```

A better approach could be to remove the initialization and test for null:

``` java
public static boolean verbose(){
    Boolean verbose = cVerbose.get();
    return (verbose != null) ? verbose : false;
}
```

---

Here follows a test case:

``` java
    @Test
    public void testDateTimeZoneBuilder() throws Exception {
        getTestDataTimeZoneBuilder().toDateTimeZone(""TestDTZ1"", true);
        Thread t = new Thread(new Runnable() {
            @Override
            public void run() {
                getTestDataTimeZoneBuilder().toDateTimeZone(""TestDTZ2"", true);
            }
        });
        t.start();
        t.join();
    }

    private DateTimeZoneBuilder getTestDataTimeZoneBuilder() {
         return new DateTimeZoneBuilder()
         .addCutover(1601, 'w', 1, 1, 1, false, 7200000)
         .setStandardOffset(3600000)
         .addRecurringSavings("""", 3600000, 1601, Integer.MAX_VALUE, 'w', 3, -1, 1, false, 7200000)
         .addRecurringSavings("""", 0, 1601, Integer.MAX_VALUE, 'w', 10, -1, 1, false, 10800000);
    }
```
",2013-04-19 14:48:33,"[{'commitHash': '57eb4cbb9044771cd46a9eee0c62016618930226', 'commitGHEventType': 'closed', 'commitUser': 'jodastephen', 'commitParents': ['6d5104753470c130336e319a64009c0553b29c96'], 'nameRev': '57eb4cbb9044771cd46a9eee0c62016618930226 tags/v2.3~39', 'commitMessage': 'Fix ZoneInfoCompiler and DateTimeZoneBuilder multi-threading\n\nFixes #18\n', 'commitDateTime': '2013-04-19 15:34:57', 'authoredDateTime': '2013-04-19 15:34:57', 'commitGitStats': [{'filePath': 'RELEASE-NOTES.txt', 'insertions': 3, 'deletions': 0, 'lines': 3}, {'filePath': 'pom.xml', 'insertions': 4, 'deletions': 0, 'lines': 4}, {'filePath': 'src/main/java/org/joda/time/tz/ZoneInfoCompiler.java', 'insertions': 5, 'deletions': 4, 'lines': 9}, {'filePath': 'src/test/java/org/joda/time/tz/TestCompiler.java', 'insertions': 24, 'deletions': 0, 'lines': 24}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'ZoneInfoCompiler.java', 'spoonMethods': [{'spoonMethodName': 'org.joda.time.tz.ZoneInfoCompiler', 'TOT': 4, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 2}]}, {'spoonFilePath': 'TestCompiler.java', 'spoonMethods': [{'spoonMethodName': 'org.joda.time.tz.TestCompiler.testDateTimeZoneBuilder()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.joda.time.tz.TestCompiler.getTestDataTimeZoneBuilder()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/JodaOrg/joda-time/issues/18,0.0002777777777777778,"['Bug', 'Fixed']",NPE in DateTimeZoneBuilder,1.0,['org.joda.time.tz.ZoneInfoCompiler'],['57eb4cbb9044771cd46a9eee0c62016618930226'],,['src/main/java/org/joda/time/tz'],5.0,4.0,9.0,1.0,0.0,1.0,4.0,1.0,1.0,2.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,joda-time
34127,2017-11-13 17:43:00,baka-kaba,"(This crash doesn't occur on 1.10.3 or 1.10.2)

I'm pulling a list of elements from a document and using several threads to parse them, but when I call ``Element.html()`` I get this exception

``
System.err: Caused by: java.lang.IllegalStateException: Current state = CODING_END, new state = CODING
System.err:     at java.nio.charset.CharsetEncoder.throwIllegalStateException(CharsetEncoder.java:1014)
System.err:     at java.nio.charset.CharsetEncoder.canEncode(CharsetEncoder.java:923)
System.err:     at java.nio.charset.CharsetEncoder.canEncode(CharsetEncoder.java:973)
System.err:     at org.jsoup.nodes.Entities.canEncode(Entities.java:298)
System.err:     at org.jsoup.nodes.Entities.escape(Entities.java:233)
System.err:     at org.jsoup.nodes.Attributes.html(Attributes.java:323)
System.err:     at org.jsoup.nodes.Element.outerHtmlHead(Element.java:1328)
System.err:     at org.jsoup.nodes.Node$OuterHtmlVisitor.head(Node.java:709)
System.err:     at org.jsoup.select.NodeTraversor.traverse(NodeTraversor.java:45)
System.err:     at org.jsoup.nodes.Node.outerHtml(Node.java:573)
System.err:     at org.jsoup.nodes.Element.html(Element.java:1366)
System.err:     at org.jsoup.nodes.Element.html(Element.java:1360)
System.err:     at com.testapp.JsoupMultithreadedTest$ParseTask.call(JsoupMultithreadedTest.java:49)
System.err:     at com.testapp.thread.JsoupMultithreadedTest$ParseTask.call(JsoupMultithreadedTest.java:39)
System.err:     at java.util.concurrent.FutureTask.run(FutureTask.java:266)
System.err:     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
System.err:     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:636)
System.err:     at java.lang.Thread.run(Thread.java:764)
``

The state can vary, I've also seen ``END_OF_INPUT`` and ``RESET``.

Here's an example that displays the behaviour on Android - it seems to work fine on my Windows 10 machine, but crashes every time on an Android device (Android 6.0.1) and emulator (Android 8.0). The site's encoding [is apparently](https://validator.w3.org/nu/?doc=https%3A%2F%2Fforums.somethingawful.com%2F) ``Windows 1252`` even though the content type is set as ``iso-8859-1``. Works fine with a single thread, and you can run it multithreaded on ``1.10.3`` with no problems.

```java
public class JsoupMultithreadedTest {

    private static final String pageUrl = ""https://forums.somethingawful.com"";
    private static final ExecutorService executor = Executors.newFixedThreadPool(2);

    public static void run() throws IOException, InterruptedException, ExecutionException {
        Document document = Jsoup.connect(pageUrl).get();
        Elements headlines = document.select("".forum"");

        List<Callable<String>> tasks = new ArrayList<>();
        for (Element headline : headlines) {
            tasks.add(new ParseTask(headline));
        }

        for (Future<String> result : executor.invokeAll(tasks)) {
            System.out.println(result.get());
        }
    }

    private static class ParseTask implements Callable<String> {

        private final Element rootElement;

        private ParseTask(Element rootElement) {
            this.rootElement = rootElement;
        }

        @Override
        public String call() throws Exception {
            return rootElement.html();
        }
    }
}
```",2017-11-15 05:38:52,"[{'commitHash': 'e38af6a351cef738885cb61fed828d9c9f6ec5a1', 'commitGHEventType': 'closed', 'commitUser': 'jhy', 'commitParents': ['3264eee7edb8eacb4da97155a7224712dceda9e0'], 'nameRev': 'e38af6a351cef738885cb61fed828d9c9f6ec5a1 tags/jsoup-1.11.2~14', 'commitMessage': 'Threadlocal encoder\n\nFixes #970\n', 'commitDateTime': '2017-11-14 21:38:38', 'authoredDateTime': '2017-11-14 21:38:38', 'commitGitStats': [{'filePath': 'CHANGES', 'insertions': 4, 'deletions': 0, 'lines': 4}, {'filePath': 'src/main/java/org/jsoup/nodes/Document.java', 'insertions': 10, 'deletions': 2, 'lines': 12}, {'filePath': 'src/main/java/org/jsoup/nodes/Entities.java', 'insertions': 1, 'deletions': 1, 'lines': 2}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'Document.java', 'spoonMethods': [{'spoonMethodName': 'org.jsoup.nodes.Document.OutputSettings', 'TOT': 2, 'UPD': 2, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.jsoup.nodes.Document.OutputSettings.prepareEncoder()', 'TOT': 9, 'UPD': 0, 'INS': 2, 'MOV': 4, 'DEL': 3}, {'spoonMethodName': 'org.jsoup.nodes.Document.OutputSettings.encoder()', 'TOT': 2, 'UPD': 0, 'INS': 2, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'Entities.java', 'spoonMethods': [{'spoonMethodName': 'org.jsoup.nodes.Entities.escape(java.lang.Appendable,java.lang.String,org.jsoup.nodes.Document$OutputSettings,boolean,boolean,boolean)', 'TOT': 3, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 1}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/jhy/jsoup/issues/970,1.0002777777777778,['bug'],Entities.canEncode() with non-fast-path encoding is not thread safe on Android (jsoup 1.11.1),1.0,"['org.jsoup.nodes.Document.OutputSettings.prepareEncoder()', 'org.jsoup.nodes.Entities.escape(java.lang.Appendable,java.lang.String,org.jsoup.nodes.Document$OutputSettings,boolean,boolean,boolean)', 'org.jsoup.nodes.Document.OutputSettings', 'org.jsoup.nodes.Document.OutputSettings.encoder()']",['e38af6a351cef738885cb61fed828d9c9f6ec5a1'],,['src/main/java/org/jsoup/nodes'],11.0,3.0,14.0,2.0,2.0,4.0,16.0,5.0,5.0,4.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,jsoup
34144,2016-08-01 12:23:33,wlk,"I stumbled on this problem:

```
java.lang.IllegalStateException: Current state = CODING_END, new state = CODING
    at java.nio.charset.CharsetEncoder.throwIllegalStateException(CharsetEncoder.java:992)
    at java.nio.charset.CharsetEncoder.canEncode(CharsetEncoder.java:904)
    at java.nio.charset.CharsetEncoder.canEncode(CharsetEncoder.java:985)
    at org.jsoup.nodes.Entities.escape(Entities.java:154)
    at org.jsoup.nodes.TextNode.outerHtmlHead(TextNode.java:100)
    at org.jsoup.nodes.Node$OuterHtmlVisitor.head(Node.java:678)
    at org.jsoup.select.NodeTraversor.traverse(NodeTraversor.java:31)
    at org.jsoup.nodes.Node.outerHtml(Node.java:551)
    at org.jsoup.nodes.Element.html(Element.java:1203)
    at org.jsoup.nodes.Element.html(Element.java:1197)
    at org.jsoup.Jsoup.clean(Jsoup.java:235)
```

Issue is most likely appearing under high load (?), this doesn't happen every time I run the cleaning for the same string

```
Jsoup.clean(input, domainName, myWhiteList, new OutputSettings().indentAmount(0).prettyPrint(false))
```
",2016-08-19 22:58:31,"[{'commitHash': '3b2440a257df45fc49884a5c67bb0c9ce631781d', 'commitGHEventType': 'closed', 'commitUser': 'jhy', 'commitParents': ['222feb1791388d1e94e2c99bb5858cf160904d16'], 'nameRev': '3b2440a257df45fc49884a5c67bb0c9ce631781d tags/jsoup-1.10.1~8', 'commitMessage': ""Don't reuse encoders, to make threadsafe\n\nFixes #740 (well, I hope. I wasn't able to replicate it.)\n"", 'commitDateTime': '2016-08-19 15:58:18', 'authoredDateTime': '2016-08-19 15:58:18', 'commitGitStats': [{'filePath': 'CHANGES', 'insertions': 3, 'deletions': 0, 'lines': 3}, {'filePath': 'src/main/java/org/jsoup/nodes/Document.java', 'insertions': 1, 'deletions': 3, 'lines': 4}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'Document.java', 'spoonMethods': [{'spoonMethodName': 'org.jsoup.nodes.Document.OutputSettings', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'org.jsoup.nodes.Document.OutputSettings.charset(java.nio.charset.Charset)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'org.jsoup.nodes.Document.OutputSettings.encoder()', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/jhy/jsoup/issues/740,18.00027777777778,['bug'],"OutputSettings is not threadsafe, throws ISE in canEncode when shared",1.0,"['org.jsoup.nodes.Document.OutputSettings', 'org.jsoup.nodes.Document.OutputSettings.charset(java.nio.charset.Charset)', 'org.jsoup.nodes.Document.OutputSettings.encoder()']",['3b2440a257df45fc49884a5c67bb0c9ce631781d'],,['src/main/java/org/jsoup/nodes'],1.0,3.0,4.0,1.0,0.0,3.0,4.0,1.0,0.0,3.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,jsoup
35019,2019-01-22 13:32:12,kkris,"I have observed that if you have multiple scenarios in a feature and you run the scenarios in parallel then you can (quite reliably) get a wrong karate.tagValues in your scenario.

I have attached a minimal project where this issue can be reproduced: [paralleltags.zip](https://github.com/intuit/karate/files/2782951/paralleltags.zip).

If you execute the SequentialRunner then all tests pass as expected, but with the ParallelRunner they don't. Sometimes tests run fine with the ParallelRunner, so I suspect there is a race condition somewhere.

Please let me know should you need any more information.
",2019-03-25 01:37:08,"[{'commitHash': 'd5070d5c9f84899f428ccf45b0c03c500f399d68', 'commitGHEventType': 'referenced', 'commitUser': 'ptrthomas', 'commitParents': ['a416c94c8c44a2294e5471048353856f61cefe15'], 'nameRev': 'd5070d5c9f84899f428ccf45b0c03c500f399d68 tags/v0.9.2~43', 'commitMessage': 'fix race condition for scenario context init #655\n', 'commitDateTime': '2019-01-22 19:32:14', 'authoredDateTime': '2019-01-22 19:32:14', 'commitGitStats': [{'filePath': 'karate-core/src/main/java/com/intuit/karate/CallContext.java', 'insertions': 0, 'deletions': 21, 'lines': 21}, {'filePath': 'karate-core/src/main/java/com/intuit/karate/Match.java', 'insertions': 1, 'deletions': 1, 'lines': 2}, {'filePath': 'karate-core/src/main/java/com/intuit/karate/StepActions.java', 'insertions': 3, 'deletions': 2, 'lines': 5}, {'filePath': 'karate-core/src/main/java/com/intuit/karate/core/FeatureBackend.java', 'insertions': 1, 'deletions': 1, 'lines': 2}, {'filePath': 'karate-core/src/main/java/com/intuit/karate/core/FeatureExecutionUnit.java', 'insertions': 1, 'deletions': 1, 'lines': 2}, {'filePath': 'karate-core/src/main/java/com/intuit/karate/core/Scenario.java', 'insertions': 2, 'deletions': 3, 'lines': 5}, {'filePath': 'karate-core/src/main/java/com/intuit/karate/core/ScenarioContext.java', 'insertions': 11, 'deletions': 4, 'lines': 15}, {'filePath': 'karate-core/src/main/java/com/intuit/karate/core/ScenarioExecutionUnit.java', 'insertions': 2, 'deletions': 9, 'lines': 11}, {'filePath': 'karate-core/src/main/java/com/intuit/karate/core/Tags.java', 'insertions': 9, 'deletions': 2, 'lines': 11}, {'filePath': 'karate-core/src/test/java/com/intuit/karate/ConfigTest.java', 'insertions': 1, 'deletions': 1, 'lines': 2}, {'filePath': 'karate-core/src/test/java/com/intuit/karate/ScriptTest.java', 'insertions': 3, 'deletions': 3, 'lines': 6}, {'filePath': 'karate-core/src/test/java/com/intuit/karate/core/MandatoryTagHook.java', 'insertions': 2, 'deletions': 2, 'lines': 4}, {'filePath': 'karate-core/src/test/java/com/intuit/karate/http/HttpClientTest.java', 'insertions': 1, 'deletions': 1, 'lines': 2}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'CallContext.java', 'spoonMethods': [{'spoonMethodName': 'com.intuit.karate.CallContext', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 2}, {'spoonMethodName': 'com.intuit.karate.CallContext.getTags()', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'com.intuit.karate.CallContext.setTags(com.intuit.karate.core.Tags)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'com.intuit.karate.CallContext.setScenarioInfo(com.intuit.karate.core.ScenarioInfo)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'com.intuit.karate.CallContext.getScenarioInfo()', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}]}, {'spoonFilePath': 'Match.java', 'spoonMethods': [{'spoonMethodName': 'com.intuit.karate.Match', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'StepActions.java', 'spoonMethods': [{'spoonMethodName': 'com.intuit.karate.StepActions', 'TOT': 2, 'UPD': 0, 'INS': 2, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'FeatureBackend.java', 'spoonMethods': [{'spoonMethodName': 'com.intuit.karate.core.FeatureBackend', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'FeatureExecutionUnit.java', 'spoonMethods': [{'spoonMethodName': 'com.intuit.karate.core.FeatureExecutionUnit.run()', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 1}]}, {'spoonFilePath': 'Scenario.java', 'spoonMethods': [{'spoonMethodName': 'com.intuit.karate.core.Scenario', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.intuit.karate.core.Scenario.getTagsEffective()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'ScenarioContext.java', 'spoonMethods': [{'spoonMethodName': 'com.intuit.karate.core.ScenarioContext', 'TOT': 16, 'UPD': 4, 'INS': 2, 'MOV': 6, 'DEL': 4}]}, {'spoonFilePath': 'ScenarioExecutionUnit.java', 'spoonMethods': [{'spoonMethodName': 'com.intuit.karate.core.ScenarioExecutionUnit', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 2}, {'spoonMethodName': 'com.intuit.karate.core.ScenarioExecutionUnit.init()', 'TOT': 4, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 3}]}, {'spoonFilePath': 'Tags.java', 'spoonMethods': [{'spoonMethodName': 'com.intuit.karate.core.Tags.merge(java.util.List[])', 'TOT': 3, 'UPD': 1, 'INS': 1, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'com.intuit.karate.core.Tags', 'TOT': 3, 'UPD': 0, 'INS': 3, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.intuit.karate.core.Tags.getTagsOriginal()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'ConfigTest.java', 'spoonMethods': [{'spoonMethodName': 'com.intuit.karate.ConfigTest.testSettingVariableViaKarateConfig()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'ScriptTest.java', 'spoonMethods': [{'spoonMethodName': 'com.intuit.karate.ScriptTest.getContext()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.intuit.karate.ScriptTest.testKarateEnvAccessFromScript()', 'TOT': 2, 'UPD': 0, 'INS': 2, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'MandatoryTagHook.java', 'spoonMethods': [{'spoonMethodName': 'com.intuit.karate.core.MandatoryTagHook.beforeScenario(com.intuit.karate.core.Scenario,com.intuit.karate.core.ScenarioContext)', 'TOT': 3, 'UPD': 1, 'INS': 1, 'MOV': 0, 'DEL': 1}]}, {'spoonFilePath': 'HttpClientTest.java', 'spoonMethods': [{'spoonMethodName': 'com.intuit.karate.http.HttpClientTest.getContext()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': 'b05a2101b188a700c4c41b95cfd1ca09fde26c4e', 'commitGHEventType': 'referenced', 'commitUser': 'ptrthomas', 'commitParents': ['d5070d5c9f84899f428ccf45b0c03c500f399d68'], 'nameRev': 'b05a2101b188a700c4c41b95cfd1ca09fde26c4e tags/v0.9.2~42', 'commitMessage': 'improved impl for prev commit #655\n', 'commitDateTime': '2019-01-22 19:46:45', 'authoredDateTime': '2019-01-22 19:46:45', 'commitGitStats': [{'filePath': 'karate-core/src/main/java/com/intuit/karate/core/Tags.java', 'insertions': 12, 'deletions': 6, 'lines': 18}, {'filePath': 'karate-core/src/test/java/com/intuit/karate/core/MandatoryTagHook.java', 'insertions': 1, 'deletions': 2, 'lines': 3}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'Tags.java', 'spoonMethods': [{'spoonMethodName': 'com.intuit.karate.core.Tags', 'TOT': 3, 'UPD': 3, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.intuit.karate.core.Tags.getTagsOriginal()', 'TOT': 2, 'UPD': 2, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.intuit.karate.core.Tags.iterator()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'MandatoryTagHook.java', 'spoonMethods': [{'spoonMethodName': 'com.intuit.karate.core.MandatoryTagHook.beforeScenario(com.intuit.karate.core.Scenario,com.intuit.karate.core.ScenarioContext)', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 1}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/intuit/karate/issues/655,61.000277777777775,"['bug', 'fixed']",karate.tagValues can be wrong when running scenarios in parallel,2.0,"['com.intuit.karate.core.MandatoryTagHook.beforeScenario(com.intuit.karate.core.Scenario,com.intuit.karate.core.ScenarioContext)', 'com.intuit.karate.core.Tags.getTagsOriginal()', 'com.intuit.karate.core.Tags', 'com.intuit.karate.CallContext.setTags(com.intuit.karate.core.Tags)', 'com.intuit.karate.core.FeatureExecutionUnit.run()', 'com.intuit.karate.Match', 'com.intuit.karate.CallContext.getTags()', 'com.intuit.karate.StepActions', 'com.intuit.karate.CallContext', 'com.intuit.karate.core.Tags.iterator()', 'com.intuit.karate.core.Tags.merge(java.util.List[])', 'com.intuit.karate.core.Scenario.getTagsEffective()', 'com.intuit.karate.core.ScenarioContext', 'com.intuit.karate.core.ScenarioExecutionUnit', 'com.intuit.karate.core.ScenarioExecutionUnit.init()', 'com.intuit.karate.core.FeatureBackend', 'com.intuit.karate.CallContext.setScenarioInfo(com.intuit.karate.core.ScenarioInfo)', 'com.intuit.karate.core.Scenario', 'com.intuit.karate.CallContext.getScenarioInfo()']","['d5070d5c9f84899f428ccf45b0c03c500f399d68', 'b05a2101b188a700c4c41b95cfd1ca09fde26c4e']",,"['karate-core/src/main/java/com/intuit/karate/core', 'karate-core/src/main/java/com/intuit/karate']",42.0,50.0,92.0,9.0,13.0,19.0,54.0,7.0,15.0,19.0,10.0,0.0,0.0,0.0,0.0,0.0,0.0,karate
36254,2017-02-08 02:22:43,ludwigb,"On our server we see the following OOM condition quite frequently:
```

Feb 08, 2017 2:17:01 AM org.mapsforge.map.writer.MapFileWriter processTile
WARNUNG: error in parallel preprocessing of ways
java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at org.mapsforge.map.writer.MapFileWriter.processTile(MapFileWriter.java:816)
	at org.mapsforge.map.writer.MapFileWriter.writeSubfile(MapFileWriter.java:906)
	at org.mapsforge.map.writer.MapFileWriter.writeFile(MapFileWriter.java:361)
	at org.mapsforge.map.writer.osmosis.MapFileWriterTask.complete(MapFileWriterTas
k.java:102)
	at org.openstreetmap.osmosis.set.v0_6.EntityMerger.run(EntityMerger.java:243)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at org.mapsforge.map.writer.util.PolyLabel.get(PolyLabel.java:124)
	at org.mapsforge.map.writer.MapFileWriter$WayPreprocessingCallable.call(MapFile
Writer.java:193)
	at org.mapsforge.map.writer.MapFileWriter$WayPreprocessingCallable.call(MapFile
Writer.java:94)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:11
42)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:6
17)
	... 1 more

Feb 08, 2017 2:17:40 AM org.mapsforge.map.writer.MapFileWriter$WayPreprocessingCallable
 call
INFORMATION: Large geometry 32951624755 (2835 coords, down from 3618 coords)
Feb 08, 2017 2:18:07 AM org.mapsforge.map.writer.MapFileWriter$WayPreprocessingCallable
 call

```

A quick look at the code shows that we do not handle the error correctly, as the way causing the OOM is silently ignored:

  ```
                  try {
                        List<Future<WayPreprocessingResult>> futures = EXECUTOR_SERVICE.invokeAll(callables);
                        for (Future<WayPreprocessingResult> wprFuture : futures) {
                            WayPreprocessingResult wpr;
                            try {
                                wpr = wprFuture.get();
                            } catch (ExecutionException e) {
                                LOGGER.log(Level.WARNING, ""error in parallel preprocessing of ways"", e);
                                continue;
                            }
                            if (wpr != null) {
                                wayBuffer.clear();
                                // increment count of ways on this zoom level                                             
                                entitiesPerZoomLevel[indexEntitiesPerZoomLevelTable][1]++;
                                if (configuration.isDebugStrings()) {
                                    writeWaySignature(wpr.getWay(), wayDataBuffer);
                                }
                                processWay(wpr, wpr.getWay(), currentTileLat, currentTileLon, wayBuffer);
                                // write size of way to way data buffer                                                   
                                wayDataBuffer.put(Serializer.getVariableByteUnsigned(wayBuffer.position()));
                                // write way data to way data buffer                                                      
                                wayDataBuffer.put(wayBuffer.array(), 0, wayBuffer.position());
                            }
                        }
                    } catch (InterruptedException e) {
                        LOGGER.log(Level.WARNING, ""error in parallel preprocessing of ways"", e);
                    }
```

The underlying reason for the error seems to be that we allocate as many threads as they are processors, on the server machine that might be 8. This then makes the thread run out of space.

I think we should terminate the process if this error condition happens, because correctness should never be compromised for efficiency. 

I also suggest that we reduce the thread pool size to 1. As I think the way processing is CPU bound it will probably not make much of a difference anyway.

",2017-03-05 18:55:35,"[{'commitHash': 'c8e07f14065b67d2b23fdee08c8e3b1cda0ef7ef', 'commitGHEventType': 'referenced', 'commitUser': 'devemux86', 'commitParents': ['6cbe7b7b158130142d658d5e8b7e72553b381066'], 'nameRev': 'c8e07f14065b67d2b23fdee08c8e3b1cda0ef7ef tags/0.8.0-rc1~23', 'commitMessage': 'Map writer: multiple threads option (default 1) #920\n', 'commitDateTime': '2017-02-09 11:33:05', 'authoredDateTime': '2017-02-09 11:33:05', 'commitGitStats': [{'filePath': 'docs/Changelog.md', 'insertions': 1, 'deletions': 0, 'lines': 1}, {'filePath': 'docs/Getting-Started-Map-Writer.md', 'insertions': 5, 'deletions': 0, 'lines': 5}, {'filePath': 'mapsforge-map-writer/src/main/java/org/mapsforge/map/writer/MapFileWriter.java', 'insertions': 4, 'deletions': 3, 'lines': 7}, {'filePath': 'mapsforge-map-writer/src/main/java/org/mapsforge/map/writer/model/MapWriterConfiguration.java', 'insertions': 17, 'deletions': 1, 'lines': 18}, {'filePath': 'mapsforge-map-writer/src/main/java/org/mapsforge/map/writer/osmosis/MapFileWriterFactory.java', 'insertions': 5, 'deletions': 1, 'lines': 6}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'MapFileWriter.java', 'spoonMethods': [{'spoonMethodName': 'org.mapsforge.map.writer.MapFileWriter', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 2}, {'spoonMethodName': 'org.mapsforge.map.writer.MapFileWriter.writeFile(org.mapsforge.map.writer.model.MapWriterConfiguration,org.mapsforge.map.writer.model.TileBasedDataProcessor)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'MapWriterConfiguration.java', 'spoonMethods': [{'spoonMethodName': 'org.mapsforge.map.writer.model.MapWriterConfiguration', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.mapsforge.map.writer.model.MapWriterConfiguration.getThreads()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.mapsforge.map.writer.model.MapWriterConfiguration.setThreads(int)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'MapFileWriterFactory.java', 'spoonMethods': [{'spoonMethodName': 'org.mapsforge.map.writer.osmosis.MapFileWriterFactory', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.mapsforge.map.writer.osmosis.MapFileWriterFactory.createTaskManagerImpl(org.openstreetmap.osmosis.core.pipeline.common.TaskConfiguration)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '5b59020a960ace68713aeb9d8ec438715f726dbf', 'commitGHEventType': 'referenced', 'commitUser': 'devemux86', 'commitParents': ['f39f108d105302a6d3d48a852b5365fbee1fa691'], 'nameRev': '5b59020a960ace68713aeb9d8ec438715f726dbf tags/0.8.0-rc1~20', 'commitMessage': 'Map writer: terminate process if OOM #920\n', 'commitDateTime': '2017-02-09 16:21:22', 'authoredDateTime': '2017-02-09 16:21:22', 'commitGitStats': [{'filePath': 'mapsforge-map-writer/src/main/java/org/mapsforge/map/writer/MapFileWriter.java', 'insertions': 2, 'deletions': 0, 'lines': 2}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'MapFileWriter.java', 'spoonMethods': [{'spoonMethodName': 'org.mapsforge.map.writer.MapFileWriter.processTile(org.mapsforge.map.writer.model.MapWriterConfiguration,org.mapsforge.map.writer.model.TileCoordinate,org.mapsforge.map.writer.model.TileBasedDataProcessor,com.google.common.cache.LoadingCache,int,java.nio.ByteBuffer,java.nio.ByteBuffer,java.nio.ByteBuffer,java.nio.ByteBuffer)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/mapsforge/mapsforge/issues/920,25.00027777777778,['bug'],MapFileWriter: OOM when processing ways,2.0,"['org.mapsforge.map.writer.osmosis.MapFileWriterFactory', 'org.mapsforge.map.writer.model.MapWriterConfiguration.setThreads(int)', 'org.mapsforge.map.writer.model.MapWriterConfiguration.getThreads()', 'org.mapsforge.map.writer.model.MapWriterConfiguration', 'org.mapsforge.map.writer.MapFileWriter.writeFile(org.mapsforge.map.writer.model.MapWriterConfiguration,org.mapsforge.map.writer.model.TileBasedDataProcessor)', 'org.mapsforge.map.writer.osmosis.MapFileWriterFactory.createTaskManagerImpl(org.openstreetmap.osmosis.core.pipeline.common.TaskConfiguration)', 'org.mapsforge.map.writer.MapFileWriter', 'org.mapsforge.map.writer.MapFileWriter.processTile(org.mapsforge.map.writer.model.MapWriterConfiguration,org.mapsforge.map.writer.model.TileCoordinate,org.mapsforge.map.writer.model.TileBasedDataProcessor,com.google.common.cache.LoadingCache,int,java.nio.ByteBuffer,java.nio.ByteBuffer,java.nio.ByteBuffer,java.nio.ByteBuffer)']","['c8e07f14065b67d2b23fdee08c8e3b1cda0ef7ef', '5b59020a960ace68713aeb9d8ec438715f726dbf']",,"['mapsforge-map-writer/src/main/java/org/mapsforge/map/writer', 'mapsforge-map-writer/src/main/java/org/mapsforge/map/writer/osmosis', 'mapsforge-map-writer/src/main/java/org/mapsforge/map/writer/model']",28.0,5.0,33.0,3.0,0.0,8.0,9.0,0.0,7.0,2.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,mapsforge
36304,2015-05-28 11:09:20,devemux86,"Fixing #513 I see we have synchronization issues with SVG internal cache handled by Salamander library in case of multiple concurrent maps.
",2015-05-28 11:12:38,"[{'commitHash': '3310bd838dc2705f38d0b69e47e1a7210bcd386e', 'commitGHEventType': 'referenced', 'commitUser': 'devemux86', 'commitParents': ['16dba11e10df3ad67d028f6a9442b710a4f0ae96'], 'nameRev': '3310bd838dc2705f38d0b69e47e1a7210bcd386e tags/0.5.2-rc1~7^2~66', 'commitMessage': 'Java SVG: fix SVG Salamander cache synchronization, #618', 'commitDateTime': '2015-05-28 14:10:45', 'authoredDateTime': '2015-05-28 14:10:45', 'commitGitStats': [{'filePath': 'mapsforge-map-awt/src/main/java/org/mapsforge/map/awt/AwtSvgBitmap.java', 'insertions': 36, 'deletions': 34, 'lines': 70}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AwtSvgBitmap.java', 'spoonMethods': [{'spoonMethodName': 'org.mapsforge.map.awt.AwtSvgBitmap.getResourceBitmap(java.io.InputStream,int,float,int,int,int)', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/mapsforge/mapsforge/issues/618,0.0002777777777777778,['bug'],SVG Salamander synchronization,1.0,"['org.mapsforge.map.awt.AwtSvgBitmap.getResourceBitmap(java.io.InputStream,int,float,int,int,int)']",['3310bd838dc2705f38d0b69e47e1a7210bcd386e'],,['mapsforge-map-awt/src/main/java/org/mapsforge/map/awt'],36.0,34.0,70.0,1.0,0.0,1.0,2.0,1.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,mapsforge
36694,2020-02-18 09:33:33,simonbasle,"## Expected Behavior
`Context.putAll(Context)` should work with custom implementations of `Context`.

## Actual Behavior
Let's imagine a project using `reactor-core` rolls its own `Context` implementation. The `stream()` method could in that case return a parallel `Stream`. However, if the default `putAll` method in `Context` isn't overridden, there is an issue: it combines both contexts by iterating their `stream()` and putting entries in a new `ContextN`, which is a (non-thread-safe) `LinkedHashMap`.
This can result in arbitrary loss of data due to concurrent writes.

## Steps to Reproduce
Run the [ContextTest#defaultPutAllWorksWithParallelStream](https://github.com/reactor/reactor-core/blob/master/reactor-core/src/test/java/reactor/util/context/ContextTest.java#L467) test several times until it fails.

## Possible Solution
The iteration-in-insertion-order aspect of `LinkedHashMap` is important, so one can either make the `ContextN#accept(Object, Object)` `synchronized`, or ensure all the `putAll` implementations that rely on `.stream().forEach(...)` force stream sequentiality with `.stream().sequential().forEach(...)`. The later seems to be slightly more performant, perhaps because most of the time the `Stream`s will already be sequential.",2020-02-18 10:56:28,"[{'commitHash': '35a44a765fc258a264cd1cb8d094edf837282e29', 'commitGHEventType': 'referenced', 'commitUser': 'simonbasle', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': 'f9cd50c565c153e4bd5b928baf3dcace65b5a163', 'commitGHEventType': 'closed', 'commitUser': 'simonbasle', 'commitParents': ['c0a7bb8f4fce039dea2e40e7b9f6c06e9762aa28'], 'nameRev': 'f9cd50c565c153e4bd5b928baf3dcace65b5a163 tags/v3.3.3.RELEASE~4', 'commitMessage': 'fix #2050 Use stream().sequential() for Context default putAll\n\nIn case an external `Context` implementation returns a parallel Stream,\r\ndefault `putAll` can fail due to `ContextN` not being thread-safe.\r\n\r\nThis commit ensures that when such operations are performed over the\r\n`stream()`, it is forced to be `sequential()`.', 'commitDateTime': '2020-02-18 11:56:26', 'authoredDateTime': '2020-02-18 11:56:26', 'commitGitStats': [{'filePath': 'reactor-core/src/main/java/reactor/util/context/Context.java', 'insertions': 2, 'deletions': 2, 'lines': 4}, {'filePath': 'reactor-core/src/main/java/reactor/util/context/ContextN.java', 'insertions': 1, 'deletions': 1, 'lines': 2}, {'filePath': 'reactor-core/src/main/java/reactor/util/context/CoreContext.java', 'insertions': 1, 'deletions': 1, 'lines': 2}, {'filePath': 'reactor-core/src/test/java/reactor/util/context/ContextTest.java', 'insertions': 10, 'deletions': 7, 'lines': 17}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'Context.java', 'spoonMethods': [{'spoonMethodName': 'reactor.util.context.putAll(reactor.util.context.Context)', 'TOT': 6, 'UPD': 2, 'INS': 2, 'MOV': 2, 'DEL': 0}]}, {'spoonFilePath': 'ContextN.java', 'spoonMethods': [{'spoonMethodName': 'reactor.util.context.ContextN.putAll(reactor.util.context.Context)', 'TOT': 3, 'UPD': 1, 'INS': 1, 'MOV': 1, 'DEL': 0}]}, {'spoonFilePath': 'CoreContext.java', 'spoonMethods': [{'spoonMethodName': 'reactor.util.context.putAll(reactor.util.context.Context)', 'TOT': 3, 'UPD': 1, 'INS': 1, 'MOV': 1, 'DEL': 0}]}, {'spoonFilePath': 'ContextTest.java', 'spoonMethods': [{'spoonMethodName': 'reactor.util.context.ContextTest.defaultPutAllWorksWithParallelStream()', 'TOT': 5, 'UPD': 0, 'INS': 1, 'MOV': 4, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/reactor/reactor-core/issues/2050,0.0002777777777777778,['type/bug'],Context.putAll can fail with a *custom* Context with parallel stream,1.0,"['reactor.util.context.putAll(reactor.util.context.Context)', 'reactor.util.context.ContextN.putAll(reactor.util.context.Context)']",['f9cd50c565c153e4bd5b928baf3dcace65b5a163'],,['reactor-core/src/main/java/reactor/util/context'],4.0,4.0,8.0,3.0,4.0,2.0,12.0,4.0,4.0,0.0,3.0,0.0,0.0,0.0,1.0,0.0,0.0,reactor-core
36702,2019-11-25 14:26:05,jacekmg,"I've stumbled upon a problem with BoundedElasticScheduler when there's high traffic creating many queued task on it. Invoking dispose() on scheduled tasks seems to break scheduler, to the point where it is not invoking any new tasks.

## Expected Behavior
Scheduler should correctly invoke tasks, after disposing all of its other tasks.

## Actual Behavior
Scheduler doesn't do anything after disposing all of its tasks, and scheduling a new one.

## Steps to Reproduce
In the following example, behavior is as follows:

* ~520 tasks are scheduled
* ~300 scheduled tasks are processed correctly
* When dispose() is called all of scheduled tasks are cancelled
* On most scheduled tasks InterruptedException occurs, except ~15 last ones (505-520)
* Scheduled task after dispose() call is never invoked.

```java
Logger logger = LoggerFactory.getLogger(""Test"");

    @Test
    public void test() throws InterruptedException {
        Scheduler scheduler =
                Schedulers.newBoundedElastic(30, Schedulers.DEFAULT_BOUNDED_ELASTIC_QUEUESIZE, ""test-scheduler"");
        Disposable subscription = Flux.range(0, 100000000)
                .flatMap(i -> {
                    logger.info(""Scheduling {}"", i);
                    return Mono.fromSupplier(() -> invokeLongOperation(i))
                            .subscribeOn(scheduler)
                            .publishOn(Schedulers.parallel())
                            .doOnError(throwable -> logger.error(""Error occurred in thread {} "", i, throwable))
                            .doOnCancel(() -> logger.info(""Thread {} cancelled"", i));
                })
                .subscribe(i -> logger.info(""Thread {} finished"", i),
                           throwable -> logger.error(""Error "", throwable));
        Thread.sleep(10000);
        logger.info(""Invoking dispose"");
        subscription.dispose();
        Thread.sleep(1000);
        Mono.fromSupplier(() -> invokeLongOperation(999999999))
                .subscribeOn(scheduler)
                .publishOn(Schedulers.parallel())
                .subscribe((i) -> logger.info(""Consumed value {} "", i),
                           throwable -> logger.error(""Exception""));
        logger.info(""Sleeping"");
        Thread.sleep(100000);
    }

    private int invokeLongOperation(Integer i) {
        logger.info(""Invoking long operation {}"", i);
        try {
            Thread.sleep(1000);
        } catch (InterruptedException e) {
            logger.error(""Thread {} interrupted"", i);
        }
        return i;
    }
```

##Environment

* Reactor version used: 3.3.0
* JVM version: 1.8.0
",2020-02-14 09:11:22,"[{'commitHash': '1fb434f73abb6cd8a10170775a745a0b24c13475', 'commitGHEventType': 'referenced', 'commitUser': 'simonbasle', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': 'ae86602c5d37658b8e0bea8323ea3ef66a9211ec', 'commitGHEventType': 'referenced', 'commitUser': 'simonbasle', 'commitParents': ['29218c1c317545d9fee4fc011cf674d3aade3482'], 'nameRev': 'ae86602c5d37658b8e0bea8323ea3ef66a9211ec tags/v3.3.3.RELEASE~9', 'commitMessage': ""fix #1973 Resolve a race that prevent decrement of REMAINING_TASK\n\nWhen the active worker becomes inactive and picks a deferred worker\r\nto serve, but at the same time that deferred worker is cancelled or\r\ndisposed, the later can be gone from the parent's pool (due to the\r\nformer polling).\r\n\r\nThe issue is that the decrementing of REMAINING_TASK was guarded\r\nby a successful removal of the deferred worker from the parent,\r\nwhich in the race is already removed and thus returns false.\r\n\r\nThe decrement must be done unconditionally at this step to fix\r\nthe issue."", 'commitDateTime': '2020-02-14 10:11:21', 'authoredDateTime': '2020-02-14 10:11:21', 'commitGitStats': [{'filePath': 'reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java', 'insertions': 2, 'deletions': 1, 'lines': 3}, {'filePath': 'reactor-core/src/test/java/reactor/core/scheduler/BoundedElasticSchedulerTest.java', 'insertions': 42, 'deletions': 0, 'lines': 42}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'BoundedElasticScheduler.java', 'spoonMethods': [{'spoonMethodName': 'reactor.core.scheduler.BoundedElasticScheduler.DeferredDirect.dispose()', 'TOT': 3, 'UPD': 0, 'INS': 0, 'MOV': 2, 'DEL': 1}]}, {'spoonFilePath': 'BoundedElasticSchedulerTest.java', 'spoonMethods': [{'spoonMethodName': 'reactor.core.scheduler.BoundedElasticSchedulerTest.raceActiveWorkerDisposeAndDeferredDirectDispose()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/reactor/reactor-core/issues/1973,80.00027777777778,['type/bug'],BoundedElasticScheduler does not invoke any tasks after interruption.,1.0,['reactor.core.scheduler.BoundedElasticScheduler.DeferredDirect.dispose()'],['ae86602c5d37658b8e0bea8323ea3ef66a9211ec'],,['reactor-core/src/main/java/reactor/core/scheduler'],2.0,1.0,3.0,1.0,0.0,1.0,3.0,2.0,0.0,1.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,reactor-core
36848,2016-12-22 15:28:36,simonbasle,"This for example causes the `WorkQueueProcessorTest#highRate` to fail:

```
@Override
public void onNext(List<String> strings) {
  int size = strings.size();
  counter += size;
  if (strings.contains(s)) {
    synchronized (s) {
      //logger.debug(""Synchronizer!"");
      s.notifyAll();
    }
  } //added to showcase the bug:
  else if (size > 1) {
    System.out.println(strings.contains(s));
  }
}
```
The bug manifests itself in the else branch above, where the sysout would print `true` (contradicting the if/else condition).",2016-12-22 16:16:51,"[{'commitHash': '18be0793a5f56e549fa2b1283c273d7e41799698', 'commitGHEventType': 'referenced', 'commitUser': 'simonbasle', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': 'ac4e9c7275f778dad23ca1a5b45b097495ec0ff4', 'commitGHEventType': 'closed', 'commitUser': 'simonbasle', 'commitParents': ['7b53aed556f7cc79f346c497eaace1a7fd7776f0'], 'nameRev': 'ac4e9c7275f778dad23ca1a5b45b097495ec0ff4 tags/v3.0.4.RELEASE~14', 'commitMessage': 'fix #320 FluxBufferTimeOrSize concurrently modified buffer\n', 'commitDateTime': '2016-12-22 17:16:50', 'authoredDateTime': '2016-12-22 16:30:06', 'commitGitStats': [{'filePath': 'src/main/java/reactor/core/publisher/FluxBufferTimeOrSize.java', 'insertions': 22, 'deletions': 11, 'lines': 33}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'FluxBufferTimeOrSize.java', 'spoonMethods': [{'spoonMethodName': 'reactor.core.publisher.FluxBufferTimeOrSize.BufferAction.checkedError(java.lang.Throwable)', 'TOT': 3, 'UPD': 0, 'INS': 1, 'MOV': 2, 'DEL': 0}, {'spoonMethodName': 'reactor.core.publisher.FluxBufferTimeOrSize.BufferAction.nextCallback(java.lang.Object)', 'TOT': 4, 'UPD': 0, 'INS': 1, 'MOV': 3, 'DEL': 0}, {'spoonMethodName': 'reactor.core.publisher.FluxBufferTimeOrSize.BufferAction.flushCallback(java.lang.Object)', 'TOT': 6, 'UPD': 0, 'INS': 4, 'MOV': 2, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/reactor/reactor-core/issues/320,0.0002777777777777778,['type/bug'],FluxBufferTimeOrSize buffer can be concurrently modified,1.0,"['reactor.core.publisher.FluxBufferTimeOrSize.BufferAction.flushCallback(java.lang.Object)', 'reactor.core.publisher.FluxBufferTimeOrSize.BufferAction.checkedError(java.lang.Throwable)', 'reactor.core.publisher.FluxBufferTimeOrSize.BufferAction.nextCallback(java.lang.Object)']",['ac4e9c7275f778dad23ca1a5b45b097495ec0ff4'],,['src/main/java/reactor/core/publisher'],22.0,11.0,33.0,1.0,0.0,3.0,13.0,7.0,6.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,reactor-core
37153,2014-01-30 21:12:39,cwilbur12,"Today we got a 500 and in looking at the stack trace we saw this to be the issue.

68636256-Caused by: java.lang.NullPointerException
68636298-       at java.util.LinkedList.clear(LinkedList.java:294)
68636350-       at org.apache.ibatis.cache.decorators.FifoCache.clear(FifoCache.java:65)
68636424-       at org.apache.ibatis.cache.decorators.ScheduledCache.clear(ScheduledCache.java:67)
68636508-       at org.apache.ibatis.cache.decorators.ScheduledCache.clearWhenStale(ScheduledCache.java:84)
68636601-       at org.apache.ibatis.cache.decorators.ScheduledCache.getObject(ScheduledCache.java:53)
68636689-       at org.apache.ibatis.cache.decorators.LoggingCache.getObject(LoggingCache.java:50)
68636773-       at org.apache.ibatis.cache.decorators.SynchronizedCache.getObject(SynchronizedCache.java:55)
68636867-       at org.apache.ibatis.executor.CachingExecutor.query(CachingExecutor.java:94)

when using grepcode.com I can see that in the clear() method 

63  public void More ...clear() {
64    delegate.clear();
65    keyList.clear();
66  }

that the keyList is null so the nullpointer is being thrown there.  We arent so sure why it is null there but wanted to see if maybe someone had some input.   We are also curious on what could make this error occur as 500s are not something that we expect to be happening out of the cache of mybatis. That cache is not supposed to be empty and this only happened once and never again.  We are currently using 3.2.2 so we are also wondering if upgrading could maybe fix this issue or if something else maybe would.  

Thank you in advance!
",2014-01-31 15:54:58,"[{'commitHash': 'cb488601eb5c45dcd1774022fdc8748e9a3668aa', 'commitGHEventType': 'closed', 'commitUser': 'emacarron', 'commitParents': ['b9ac317b88d37abef1732dcd1063738d1cdeea1a'], 'nameRev': 'cb488601eb5c45dcd1774022fdc8748e9a3668aa tags/mybatis-3.2.5~6', 'commitMessage': 'Fixes #138. Yet another concurrency issue when reading from caches', 'commitDateTime': '2014-01-31 16:54:55', 'authoredDateTime': '2014-01-31 16:54:55', 'commitGitStats': [{'filePath': 'src/main/java/org/apache/ibatis/cache/decorators/FifoCache.java', 'insertions': 2, 'deletions': 2, 'lines': 4}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'FifoCache.java', 'spoonMethods': [{'spoonMethodName': 'org.apache.ibatis.cache.decorators.FifoCache', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'org.apache.ibatis.cache.decorators.FifoCache.clear()', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 1}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/mybatis/mybatis-3/issues/138,0.0002777777777777778,['bug'],NullPointer in FIFOCache.java in the clear() method,1.0,"['org.apache.ibatis.cache.decorators.FifoCache.clear()', 'org.apache.ibatis.cache.decorators.FifoCache']",['cb488601eb5c45dcd1774022fdc8748e9a3668aa'],,['src/main/java/org/apache/ibatis/cache/decorators'],2.0,2.0,4.0,1.0,0.0,2.0,3.0,0.0,1.0,2.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,mybatis-3
37446,2017-11-23 10:06:57,jpilgrim,"At enfore we have additional, non-public tests using non-public test projects building external libraries. These tests are failing at the moment. These failing tests indicate general problems in N4JS and need to be fixed. If possible, internal tests are to be made public and title of this bug may be adjusted accordingly.

The internal details are described in internal project task IDE-2911.",2017-11-28 12:29:40,"[{'commitHash': 'ce2a06fbd53ec106c89d37d3e1d62f80c5d7f22b', 'commitGHEventType': 'referenced', 'commitUser': 'mor-n4', 'commitParents': ['869c9ef7ef2a354798abb5130db395c3804ce5c5'], 'nameRev': 'ce2a06fbd53ec106c89d37d3e1d62f80c5d7f22b remotes/origin/postponed-GH-1275~430', 'commitMessage': 'GH-373: Fix random Concurrent Modification exception (#383)\n\n* retain workspace lock while reloading external libraries\r\n\r\n* fix a typo\r\n', 'commitDateTime': '2017-11-28 13:28:14', 'authoredDateTime': '2017-11-28 13:28:14', 'commitGitStats': [{'filePath': 'plugins/org.eclipse.n4js.ui/src/org/eclipse/n4js/ui/external/ExternalLibrariesReloadHelper.java', 'insertions': 17, 'deletions': 22, 'lines': 39}, {'filePath': 'plugins/org.eclipse.n4js.ui/src/org/eclipse/n4js/ui/preferences/external/ExternalLibraryPreferencePage.java', 'insertions': 1, 'deletions': 1, 'lines': 2}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'ExternalLibrariesReloadHelper.java', 'spoonMethods': [{'spoonMethodName': 'org.eclipse.n4js.ui.external.ExternalLibrariesReloadHelper.waitForWorkspaceLock(org.eclipse.core.runtime.IProgressMonitor)', 'TOT': 8, 'UPD': 5, 'INS': 0, 'MOV': 2, 'DEL': 1}, {'spoonMethodName': 'org.eclipse.n4js.ui.external.ExternalLibrariesReloadHelper.reloadLibraries(boolean,org.eclipse.core.runtime.IProgressMonitor)', 'TOT': 9, 'UPD': 1, 'INS': 1, 'MOV': 4, 'DEL': 3}, {'spoonMethodName': 'org.eclipse.n4js.ui.external.ExternalLibrariesReloadHelper.reloadLibrariesInternal(boolean,org.eclipse.core.runtime.IProgressMonitor)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'ExternalLibraryPreferencePage.java', 'spoonMethods': [{'spoonMethodName': 'org.eclipse.n4js.ui.preferences.external.ExternalLibraryPreferencePage.maintenanceUpateState(org.eclipse.n4js.ui.preferences.external.MaintenanceActionsChoice,org.eclipse.core.runtime.MultiStatus,org.eclipse.core.runtime.IProgressMonitor)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/eclipse/n4js/issues/373,5.000277777777778,['bug'],Fix random Concurrent Modification exception,1.0,"['org.eclipse.n4js.ui.external.ExternalLibrariesReloadHelper.waitForWorkspaceLock(org.eclipse.core.runtime.IProgressMonitor)', 'org.eclipse.n4js.ui.preferences.external.ExternalLibraryPreferencePage.maintenanceUpateState(org.eclipse.n4js.ui.preferences.external.MaintenanceActionsChoice,org.eclipse.core.runtime.MultiStatus,org.eclipse.core.runtime.IProgressMonitor)', 'org.eclipse.n4js.ui.external.ExternalLibrariesReloadHelper.reloadLibrariesInternal(boolean,org.eclipse.core.runtime.IProgressMonitor)', 'org.eclipse.n4js.ui.external.ExternalLibrariesReloadHelper.reloadLibraries(boolean,org.eclipse.core.runtime.IProgressMonitor)']",['ce2a06fbd53ec106c89d37d3e1d62f80c5d7f22b'],,"['plugins/org.eclipse.n4js.ui/src/org/eclipse/n4js/ui/external', 'plugins/org.eclipse.n4js.ui/src/org/eclipse/n4js/ui/preferences/external']",18.0,23.0,41.0,2.0,7.0,4.0,19.0,6.0,2.0,4.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,n4js
37912,2016-02-03 19:07:54,carl-mastrangelo,"It appears there might be a race in `io.netty.buffer.PoolArena.allocate`, where `allocationsTiny` is incremented.  While the value is incremented under a lock, it also appears to be mutated elsewhere in the file without a lock.  A race detector suggested the two places where the class accesses this field in an unsynchronized way:

```
  Read of size 8 at 0x7f2f7be16108 by thread T53 (mutexes: write M95278):
    #0 io.netty.buffer.PoolArena.allocate(Lio/netty/buffer/PoolThreadCache;Lio/netty/buffer/PooledByteBuf;I)V (PoolArena.java:198)  
    #1 io.netty.buffer.PoolArena.reallocate(Lio/netty/buffer/PooledByteBuf;IZ)V (PoolArena.java:359)  
    #2 io.netty.buffer.PooledByteBuf.capacity(I)Lio/netty/buffer/ByteBuf; (PooledByteBuf.java:120)  
    #3 io.netty.buffer.AbstractByteBuf.ensureWritable0(I)V (AbstractByteBuf.java:269)  
    #4 io.netty.buffer.AbstractByteBuf.ensureWritable(I)Lio/netty/buffer/ByteBuf; (AbstractByteBuf.java:250)  
```

and

```
    #0 io.netty.buffer.PoolArena.allocate(Lio/netty/buffer/PoolThreadCache;Lio/netty/buffer/PooledByteBuf;I)V (PoolArena.java:198)  
    #1 io.netty.buffer.PoolArena.allocate(Lio/netty/buffer/PoolThreadCache;II)Lio/netty/buffer/PooledByteBuf; (PoolArena.java:133)  
    #2 io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(II)Lio/netty/buffer/ByteBuf; (PooledByteBufAllocator.java:262)  
    #3 io.netty.buffer.AbstractByteBufAllocator.directBuffer(II)Lio/netty/buffer/ByteBuf; (AbstractByteBufAllocator.java:157)  
    #4 io.netty.buffer.AbstractByteBufAllocator.directBuffer(I)Lio/netty/buffer/ByteBuf; (AbstractByteBufAllocator.java:148)  
```

cc: @nmittler & @normanmaurer 
",2016-03-14 07:59:04,"[{'commitHash': '35771dd1cdd20e291a4a1a15ef04da57329c41db', 'commitGHEventType': 'closed', 'commitUser': 'normanmaurer', 'commitParents': ['e7ee6abd707f3f0b3cae6a4190b7bb9415a1a568'], 'nameRev': '35771dd1cdd20e291a4a1a15ef04da57329c41db tags/netty-4.1.0.CR4~33', 'commitMessage': 'Fix race in PoolArena.allocate. Fixes #4829\n\nMotivation:\n\nThe statistic counters PoolArena.(allocationsTiny|allocationsSmall) are\nnot protected by a per arena lock, but by a per size class lock. Thus,\ntwo concurrent allocations of different size (class) could lead to a\nrace and ultimately to wrong statistics.\n\nModifications:\n\nUse a thread-safe LongCounter instead of a plain long data type.\n\nResult:\n\nFewer data races.\n', 'commitDateTime': '2016-03-14 08:57:46', 'authoredDateTime': '2016-03-13 22:27:56', 'commitGitStats': [{'filePath': 'buffer/src/main/java/io/netty/buffer/PoolArena.java', 'insertions': 7, 'deletions': 7, 'lines': 14}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'PoolArena.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.buffer.PoolArena', 'TOT': 4, 'UPD': 2, 'INS': 2, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.buffer.PoolArena.allocate(io.netty.buffer.PoolThreadCache,io.netty.buffer.PooledByteBuf,int)', 'TOT': 4, 'UPD': 0, 'INS': 2, 'MOV': 0, 'DEL': 2}, {'spoonMethodName': 'io.netty.buffer.PoolArena.numTinyAllocations()', 'TOT': 3, 'UPD': 0, 'INS': 1, 'MOV': 2, 'DEL': 0}, {'spoonMethodName': 'io.netty.buffer.PoolArena.numSmallAllocations()', 'TOT': 3, 'UPD': 0, 'INS': 1, 'MOV': 2, 'DEL': 0}, {'spoonMethodName': 'io.netty.buffer.PoolArena.numAllocations()', 'TOT': 4, 'UPD': 0, 'INS': 2, 'MOV': 2, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '47b598e6ce6c1f0e57d6955d10652dd5e5bc1462', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': ['ec946f8a3e533c4a86507e87839b6d604566d384'], 'nameRev': '47b598e6ce6c1f0e57d6955d10652dd5e5bc1462 tags/netty-4.0.35.Final~21', 'commitMessage': 'Fix race in PoolArena.allocate. Fixes #4829\n\nMotivation:\n\nThe statistic counters PoolArena.(allocationsTiny|allocationsSmall) are\nnot protected by a per arena lock, but by a per size class lock. Thus,\ntwo concurrent allocations of different size (class) could lead to a\nrace and ultimately to wrong statistics.\n\nModifications:\n\nUse a thread-safe LongCounter instead of a plain long data type.\n\nResult:\n\nFewer data races.\n', 'commitDateTime': '2016-03-14 08:58:15', 'authoredDateTime': '2016-03-13 22:27:56', 'commitGitStats': [{'filePath': 'buffer/src/main/java/io/netty/buffer/PoolArena.java', 'insertions': 7, 'deletions': 7, 'lines': 14}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'PoolArena.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.buffer.PoolArena', 'TOT': 4, 'UPD': 2, 'INS': 2, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.buffer.PoolArena.allocate(io.netty.buffer.PoolThreadCache,io.netty.buffer.PooledByteBuf,int)', 'TOT': 4, 'UPD': 0, 'INS': 2, 'MOV': 0, 'DEL': 2}, {'spoonMethodName': 'io.netty.buffer.PoolArena.numTinyAllocations()', 'TOT': 3, 'UPD': 0, 'INS': 1, 'MOV': 2, 'DEL': 0}, {'spoonMethodName': 'io.netty.buffer.PoolArena.numSmallAllocations()', 'TOT': 3, 'UPD': 0, 'INS': 1, 'MOV': 2, 'DEL': 0}, {'spoonMethodName': 'io.netty.buffer.PoolArena.numAllocations()', 'TOT': 4, 'UPD': 0, 'INS': 2, 'MOV': 2, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/netty/netty/issues/4829,39.000277777777775,['defect'],Race in PoolArena.allocate,1.0,"['io.netty.buffer.PoolArena', 'io.netty.buffer.PoolArena.numTinyAllocations()', 'io.netty.buffer.PoolArena.numAllocations()', 'io.netty.buffer.PoolArena.allocate(io.netty.buffer.PoolThreadCache,io.netty.buffer.PooledByteBuf,int)', 'io.netty.buffer.PoolArena.numSmallAllocations()']",['35771dd1cdd20e291a4a1a15ef04da57329c41db'],,['buffer/src/main/java/io/netty/buffer'],7.0,7.0,14.0,1.0,2.0,5.0,18.0,6.0,8.0,2.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,netty
37994,2015-09-06 02:22:27,ninja-,"out of nowhere...

Caused by: java.lang.NullPointerException
    at io.netty.buffer.PoolChunk.initBufWithSubpage(PoolChunk.java:381)
    at io.netty.buffer.PoolChunk.initBufWithSubpage(PoolChunk.java:369)
    at io.netty.buffer.PoolArena.allocate(PoolArena.java:194)
    at io.netty.buffer.PoolArena.allocate(PoolArena.java:132)
    at io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:262)
    at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:157)
    at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:148)
    at io.netty.buffer.AbstractByteBufAllocator.ioBuffer(AbstractByteBufAllocator.java:101)
    at io.netty.handler.codec.MessageToByteEncoder.allocateBuffer(MessageToByteEncoder.java:140)
    at io.netty.handler.codec.MessageToByteEncoder.write(MessageToByteEncoder.java:105)
    ... 39 more

netty 4.1 beta 6
",2015-10-27 06:43:11,"[{'commitHash': '4e634e916fd0fea5a22700eed188444f7dc83215', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': '777c2bf04c684fcdd2d776b43bf58646852d68c1', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': 'b59cb9406bcd89ee0b56e11f60534db514cb5379', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': '014c97f93281c088f266a4a9da908877b39017c4', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': 'e01484eda2af057695f6f0594e5077bb5e94e240', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': 'c0fd74858a16cd3783e1bff0bb0e3da060c63971', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': ['1c72b34be261dfe3cdbd7636e741a4b522427b2a'], 'nameRev': 'c0fd74858a16cd3783e1bff0bb0e3da060c63971 tags/netty-4.0.33.Final~13', 'commitMessage': '[#4198] Fix race-condition when allocate from multiple-thread.\n\nMotivation:\n\nFix a race condition that was introduced by f18990a8a507d52fc40416d169db340105b10ec0 that could lead to a NPE when allocate from the PooledByteBufAllocator concurrently by many threads.\n\nModifications:\n\nCorrectly synchronize on the PoolSubPage head.\n\nResult:\n\nNo more race.\n', 'commitDateTime': '2015-10-27 07:37:33', 'authoredDateTime': '2015-10-22 22:09:41', 'commitGitStats': [{'filePath': 'buffer/src/main/java/io/netty/buffer/PoolArena.java', 'insertions': 2, 'deletions': 2, 'lines': 4}, {'filePath': 'buffer/src/main/java/io/netty/buffer/PoolChunk.java', 'insertions': 29, 'deletions': 18, 'lines': 47}, {'filePath': 'buffer/src/main/java/io/netty/buffer/PoolSubpage.java', 'insertions': 36, 'deletions': 59, 'lines': 95}, {'filePath': 'buffer/src/test/java/io/netty/buffer/PooledByteBufAllocatorTest.java', 'insertions': 140, 'deletions': 0, 'lines': 140}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'PoolArena.java', 'spoonMethods': []}, {'spoonFilePath': 'PoolChunk.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.buffer.PoolChunk.allocateSubpage(int)', 'TOT': 17, 'UPD': 1, 'INS': 3, 'MOV': 12, 'DEL': 1}, {'spoonMethodName': 'io.netty.buffer.PoolChunk.free(long)', 'TOT': 6, 'UPD': 0, 'INS': 3, 'MOV': 2, 'DEL': 1}]}, {'spoonFilePath': 'PoolSubpage.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.buffer.PoolSubpage.init(int)', 'TOT': 3, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 2}, {'spoonMethodName': 'io.netty.buffer.PoolSubpage.allocate()', 'TOT': 10, 'UPD': 0, 'INS': 0, 'MOV': 8, 'DEL': 2}, {'spoonMethodName': 'io.netty.buffer.PoolSubpage.free(int)', 'TOT': 9, 'UPD': 0, 'INS': 0, 'MOV': 7, 'DEL': 2}, {'spoonMethodName': 'io.netty.buffer.PoolSubpage', 'TOT': 2, 'UPD': 0, 'INS': 2, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.buffer.PoolSubpage.init(io.netty.buffer.PoolSubpage,int)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.buffer.PoolSubpage.free(io.netty.buffer.PoolSubpage,int)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'PooledByteBufAllocatorTest.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.buffer.PooledByteBufAllocatorTest', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': 'd93f9062554d07eec6d376d85d886e40b6a907f7', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': ['85236d5446b4858df20dc6bd270735490c807666'], 'nameRev': 'd93f9062554d07eec6d376d85d886e40b6a907f7 tags/netty-4.1.0.Beta8~34', 'commitMessage': '[#4198] Fix race-condition when allocate from multiple-thread.\n\nMotivation:\n\nFix a race condition that was introduced by f18990a8a507d52fc40416d169db340105b10ec0 that could lead to a NPE when allocate from the PooledByteBufAllocator concurrently by many threads.\n\nModifications:\n\nCorrectly synchronize on the PoolSubPage head.\n\nResult:\n\nNo more race.\n', 'commitDateTime': '2015-10-27 07:39:42', 'authoredDateTime': '2015-10-22 22:09:41', 'commitGitStats': [{'filePath': 'buffer/src/main/java/io/netty/buffer/PoolArena.java', 'insertions': 2, 'deletions': 2, 'lines': 4}, {'filePath': 'buffer/src/main/java/io/netty/buffer/PoolChunk.java', 'insertions': 29, 'deletions': 18, 'lines': 47}, {'filePath': 'buffer/src/main/java/io/netty/buffer/PoolSubpage.java', 'insertions': 36, 'deletions': 59, 'lines': 95}, {'filePath': 'buffer/src/test/java/io/netty/buffer/PooledByteBufAllocatorTest.java', 'insertions': 140, 'deletions': 0, 'lines': 140}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'PoolArena.java', 'spoonMethods': []}, {'spoonFilePath': 'PoolChunk.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.buffer.PoolChunk.allocateSubpage(int)', 'TOT': 17, 'UPD': 1, 'INS': 3, 'MOV': 12, 'DEL': 1}, {'spoonMethodName': 'io.netty.buffer.PoolChunk.free(long)', 'TOT': 6, 'UPD': 0, 'INS': 3, 'MOV': 2, 'DEL': 1}]}, {'spoonFilePath': 'PoolSubpage.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.buffer.PoolSubpage.init(int)', 'TOT': 3, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 2}, {'spoonMethodName': 'io.netty.buffer.PoolSubpage.allocate()', 'TOT': 10, 'UPD': 0, 'INS': 0, 'MOV': 8, 'DEL': 2}, {'spoonMethodName': 'io.netty.buffer.PoolSubpage.free(int)', 'TOT': 9, 'UPD': 0, 'INS': 0, 'MOV': 7, 'DEL': 2}, {'spoonMethodName': 'io.netty.buffer.PoolSubpage', 'TOT': 2, 'UPD': 0, 'INS': 2, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.buffer.PoolSubpage.init(io.netty.buffer.PoolSubpage,int)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.buffer.PoolSubpage.free(io.netty.buffer.PoolSubpage,int)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'PooledByteBufAllocatorTest.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.buffer.PooledByteBufAllocatorTest', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '9feba624da65e09bdfd3cbf9d62f806e196cad4d', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': ['9f2b54753120e2b68771f82681511dc2679afcd5'], 'nameRev': '9feba624da65e09bdfd3cbf9d62f806e196cad4d remotes/origin/master_deprecated~83', 'commitMessage': '[#4198] Fix race-condition when allocate from multiple-thread.\n\nMotivation:\n\nFix a race condition that was introduced by f18990a8a507d52fc40416d169db340105b10ec0 that could lead to a NPE when allocate from the PooledByteBufAllocator concurrently by many threads.\n\nModifications:\n\nCorrectly synchronize on the PoolSubPage head.\n\nResult:\n\nNo more race.\n', 'commitDateTime': '2015-10-27 07:39:53', 'authoredDateTime': '2015-10-22 22:09:41', 'commitGitStats': [{'filePath': 'buffer/src/main/java/io/netty/buffer/PoolArena.java', 'insertions': 2, 'deletions': 2, 'lines': 4}, {'filePath': 'buffer/src/main/java/io/netty/buffer/PoolChunk.java', 'insertions': 29, 'deletions': 18, 'lines': 47}, {'filePath': 'buffer/src/main/java/io/netty/buffer/PoolSubpage.java', 'insertions': 36, 'deletions': 59, 'lines': 95}, {'filePath': 'buffer/src/test/java/io/netty/buffer/PooledByteBufAllocatorTest.java', 'insertions': 140, 'deletions': 0, 'lines': 140}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'PoolArena.java', 'spoonMethods': []}, {'spoonFilePath': 'PoolChunk.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.buffer.PoolChunk.allocateSubpage(int)', 'TOT': 17, 'UPD': 1, 'INS': 3, 'MOV': 12, 'DEL': 1}, {'spoonMethodName': 'io.netty.buffer.PoolChunk.free(long)', 'TOT': 6, 'UPD': 0, 'INS': 3, 'MOV': 2, 'DEL': 1}]}, {'spoonFilePath': 'PoolSubpage.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.buffer.PoolSubpage.init(int)', 'TOT': 3, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 2}, {'spoonMethodName': 'io.netty.buffer.PoolSubpage.allocate()', 'TOT': 10, 'UPD': 0, 'INS': 0, 'MOV': 8, 'DEL': 2}, {'spoonMethodName': 'io.netty.buffer.PoolSubpage.free(int)', 'TOT': 9, 'UPD': 0, 'INS': 0, 'MOV': 7, 'DEL': 2}, {'spoonMethodName': 'io.netty.buffer.PoolSubpage', 'TOT': 2, 'UPD': 0, 'INS': 2, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.buffer.PoolSubpage.init(io.netty.buffer.PoolSubpage,int)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.buffer.PoolSubpage.free(io.netty.buffer.PoolSubpage,int)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'PooledByteBufAllocatorTest.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.buffer.PooledByteBufAllocatorTest', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/netty/netty/issues/4198,51.000277777777775,['defect'],NullPointer @     at io.netty.buffer.PoolChunk.initBufWithSubpage,1.0,"['io.netty.buffer.PoolSubpage.allocate()', 'io.netty.buffer.PoolSubpage', 'io.netty.buffer.PoolChunk.allocateSubpage(int)', 'io.netty.buffer.PoolSubpage.init(io.netty.buffer.PoolSubpage,int)', 'io.netty.buffer.PoolSubpage.init(int)', 'io.netty.buffer.PoolChunk.free(long)', 'io.netty.buffer.PoolSubpage.free(int)', 'io.netty.buffer.PoolSubpage.free(io.netty.buffer.PoolSubpage,int)']",['c0fd74858a16cd3783e1bff0bb0e3da060c63971'],,['buffer/src/main/java/io/netty/buffer'],67.0,79.0,146.0,3.0,1.0,8.0,49.0,30.0,10.0,8.0,2.0,0.0,0.0,0.0,7.0,0.0,0.0,netty
38229,2014-07-17 02:35:17,lokeshhctm,"Exception in thread ""pool-2-thread-1"" java.lang.NullPointerException
        at io.netty.util.HashedWheelTimer$HashedWheelBucket.addTimeout(HashedWheelTimer.java:625)
        at io.netty.util.HashedWheelTimer$Worker.transferTimeoutsToBuckets(HashedWheelTimer.java:411)
        at io.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:368)
        at java.lang.Thread.run(Thread.java:744)
",2014-07-25 04:35:29,"[{'commitHash': 'f30e4efb421380edea93445f91f99252c4950b68', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': '44ea769f537bf16b833d03db844b1f3067b3acd7', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': ['581374c111d9979d9efbaff8fba647687918db20'], 'nameRev': '44ea769f537bf16b833d03db844b1f3067b3acd7 tags/netty-5.0.0.Alpha2~461', 'commitMessage': ""[#2662] Fix race in cancellation of TimerTasks which could let to NPE\n\nMotivation:\n\nDue some race-condition while handling canellation of TimerTasks it was possibleto corrupt the linked-list structure that is represent by HashedWheelBucket and so produce a NPE.\n\nModification:\n\nFix the problem by adding another MpscLinkedQueue which holds the cancellation tasks and process them on each tick. This allows to use no synchronization / locking at all while introduce a latency of max 1 tick before the TimerTask can be GC'ed.\n\nResult:\n\nNo more NPE\n"", 'commitDateTime': '2014-07-25 06:29:28', 'authoredDateTime': '2014-07-21 14:17:35', 'commitGitStats': [{'filePath': 'common/src/main/java/io/netty/util/HashedWheelTimer.java', 'insertions': 40, 'deletions': 26, 'lines': 66}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'HashedWheelTimer.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.util.HashedWheelTimer.Worker.run()', 'TOT': 8, 'UPD': 1, 'INS': 2, 'MOV': 4, 'DEL': 1}, {'spoonMethodName': 'io.netty.util.HashedWheelTimer.HashedWheelTimeout.cancel()', 'TOT': 6, 'UPD': 0, 'INS': 1, 'MOV': 3, 'DEL': 2}, {'spoonMethodName': 'io.netty.util.HashedWheelTimer.HashedWheelBucket', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'io.netty.util.HashedWheelTimer', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.util.HashedWheelTimer.Worker.processCancelledTasks()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '5b2bdd844db56079e6cb75b0e1142d337723e1c3', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': ['e1cc1fbabce9b0d7247e6a955496c78e3772390a'], 'nameRev': '5b2bdd844db56079e6cb75b0e1142d337723e1c3 tags/netty-4.1.0.Beta2~57', 'commitMessage': ""[#2662] Fix race in cancellation of TimerTasks which could let to NPE\n\nMotivation:\n\nDue some race-condition while handling canellation of TimerTasks it was possibleto corrupt the linked-list structure that is represent by HashedWheelBucket and so produce a NPE.\n\nModification:\n\nFix the problem by adding another MpscLinkedQueue which holds the cancellation tasks and process them on each tick. This allows to use no synchronization / locking at all while introduce a latency of max 1 tick before the TimerTask can be GC'ed.\n\nResult:\n\nNo more NPE\n"", 'commitDateTime': '2014-07-25 06:34:35', 'authoredDateTime': '2014-07-21 14:17:35', 'commitGitStats': [{'filePath': 'common/src/main/java/io/netty/util/HashedWheelTimer.java', 'insertions': 40, 'deletions': 26, 'lines': 66}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'HashedWheelTimer.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.util.HashedWheelTimer.Worker.run()', 'TOT': 8, 'UPD': 1, 'INS': 2, 'MOV': 4, 'DEL': 1}, {'spoonMethodName': 'io.netty.util.HashedWheelTimer.HashedWheelTimeout.cancel()', 'TOT': 6, 'UPD': 0, 'INS': 1, 'MOV': 3, 'DEL': 2}, {'spoonMethodName': 'io.netty.util.HashedWheelTimer.HashedWheelBucket', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'io.netty.util.HashedWheelTimer', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.util.HashedWheelTimer.Worker.processCancelledTasks()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': 'd989b243511ff76727972b45f52ea94fbb6d1118', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': ['f5faada77c90625a2bb48c38cb8148762d5e74b0'], 'nameRev': 'd989b243511ff76727972b45f52ea94fbb6d1118 tags/netty-4.0.22.Final~37', 'commitMessage': ""[#2662] Fix race in cancellation of TimerTasks which could let to NPE\n\nMotivation:\n\nDue some race-condition while handling canellation of TimerTasks it was possibleto corrupt the linked-list structure that is represent by HashedWheelBucket and so produce a NPE.\n\nModification:\n\nFix the problem by adding another MpscLinkedQueue which holds the cancellation tasks and process them on each tick. This allows to use no synchronization / locking at all while introduce a latency of max 1 tick before the TimerTask can be GC'ed.\n\nResult:\n\nNo more NPE\n"", 'commitDateTime': '2014-07-25 06:34:49', 'authoredDateTime': '2014-07-21 14:17:35', 'commitGitStats': [{'filePath': 'common/src/main/java/io/netty/util/HashedWheelTimer.java', 'insertions': 40, 'deletions': 26, 'lines': 66}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'HashedWheelTimer.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.util.HashedWheelTimer.Worker.run()', 'TOT': 8, 'UPD': 1, 'INS': 2, 'MOV': 4, 'DEL': 1}, {'spoonMethodName': 'io.netty.util.HashedWheelTimer.HashedWheelTimeout.cancel()', 'TOT': 6, 'UPD': 0, 'INS': 1, 'MOV': 3, 'DEL': 2}, {'spoonMethodName': 'io.netty.util.HashedWheelTimer.HashedWheelBucket', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'io.netty.util.HashedWheelTimer', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.util.HashedWheelTimer.Worker.processCancelledTasks()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/netty/netty/issues/2662,8.000277777777777,['defect'],exception in hashedwheelTimer,1.0,"['io.netty.util.HashedWheelTimer.HashedWheelBucket', 'io.netty.util.HashedWheelTimer.Worker.processCancelledTasks()', 'io.netty.util.HashedWheelTimer.Worker.run()', 'io.netty.util.HashedWheelTimer.HashedWheelTimeout.cancel()', 'io.netty.util.HashedWheelTimer']",['44ea769f537bf16b833d03db844b1f3067b3acd7'],,['common/src/main/java/io/netty/util'],40.0,26.0,66.0,1.0,1.0,5.0,18.0,8.0,5.0,4.0,1.0,0.0,0.0,0.0,3.0,0.0,0.0,netty
38306,2014-01-22 02:20:42,shevek,"I made a LocalTransport server/client pair.
I made them share a LocalEventLoopGroup(1)

Server.channelRead() calls ctx.close()

I then get:
    [WARN] SingleThreadEventExecutor - Unexpected exception from an event executor:  <java.lang.NullPointerException>java.lang.NullPointerException
        at io.netty.channel.local.LocalChannel.access$300(LocalChannel.java:44)
        at io.netty.channel.local.LocalChannel$4.run(LocalChannel.java:169)
        at io.netty.channel.local.LocalEventLoop.run(LocalEventLoop.java:33)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:101)
        at java.lang.Thread.run(Thread.java:724)

That's the connectPromise.

If I allow the EventLoopGroup 2 threads, then it does not throw NPE.

But I really want 1 thread to synchronize logging, as I'm in a test suite.
",2014-04-17 13:19:45,"[{'commitHash': 'f0c7c901d08dc5acaea2fa3a22119f9f4a5e8ada', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': ['d67184b488d163be03437e68ee2ec33ea7add2f6'], 'nameRev': 'f0c7c901d08dc5acaea2fa3a22119f9f4a5e8ada tags/netty-4.0.16.Final~83', 'commitMessage': 'Add testcase to try to reproduce #2144\n', 'commitDateTime': '2014-01-23 07:04:42', 'authoredDateTime': '2014-01-23 07:04:42', 'commitGitStats': [{'filePath': 'transport/src/test/java/io/netty/channel/local/LocalChannelTest.java', 'insertions': 34, 'deletions': 0, 'lines': 34}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'LocalChannelTest.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.channel.local.LocalChannelTest.testServerCloseChannelSameEventLoop()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': 'cac449a5e497922055784f99535adf2d3a4176fe', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': ['2b63d2d0cdee700b299b258bb0aeb46b07e6c335'], 'nameRev': 'cac449a5e497922055784f99535adf2d3a4176fe tags/netty-5.0.0.Alpha2~955', 'commitMessage': 'Add testcase to try to reproduce #2144\n', 'commitDateTime': '2014-01-23 07:10:08', 'authoredDateTime': '2014-01-23 07:04:42', 'commitGitStats': [{'filePath': 'transport/src/test/java/io/netty/channel/local/LocalChannelTest.java', 'insertions': 34, 'deletions': 0, 'lines': 34}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'LocalChannelTest.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.channel.local.LocalChannelTest.testServerCloseChannelSameEventLoop()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': 'c97c8b7b8eaad8f79aeb73f871ea0135dc133baa', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': ['f5d4e6b10e6ab6ec8a8876e34b6295e5ce68eaf8'], 'nameRev': 'c97c8b7b8eaad8f79aeb73f871ea0135dc133baa tags/netty-4.0.19.Final~23', 'commitMessage': '[#2144] Fix NPE in Local transport caused by a race\n\nMotivation:\nAt the moment it is possible to see a NPE when the LocalSocketChannels doRegister() method is called and the LocalSocketChannels doClose() method is called before the registration was completed.\n\nModifications:\nMake sure we delay the actual close until the registration task was executed.\n\nResult:\nNo more NPE\n', 'commitDateTime': '2014-04-17 14:24:36', 'authoredDateTime': '2014-04-17 14:24:36', 'commitGitStats': [{'filePath': 'transport/src/main/java/io/netty/channel/local/LocalChannel.java', 'insertions': 16, 'deletions': 1, 'lines': 17}, {'filePath': 'transport/src/test/java/io/netty/channel/local/LocalChannelTest.java', 'insertions': 71, 'deletions': 0, 'lines': 71}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'LocalChannel.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.channel.local.LocalChannel.doRegister()', 'TOT': 10, 'UPD': 1, 'INS': 5, 'MOV': 2, 'DEL': 2}, {'spoonMethodName': 'io.netty.channel.local.LocalChannel.doRegister().4.run()', 'TOT': 5, 'UPD': 0, 'INS': 3, 'MOV': 0, 'DEL': 2}, {'spoonMethodName': 'io.netty.channel.local.LocalChannel', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.channel.local.LocalChannel.doClose()', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 0}]}, {'spoonFilePath': 'LocalChannelTest.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.channel.local.LocalChannelTest.localChannelRaceCondition()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '9b670d819f321bf8f58084d928ff210dce4a8b9c', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': ['199d2b499c0091d992f8693d76b879c247f09d11'], 'nameRev': '9b670d819f321bf8f58084d928ff210dce4a8b9c tags/netty-4.1.0.Beta1~204', 'commitMessage': '[#2144] Fix NPE in Local transport caused by a race\n\nMotivation:\nAt the moment it is possible to see a NPE when the LocalSocketChannels doRegister() method is called and the LocalSocketChannels doClose() method is called before the registration was completed.\n\nModifications:\nMake sure we delay the actual close until the registration task was executed.\n\nResult:\nNo more NPE\n', 'commitDateTime': '2014-04-17 15:06:52', 'authoredDateTime': '2014-04-17 14:24:36', 'commitGitStats': [{'filePath': 'transport/src/main/java/io/netty/channel/local/LocalChannel.java', 'insertions': 16, 'deletions': 1, 'lines': 17}, {'filePath': 'transport/src/test/java/io/netty/channel/local/LocalChannelTest.java', 'insertions': 70, 'deletions': 0, 'lines': 70}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'LocalChannel.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.channel.local.LocalChannel.doRegister()', 'TOT': 10, 'UPD': 1, 'INS': 5, 'MOV': 2, 'DEL': 2}, {'spoonMethodName': 'io.netty.channel.local.LocalChannel.doRegister().4.run()', 'TOT': 5, 'UPD': 0, 'INS': 3, 'MOV': 0, 'DEL': 2}, {'spoonMethodName': 'io.netty.channel.local.LocalChannel', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.channel.local.LocalChannel.doClose()', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 0}]}, {'spoonFilePath': 'LocalChannelTest.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.channel.local.LocalChannelTest.localChannelRaceCondition()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': 'dfd6b9009c5e9929308ff2e9c0181dff55c60e26', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': ['5babc1a498a9463d856af7d7728b9c8cd0f681d6'], 'nameRev': 'dfd6b9009c5e9929308ff2e9c0181dff55c60e26 tags/netty-5.0.0.Alpha2~753', 'commitMessage': '[#2144] Fix NPE in Local transport caused by a race\n\nMotivation:\nAt the moment it is possible to see a NPE when the LocalSocketChannels doRegister() method is called and the LocalSocketChannels doClose() method is called before the registration was completed.\n\nModifications:\nMake sure we delay the actual close until the registration task was executed.\n\nResult:\nNo more NPE\n', 'commitDateTime': '2014-04-17 15:19:23', 'authoredDateTime': '2014-04-17 14:24:36', 'commitGitStats': [{'filePath': 'transport/src/main/java/io/netty/channel/local/LocalChannel.java', 'insertions': 16, 'deletions': 1, 'lines': 17}, {'filePath': 'transport/src/test/java/io/netty/channel/local/LocalChannelTest.java', 'insertions': 71, 'deletions': 0, 'lines': 71}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'LocalChannel.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.channel.local.LocalChannel.doRegister()', 'TOT': 10, 'UPD': 1, 'INS': 5, 'MOV': 2, 'DEL': 2}, {'spoonMethodName': 'io.netty.channel.local.LocalChannel.doRegister().4.run()', 'TOT': 5, 'UPD': 0, 'INS': 3, 'MOV': 0, 'DEL': 2}, {'spoonMethodName': 'io.netty.channel.local.LocalChannel', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.channel.local.LocalChannel.doClose()', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 0}]}, {'spoonFilePath': 'LocalChannelTest.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.channel.local.LocalChannelTest.localChannelRaceCondition()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/netty/netty/issues/2144,85.00027777777778,['defect'],NullPointerException in LocalTransport,1.0,"['io.netty.channel.local.LocalChannel.doRegister()', 'io.netty.channel.local.LocalChannel.doRegister().4.run()', 'io.netty.channel.local.LocalChannel.doClose()', 'io.netty.channel.local.LocalChannel']","['f0c7c901d08dc5acaea2fa3a22119f9f4a5e8ada', 'c97c8b7b8eaad8f79aeb73f871ea0135dc133baa']",,['transport/src/main/java/io/netty/channel/local'],16.0,1.0,17.0,1.0,1.0,4.0,18.0,3.0,10.0,4.0,1.0,0.0,0.0,0.0,3.0,0.0,0.0,netty
38315,2014-01-05 12:03:17,fredericBregier,"Hi,

I tried to upgrade to Netty 3.9.0 this morning, but I found out a blocking issue on SSL support. I check the change from 3.8.0 (where it works like a charm), and I only found the changes related to issue #1905 .

I have great difficulties to find out what is going on, but here are however what I've seen:
- The connection is opened
- The sslHandler is created correctly
- Then it blocks and never answer
- After a while (a time-out in my application), the channel is forcely closed and I get this exception 
  
    05/01/2014/12:06:36.501 DEBUG [org.jboss.netty.handler.ssl.SslHandler] [New I/O worker #125] org.jboss.netty.logging.InternalLoggerFactory$1.debug(InternalLoggerFactory.java:85) : Swallowing an exception raised while writing non-app data
  java.nio.channels.ClosedChannelException: null
  at org.jboss.netty.channel.socket.nio.AbstractNioWorker.cleanUpWriteBuffer(AbstractNioWorker.java:433) ~[netty-3.9.0.Final.jar:na]
  at org.jboss.netty.channel.socket.nio.AbstractNioWorker.writeFromUserCode(AbstractNioWorker.java:128) ~[netty-3.9.0.Final.jar:na]
  at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:99) ~[netty-3.9.0.Final.jar:na]
  at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36) ~[netty-3.9.0.Final.jar:na]
  at org.jboss.netty.channel.Channels.write(Channels.java:725) ~[netty-3.9.0.Final.jar:na]
  at org.jboss.netty.channel.Channels.write(Channels.java:686) ~[netty-3.9.0.Final.jar:na]
  at org.jboss.netty.handler.ssl.SslHandler.wrapNonAppData(SslHandler.java:1177) ~[netty-3.9.0.Final.jar:na]
  at org.jboss.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1294) ~[netty-3.9.0.Final.jar:na]
  at org.jboss.netty.handler.ssl.SslHandler.channelDisconnected(SslHandler.java:668) ~[netty-3.9.0.Final.jar:na]
  at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:396) ~[netty-3.9.0.Final.jar:na]
  at org.jboss.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360) ~[netty-3.9.0.Final.jar:na]
  at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:93) ~[netty-3.9.0.Final.jar:na]
  at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) ~[netty-3.9.0.Final.jar:na]
  at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318) ~[netty-3.9.0.Final.jar:na]
  at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) ~[netty-3.9.0.Final.jar:na]
  at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) ~[netty-3.9.0.Final.jar:na]
  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895) [na:1.6.0_43]
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918) [na:1.6.0_43]
  at java.lang.Thread.run(Thread.java:662) [na:1.6.0_43]

I tested this issue with several ways:
- using an http client (Firefox, Chrome)
- using native Netty client

But when I tried the native HTTP Netty server example, it works like a charm. So I'm lost. I checked most of the code, trying various ways:
- enabling (default)/disabling automatic handshake
- enabling / disabling (default) renegociation
  When putting some logs in Netty, it appears that it never goes further than the handshake (the handshake never finished, except in error).

Any idea ?

Frederic
",2014-01-09 09:02:37,"[{'commitHash': '8198f23dbb8060e9a6912aaa281203fe5e5d592e', 'commitGHEventType': 'referenced', 'commitUser': 'trustin', 'commitParents': ['fc9a794613608217410b5ac93df6185193ebe575'], 'nameRev': '8198f23dbb8060e9a6912aaa281203fe5e5d592e tags/netty-3.9.1.Final~36', 'commitMessage': 'Fix a regression in SslHandler where delegated tasks run in a different executor makes the session hang\n\n- Fixes #2098\n', 'commitDateTime': '2014-01-09 15:13:31', 'authoredDateTime': '2014-01-09 15:13:31', 'commitGitStats': [{'filePath': 'src/main/java/org/jboss/netty/handler/ssl/SslHandler.java', 'insertions': 60, 'deletions': 15, 'lines': 75}, {'filePath': 'src/test/java/org/jboss/netty/handler/ssl/AbstractSocketSslEchoTest.java', 'insertions': 38, 'deletions': 3, 'lines': 41}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'SslHandler.java', 'spoonMethods': [{'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.runDelegatedTasks().4.run()', 'TOT': 5, 'UPD': 0, 'INS': 0, 'MOV': 4, 'DEL': 1}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.runDelegatedTasks()', 'TOT': 6, 'UPD': 0, 'INS': 0, 'MOV': 5, 'DEL': 1}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.SSLEngineInboundCloseFuture', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.runDelegatedTasks(boolean)', 'TOT': 2, 'UPD': 0, 'INS': 2, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.handshake()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.wrapNonAppData(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.Channel)', 'TOT': 2, 'UPD': 0, 'INS': 2, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.runDelegatedTasks(boolean).4.run()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.unwrap(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.Channel,org.jboss.netty.buffer.ChannelBuffer,int,int)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.wrap(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.Channel)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.runDelegatedTasks(boolean).4.run().1.run()', 'TOT': 2, 'UPD': 0, 'INS': 2, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'AbstractSocketSslEchoTest.java', 'spoonMethods': [{'spoonMethodName': 'org.jboss.netty.handler.ssl.AbstractSocketSslEchoTest.testSslEcho()', 'TOT': 5, 'UPD': 0, 'INS': 0, 'MOV': 4, 'DEL': 1}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.AbstractSocketSslEchoTest.testSslEcho1()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.AbstractSocketSslEchoTest.testSslEcho2()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.AbstractSocketSslEchoTest.testSslEcho3()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.AbstractSocketSslEchoTest.testSslEcho4()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.AbstractSocketSslEchoTest.testSslEcho(boolean,boolean)', 'TOT': 7, 'UPD': 0, 'INS': 7, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': 'bc21443ea904ba37f46c1fb3b9ab99322e3bbb0d', 'commitGHEventType': 'referenced', 'commitUser': 'trustin', 'commitParents': ['0bbc3facec1fa6c83b0bec04bdfbefc424230a3f'], 'nameRev': 'bc21443ea904ba37f46c1fb3b9ab99322e3bbb0d tags/netty-4.0.15.Final~47', 'commitMessage': 'Fix a regression in SslHandler where delegated tasks run in a different executor makes the session hang\n\n- Fixes #2098\n- Deprecate specifying an alternative Executor for delegated tasks for SslHandler\n', 'commitDateTime': '2014-01-09 18:08:05', 'authoredDateTime': '2014-01-09 18:07:36', 'commitGitStats': [{'filePath': 'handler/src/main/java/io/netty/handler/ssl/SslHandler.java', 'insertions': 65, 'deletions': 23, 'lines': 88}, {'filePath': 'testsuite/src/test/java/io/netty/testsuite/transport/socket/AbstractSocketTest.java', 'insertions': 5, 'deletions': 2, 'lines': 7}, {'filePath': 'testsuite/src/test/java/io/netty/testsuite/transport/socket/SocketSslEchoTest.java', 'insertions': 53, 'deletions': 37, 'lines': 90}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'SslHandler.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.handler.ssl.SslHandler.handshake().3', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.handler.ssl.SslHandler.safeClose(io.netty.channel.ChannelHandlerContext,io.netty.channel.ChannelFuture,io.netty.channel.ChannelPromise).6', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.handler.ssl.SslHandler.runDelegatedTasks()', 'TOT': 4, 'UPD': 1, 'INS': 1, 'MOV': 2, 'DEL': 0}, {'spoonMethodName': 'io.netty.handler.ssl.SslHandler.handshake()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.handler.ssl.SslHandler.channelActive(io.netty.channel.ChannelHandlerContext).4', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.handler.ssl.SslHandler.safeClose(io.netty.channel.ChannelHandlerContext,io.netty.channel.ChannelFuture,io.netty.channel.ChannelPromise)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.handler.ssl.SslHandler.handshake().2', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.handler.ssl.SslHandler.safeClose(io.netty.channel.ChannelHandlerContext,io.netty.channel.ChannelFuture,io.netty.channel.ChannelPromise).5', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.handler.ssl.SslHandler.handshake().2.run()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.handler.ssl.SslHandler', 'TOT': 5, 'UPD': 0, 'INS': 4, 'MOV': 0, 'DEL': 1}]}, {'spoonFilePath': 'AbstractSocketTest.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.testsuite.transport.socket.AbstractSocketTest.run()', 'TOT': 4, 'UPD': 0, 'INS': 3, 'MOV': 1, 'DEL': 0}]}, {'spoonFilePath': 'SocketSslEchoTest.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEcho(io.netty.bootstrap.ServerBootstrap,io.netty.bootstrap.Bootstrap)', 'TOT': 8, 'UPD': 3, 'INS': 4, 'MOV': 1, 'DEL': 0}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEcho0(io.netty.bootstrap.ServerBootstrap,io.netty.bootstrap.Bootstrap,boolean,boolean)', 'TOT': 5, 'UPD': 1, 'INS': 0, 'MOV': 1, 'DEL': 3}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEchoWithChunkHandler(io.netty.bootstrap.ServerBootstrap,io.netty.bootstrap.Bootstrap)', 'TOT': 5, 'UPD': 2, 'INS': 0, 'MOV': 2, 'DEL': 1}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEchoComposite()', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEchoComposite(io.netty.bootstrap.ServerBootstrap,io.netty.bootstrap.Bootstrap)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEchoWithChunkHandler()', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEchoWithChunkHandlerComposite()', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEchoWithChunkHandlerComposite(io.netty.bootstrap.ServerBootstrap,io.netty.bootstrap.Bootstrap)', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEcho0(io.netty.bootstrap.ServerBootstrap,io.netty.bootstrap.Bootstrap,boolean,boolean).2.initChannel(io.netty.channel.socket.SocketChannel)', 'TOT': 3, 'UPD': 0, 'INS': 0, 'MOV': 2, 'DEL': 1}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEcho0(io.netty.bootstrap.ServerBootstrap,io.netty.bootstrap.Bootstrap,boolean,boolean).1.initChannel(io.netty.channel.socket.SocketChannel)', 'TOT': 3, 'UPD': 0, 'INS': 0, 'MOV': 2, 'DEL': 1}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest', 'TOT': 6, 'UPD': 0, 'INS': 6, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.data()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEcho(io.netty.bootstrap.ServerBootstrap,io.netty.bootstrap.Bootstrap).1.initChannel(io.netty.channel.socket.SocketChannel)', 'TOT': 3, 'UPD': 0, 'INS': 3, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEcho(io.netty.bootstrap.ServerBootstrap,io.netty.bootstrap.Bootstrap).2.initChannel(io.netty.channel.socket.SocketChannel)', 'TOT': 3, 'UPD': 0, 'INS': 3, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEcho()', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '53110a83b3ce8a71eb0ec4a9087c4754640bdd5f', 'commitGHEventType': 'referenced', 'commitUser': 'trustin', 'commitParents': ['0b8e732c6cce5bade1cf5d8a8d87e5c788c6d20f'], 'nameRev': '53110a83b3ce8a71eb0ec4a9087c4754640bdd5f tags/netty-5.0.0.Alpha2~1009', 'commitMessage': 'Fix a regression in SslHandler where delegated tasks run in a different executor makes the session hang\n\n- Fixes #2098\n- Deprecate specifying an alternative Executor for delegated tasks for SslHandler\n', 'commitDateTime': '2014-01-09 18:08:48', 'authoredDateTime': '2014-01-09 18:07:36', 'commitGitStats': [{'filePath': 'handler/src/main/java/io/netty/handler/ssl/SslHandler.java', 'insertions': 65, 'deletions': 23, 'lines': 88}, {'filePath': 'testsuite/src/test/java/io/netty/testsuite/transport/socket/AbstractSocketTest.java', 'insertions': 5, 'deletions': 2, 'lines': 7}, {'filePath': 'testsuite/src/test/java/io/netty/testsuite/transport/socket/SocketSslEchoTest.java', 'insertions': 53, 'deletions': 37, 'lines': 90}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'SslHandler.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.handler.ssl.SslHandler.handshake().3', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.handler.ssl.SslHandler.safeClose(io.netty.channel.ChannelHandlerContext,io.netty.channel.ChannelFuture,io.netty.channel.ChannelPromise).6', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.handler.ssl.SslHandler.runDelegatedTasks()', 'TOT': 4, 'UPD': 1, 'INS': 1, 'MOV': 2, 'DEL': 0}, {'spoonMethodName': 'io.netty.handler.ssl.SslHandler.handshake()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.handler.ssl.SslHandler.channelActive(io.netty.channel.ChannelHandlerContext).4', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.handler.ssl.SslHandler.safeClose(io.netty.channel.ChannelHandlerContext,io.netty.channel.ChannelFuture,io.netty.channel.ChannelPromise)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.handler.ssl.SslHandler.handshake().2', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.handler.ssl.SslHandler.safeClose(io.netty.channel.ChannelHandlerContext,io.netty.channel.ChannelFuture,io.netty.channel.ChannelPromise).5', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.handler.ssl.SslHandler.handshake().2.run()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.handler.ssl.SslHandler', 'TOT': 5, 'UPD': 0, 'INS': 4, 'MOV': 0, 'DEL': 1}]}, {'spoonFilePath': 'AbstractSocketTest.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.testsuite.transport.socket.AbstractSocketTest.run()', 'TOT': 4, 'UPD': 0, 'INS': 3, 'MOV': 1, 'DEL': 0}]}, {'spoonFilePath': 'SocketSslEchoTest.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEcho(io.netty.bootstrap.ServerBootstrap,io.netty.bootstrap.Bootstrap)', 'TOT': 8, 'UPD': 3, 'INS': 4, 'MOV': 1, 'DEL': 0}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEcho0(io.netty.bootstrap.ServerBootstrap,io.netty.bootstrap.Bootstrap,boolean,boolean)', 'TOT': 5, 'UPD': 1, 'INS': 0, 'MOV': 1, 'DEL': 3}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEchoWithChunkHandler(io.netty.bootstrap.ServerBootstrap,io.netty.bootstrap.Bootstrap)', 'TOT': 5, 'UPD': 2, 'INS': 0, 'MOV': 2, 'DEL': 1}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEchoComposite()', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEchoComposite(io.netty.bootstrap.ServerBootstrap,io.netty.bootstrap.Bootstrap)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEchoWithChunkHandler()', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEchoWithChunkHandlerComposite()', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEchoWithChunkHandlerComposite(io.netty.bootstrap.ServerBootstrap,io.netty.bootstrap.Bootstrap)', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEcho0(io.netty.bootstrap.ServerBootstrap,io.netty.bootstrap.Bootstrap,boolean,boolean).2.initChannel(io.netty.channel.socket.SocketChannel)', 'TOT': 3, 'UPD': 0, 'INS': 0, 'MOV': 2, 'DEL': 1}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEcho0(io.netty.bootstrap.ServerBootstrap,io.netty.bootstrap.Bootstrap,boolean,boolean).1.initChannel(io.netty.channel.socket.SocketChannel)', 'TOT': 3, 'UPD': 0, 'INS': 0, 'MOV': 2, 'DEL': 1}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest', 'TOT': 6, 'UPD': 0, 'INS': 6, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.data()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEcho(io.netty.bootstrap.ServerBootstrap,io.netty.bootstrap.Bootstrap).1.initChannel(io.netty.channel.socket.SocketChannel)', 'TOT': 3, 'UPD': 0, 'INS': 3, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEcho(io.netty.bootstrap.ServerBootstrap,io.netty.bootstrap.Bootstrap).2.initChannel(io.netty.channel.socket.SocketChannel)', 'TOT': 3, 'UPD': 0, 'INS': 3, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.testsuite.transport.socket.SocketSslEchoTest.testSslEcho()', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/netty/netty/issues/2098,3.000277777777778,['defect'],Issue on SslHandler with Netty 3.9.0,1.0,"['org.jboss.netty.handler.ssl.SslHandler.wrapNonAppData(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.Channel)', 'org.jboss.netty.handler.ssl.SslHandler.runDelegatedTasks(boolean)', 'org.jboss.netty.handler.ssl.SslHandler.SSLEngineInboundCloseFuture', 'org.jboss.netty.handler.ssl.SslHandler.runDelegatedTasks()', 'org.jboss.netty.handler.ssl.SslHandler.runDelegatedTasks(boolean).4.run().1.run()', 'org.jboss.netty.handler.ssl.SslHandler.runDelegatedTasks(boolean).4.run()', 'org.jboss.netty.handler.ssl.SslHandler.wrap(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.Channel)', 'org.jboss.netty.handler.ssl.SslHandler.runDelegatedTasks().4.run()', 'org.jboss.netty.handler.ssl.SslHandler.unwrap(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.Channel,org.jboss.netty.buffer.ChannelBuffer,int,int)', 'org.jboss.netty.handler.ssl.SslHandler.handshake()']",['8198f23dbb8060e9a6912aaa281203fe5e5d592e'],,['src/main/java/org/jboss/netty/handler/ssl'],60.0,15.0,75.0,1.0,0.0,10.0,22.0,9.0,10.0,3.0,1.0,0.0,0.0,0.0,2.0,0.0,0.0,netty
38317,2014-01-01 00:38:41,tom1401,"Hello,

I believe there is a deadlock situation in Netty 3.7(likely still exists in 3.8 and 3.9) when using SslHandler with SPDY/3:

At the high level, the channel pipeline is setup as:
SslHandler
SpdyFrameCodec
SpdySessionHandler

The situation encountered showed the following call sequence in the stack trace:

First invocation:
        at org.jboss.netty.handler.ssl.SslHandler.channelClosed(SslHandler.java:1566)
Second invocation:
        at org.jboss.netty.handler.ssl.SslHandler$6.run(SslHandler.java:1593)
Third invocation:
        at org.jboss.netty.handler.ssl.SslHandler.handleDownstream(SslHandler.java:623)

Netty 3.7 SSLHandler.java source code: http://netty.io/3.7/xref/org/jboss/netty/handler/ssl/SslHandler.html

First invocation takes the pendingUnencryptedWritesLock at line 1568 and releases lock at line 1596, but at line 1593, it indirectly invoked SslHandler::handleDownstream() which attempts to take the pendingUnencryptedWritesLock at line 623.

Would you mind confirming if this is a new bug or some known issue? Thanks!

---

Full stack trace:

""New I/O worker #1"" prio=10 tid=0x00002aaad4a0f000 nid=0x1833 waiting on condition [0x0000000040b5d000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000007d51b93e8> (a org.jboss.netty.util.internal.NonReentrantLock)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
        at org.jboss.netty.util.internal.NonReentrantLock.lock(NonReentrantLock.java:31)
        at org.jboss.netty.handler.ssl.SslHandler.handleDownstream(SslHandler.java:623)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
        at org.jboss.netty.channel.SimpleChannelHandler.writeRequested(SimpleChannelHandler.java:292)
        at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
        at org.jboss.netty.channel.SimpleChannelHandler.writeRequested(SimpleChannelHandler.java:292)
        at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
        at org.jboss.netty.channel.Channels.write(Channels.java:725)
        at org.jboss.netty.handler.codec.spdy.SpdyFrameEncoder.handleDownstream(SpdyFrameEncoder.java:193)
        at org.jboss.netty.handler.codec.spdy.SpdyFrameCodec.handleDownstream(SpdyFrameCodec.java:62)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
        at org.jboss.netty.channel.Channels.write(Channels.java:725)
        at org.jboss.netty.handler.codec.spdy.SpdySessionHandler.issueStreamError(SpdySessionHandler.java:658)
        at org.jboss.netty.handler.codec.spdy.SpdySessionHandler.access$000(SpdySessionHandler.java:39)
        at org.jboss.netty.handler.codec.spdy.SpdySessionHandler$2.operationComplete(SpdySessionHandler.java:501)
        at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:427)
        at org.jboss.netty.channel.DefaultChannelFuture.notifyListeners(DefaultChannelFuture.java:413)
        at org.jboss.netty.channel.DefaultChannelFuture.setFailure(DefaultChannelFuture.java:380)
        at org.jboss.netty.handler.ssl.SslHandler$6.run(SslHandler.java:1593)
        at org.jboss.netty.channel.socket.ChannelRunnableWrapper.run(ChannelRunnableWrapper.java:40)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:71)
        at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:57)
        at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
        at org.jboss.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
        at org.jboss.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)
        at org.jboss.netty.handler.ssl.SslHandler.channelClosed(SslHandler.java:1566)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:88)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireChannelClosed(Channels.java:468)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:376)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:93)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
",2014-08-06 00:42:26,"[{'commitHash': '2b16d9f67084350ce936815639e770a0b3bca3d0', 'commitGHEventType': 'referenced', 'commitUser': 'trustin', 'commitParents': ['2c17b75a9935232fb812d5387a154349b005f423'], 'nameRev': '2b16d9f67084350ce936815639e770a0b3bca3d0 tags/netty-3.9.3.Final~4', 'commitMessage': 'Fix the dead lock while failing SSL pending writes on a closed channel\n\nRelated issue: #2093\n\nMotivation:\n\nWhen a channel with SslHandler is closed with pending writes (either\nunencrypted or encrypted), SslHandler.channelClosed() attempts to\nacquire a lock and fail the futures of the pending writes.\n\nIf a user added a listener to any of the failed futures and the listener\nattempts to write something, it will make SslHandler.handleDownstream()\ninvoked, which also attempts to acquire the same lock.\n\nBecause the lock is non-reentrant, even if these two lock acquisitions\nare taking place in the same thread, the second attempt will block for\never.\n\nModification:\n\nDo not fail the futures while holding a lock.  Notify them after\nreleasing the lock.\n\nResult:\n\nOne less dead lock\n', 'commitDateTime': '2014-08-05 17:35:11', 'authoredDateTime': '2014-08-05 17:35:11', 'commitGitStats': [{'filePath': 'src/main/java/org/jboss/netty/handler/ssl/SslHandler.java', 'insertions': 13, 'deletions': 8, 'lines': 21}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'SslHandler.java', 'spoonMethods': [{'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.channelClosed(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent).7.run()', 'TOT': 19, 'UPD': 14, 'INS': 3, 'MOV': 2, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/netty/netty/issues/2093,217.00027777777777,['defect'],Deadlock when use SslHandler with SPDY,1.0,"['org.jboss.netty.handler.ssl.SslHandler.channelClosed(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent).7.run()']",['2b16d9f67084350ce936815639e770a0b3bca3d0'],,['src/main/java/org/jboss/netty/handler/ssl'],13.0,8.0,21.0,1.0,14.0,1.0,19.0,2.0,3.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,netty
38318,2013-12-24 18:20:15,valodzka,"In rare circumstances I'm getting following exception:

```
WARN io.netty.channel.nio.NioEventLoop Unexpected exception in the selector loop.
java.lang.NullPointerException: null 
    at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:251) 
    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:502) 
    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:452) 
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:346) 
    at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:101) 
    at java.lang.Thread.run(Thread.java:744)
```

It's rare and happens a few times a week on production servers and I don't know simple way to reproduce it. 

```
java version ""1.7.0_45""
Java(TM) SE Runtime Environment (build 1.7.0_45-b18)
Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode)
netty 4.0.14.Final, nio, client mode    
```
",2014-01-09 07:24:18,"[{'commitHash': '0bbc3facec1fa6c83b0bec04bdfbefc424230a3f', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': ['393f7b2306f282df852bb487173c725396607ea7'], 'nameRev': '0bbc3facec1fa6c83b0bec04bdfbefc424230a3f tags/netty-4.0.15.Final~48', 'commitMessage': '[#2086] Fix race which could produce NPE in AbstractNioUnsafe.finishConnect\n', 'commitDateTime': '2014-01-09 08:22:34', 'authoredDateTime': '2014-01-09 08:22:34', 'commitGitStats': [{'filePath': 'transport/src/main/java/io/netty/channel/nio/AbstractNioChannel.java', 'insertions': 7, 'deletions': 2, 'lines': 9}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AbstractNioChannel.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.finishConnect()', 'TOT': 5, 'UPD': 0, 'INS': 3, 'MOV': 1, 'DEL': 1}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '0b8e732c6cce5bade1cf5d8a8d87e5c788c6d20f', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': ['06823e3afff6f1eab55f9df58a9e4c1c68087486'], 'nameRev': '0b8e732c6cce5bade1cf5d8a8d87e5c788c6d20f tags/netty-5.0.0.Alpha2~1010', 'commitMessage': '[#2086] Fix race which could produce NPE in AbstractNioUnsafe.finishConnect\n', 'commitDateTime': '2014-01-09 08:22:56', 'authoredDateTime': '2014-01-09 08:22:34', 'commitGitStats': [{'filePath': 'transport/src/main/java/io/netty/channel/nio/AbstractNioChannel.java', 'insertions': 7, 'deletions': 2, 'lines': 9}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AbstractNioChannel.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.finishConnect()', 'TOT': 5, 'UPD': 0, 'INS': 3, 'MOV': 1, 'DEL': 1}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '751943ed00e864f1104806c7e03a1207797cf1b6', 'commitGHEventType': 'referenced', 'commitUser': 'trustin', 'commitParents': ['1b79781b4ef125b55819d8429b137e3a3e670d38'], 'nameRev': '751943ed00e864f1104806c7e03a1207797cf1b6 tags/netty-4.0.15.Final~44', 'commitMessage': 'Fix a potential NPE due to the race between a connection attempt and its cancellation\n\n- should fix #2086\n', 'commitDateTime': '2014-01-09 19:24:44', 'authoredDateTime': '2014-01-09 19:24:44', 'commitGitStats': [{'filePath': 'transport/src/main/java/io/netty/channel/nio/AbstractNioChannel.java', 'insertions': 17, 'deletions': 10, 'lines': 27}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AbstractNioChannel.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.finishConnect()', 'TOT': 7, 'UPD': 1, 'INS': 0, 'MOV': 3, 'DEL': 3}, {'spoonMethodName': 'io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.fulfillConnectPromise(io.netty.channel.ChannelPromise,java.lang.Throwable)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.fulfillConnectPromise(io.netty.channel.ChannelPromise,boolean)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '2af14190564a39b7170a981ca2a970c65a2c4df0', 'commitGHEventType': 'referenced', 'commitUser': 'trustin', 'commitParents': ['2338bc52cbbf5eaf5aaf422fabfab89ba5cd887b'], 'nameRev': '2af14190564a39b7170a981ca2a970c65a2c4df0 tags/netty-5.0.0.Alpha2~1006', 'commitMessage': 'Fix a potential NPE due to the race between a connection attempt and its cancellation\n\n- should fix #2086\n', 'commitDateTime': '2014-01-09 19:25:08', 'authoredDateTime': '2014-01-09 19:24:44', 'commitGitStats': [{'filePath': 'transport/src/main/java/io/netty/channel/nio/AbstractNioChannel.java', 'insertions': 17, 'deletions': 10, 'lines': 27}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AbstractNioChannel.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.finishConnect()', 'TOT': 7, 'UPD': 1, 'INS': 0, 'MOV': 3, 'DEL': 3}, {'spoonMethodName': 'io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.fulfillConnectPromise(io.netty.channel.ChannelPromise,java.lang.Throwable)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.fulfillConnectPromise(io.netty.channel.ChannelPromise,boolean)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/netty/netty/issues/2086,15.000277777777777,['defect'],NPE in AbstractNioUnsafe.finishConnect,2.0,"['io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.finishConnect()', 'io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.fulfillConnectPromise(io.netty.channel.ChannelPromise,java.lang.Throwable)', 'io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.fulfillConnectPromise(io.netty.channel.ChannelPromise,boolean)']","['0bbc3facec1fa6c83b0bec04bdfbefc424230a3f', '751943ed00e864f1104806c7e03a1207797cf1b6']",,['transport/src/main/java/io/netty/channel/nio'],24.0,12.0,36.0,1.0,1.0,3.0,14.0,4.0,5.0,4.0,1.0,0.0,0.0,0.0,2.0,0.0,0.0,netty
38332,2013-11-28 11:16:08,normanmaurer,"Because of a race-condition it is possible that OioSocketChannel.close(...) / OioDatagramChannel.close(...) will interrupt the wrong thread.
",2013-12-03 06:09:03,"[{'commitHash': 'b8de307b0da720532d945544d6b54036514ecd85', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': ['4a5c21b2e1170602f08ce59c45108865e10caa24'], 'nameRev': 'b8de307b0da720532d945544d6b54036514ecd85 tags/netty-3.2.10.Final~2', 'commitMessage': '[#2013] Fix a race which could happen to have the worker Thread interrupted during close(...) when using OIO\n', 'commitDateTime': '2013-11-28 12:17:35', 'authoredDateTime': '2013-11-25 18:43:18', 'commitGitStats': [{'filePath': 'src/main/java/org/jboss/netty/channel/socket/oio/OioDatagramWorker.java', 'insertions': 18, 'deletions': 6, 'lines': 24}, {'filePath': 'src/main/java/org/jboss/netty/channel/socket/oio/OioWorker.java', 'insertions': 18, 'deletions': 6, 'lines': 24}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'OioDatagramWorker.java', 'spoonMethods': [{'spoonMethodName': 'org.jboss.netty.channel.socket.oio.OioDatagramWorker.run()', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.channel.socket.oio.OioDatagramWorker.close(org.jboss.netty.channel.socket.oio.OioDatagramChannel,org.jboss.netty.channel.ChannelFuture)', 'TOT': 3, 'UPD': 0, 'INS': 1, 'MOV': 2, 'DEL': 0}]}, {'spoonFilePath': 'OioWorker.java', 'spoonMethods': [{'spoonMethodName': 'org.jboss.netty.channel.socket.oio.OioWorker.run()', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.channel.socket.oio.OioWorker.close(org.jboss.netty.channel.socket.oio.OioSocketChannel,org.jboss.netty.channel.ChannelFuture)', 'TOT': 3, 'UPD': 0, 'INS': 1, 'MOV': 2, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '6c8299c09d66cb1f97f84f59b977876904010fab', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': ['78a840fcfe72137059283cafc86c3a2ca45db3dd'], 'nameRev': '6c8299c09d66cb1f97f84f59b977876904010fab tags/netty-3.6.7.Final~1', 'commitMessage': '[#2013] Fix a race which could happen to have the worker Thread interrupted during close(...) when using OIO\n', 'commitDateTime': '2013-12-03 06:35:03', 'authoredDateTime': '2013-12-03 06:35:03', 'commitGitStats': [{'filePath': 'src/main/java/org/jboss/netty/channel/socket/oio/AbstractOioWorker.java', 'insertions': 20, 'deletions': 7, 'lines': 27}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AbstractOioWorker.java', 'spoonMethods': [{'spoonMethodName': 'org.jboss.netty.channel.socket.oio.AbstractOioWorker.run()', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.channel.socket.oio.AbstractOioWorker.close(org.jboss.netty.channel.socket.oio.AbstractOioChannel,org.jboss.netty.channel.ChannelFuture,boolean)', 'TOT': 3, 'UPD': 0, 'INS': 1, 'MOV': 2, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': 'a30be3e73e4bdbcbf79c1dfff9c917e300613db4', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': ['1fc171b58d87c2f7d429fe95f1e46c63644e50ba'], 'nameRev': 'a30be3e73e4bdbcbf79c1dfff9c917e300613db4 tags/netty-3.9.0.Final~10', 'commitMessage': '[#2013] Fix a race which could happen to have the worker Thread interrupted during close(...) when using OIO\n', 'commitDateTime': '2013-12-03 08:07:50', 'authoredDateTime': '2013-12-03 06:35:03', 'commitGitStats': [{'filePath': 'src/main/java/org/jboss/netty/channel/socket/oio/AbstractOioWorker.java', 'insertions': 20, 'deletions': 7, 'lines': 27}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AbstractOioWorker.java', 'spoonMethods': [{'spoonMethodName': 'org.jboss.netty.channel.socket.oio.AbstractOioWorker.run()', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.channel.socket.oio.AbstractOioWorker.close(org.jboss.netty.channel.socket.oio.AbstractOioChannel,org.jboss.netty.channel.ChannelFuture,boolean)', 'TOT': 3, 'UPD': 0, 'INS': 1, 'MOV': 2, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/netty/netty/issues/2013,4.000277777777778,['defect'],OioSocketChannel.close(...) and OioDatagramChannel.close(...) may interrupt wrong thread,1.0,"['org.jboss.netty.channel.socket.oio.OioWorker.run()', 'org.jboss.netty.channel.socket.oio.OioDatagramWorker.run()', 'org.jboss.netty.channel.socket.oio.OioWorker.close(org.jboss.netty.channel.socket.oio.OioSocketChannel,org.jboss.netty.channel.ChannelFuture)', 'org.jboss.netty.channel.socket.oio.OioDatagramWorker.close(org.jboss.netty.channel.socket.oio.OioDatagramChannel,org.jboss.netty.channel.ChannelFuture)']",['b8de307b0da720532d945544d6b54036514ecd85'],,['src/main/java/org/jboss/netty/channel/socket/oio'],36.0,12.0,48.0,2.0,0.0,4.0,10.0,6.0,4.0,0.0,2.0,0.0,0.0,0.0,2.0,0.0,0.0,netty
38335,2013-11-18 07:31:48,trustin,"`io.netty.testsuite.transport.socket.SocketConnectionAttemptTest.testConnectCancellation`
### Error Message

```
Expected: is <true>
     but: was <false>
```
### Stacktrace

```
java.lang.AssertionError: 
Expected: is <true>
     but: was <false>
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.junit.Assert.assertThat(Assert.java:865)
    at org.junit.Assert.assertThat(Assert.java:832)
    at io.netty.testsuite.transport.socket.SocketConnectionAttemptTest.testConnectCancellation(SocketConnectionAttemptTest.java:94)
```
### Standard Output

```
01:40:58.760 [main] DEBUG i.n.t.t.s.SocketConnectionAttemptTest - -Dio.netty.testsuite.badHost: 255.255.255.0
01:40:58.771 [main] INFO  i.n.t.t.s.SocketConnectionAttemptTest - Running: testConnectCancellation 1 of 2 with UnpooledByteBufAllocator
01:40:59.774 [testsuite-nio-worker-3-3] WARN  io.netty.channel.nio.NioEventLoop - Unexpected exception in the selector loop.
java.lang.IllegalStateException: complete already: DefaultChannelPromise@3f0cf5cb(failure(java.util.concurrent.CancellationException)
    at io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:401) ~[netty-common-4.0.13.Final-SNAPSHOT.jar:na]
    at io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:87) ~[netty-transport-4.0.13.Final-SNAPSHOT.jar:na]
    at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:240) ~[netty-transport-4.0.13.Final-SNAPSHOT.jar:na]
    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:502) ~[netty-transport-4.0.13.Final-SNAPSHOT.jar:na]
    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:452) ~[netty-transport-4.0.13.Final-SNAPSHOT.jar:na]
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:346) ~[netty-transport-4.0.13.Final-SNAPSHOT.jar:na]
    at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:101) [netty-common-4.0.13.Final-SNAPSHOT.jar:na]
    at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]
Caused by: java.net.SocketException: Network is unreachable
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[na:1.7.0_45]
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735) ~[na:1.7.0_45]
    at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:191) ~[netty-transport-4.0.13.Final-SNAPSHOT.jar:na]
    at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:228) ~[netty-transport-4.0.13.Final-SNAPSHOT.jar:na]
    ... 5 common frames omitted
```
",2013-11-18 07:33:57,"[{'commitHash': '54d3c99469b923b409ce1a52b6d1181984470b11', 'commitGHEventType': 'referenced', 'commitUser': 'trustin', 'commitParents': ['b5a587fadfd0821088d37d9d23388c343d73eb45'], 'nameRev': '54d3c99469b923b409ce1a52b6d1181984470b11 tags/netty-4.0.13.Final~18', 'commitMessage': 'Fix an unexpected IllegalStateException from a selector loop when a user cancels a connection attempt\n\n- Fixes #1986\n', 'commitDateTime': '2013-11-18 16:33:21', 'authoredDateTime': '2013-11-18 16:32:52', 'commitGitStats': [{'filePath': 'transport/src/main/java/io/netty/channel/nio/AbstractNioChannel.java', 'insertions': 2, 'deletions': 1, 'lines': 3}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AbstractNioChannel.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.finishConnect()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '38f57adb704613abd132d58bfa8f6c824b552c41', 'commitGHEventType': 'closed', 'commitUser': 'trustin', 'commitParents': ['786fdbd6e05d3cd3bd8e43f4f47cc5d0f1f9da9a'], 'nameRev': '38f57adb704613abd132d58bfa8f6c824b552c41 tags/netty-5.0.0.Alpha1~104', 'commitMessage': 'Fix an unexpected IllegalStateException from a selector loop when a user cancels a connection attempt\n\n- Fixes #1986\n', 'commitDateTime': '2013-11-18 16:33:49', 'authoredDateTime': '2013-11-18 16:32:52', 'commitGitStats': [{'filePath': 'transport/src/main/java/io/netty/channel/nio/AbstractNioChannel.java', 'insertions': 2, 'deletions': 1, 'lines': 3}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AbstractNioChannel.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.finishConnect()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': 'd0e928db707460e00bb1269dc042401246c76324', 'commitGHEventType': 'referenced', 'commitUser': 'trustin', 'commitParents': ['38f57adb704613abd132d58bfa8f6c824b552c41'], 'nameRev': 'd0e928db707460e00bb1269dc042401246c76324 tags/netty-5.0.0.Alpha1~103', 'commitMessage': 'Additional fix for potential race condition which occurs when a user cancels a connection attempt\n\n- Fixes #1986\n', 'commitDateTime': '2013-11-18 17:00:23', 'authoredDateTime': '2013-11-18 17:00:23', 'commitGitStats': [{'filePath': 'transport/src/main/java/io/netty/channel/nio/AbstractNioChannel.java', 'insertions': 18, 'deletions': 8, 'lines': 26}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AbstractNioChannel.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.finishConnect()', 'TOT': 2, 'UPD': 1, 'INS': 0, 'MOV': 1, 'DEL': 0}, {'spoonMethodName': 'io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.connect(java.net.SocketAddress,java.net.SocketAddress,io.netty.channel.ChannelPromise)', 'TOT': 4, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 2}, {'spoonMethodName': 'io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.fulfillConnectPromise(io.netty.channel.ChannelPromise,boolean)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': 'fd776274c91e338af1364b75fae48547c8214c45', 'commitGHEventType': 'referenced', 'commitUser': 'trustin', 'commitParents': ['54d3c99469b923b409ce1a52b6d1181984470b11'], 'nameRev': 'fd776274c91e338af1364b75fae48547c8214c45 tags/netty-4.0.13.Final~17', 'commitMessage': 'Additional fix for potential race condition which occurs when a user cancels a connection attempt\n\n- Fixes #1986\n', 'commitDateTime': '2013-11-18 17:00:43', 'authoredDateTime': '2013-11-18 17:00:23', 'commitGitStats': [{'filePath': 'transport/src/main/java/io/netty/channel/nio/AbstractNioChannel.java', 'insertions': 18, 'deletions': 8, 'lines': 26}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AbstractNioChannel.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.finishConnect()', 'TOT': 2, 'UPD': 1, 'INS': 0, 'MOV': 1, 'DEL': 0}, {'spoonMethodName': 'io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.connect(java.net.SocketAddress,java.net.SocketAddress,io.netty.channel.ChannelPromise)', 'TOT': 4, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 2}, {'spoonMethodName': 'io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.fulfillConnectPromise(io.netty.channel.ChannelPromise,boolean)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/netty/netty/issues/1986,0.0002777777777777778,['defect'],IllegalStateException in the selector loop when a connection attempt is cancelled.,2.0,"['io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.finishConnect()', 'io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.fulfillConnectPromise(io.netty.channel.ChannelPromise,boolean)', 'io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe.connect(java.net.SocketAddress,java.net.SocketAddress,io.netty.channel.ChannelPromise)']","['54d3c99469b923b409ce1a52b6d1181984470b11', 'd0e928db707460e00bb1269dc042401246c76324']",,['transport/src/main/java/io/netty/channel/nio'],20.0,9.0,29.0,1.0,2.0,3.0,8.0,2.0,2.0,2.0,1.0,0.0,0.0,0.0,2.0,0.0,0.0,netty
38365,2013-09-08 23:49:42,huntc,"We've received a customer report in respect of Play 2.1.2, .3, .4 and Netty 3.6.3 where an internal Play upstream handler handles an exception and closes the channel, and there is SSL handling code also in the midst of handshaking. This then causes a deadlock.

The following mail thread yields the details including the stack trace surrounding the deadlock:

https://groups.google.com/d/msg/play-framework/-4DUxhxNa_g/5wbmPZdqar0J

We have also tried Netty 3.6.6 and see the problem after suspecting the following issues having been resolved since 3.6.3:

https://github.com/netty/netty/issues/1181
https://github.com/netty/netty/issues/1310

```
Found one Java-level deadlock:
=============================
""play-internal-execution-context-1"":
  waiting to lock monitor 0x00007fd80dab2968 (object 0x00000007dcd19300, a java.lang.Object),
  which is held by ""New I/O worker #19""
""New I/O worker #19"":
  waiting to lock monitor 0x00007fd80da852e8 (object 0x00000007dcd194c0, a java.lang.Object),
  which is held by ""play-internal-execution-context-1""

Java stack information for the threads listed above:
===================================================
""play-internal-execution-context-1"":
at org.jboss.netty.channel.socket.nio.AbstractNioWorker.cleanUpWriteBuffer(AbstractNioWorker.java:374)
- waiting to lock <0x00000007dcd19300> (a java.lang.Object)
at org.jboss.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:349)
at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)
at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:779)
at org.jboss.netty.handler.ssl.SslHandler.closeOutboundAndChannel(SslHandler.java:1458)
at org.jboss.netty.handler.ssl.SslHandler.handleDownstream(SslHandler.java:591)
at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:54)
at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
at org.jboss.netty.channel.SimpleChannelHandler.closeRequested(SimpleChannelHandler.java:334)
at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:260)
at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
at org.jboss.netty.channel.Channels.close(Channels.java:812)
at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
at play.core.server.netty.PlayDefaultUpstreamHandler.exceptionCaught(PlayDefaultUpstreamHandler.scala:37)
at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
at org.jboss.netty.channel.SimpleChannelHandler.exceptionCaught(SimpleChannelHandler.java:156)
at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:130)
at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
at org.jboss.netty.channel.SimpleChannelUpstreamHandler.exceptionCaught(SimpleChannelUpstreamHandler.java:153)
at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
at org.jboss.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:536)
at org.jboss.netty.handler.ssl.SslHandler.handshake(SslHandler.java:468)
- locked <0x00000007dcd194c0> (a java.lang.Object)
at org.jboss.netty.handler.ssl.SslHandler.handleRenegotiation(SslHandler.java:1338)
- locked <0x00000007dcd194c0> (a java.lang.Object)
at org.jboss.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1238)
- locked <0x00000007dcd194c0> (a java.lang.Object)
at org.jboss.netty.handler.ssl.SslHandler.closeOutboundAndChannel(SslHandler.java:1434)
at org.jboss.netty.handler.ssl.SslHandler.handleDownstream(SslHandler.java:591)
at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:54)
at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
at org.jboss.netty.channel.SimpleChannelHandler.closeRequested(SimpleChannelHandler.java:334)
at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:260)
at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
at org.jboss.netty.channel.Channels.close(Channels.java:812)
at play.core.server.netty.PlayDefaultUpstreamHandler$$anonfun$play$core$server$netty$PlayDefaultUpstreamHandler$$handle$1$6$$anonfun$apply$9.apply(PlayDefaultUpstreamHandler.scala:219)
at play.core.server.netty.PlayDefaultUpstreamHandler$$anonfun$play$core$server$netty$PlayDefaultUpstreamHandler$$handle$1$6$$anonfun$apply$9.apply(PlayDefaultUpstreamHandler.scala:215)
at play.api.libs.concurrent.PlayPromise$$anonfun$extend1$1.apply(Promise.scala:113)
at play.api.libs.concurrent.PlayPromise$$anonfun$extend1$1.apply(Promise.scala:113)
at play.api.libs.concurrent.PlayPromise$$anonfun$extend$1$$anonfun$apply$1.apply(Promise.scala:104)
at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:722)
""New I/O worker #19"":
at org.jboss.netty.handler.ssl.SslHandler.wrapNonAppData(SslHandler.java:1127)
- waiting to lock <0x00000007dcd194c0> (a java.lang.Object)
at org.jboss.netty.handler.ssl.SslHandler.closeOutboundAndChannel(SslHandler.java:1445)
at org.jboss.netty.handler.ssl.SslHandler.handleDownstream(SslHandler.java:591)
at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:54)
at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
at org.jboss.netty.channel.SimpleChannelHandler.closeRequested(SimpleChannelHandler.java:334)
at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:260)
at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
at org.jboss.netty.channel.Channels.close(Channels.java:812)
at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:41)
at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:427)
at org.jboss.netty.channel.DefaultChannelFuture.notifyListeners(DefaultChannelFuture.java:413)
at org.jboss.netty.channel.DefaultChannelFuture.setSuccess(DefaultChannelFuture.java:362)
at org.jboss.netty.channel.socket.nio.AbstractNioWorker.write0(AbstractNioWorker.java:217)
- locked <0x00000007dcd19300> (a java.lang.Object)
at org.jboss.netty.channel.socket.nio.AbstractNioWorker.writeFromTaskLoop(AbstractNioWorker.java:150)
at org.jboss.netty.channel.socket.nio.AbstractNioChannel$WriteTask.run(AbstractNioChannel.java:335)
at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)
at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)
at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:722)

Found 1 deadlock.
```
",2013-10-18 06:17:42,"[{'commitHash': 'd3c3b1dc899f68e0c3bea5fa964a748e8e16db4e', 'commitGHEventType': 'referenced', 'commitUser': 'trustin', 'commitParents': ['9278b29ea486dc0a985b5b2521cc16769432aa63'], 'nameRev': 'd3c3b1dc899f68e0c3bea5fa964a748e8e16db4e tags/netty-3.8.0.Final~13', 'commitMessage': 'Fix a dead lock which occurs when multiple threads attempt to close the same SSL connection\n\n- Fixes #1817\n', 'commitDateTime': '2013-10-17 23:41:56', 'authoredDateTime': '2013-10-17 23:41:56', 'commitGitStats': [{'filePath': 'src/main/java/org/jboss/netty/handler/ssl/SslHandler.java', 'insertions': 24, 'deletions': 6, 'lines': 30}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'SslHandler.java', 'spoonMethods': [{'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler', 'TOT': 10, 'UPD': 4, 'INS': 6, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.handleDownstream(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelEvent)', 'TOT': 5, 'UPD': 2, 'INS': 2, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.closeOutboundAndChannel(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent)', 'TOT': 6, 'UPD': 2, 'INS': 3, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.channelDisconnected(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent)', 'TOT': 3, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 1}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '597f40e0d92dd7c1c5f542bb9ff3ab30a78e4114', 'commitGHEventType': 'referenced', 'commitUser': 'trustin', 'commitParents': ['d3c3b1dc899f68e0c3bea5fa964a748e8e16db4e'], 'nameRev': '597f40e0d92dd7c1c5f542bb9ff3ab30a78e4114 tags/netty-3.8.0.Final~12', 'commitMessage': 'Better fix for #1817 - do not close immediately on concurrent closeOutboundAndChannel()\n', 'commitDateTime': '2013-10-18 00:03:13', 'authoredDateTime': '2013-10-18 00:03:13', 'commitGitStats': [{'filePath': 'src/main/java/org/jboss/netty/handler/ssl/SslHandler.java', 'insertions': 8, 'deletions': 1, 'lines': 9}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'SslHandler.java', 'spoonMethods': [{'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.channelClosed(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.channelClosed(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent).6', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.channelConnected(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent).5', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.channelClosed(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent).6.run()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.closeOutboundAndChannel(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent)', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/netty/netty/issues/1817,39.000277777777775,['defect'],Deadlock for SSL connections when closing during an exception handler,2.0,"['org.jboss.netty.handler.ssl.SslHandler.closeOutboundAndChannel(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent)', 'org.jboss.netty.handler.ssl.SslHandler.channelDisconnected(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent)', 'org.jboss.netty.handler.ssl.SslHandler.channelClosed(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent)', 'org.jboss.netty.handler.ssl.SslHandler.handleDownstream(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelEvent)', 'org.jboss.netty.handler.ssl.SslHandler', 'org.jboss.netty.handler.ssl.SslHandler.channelClosed(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent).6', 'org.jboss.netty.handler.ssl.SslHandler.channelConnected(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent).5', 'org.jboss.netty.handler.ssl.SslHandler.channelClosed(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent).6.run()']","['d3c3b1dc899f68e0c3bea5fa964a748e8e16db4e', '597f40e0d92dd7c1c5f542bb9ff3ab30a78e4114']",,['src/main/java/org/jboss/netty/handler/ssl'],32.0,7.0,39.0,1.0,12.0,8.0,30.0,2.0,13.0,3.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,netty
38382,2013-08-14 14:35:50,normanmaurer,"During benchmarking I noticed that an exception is thrown as soon as we write a DuplicatedByteBuf as part of a gathering write operation.

Not sure yet why:

```
java.lang.IllegalArgumentException
    at java.nio.Buffer.position(Buffer.java:236)
    at sun.nio.ch.IOUtil.write(IOUtil.java:159)
    at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:499)
    at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:249)
    at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:680)
    at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:256)
    at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:649)
    at io.netty.channel.DefaultChannelPipeline$HeadHandler.flush(DefaultChannelPipeline.java:1038)
    at io.netty.channel.DefaultChannelHandlerContext.invokeFlush(DefaultChannelHandlerContext.java:676)
    at io.netty.channel.DefaultChannelHandlerContext.flush(DefaultChannelHandlerContext.java:657)
    at io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:115)
    at io.netty.channel.DefaultChannelHandlerContext.invokeFlush(DefaultChannelHandlerContext.java:676)
    at io.netty.channel.DefaultChannelHandlerContext.writeAndFlush(DefaultChannelHandlerContext.java:693)
    at io.netty.channel.DefaultChannelHandlerContext.writeAndFlush(DefaultChannelHandlerContext.java:716)
    at hello.HelloServerHandler.writeResponse(HelloServerHandler.java:75)
    at hello.HelloServerHandler.channelRead0(HelloServerHandler.java:47)
    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:98)
    at io.netty.channel.DefaultChannelHandlerContext.invokeChannelRead(DefaultChannelHandlerContext.java:334)
    at io.netty.channel.DefaultChannelHandlerContext.fireChannelRead(DefaultChannelHandlerContext.java:320)
    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:173)
    at io.netty.channel.DefaultChannelHandlerContext.invokeChannelRead(DefaultChannelHandlerContext.java:334)
    at io.netty.channel.DefaultChannelHandlerContext.fireChannelRead(DefaultChannelHandlerContext.java:320)
    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:785)
    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:100)
    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:479)
    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:447)
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:341)
    at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:101)
    at java.lang.Thread.run(Thread.java:724)
```
",2013-08-20 05:29:17,"[{'commitHash': '20894bc99e28191cc3926ae95a23f5c7d3969a33', 'commitGHEventType': 'closed', 'commitUser': 'trustin', 'commitParents': ['a383988cdb57e56e5211fdf592545fe67331ff6e'], 'nameRev': '20894bc99e28191cc3926ae95a23f5c7d3969a33 tags/netty-4.0.8.Final~18', 'commitMessage': ""Fix a bug in internalNioBuffer() implementations of derived buffers\n\n- A user can create multiple duplicates of a buffer and access their internal NIO buffers. (e.g. write multiple duplicates to multiple channels assigned to different event loop.)  Because the derived buffers' internalNioBuffer() simply delegates the call to the original buffer, all derived buffers and the original buffer's internalNioBuffer() will return the same buffer, which will lead to a race condition.\n- Fixes #1739\n"", 'commitDateTime': '2013-08-20 14:28:50', 'authoredDateTime': '2013-08-20 14:28:50', 'commitGitStats': [{'filePath': 'buffer/src/main/java/io/netty/buffer/DuplicatedByteBuf.java', 'insertions': 1, 'deletions': 1, 'lines': 2}, {'filePath': 'buffer/src/main/java/io/netty/buffer/ReadOnlyByteBuf.java', 'insertions': 1, 'deletions': 1, 'lines': 2}, {'filePath': 'buffer/src/main/java/io/netty/buffer/SlicedByteBuf.java', 'insertions': 1, 'deletions': 1, 'lines': 2}, {'filePath': 'buffer/src/main/java/io/netty/buffer/SwappedByteBuf.java', 'insertions': 1, 'deletions': 2, 'lines': 3}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'DuplicatedByteBuf.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.buffer.DuplicatedByteBuf.internalNioBuffer(int,int)', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 0}]}, {'spoonFilePath': 'ReadOnlyByteBuf.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.buffer.ReadOnlyByteBuf.internalNioBuffer(int,int)', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 0}]}, {'spoonFilePath': 'SlicedByteBuf.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.buffer.SlicedByteBuf.internalNioBuffer(int,int)', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 0}]}, {'spoonFilePath': 'SwappedByteBuf.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.buffer.SwappedByteBuf.internalNioBuffer(int,int)', 'TOT': 3, 'UPD': 1, 'INS': 1, 'MOV': 1, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/netty/netty/issues/1739,5.000277777777778,['defect'],java.lang.IllegalArgumentException when gathering write a DuplicatedByteBuf,1.0,"['io.netty.buffer.SwappedByteBuf.internalNioBuffer(int,int)', 'io.netty.buffer.ReadOnlyByteBuf.internalNioBuffer(int,int)', 'io.netty.buffer.SlicedByteBuf.internalNioBuffer(int,int)', 'io.netty.buffer.DuplicatedByteBuf.internalNioBuffer(int,int)']",['20894bc99e28191cc3926ae95a23f5c7d3969a33'],,['buffer/src/main/java/io/netty/buffer'],4.0,5.0,9.0,4.0,1.0,4.0,9.0,4.0,4.0,0.0,4.0,0.0,0.0,0.0,0.0,0.0,0.0,netty
38390,2013-08-05 11:02:28,trustin,"```
Found one Java-level deadlock:
=============================
""New I/O worker #1"":
  waiting to lock monitor 0x0000000000e2a278 (object 0x00000006fafed930, a java.util.logging.LogManager),
  which is held by ""main""
""main"":
  waiting to lock monitor 0x0000000000e28c78 (object 0x00000006fafee5e0, a java.util.logging.LogManager$LoggerContext),
  which is held by ""New I/O worker #1""

Java stack information for the threads listed above:
===================================================
""New I/O worker #1"":
at java.util.logging.LogManager.drainLoggerRefQueueBounded(LogManager.java:811)
- waiting to lock <0x00000006fafed930> (a java.util.logging.LogManager)
at java.util.logging.LogManager$LoggerContext.addLocalLogger(LogManager.java:511)
- locked <0x00000006fafee5e0> (a java.util.logging.LogManager$LoggerContext)
at java.util.logging.LogManager.addLogger(LogManager.java:848)
at java.util.logging.LogManager.demandLogger(LogManager.java:405)
at java.util.logging.Logger.demandLogger(Logger.java:317)
at java.util.logging.Logger.getLogger(Logger.java:361)
at org.jboss.netty.logging.JdkLoggerFactory.newInstance(JdkLoggerFactory.java:30)
at org.jboss.netty.logging.InternalLoggerFactory.getInstance(InternalLoggerFactory.java:76)
at org.jboss.netty.logging.InternalLoggerFactory.getInstance(InternalLoggerFactory.java:69)
at org.jboss.netty.channel.socket.nio.SelectorUtil.<clinit>(SelectorUtil.java:28)
at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:199)
at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:722)

""main"":
at java.util.logging.LogManager$LoggerContext.findLogger(LogManager.java:489)
- waiting to lock <0x00000006fafee5e0> (a java.util.logging.LogManager$LoggerContext)
at java.util.logging.LogManager.getLogger(LogManager.java:910)
at java.util.logging.LogManager.demandLogger(LogManager.java:400)
at java.util.logging.Logger.demandLogger(Logger.java:317)
at java.util.logging.Logger.getLogger(Logger.java:361)
at org.jboss.netty.logging.JdkLoggerFactory.newInstance(JdkLoggerFactory.java:30)
at org.jboss.netty.logging.InternalLoggerFactory.getInstance(InternalLoggerFactory.java:76)
at org.jboss.netty.logging.InternalLoggerFactory.getInstance(InternalLoggerFactory.java:69)
at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.<clinit>(NioClientSocketPipelineSink.java:36)
at org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory.<init>(NioClientSocketChannelFactory.java:208)
at org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory.<init>(NioClientSocketChannelFactory.java:185)
...
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
at java.lang.Class.newInstance0(Class.java:374)
at java.lang.Class.newInstance(Class.java:327)
at java.util.logging.LogManager$4.run(LogManager.java:685)
at java.security.AccessController.doPrivileged(Native Method)
at java.util.logging.LogManager.loadLoggerHandlers(LogManager.java:678)
at java.util.logging.LogManager.initializeGlobalHandlers(LogManager.java:1249)
- locked <0x00000006fafed930> (a java.util.logging.LogManager)
at java.util.logging.LogManager.access$1300(LogManager.java:150)
at java.util.logging.LogManager$RootLogger.getHandlers(LogManager.java:1332)
at java.util.logging.Logger.log(Logger.java:564)
at java.util.logging.Logger.doLog(Logger.java:586)
at java.util.logging.Logger.log(Logger.java:609)
at java.util.logging.Logger.info(Logger.java:1128)
...
```
",2013-09-05 09:58:01,"[{'commitHash': '657d8122e2b94a00d462cb404cad5df180199438', 'commitGHEventType': 'referenced', 'commitUser': 'trustin', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': '05ffb0bbe5692480243f7e5a0c35a7d89c6b56bf', 'commitGHEventType': 'referenced', 'commitUser': 'trustin', 'commitParents': ['1f6a0407b6e73414c3e4f5cd4a5fe0f0642cbc07'], 'nameRev': '05ffb0bbe5692480243f7e5a0c35a7d89c6b56bf tags/netty-3.7.0.Final~1', 'commitMessage': ""Ensure that SelectorUtil is initialized when AbstractNioSelector is initialized\n\n- Related issue: #1701\n- The dead lock reported in #1701 is basically because SelectorUtil's logger is initialized in a different thread, so I hope this will fix this specific issue.\n"", 'commitDateTime': '2013-09-05 17:38:47', 'authoredDateTime': '2013-08-23 15:41:46', 'commitGitStats': [{'filePath': 'src/main/java/org/jboss/netty/channel/socket/nio/AbstractNioSelector.java', 'insertions': 2, 'deletions': 2, 'lines': 4}, {'filePath': 'src/main/java/org/jboss/netty/channel/socket/nio/SelectorUtil.java', 'insertions': 8, 'deletions': 4, 'lines': 12}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AbstractNioSelector.java', 'spoonMethods': [{'spoonMethodName': 'org.jboss.netty.channel.socket.nio.AbstractNioSelector.rebuildSelector()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.channel.socket.nio.AbstractNioSelector.openSelector(org.jboss.netty.util.ThreadNameDeterminer)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'SelectorUtil.java', 'spoonMethods': [{'spoonMethodName': 'org.jboss.netty.channel.socket.nio.SelectorUtil.open()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/netty/netty/issues/1701,30.00027777777778,['defect'],Intermittent dead lock when java.util.logging uses a LoggingHandler writtin on top of Netty.,1.0,"['org.jboss.netty.channel.socket.nio.SelectorUtil.open()', 'org.jboss.netty.channel.socket.nio.AbstractNioSelector.openSelector(org.jboss.netty.util.ThreadNameDeterminer)', 'org.jboss.netty.channel.socket.nio.AbstractNioSelector.rebuildSelector()']",['05ffb0bbe5692480243f7e5a0c35a7d89c6b56bf'],,['src/main/java/org/jboss/netty/channel/socket/nio'],10.0,6.0,16.0,2.0,2.0,3.0,3.0,0.0,1.0,0.0,2.0,0.0,0.0,0.0,1.0,0.0,0.0,netty
38502,2013-04-25 17:06:09,lhrios,"I believe that I have found a deadlock on Netty 3.6.5.Final. It seems that ""New I/O worker #2"" thread fires an event while it is holding a lock (AbstractNioWorker.java:343 - channel.writeLock). It waits on a MemoryAwareThreadPoolExecutor$Limiter (because the available space has been exceeded) instance but the ""OrderedMemoryAwareThreadPoolExecutor thread #3"" can not notify it because ""New I/O worker #2"" is holding the lock ""OrderedMemoryAwareThreadPoolExecutor thread #3"" requires to continue processing its requests.

Here are the stack traces:

```
""New I/O worker #2"" prio=6 tid=0x16f1f800 nid=0x9d0 in Object.wait() [0x1758f000]
   java.lang.Thread.State: WAITING (on object monitor)
    at java.lang.Object.wait(Native Method)
    at java.lang.Object.wait(Object.java:503)
    at org.jboss.netty.handler.execution.MemoryAwareThreadPoolExecutor$Limiter.increase(MemoryAwareThreadPoolExecutor.java:640)
    - locked <0x080da358> (a org.jboss.netty.handler.execution.MemoryAwareThreadPoolExecutor$Limiter)
    at org.jboss.netty.handler.execution.MemoryAwareThreadPoolExecutor.increaseCounter(MemoryAwareThreadPoolExecutor.java:501)
    at org.jboss.netty.handler.execution.MemoryAwareThreadPoolExecutor.execute(MemoryAwareThreadPoolExecutor.java:435)
    at org.jboss.netty.handler.execution.ExecutionHandler.handleUpstream(ExecutionHandler.java:173)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.oneone.OneToOneDecoder.handleUpstream(OneToOneDecoder.java:60)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.channelUnbound(SimpleChannelUpstreamHandler.java:216)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:95)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireChannelUnbound(Channels.java:432)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:343)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.write0(AbstractNioWorker.java:254)
    - locked <0x08454ce0> (a java.lang.Object)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.writeFromTaskLoop(AbstractNioWorker.java:150)
    at org.jboss.netty.channel.socket.nio.AbstractNioChannel$WriteTask.run(AbstractNioChannel.java:335)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)

   Locked ownable synchronizers:
    - <0x080c36d0> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""OrderedMemoryAwareThreadPoolExecutor thread #3"" prio=6 tid=0x16ef8800 nid=0x914 waiting for monitor entry [0x17f8e000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.cleanUpWriteBuffer(AbstractNioWorker.java:374)
    - waiting to lock <0x08454ce0> (a java.lang.Object)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.writeFromUserCode(AbstractNioWorker.java:127)
    at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:99)
    at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:779)
    at org.jboss.netty.handler.ssl.SslHandler.flushPendingEncryptedWrites(SslHandler.java:1109)
    at org.jboss.netty.handler.ssl.SslHandler.wrap(SslHandler.java:1048)
    at org.jboss.netty.handler.ssl.SslHandler.handleDownstream(SslHandler.java:631)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
    at org.jboss.netty.channel.Channels.write(Channels.java:725)
    at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)
    at org.jboss.netty.handler.codec.oneone.OneToOneStrictEncoder.doEncode(OneToOneStrictEncoder.java:35)
    - locked <0x083e9968> (a org.jboss.netty.channel.socket.nio.NioAcceptedSocketChannel)
    at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
    at org.jboss.netty.handler.codec.compression.JdkZlibEncoder.handleDownstream(JdkZlibEncoder.java:221)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
    at org.jboss.netty.handler.execution.ExecutionHandler.handleDownstream(ExecutionHandler.java:186)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
    [my project's classes...]
    at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
    at org.jboss.netty.channel.Channels.write(Channels.java:704)
    at org.jboss.netty.channel.Channels.write(Channels.java:671)
    at org.jboss.netty.channel.AbstractChannel.write(AbstractChannel.java:248)
    [my project's classes...]
    at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
    at org.jboss.netty.channel.Channels.write(Channels.java:704)
    at org.jboss.netty.channel.Channels.write(Channels.java:671)
    at org.jboss.netty.channel.AbstractChannel.write(AbstractChannel.java:248)
    [my project's classes...]
    - locked <0x083db908> ([my project's classes...])
    [my project's classes...]
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:43)
    at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:67)
    at org.jboss.netty.handler.execution.OrderedMemoryAwareThreadPoolExecutor$ChildExecutor.run(OrderedMemoryAwareThreadPoolExecutor.java:314)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)

   Locked ownable synchronizers:
    - <0x083d62a8> (a java.util.concurrent.ThreadPoolExecutor$Worker)
    - <0x0841d6a8> (a java.util.concurrent.locks.ReentrantLock$FairSync)
    - <0x0841e978> (a java.util.concurrent.locks.ReentrantLock$FairSync)
    - <0x0841ea10> (a java.util.concurrent.locks.ReentrantLock$FairSync)
```
",2013-05-08 05:32:33,"[{'commitHash': '960067c78a82a584dc2529472c80d9e1e5a56fa7', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': ['3371f4693bfcc14dbce31d5cc81224106433b525'], 'nameRev': '960067c78a82a584dc2529472c80d9e1e5a56fa7 tags/netty-3.6.6.Final~8', 'commitMessage': '[#1310] Fix deadlock which can happen if limit of MemoryAwareThreadPoolExecutor was exceed and a close is triggered by a write\n\n* Close must be handled from outside the write lock to fix a possible deadlock\n  which can happen when MemoryAwareThreadPoolExecutor is used and the limit is exceed\n  and a close is triggered while the lock is hold. This is because the close(..)\n  may try to submit a task to handle it via the ExecutorHandler which then deadlocks.\n', 'commitDateTime': '2013-05-06 12:45:13', 'authoredDateTime': '2013-05-06 12:45:13', 'commitGitStats': [{'filePath': 'src/main/java/org/jboss/netty/channel/socket/nio/AbstractNioWorker.java', 'insertions': 8, 'deletions': 1, 'lines': 9}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AbstractNioWorker.java', 'spoonMethods': [{'spoonMethodName': 'org.jboss.netty.channel.socket.nio.AbstractNioWorker.write0(org.jboss.netty.channel.socket.nio.AbstractNioChannel)', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '60695012ab66170083c10607393b70a507b71390', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': ['960067c78a82a584dc2529472c80d9e1e5a56fa7'], 'nameRev': '60695012ab66170083c10607393b70a507b71390 tags/netty-3.6.6.Final~7', 'commitMessage': '[#1310] Fix deadlock which can happen if limit of MemoryAwareThreadPoolExecutor was exceed and an exceptionCaught(..) is triggered by a write\n', 'commitDateTime': '2013-05-06 13:54:25', 'authoredDateTime': '2013-05-06 13:54:25', 'commitGitStats': [{'filePath': 'src/main/java/org/jboss/netty/channel/socket/nio/AbstractNioWorker.java', 'insertions': 22, 'deletions': 4, 'lines': 26}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AbstractNioWorker.java', 'spoonMethods': [{'spoonMethodName': 'org.jboss.netty.channel.socket.nio.AbstractNioWorker.write0(org.jboss.netty.channel.socket.nio.AbstractNioChannel)', 'TOT': 8, 'UPD': 2, 'INS': 4, 'MOV': 1, 'DEL': 1}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/netty/netty/issues/1310,12.000277777777777,['defect'],Deadlock can happen when MemoryAwareThreadPoolExecutor with limit is used,2.0,['org.jboss.netty.channel.socket.nio.AbstractNioWorker.write0(org.jboss.netty.channel.socket.nio.AbstractNioChannel)'],"['960067c78a82a584dc2529472c80d9e1e5a56fa7', '60695012ab66170083c10607393b70a507b71390']",,['src/main/java/org/jboss/netty/channel/socket/nio'],30.0,5.0,35.0,1.0,2.0,1.0,10.0,2.0,5.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,netty
38549,2013-03-20 12:10:16,demarsi,"Hi,

I suspect I found a bug in the SSLHandler class.
One of our integration tests fails systematically; from the stacktraces it appears to be caused by a thread deadlock.

I've tried with several versions, from 3.4 on - it happens with all of them, including 3.6.4.Final-SNAPSHOT build #122 which is the one relative to the stacktraces below; 

To give an idea of the scenario the netty server is serving several https download which are interrupted by the clients before termination.

<pre>
Found one Java-level deadlock:
=============================
""com.spreaker.mi.station.download.HttpDownloader@4a6f19d5"":
  waiting to lock monitor 7f839e82c058 (object 7db0310f0, a java.lang.Object),
  which is held by ""New I/O worker #15""
""New I/O worker #15"":
  waiting to lock monitor 7f839b131e60 (object 7db031130, a java.util.LinkedList),
  which is held by ""com.spreaker.mi.station.download.HttpDownloader@4a6f19d5""

Java stack information for the threads listed above:
===================================================
""com.spreaker.mi.station.download.HttpDownloader@4a6f19d5"":
    at org.jboss.netty.handler.ssl.SslHandler.wrap(SslHandler.java:959)
    - waiting to lock <7db0310f0> (a java.lang.Object)
    - locked <7db031130> (a java.util.LinkedList)
    at org.jboss.netty.handler.ssl.SslHandler.handleDownstream(SslHandler.java:627)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:587)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:780)
    at org.jboss.netty.channel.SimpleChannelDownstreamHandler.writeRequested(SimpleChannelDownstreamHandler.java:108)
    at org.jboss.netty.handler.timeout.WriteTimeoutHandler.writeRequested(WriteTimeoutHandler.java:159)
    at org.jboss.netty.channel.SimpleChannelDownstreamHandler.handleDownstream(SimpleChannelDownstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:587)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:780)
    at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:60)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:587)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:780)
    at org.jboss.netty.channel.SimpleChannelHandler.writeRequested(SimpleChannelHandler.java:292)
    at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:587)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:780)
    at org.jboss.netty.handler.execution.ExecutionHandler.handleDownstream(ExecutionHandler.java:186)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:587)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:780)
    at org.jboss.netty.channel.SimpleChannelHandler.writeRequested(SimpleChannelHandler.java:292)
    at com.spreaker.mi.net.icecast.StreamingServerStatsdHandler.writeRequested(StreamingServerStatsdHandler.java:92)
    at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:587)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:780)
    at org.jboss.netty.channel.SimpleChannelHandler.writeRequested(SimpleChannelHandler.java:292)
    at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:587)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:780)
    at org.jboss.netty.channel.SimpleChannelHandler.writeRequested(SimpleChannelHandler.java:292)
    at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:587)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:780)
    at org.jboss.netty.channel.SimpleChannelHandler.writeRequested(SimpleChannelHandler.java:292)
    at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:587)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:780)
    at org.jboss.netty.channel.SimpleChannelHandler.writeRequested(SimpleChannelHandler.java:292)
    at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:587)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:780)
    at com.spreaker.mi.core.net.http.HttpServerProtocolHandler._writeRequested(HttpServerProtocolHandler.java:152)
    at com.spreaker.mi.core.net.http.HttpServerProtocolHandler.writeRequested(HttpServerProtocolHandler.java:92)
    at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:587)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:780)
    at org.jboss.netty.channel.Channels.write(Channels.java:725)
    at com.spreaker.mi.core.net.DebugChunkedWriteHandler.flush(DebugChunkedWriteHandler.java:271)
    at com.spreaker.mi.core.net.DebugChunkedWriteHandler.resumeTransfer(DebugChunkedWriteHandler.java:85)
    at com.spreaker.mi.core.net.PacketChunkedStreamHandler.onResumeTransfer(PacketChunkedStreamHandler.java:49)
    at com.spreaker.mi.core.net.PacketChunkedStream.addPacket(PacketChunkedStream.java:114)
    at com.spreaker.mi.station.download.HttpDownloader._downloadBytes(HttpDownloader.java:83)
    at com.spreaker.mi.station.download.HttpEngine.run(HttpEngine.java:131)
    at java.lang.Thread.run(Thread.java:680)
""New I/O worker #15"":
    at org.jboss.netty.handler.ssl.SslHandler$6.run(SslHandler.java:1562)
    - waiting to lock <7db031130> (a java.util.LinkedList)
    at org.jboss.netty.channel.socket.ChannelRunnableWrapper.run(ChannelRunnableWrapper.java:40)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:69)
    at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:55)
    at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
    at org.jboss.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
    at org.jboss.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:632)
    at org.jboss.netty.handler.ssl.SslHandler.channelClosed(SslHandler.java:1557)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:88)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)
    at org.jboss.netty.channel.SimpleChannelHandler.channelClosed(SimpleChannelHandler.java:216)
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:106)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)
    at org.jboss.netty.channel.Channels.fireChannelClosed(Channels.java:468)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:351)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.write0(AbstractNioWorker.java:254)
    - locked <7db031868> (a java.lang.Object)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.writeFromUserCode(AbstractNioWorker.java:145)
    at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:99)
    at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:775)
    at org.jboss.netty.channel.SimpleChannelHandler.writeRequested(SimpleChannelHandler.java:292)
    at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:587)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:780)
    at org.jboss.netty.channel.Channels.write(Channels.java:725)
    at org.jboss.netty.channel.Channels.write(Channels.java:686)
    at org.jboss.netty.handler.ssl.SslHandler.wrapNonAppData(SslHandler.java:1144)
    at org.jboss.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1233)
    - locked <7db0310f0> (a java.lang.Object)
    at org.jboss.netty.handler.ssl.SslHandler.decode(SslHandler.java:910)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)
    at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:142)
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
    at java.lang.Thread.run(Thread.java:680)
</pre>


Please let me know if any additional information is required or if this issue might be caused by a misuse of the SSLHandler class on our side.

Thanks,

Andrea.
",2013-04-05 00:13:29,"[{'commitHash': 'a723a1d7dfa6741af7d842629545a274056026cb', 'commitGHEventType': 'referenced', 'commitUser': 'trustin', 'commitParents': ['9d2ceb82c6573baf0e1fa10a71ca07591e83743f'], 'nameRev': 'a723a1d7dfa6741af7d842629545a274056026cb tags/netty-3.6.4.Final~1', 'commitMessage': 'Potential fix for SslHandler dead lock\n\n- Related with: #1181\n', 'commitDateTime': '2013-04-02 12:56:32', 'authoredDateTime': '2013-04-02 12:55:36', 'commitGitStats': [{'filePath': 'src/main/java/org/jboss/netty/handler/ssl/SslHandler.java', 'insertions': 24, 'deletions': 8, 'lines': 32}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'SslHandler.java', 'spoonMethods': [{'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.offerEncryptedWriteRequest(org.jboss.netty.channel.MessageEvent)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.handleDownstream(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelEvent)', 'TOT': 6, 'UPD': 0, 'INS': 2, 'MOV': 3, 'DEL': 1}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.wrap(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.Channel)', 'TOT': 14, 'UPD': 0, 'INS': 4, 'MOV': 8, 'DEL': 2}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler.channelClosed(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent).6.run()', 'TOT': 6, 'UPD': 0, 'INS': 2, 'MOV': 3, 'DEL': 1}, {'spoonMethodName': 'org.jboss.netty.handler.ssl.SslHandler', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/netty/netty/issues/1181,15.000277777777777,['defect'],Deadlock in SSLHandler,1.0,"['org.jboss.netty.handler.ssl.SslHandler.wrap(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.Channel)', 'org.jboss.netty.handler.ssl.SslHandler.channelClosed(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent).6.run()', 'org.jboss.netty.handler.ssl.SslHandler.handleDownstream(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelEvent)', 'org.jboss.netty.handler.ssl.SslHandler.offerEncryptedWriteRequest(org.jboss.netty.channel.MessageEvent)', 'org.jboss.netty.handler.ssl.SslHandler']",['a723a1d7dfa6741af7d842629545a274056026cb'],,['src/main/java/org/jboss/netty/handler/ssl'],24.0,8.0,32.0,1.0,1.0,5.0,28.0,14.0,9.0,4.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,netty
38550,2013-03-19 06:12:41,md-5,"The majority of this discussion will be stemming from https://github.com/ElasticPortalSuite/BungeeCord/issues/206 an issue currently plaguing my software, which may or may not be Netty related, but it happens to basically every user.

Basically after an undetermined amount of time, all existing connections are terminated (via my read timeout handler) and no new connections are made. Unfortunately I cannot reproduce this locally, probably due to the fact I do not run a production server that cycles through 1000+ connections a minute, however given the amount of users reporting this (all Linux, Java 7), it doesn't appear to be constrained to one particular setup.

The biggest breakthrough of information I have so far, is this screenshot from VisualVM before and after the issue. Beforehand, the Netty IO threads are runnable (as is the case even when no connections are present), and after the issue occurs and all channels have been dropped, they appear to enter some sort of wait; no new connections can be made.
You can view the screenshot here: http://d.pr/i/mJb7

Unfortunately the VisualVM snapshot provided was useless, however I have informed my users of YourKit, and some of them have downloaded the free trial, and promise to get me useful CPU and heap dumps as soon as possible.

Thanks for the hard work you put into Netty and I hope we can solve this final blocker for me (and I'm assuming 4.0 can't ship with this)
",2013-03-22 00:10:12,"[{'commitHash': '430fbc7ead5e92016e6c7620f7696d42d08b60be', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': 'bbd9500db725ea565d0886e730ed819348b65710', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': 'e4765b0a6e8c0c5915add354464800416dd6d4ca', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': 'cb023307495aba84d0f24cd355e14713d65c3ede', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': '2aa0bf73dc7b44175622055b891e8eec6f65de46', 'commitGHEventType': 'referenced', 'commitUser': 'trustin', 'commitParents': ['a980638190c055a596145ec2a6963fa8cd30126a'], 'nameRev': '2aa0bf73dc7b44175622055b891e8eec6f65de46 tags/netty-4.0.0.CR1~27', 'commitMessage': 'Add a unit test that reproduces the dead lock described in #1175\n\n- The offending test case is annotated with `@Ignore`\n- Also fixed a bug where channel initialization failure swallows the original cause of initialization failure\n', 'commitDateTime': '2013-03-21 18:43:03', 'authoredDateTime': '2013-03-21 18:43:03', 'commitGitStats': [{'filePath': 'transport/src/main/java/io/netty/bootstrap/Bootstrap.java', 'insertions': 5, 'deletions': 1, 'lines': 6}, {'filePath': 'transport/src/test/java/io/netty/bootstrap/BootstrapTest.java', 'insertions': 90, 'deletions': 0, 'lines': 90}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'Bootstrap.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.bootstrap.Bootstrap.doBind(java.net.SocketAddress)', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 0}]}, {'spoonFilePath': 'BootstrapTest.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.bootstrap.BootstrapTest', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': 'c08919d0a0a8b391598fc1be49956a5ae5c7b375', 'commitGHEventType': 'referenced', 'commitUser': 'trustin', 'commitParents': ['bd8d53eaedbe7c6251df76716a6cdae79ef6f8b9'], 'nameRev': 'c08919d0a0a8b391598fc1be49956a5ae5c7b375 tags/netty-4.0.0.CR1~24', 'commitMessage': ""Fix the dead lock described in #1175\n\n- Similar to @normanmaurer's fix in that this commit also makes Bootstrap.init(Channel) asynchronous, but it is simpler and less invasive.\n- Also made sure a connection attempt failure in the local transport does not trigger an exceptionCaught event\n"", 'commitDateTime': '2013-03-21 19:19:14', 'authoredDateTime': '2013-03-21 19:19:14', 'commitGitStats': [{'filePath': 'transport/src/main/java/io/netty/bootstrap/Bootstrap.java', 'insertions': 48, 'deletions': 32, 'lines': 80}, {'filePath': 'transport/src/main/java/io/netty/channel/AbstractChannel.java', 'insertions': 1, 'deletions': 6, 'lines': 7}, {'filePath': 'transport/src/main/java/io/netty/channel/local/LocalChannel.java', 'insertions': 1, 'deletions': 3, 'lines': 4}, {'filePath': 'transport/src/test/java/io/netty/bootstrap/BootstrapTest.java', 'insertions': 47, 'deletions': 3, 'lines': 50}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'Bootstrap.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.bootstrap.Bootstrap.init(io.netty.channel.Channel)', 'TOT': 15, 'UPD': 1, 'INS': 4, 'MOV': 8, 'DEL': 2}, {'spoonMethodName': 'io.netty.bootstrap.Bootstrap.doConnect(java.net.SocketAddress,java.net.SocketAddress)', 'TOT': 7, 'UPD': 1, 'INS': 2, 'MOV': 3, 'DEL': 1}, {'spoonMethodName': 'io.netty.bootstrap.Bootstrap.doBind(java.net.SocketAddress)', 'TOT': 5, 'UPD': 0, 'INS': 2, 'MOV': 2, 'DEL': 1}]}, {'spoonFilePath': 'AbstractChannel.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.channel.AbstractChannel.AbstractUnsafe.register0(io.netty.channel.ChannelPromise)', 'TOT': 3, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 1}]}, {'spoonFilePath': 'LocalChannel.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.channel.local.LocalChannel.LocalUnsafe.connect(java.net.SocketAddress,java.net.SocketAddress,io.netty.channel.ChannelPromise)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}]}, {'spoonFilePath': 'BootstrapTest.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.bootstrap.BootstrapTest.testInitializationDeadLock()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.bootstrap.BootstrapTest.testConnectDeadLock()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '4fa7e854933c8b388a5ad52f600661ff9b38910f', 'commitGHEventType': 'referenced', 'commitUser': 'trustin', 'commitParents': ['b6dd5938ab748f4d3482f58106bd8018f5cf4bb4'], 'nameRev': '4fa7e854933c8b388a5ad52f600661ff9b38910f tags/netty-4.0.0.CR1~22', 'commitMessage': 'Make sure ChannelFuture of Bootstrap.bind() and connect() notify with the right cause when registration fails\n\n- Related: #1175\n', 'commitDateTime': '2013-03-21 19:41:38', 'authoredDateTime': '2013-03-21 19:40:52', 'commitGitStats': [{'filePath': 'transport/src/main/java/io/netty/bootstrap/Bootstrap.java', 'insertions': 50, 'deletions': 9, 'lines': 59}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'Bootstrap.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.bootstrap.Bootstrap.doConnect(java.net.SocketAddress,java.net.SocketAddress)', 'TOT': 25, 'UPD': 3, 'INS': 1, 'MOV': 18, 'DEL': 3}, {'spoonMethodName': 'io.netty.bootstrap.Bootstrap.doBind(java.net.SocketAddress)', 'TOT': 11, 'UPD': 0, 'INS': 2, 'MOV': 8, 'DEL': 1}, {'spoonMethodName': 'io.netty.bootstrap.Bootstrap.doBind0(io.netty.channel.ChannelFuture,io.netty.channel.Channel,java.net.SocketAddress,io.netty.channel.ChannelPromise)', 'TOT': 2, 'UPD': 0, 'INS': 2, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.bootstrap.Bootstrap.doConnect0(io.netty.channel.ChannelFuture,io.netty.channel.Channel,java.net.SocketAddress,java.net.SocketAddress,io.netty.channel.ChannelPromise)', 'TOT': 3, 'UPD': 0, 'INS': 3, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '8fb80e91796e8567aca5e2b8244f853f8631217b', 'commitGHEventType': 'referenced', 'commitUser': 'trustin', 'commitParents': ['9b208028ef1300b1a49eb1315ba58e58c2b87152'], 'nameRev': '8fb80e91796e8567aca5e2b8244f853f8631217b tags/netty-4.0.0.CR1~18', 'commitMessage': 'Fix a dead lock in ServerBootstrap as described in #1175\n\n- Reduce code duplication between bootstrap implementations\n', 'commitDateTime': '2013-03-21 21:34:13', 'authoredDateTime': '2013-03-21 21:34:13', 'commitGitStats': [{'filePath': 'transport/src/main/java/io/netty/bootstrap/AbstractBootstrap.java', 'insertions': 71, 'deletions': 1, 'lines': 72}, {'filePath': 'transport/src/main/java/io/netty/bootstrap/Bootstrap.java', 'insertions': 44, 'deletions': 95, 'lines': 139}, {'filePath': 'transport/src/main/java/io/netty/bootstrap/ServerBootstrap.java', 'insertions': 4, 'deletions': 21, 'lines': 25}, {'filePath': 'transport/src/main/java/io/netty/channel/aio/AbstractAioChannel.java', 'insertions': 1, 'deletions': 4, 'lines': 5}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'AbstractBootstrap.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.bootstrap.AbstractBootstrap.initAndRegister()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.bootstrap.AbstractBootstrap.init(io.netty.channel.Channel)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.bootstrap.AbstractBootstrap.doBind0(io.netty.channel.ChannelFuture,io.netty.channel.Channel,java.net.SocketAddress,io.netty.channel.ChannelPromise)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.bootstrap.AbstractBootstrap.doBind(java.net.SocketAddress)', 'TOT': 8, 'UPD': 0, 'INS': 7, 'MOV': 1, 'DEL': 0}]}, {'spoonFilePath': 'Bootstrap.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.bootstrap.Bootstrap.doConnect(java.net.SocketAddress,java.net.SocketAddress)', 'TOT': 13, 'UPD': 11, 'INS': 0, 'MOV': 2, 'DEL': 0}, {'spoonMethodName': 'io.netty.bootstrap.Bootstrap.doConnect0(io.netty.channel.ChannelFuture,io.netty.channel.Channel,java.net.SocketAddress,java.net.SocketAddress,io.netty.channel.ChannelPromise)', 'TOT': 9, 'UPD': 3, 'INS': 4, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'io.netty.bootstrap.Bootstrap.init(io.netty.channel.Channel)', 'TOT': 14, 'UPD': 1, 'INS': 2, 'MOV': 6, 'DEL': 5}, {'spoonMethodName': 'io.netty.bootstrap.Bootstrap.doConnect(java.net.SocketAddress,java.net.SocketAddress).2', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.bootstrap.Bootstrap.doConnect(java.net.SocketAddress,java.net.SocketAddress).2.operationComplete(io.netty.channel.ChannelFuture)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.netty.bootstrap.Bootstrap.doBind(java.net.SocketAddress)', 'TOT': 3, 'UPD': 0, 'INS': 0, 'MOV': 2, 'DEL': 1}, {'spoonMethodName': 'io.netty.bootstrap.Bootstrap.doBind0(io.netty.channel.ChannelFuture,io.netty.channel.Channel,java.net.SocketAddress,io.netty.channel.ChannelPromise)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'io.netty.bootstrap.Bootstrap.doBind(java.net.SocketAddress).1.operationComplete(io.netty.channel.ChannelFuture)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 0}]}, {'spoonFilePath': 'ServerBootstrap.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.bootstrap.ServerBootstrap.doBind(java.net.SocketAddress)', 'TOT': 6, 'UPD': 4, 'INS': 0, 'MOV': 2, 'DEL': 0}]}, {'spoonFilePath': 'AbstractAioChannel.java', 'spoonMethods': [{'spoonMethodName': 'io.netty.channel.aio.AbstractAioChannel.DefaultAioUnsafe.connectSuccess()', 'TOT': 3, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 2}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/netty/netty/issues/1175,2.000277777777778,['defect'],Many Bootsraps attempting to register to the event loop will cause deadlocks,4.0,"['io.netty.bootstrap.Bootstrap.doBind(java.net.SocketAddress)', 'io.netty.bootstrap.Bootstrap.doConnect(java.net.SocketAddress,java.net.SocketAddress)', 'io.netty.bootstrap.Bootstrap.init(io.netty.channel.Channel)', 'io.netty.bootstrap.Bootstrap.doConnect0(io.netty.channel.ChannelFuture,io.netty.channel.Channel,java.net.SocketAddress,java.net.SocketAddress,io.netty.channel.ChannelPromise)', 'io.netty.bootstrap.Bootstrap.doBind0(io.netty.channel.ChannelFuture,io.netty.channel.Channel,java.net.SocketAddress,io.netty.channel.ChannelPromise)', 'io.netty.bootstrap.Bootstrap.doConnect(java.net.SocketAddress,java.net.SocketAddress).2', 'io.netty.bootstrap.AbstractBootstrap.initAndRegister()', 'io.netty.channel.local.LocalChannel.LocalUnsafe.connect(java.net.SocketAddress,java.net.SocketAddress,io.netty.channel.ChannelPromise)', 'io.netty.bootstrap.AbstractBootstrap.init(io.netty.channel.Channel)', 'io.netty.bootstrap.ServerBootstrap.doBind(java.net.SocketAddress)', 'io.netty.bootstrap.Bootstrap.doConnect(java.net.SocketAddress,java.net.SocketAddress).2.operationComplete(io.netty.channel.ChannelFuture)', 'io.netty.bootstrap.AbstractBootstrap.doBind0(io.netty.channel.ChannelFuture,io.netty.channel.Channel,java.net.SocketAddress,io.netty.channel.ChannelPromise)', 'io.netty.bootstrap.AbstractBootstrap.doBind(java.net.SocketAddress)', 'io.netty.bootstrap.Bootstrap.doBind(java.net.SocketAddress).1.operationComplete(io.netty.channel.ChannelFuture)', 'io.netty.channel.aio.AbstractAioChannel.DefaultAioUnsafe.connectSuccess()', 'io.netty.channel.AbstractChannel.AbstractUnsafe.register0(io.netty.channel.ChannelPromise)']","['2aa0bf73dc7b44175622055b891e8eec6f65de46', 'c08919d0a0a8b391598fc1be49956a5ae5c7b375', '4fa7e854933c8b388a5ad52f600661ff9b38910f', '8fb80e91796e8567aca5e2b8244f853f8631217b']",,"['transport/src/main/java/io/netty/bootstrap', 'transport/src/main/java/io/netty/channel/local', 'transport/src/main/java/io/netty/channel/aio', 'transport/src/main/java/io/netty/channel']",225.0,172.0,397.0,6.0,26.0,16.0,137.0,57.0,34.0,20.0,6.0,0.0,0.0,0.0,4.0,0.0,0.0,netty
38732,2012-07-05 21:14:58,beidi,"When both the client and server disconnect the socket channel at the same time, AbstractNioWorker.close(AbstractNioChannel<?> channel, ChannelFuture future) can be called by 2 different Netty Daemon threads at the same time.
The race condition happens when:
Thread 1 runs into ""if (channel.setClosed())"" where ""channel"" is a NioSocketChannel object. It context-switches to Thread 2 right before ""return super.setClosed();"" inside of ""NioSocketChannel.setClosed()"".
Since Thread 1 has ""connected"" true and ""channel.setClosed()"" false, so fireChannelDisconnected() will not be triggered.
And Thread 2 has ""connected"" false and ""channel.setClosed()"" true, so fireChannelDisconnected() will not be triggered either.

I made a fix in ""NioSocketChannel.setClosed()"" by doing:
    @Override
    protected boolean setClosed() {
        if (super.setClosed()) {
            state = ST_CLOSED;
            return true;
        } else {
            return false;
        }
    }
The issue seems fixed.
",2012-07-06 05:40:21,"[{'commitHash': '6421bd388521945329b2bffa15a4636a59ca2b49', 'commitGHEventType': 'referenced', 'commitUser': 'normanmaurer', 'commitParents': ['991ca860ea2a0b1681caa8795548b83f2dbc63e2'], 'nameRev': '6421bd388521945329b2bffa15a4636a59ca2b49 tags/netty-3.5.3.Final~25', 'commitMessage': 'Fix a race which could lead to have channelDisconnected event not fired. See #440\n', 'commitDateTime': '2012-07-06 07:39:41', 'authoredDateTime': '2012-07-06 07:39:41', 'commitGitStats': [{'filePath': 'src/main/java/org/jboss/netty/channel/socket/nio/NioSocketChannel.java', 'insertions': 5, 'deletions': 2, 'lines': 7}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'NioSocketChannel.java', 'spoonMethods': [{'spoonMethodName': 'org.jboss.netty.channel.socket.nio.NioSocketChannel.setClosed()', 'TOT': 5, 'UPD': 0, 'INS': 2, 'MOV': 2, 'DEL': 1}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/netty/netty/issues/440,0.0002777777777777778,['defect'],"SimpleChannelHandler.channelDisconnected() not called sometimes due to a race condition in AbstractNioWorker.close(AbstractNioChannel<?> channel, ChannelFuture future)",1.0,['org.jboss.netty.channel.socket.nio.NioSocketChannel.setClosed()'],['6421bd388521945329b2bffa15a4636a59ca2b49'],,['src/main/java/org/jboss/netty/channel/socket/nio'],5.0,2.0,7.0,1.0,0.0,1.0,5.0,2.0,2.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,netty
40104,2018-01-16 02:29:37,koo-taejin,"#### Description

If addDeadlockedThread and getDeadlockedThreadIdSet operation performed concurrently on deadlockedThreadIdSet in DeadlockThreadRegistry class.

Concurrency issue will occur.
```java
public class DeadlockThreadRegistry implements DeadlockThreadLocator {

    private final Set<Long> deadlockedThreadIdSet = new HashSet<Long>();

    boolean addDeadlockedThread(long threadId);

    public Set<Long> getDeadlockedThreadIdSet();
}
```


related issue #2941 

",2018-02-23 10:28:28,"[{'commitHash': '120fdc8e116e2f5c32266f8d381b90ec450b8af0', 'commitGHEventType': 'referenced', 'commitUser': 'koo-taejin', 'commitParents': ['b2da64b7eb42cb5fc248e314ca32f3cc2ba8b05e'], 'nameRev': '120fdc8e116e2f5c32266f8d381b90ec450b8af0 tags/1.7.2~103', 'commitMessage': '[#3712] Fix concurrency issue in DeadlockThreadRegistry\n\nFix concurrency issue in DeadlockThreadRegistry\n', 'commitDateTime': '2018-01-16 14:37:58', 'authoredDateTime': '2018-01-16 14:12:35', 'commitGitStats': [{'filePath': 'profiler/src/main/java/com/navercorp/pinpoint/profiler/monitor/DeadlockThreadRegistry.java', 'insertions': 15, 'deletions': 6, 'lines': 21}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'DeadlockThreadRegistry.java', 'spoonMethods': [{'spoonMethodName': 'com.navercorp.pinpoint.profiler.monitor.DeadlockThreadRegistry', 'TOT': 4, 'UPD': 3, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.navercorp.pinpoint.profiler.monitor.DeadlockThreadRegistry.getDeadlockedThreadIdSet()', 'TOT': 8, 'UPD': 3, 'INS': 2, 'MOV': 1, 'DEL': 2}, {'spoonMethodName': 'com.navercorp.pinpoint.profiler.monitor.DeadlockThreadRegistry.addDeadlockedThread(long)', 'TOT': 6, 'UPD': 1, 'INS': 2, 'MOV': 2, 'DEL': 1}, {'spoonMethodName': 'com.navercorp.pinpoint.profiler.monitor.DeadlockThreadRegistry.toString()', 'TOT': 3, 'UPD': 1, 'INS': 1, 'MOV': 1, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/naver/pinpoint/issues/3712,38.000277777777775,"['bug', 'module:agent']",Fix concurrency issue in DeadlockThreadRegistry,1.0,"['com.navercorp.pinpoint.profiler.monitor.DeadlockThreadRegistry.getDeadlockedThreadIdSet()', 'com.navercorp.pinpoint.profiler.monitor.DeadlockThreadRegistry.addDeadlockedThread(long)', 'com.navercorp.pinpoint.profiler.monitor.DeadlockThreadRegistry.toString()', 'com.navercorp.pinpoint.profiler.monitor.DeadlockThreadRegistry']",['120fdc8e116e2f5c32266f8d381b90ec450b8af0'],,['profiler/src/main/java/com/navercorp/pinpoint/profiler/monitor'],15.0,6.0,21.0,1.0,8.0,4.0,21.0,4.0,6.0,3.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,pinpoint
40115,2017-09-19 05:31:09,koo-taejin,"#### Description
Current previous value is stored as a String, if input value is [null, null, ""value"", ""value""], it is impossible to distinguish between the initial value and the previous value.
To save the previous value, change it to AtomicReference.
",2017-09-19 07:45:48,"[{'commitHash': 'd8bcbd158f790e4ef9757dad899148ccfc4f5bb6', 'commitGHEventType': 'referenced', 'commitUser': 'koo-taejin', 'commitParents': ['8df51a3a3e872a90e1733d72433b07d04cdc37ea'], 'nameRev': 'd8bcbd158f790e4ef9757dad899148ccfc4f5bb6 tags/1.7.0-RC1~125', 'commitMessage': '[#3369] Changes object type of the previous value in StringRepeatCountEncodingStrategy\n\n Current previous value is stored as a String, if input value is [null, null, ""value"", ""value""], it is impossible to distinguish between the initial value and the previous value.\n To save the previous value, change it to AtomicReference.\n', 'commitDateTime': '2017-09-19 16:23:33', 'authoredDateTime': '2017-09-19 15:28:45', 'commitGitStats': [{'filePath': 'commons-server/src/main/java/com/navercorp/pinpoint/common/server/bo/codec/strategy/impl/StringRepeatCountEncodingStrategy.java', 'insertions': 21, 'deletions': 7, 'lines': 28}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'StringRepeatCountEncodingStrategy.java', 'spoonMethods': [{'spoonMethodName': 'com.navercorp.pinpoint.common.server.bo.codec.strategy.impl.StringRepeatCountEncodingStrategy.encodeValues(com.navercorp.pinpoint.common.buffer.Buffer,java.util.List)', 'TOT': 20, 'UPD': 7, 'INS': 6, 'MOV': 5, 'DEL': 2}, {'spoonMethodName': 'com.navercorp.pinpoint.common.server.bo.codec.strategy.impl.StringRepeatCountEncodingStrategy.StringReference', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/naver/pinpoint/issues/3369,0.0002777777777777778,"['bug', 'module:collector']",Changes object type of the previous value in StringRepeatCountEncodingStrategy,1.0,"['com.navercorp.pinpoint.common.server.bo.codec.strategy.impl.StringRepeatCountEncodingStrategy.StringReference', 'com.navercorp.pinpoint.common.server.bo.codec.strategy.impl.StringRepeatCountEncodingStrategy.encodeValues(com.navercorp.pinpoint.common.buffer.Buffer,java.util.List)']",['d8bcbd158f790e4ef9757dad899148ccfc4f5bb6'],,['commons-server/src/main/java/com/navercorp/pinpoint/common/server/bo'],21.0,7.0,28.0,1.0,7.0,2.0,21.0,5.0,7.0,2.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,pinpoint
40125,2017-06-02 06:04:27,Xylus,"#### Description
Multiple threads may collect server meta data and every time they do, they notify their collection to the listeners.
When notifying the listeners, a `ServerMetaData` object is created based on everything collected up to that point and this is passed on to the listeners.
However, these 2 operations are not atomic so a thread that have created the object first could publish it later, resulting in an out of order operation.",2017-06-08 05:14:08,"[{'commitHash': 'f349c7a0df72ead709a9a1ed0586ceb0a99536bd', 'commitGHEventType': 'referenced', 'commitUser': 'koo-taejin', 'commitParents': ['a4df90a241fd72efcd50942c383dae6d522cce73'], 'nameRev': 'f349c7a0df72ead709a9a1ed0586ceb0a99536bd tags/1.6.2~1', 'commitMessage': '[#2993] Server metadata listener notification has a race condition\n\nuse synchronized to prevent race condition\n', 'commitDateTime': '2017-06-02 15:44:43', 'authoredDateTime': '2017-06-02 15:08:20', 'commitGitStats': [{'filePath': 'profiler/src/main/java/com/navercorp/pinpoint/profiler/context/DefaultServerMetaDataHolder.java', 'insertions': 5, 'deletions': 5, 'lines': 10}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'DefaultServerMetaDataHolder.java', 'spoonMethods': [{'spoonMethodName': 'com.navercorp.pinpoint.profiler.context.DefaultServerMetaDataHolder.notifyListeners()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/naver/pinpoint/issues/2993,5.000277777777778,['bug'],Server metadata listener notification has a race condition,1.0,['com.navercorp.pinpoint.profiler.context.DefaultServerMetaDataHolder.notifyListeners()'],['f349c7a0df72ead709a9a1ed0586ceb0a99536bd'],,['profiler/src/main/java/com/navercorp/pinpoint/profiler/context'],5.0,5.0,10.0,1.0,0.0,1.0,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,pinpoint
40150,2016-11-17 10:03:58,emeroad,"InterceptorScopePool Field of DefaultProfilerPluginContext is not thread safe.

fixed version : 1.6.0-RC2",2016-11-22 02:45:30,"[{'commitHash': 'ca24c0592cd8973520229e864994f0a1fc80c134', 'commitGHEventType': 'referenced', 'commitUser': 'emeroad', 'commitParents': ['6465a0f3fddb959c9a7f029d36210db75d6c9ca6'], 'nameRev': 'ca24c0592cd8973520229e864994f0a1fc80c134 tags/1.6.0-RC2~5', 'commitMessage': '#2259 Fix potential thread safe problem in InterceptorScope\n', 'commitDateTime': '2016-11-17 20:31:10', 'authoredDateTime': '2016-11-17 19:36:51', 'commitGitStats': [{'filePath': 'profiler/src/main/java/com/navercorp/pinpoint/profiler/context/scope/ConcurrentPool.java', 'insertions': 68, 'deletions': 0, 'lines': 68}, {'filePath': 'profiler/src/main/java/com/navercorp/pinpoint/profiler/context/scope/InterceptorScopeFactory.java', 'insertions': 31, 'deletions': 0, 'lines': 31}, {'filePath': 'profiler/src/main/java/com/navercorp/pinpoint/profiler/context/scope/Pool.java', 'insertions': 29, 'deletions': 0, 'lines': 29}, {'filePath': 'profiler/src/main/java/com/navercorp/pinpoint/profiler/context/scope/PoolObjectFactory.java', 'insertions': 25, 'deletions': 0, 'lines': 25}, {'filePath': 'profiler/src/main/java/com/navercorp/pinpoint/profiler/plugin/DefaultProfilerPluginContext.java', 'insertions': 7, 'deletions': 11, 'lines': 18}, {'filePath': 'profiler/src/main/java/com/navercorp/pinpoint/profiler/plugin/xml/interceptor/AnnotatedInterceptorInjector.java', 'insertions': 1, 'deletions': 1, 'lines': 2}, {'filePath': 'profiler/src/main/java/com/navercorp/pinpoint/profiler/util/NameValueList.java', 'insertions': 2, 'deletions': 1, 'lines': 3}, {'filePath': 'profiler/src/test/java/com/navercorp/pinpoint/profiler/context/scope/ConcurrentPoolTest.java', 'insertions': 47, 'deletions': 0, 'lines': 47}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'ConcurrentPool.java', 'spoonMethods': [{'spoonMethodName': 'com.navercorp.pinpoint.profiler.context.scope.ConcurrentPool', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'InterceptorScopeFactory.java', 'spoonMethods': [{'spoonMethodName': 'com.navercorp.pinpoint.profiler.context.scope.InterceptorScopeFactory', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'Pool.java', 'spoonMethods': [{'spoonMethodName': 'com.navercorp.pinpoint.profiler.context.scope', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'PoolObjectFactory.java', 'spoonMethods': [{'spoonMethodName': 'com.navercorp.pinpoint.profiler.context.scope', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'DefaultProfilerPluginContext.java', 'spoonMethods': [{'spoonMethodName': 'com.navercorp.pinpoint.profiler.plugin.DefaultProfilerPluginContext', 'TOT': 3, 'UPD': 3, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'com.navercorp.pinpoint.profiler.plugin.DefaultProfilerPluginContext.getInterceptorScope(java.lang.String)', 'TOT': 7, 'UPD': 1, 'INS': 1, 'MOV': 2, 'DEL': 3}]}, {'spoonFilePath': 'AnnotatedInterceptorInjector.java', 'spoonMethods': []}, {'spoonFilePath': 'NameValueList.java', 'spoonMethods': [{'spoonMethodName': 'com.navercorp.pinpoint.profiler.util.NameValueList', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'ConcurrentPoolTest.java', 'spoonMethods': [{'spoonMethodName': 'com.navercorp.pinpoint.profiler.context.scope.ConcurrentPoolTest', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/naver/pinpoint/issues/2259,4.000277777777778,['bug'],Fix potential thread safe problem in InterceptorScope,1.0,"['com.navercorp.pinpoint.profiler.context.scope', 'com.navercorp.pinpoint.profiler.util.NameValueList', 'com.navercorp.pinpoint.profiler.context.scope.InterceptorScopeFactory', 'com.navercorp.pinpoint.profiler.plugin.DefaultProfilerPluginContext.getInterceptorScope(java.lang.String)', 'com.navercorp.pinpoint.profiler.plugin.DefaultProfilerPluginContext', 'com.navercorp.pinpoint.profiler.context.scope.ConcurrentPool']",['ca24c0592cd8973520229e864994f0a1fc80c134'],,"['profiler/src/main/java/com/navercorp/pinpoint/profiler/util', 'profiler/src/main/java/com/navercorp/pinpoint/profiler/plugin', 'profiler/src/main/java/com/navercorp/pinpoint/profiler/context/scope', 'profiler/src/main/java/com/navercorp/pinpoint/profiler/plugin/xml']",163.0,13.0,176.0,7.0,5.0,6.0,15.0,2.0,5.0,3.0,6.0,0.0,0.0,0.0,0.0,0.0,0.0,pinpoint
41137,2019-07-24 08:45:56,HeChuanXUPT,"**Describe the bug**
healthcheck api report error

**To Reproduce**
Steps to reproduce the behavior:
1. access http://127.0.0.1:8080/admin/v2/brokers/health
2. see errors
```
 --- An unexpected error occurred in the server ---

Message: org.apache.pulsar.client.api.PulsarClientException$IncompatibleSchemaException: Trying to subscribe with incompatible schema

Stacktrace:

java.util.concurrent.CompletionException: org.apache.pulsar.client.api.PulsarClientException$IncompatibleSchemaException: Trying to subscribe with incompatible schema
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:700)
	at java.util.concurrent.CompletableFuture$UniRun.tryFire(CompletableFuture.java:687)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.pulsar.client.impl.ClientCnx.handleError(ClientCnx.java:597)
	at org.apache.pulsar.common.protocol.PulsarDecoder.channelRead(PulsarDecoder.java:154)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:323)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:297)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1434)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:965)
	at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:799)
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:433)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:330)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:909)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.pulsar.client.api.PulsarClientException$IncompatibleSchemaException: Trying to subscribe with incompatible schema
	at org.apache.pulsar.client.impl.ClientCnx.getPulsarClientException(ClientCnx.java:905)
	... 20 more
```

**Expected behavior**

**Screenshots**


**Desktop (please complete the following information):**
 - OS: [Linux]

**Additional context**
version: apache-pulsar-2.4.0
access other broker/cluster api not encounter errors, like get http://127.0.0.1:8080/admin/v2/clusters successfully
use bin/pulsar-client to produce or consume data is ok",2019-11-06 06:05:18,"[{'commitHash': '86ccfe877e483cce8c8481f5933dfb906f92ab42', 'commitGHEventType': 'referenced', 'commitUser': 'sijie', 'commitParents': ['e0b36f9cc95aebb7767d4864e46026dfb7e89e08'], 'nameRev': '86ccfe877e483cce8c8481f5933dfb906f92ab42 tags/v2.5.0-candidate-1~119', 'commitMessage': 'Add handle exception KeeperException.BadVersionException (#5563)\n\nFixes https://github.com/apache/pulsar/issues/4790\r\n\r\n\r\nMaster Issue: https://github.com/apache/pulsar/issues/4790\r\n\r\n### Motivation\r\nCurrently, when the brokerDeleteInactiveTopicsEnabled policy is enabled, the topic pulsar/standalone/127.0.0.1:8080/healthcheck will be deleted periodically, when creating producer and consumer for the second time https://github.com/apache/pulsar/blob/master/pulsar-broker/src/main/java/org/apache/pulsar/broker/admin/impl/BrokersBase.java#L272, because both producer and consumer call putSchema in a short period of time, resulting in race conditions, Throwing exceptions of schema badVersion, only one of them could succeed. therefore, when badVersion exception occurs, call putSchema should be performed again. \r\n### Modifications\r\n\r\n* Add operation to handle exception KeeperException.BadVersionException\r\n\r\n### Verifying this change\r\n\r\n```\r\n./bin/pulsar standalone -nss -a 127.0.0.1\r\ncurl -v http://127.0.0.1:8080/admin/v2/brokers/health\r\n\r\n# Waiting for more than 60 seconds, waiting for the topic pulsar/standalone/127.0.0.1:8080/healthcheck to be deleted\r\n\r\ncurl -v http://127.0.0.1:8080/admin/v2/brokers/health\r\n```', 'commitDateTime': '2019-11-06 14:05:17', 'authoredDateTime': '2019-11-06 14:05:17', 'commitGitStats': [{'filePath': 'pulsar-broker/src/main/java/org/apache/pulsar/broker/service/schema/BookkeeperSchemaStorage.java', 'insertions': 2, 'deletions': 1, 'lines': 3}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'BookkeeperSchemaStorage.java', 'spoonMethods': [{'spoonMethodName': 'org.apache.pulsar.broker.service.schema.BookkeeperSchemaStorage.putSchema(java.lang.String,byte[],byte[])', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/apache/pulsar/issues/4790,104.00027777777778,"['component/schemaregistry', 'triage/week-30', 'type/bug']",healthcheck report error: Trying to subscribe with incompatible schema,1.0,"['org.apache.pulsar.broker.service.schema.BookkeeperSchemaStorage.putSchema(java.lang.String,byte[],byte[])']",['86ccfe877e483cce8c8481f5933dfb906f92ab42'],,['pulsar-broker/src/main/java/org/apache/pulsar/broker/service/schema'],2.0,1.0,3.0,1.0,0.0,1.0,2.0,1.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,pulsar
41782,2015-11-05 03:36:03,RobertoArtiles,"Hi there,

I'm trying to locate a dead-lock which is happening somewhere inside Realm.
By logging I found that I get an infinite ANR in `onCreate()` of the MainActivity on the line `Realm.getDefaultInstance()`. Another place where it hangs is a worker thread on the line `Realm.close()`.

Here is how the worker thread body looks like (without the domain details):

``` java
Realm realm = null;
try {
    realm = Realm.getDefaultInstance();
    RealmResults<A> results = realm.where(A.class).findAll();
    realm.beginTransaction();
    for (int i = results.size() - 1; i >= 0; i--) {
        final A item = results.get(i);
        String value = item.getSomeStringValue();
        callSomeMethod(value); // this method throws an exception not related to realm when i == 1
    }
    realm.commitTransaction();
} finally {
    if (realm != null) {
        realm.close(); // hangs on this line based on the logs
    }
}
```

I was able to narrow down where exactly Realm.close() hangs. It is the line `synchronized (BaseRealm.class)`:

``` java
protected void lastLocalInstanceClosed() {
        // validatedRealmFiles must not modified while other thread is executing createAndValidate()
        synchronized (BaseRealm.class) {
            validatedRealmFiles.remove(configuration.getPath());
        }
        realmsCache.get().remove(configuration);
    }
}
```

Here is the sequence of actions accordingly to logs:
1. Application.onCreate() called and starts the worker thread
2. Worker thread is going through the items
3. MainActivity.onCreate() called and it hangs on Realm.getDefaultInstance()
4. Exception thrown in the worker thread, and it hangs on realm.close()
5. Infinite ANR in MainActivity

It's very hard to locate the problem because it's not 100% reproducible and time-dependent. If I remove transaction, or there is no exception then the dead-lock doesn't happen (could be just due to timing). Also, I have another running worker thread in background which uses realm. But it successfully finishes and doesn't hang on `realm.close()`. 

I tried to reproduce it in a small project, but it didn't work out.

I hope this description could lead you to some ideas what could go wrong, or what I can check to locate the exact problem.
",2015-11-11 08:32:11,"[{'commitHash': 'f11f9c5d65f11101dc18b6b8226e909007e99b82', 'commitGHEventType': 'referenced', 'commitUser': 'beeender', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': '141e6977fa7b206309c555ce522546850169b4c8', 'commitGHEventType': 'referenced', 'commitUser': 'beeender', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': '492771a7bd6d703c197fce1e03b071378a8c6d40', 'commitGHEventType': 'referenced', 'commitUser': 'beeender', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': '23282de28b37b8f30e5b687d308c8d978652e20b', 'commitGHEventType': 'referenced', 'commitUser': 'beeender', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': '6d6c2d3d64e1caf6a8cbb056b36709416c5adda9', 'commitGHEventType': 'referenced', 'commitUser': 'beeender', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': '312d4c9b423ca3abbe46b68dab2ef4fce5dc79c3', 'commitGHEventType': 'referenced', 'commitUser': 'beeender', 'commitParents': ['393079a9c462dfb3d188ec001e729411c27cd05a'], 'nameRev': '312d4c9b423ca3abbe46b68dab2ef4fce5dc79c3 tags/v0.84.2~6^2', 'commitMessage': 'Add counter for Realm instance for all threads.\n\nTo close #1728\nThe only usage of the Realm instance counter for now is to clean up\nvalidatedRealmFiles.\nWe do need to refactor the whole counter/cache part.\n', 'commitDateTime': '2015-11-11 16:00:50', 'authoredDateTime': '2015-11-05 22:12:30', 'commitGitStats': [{'filePath': 'changelog.txt', 'insertions': 2, 'deletions': 1, 'lines': 3}, {'filePath': 'realm/realm-library/src/androidTest/java/io/realm/RealmTest.java', 'insertions': 58, 'deletions': 0, 'lines': 58}, {'filePath': 'realm/realm-library/src/main/java/io/realm/Realm.java', 'insertions': 49, 'deletions': 1, 'lines': 50}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'RealmTest.java', 'spoonMethods': [{'spoonMethodName': 'io.realm.RealmTest.testOpenRealmWhileTransactionInAnotherThread()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'Realm.java', 'spoonMethods': [{'spoonMethodName': 'io.realm.Realm', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.realm.Realm.acquireRealmFileReference(io.realm.RealmConfiguration)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.realm.Realm.releaseRealmFileReference(io.realm.RealmConfiguration)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.realm.Realm.createAndValidate(io.realm.RealmConfiguration,java.lang.Boolean,boolean)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'io.realm.Realm.lastLocalInstanceClosed()', 'TOT': 2, 'UPD': 0, 'INS': 1, 'MOV': 1, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '6761cc892765c8099162e4dfe09921fc18eca705', 'commitGHEventType': 'referenced', 'commitUser': 'beeender', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}]",https://github.com/realm/realm-java/issues/1728,6.000277777777778,['T-Bug'],Trying to catch a dead lock,1.0,"['io.realm.Realm.acquireRealmFileReference(io.realm.RealmConfiguration)', 'io.realm.Realm.releaseRealmFileReference(io.realm.RealmConfiguration)', 'io.realm.Realm', 'io.realm.Realm.lastLocalInstanceClosed()', 'io.realm.Realm.createAndValidate(io.realm.RealmConfiguration,java.lang.Boolean,boolean)']",['312d4c9b423ca3abbe46b68dab2ef4fce5dc79c3'],,['realm/realm-library/src/main/java/io/realm'],49.0,1.0,50.0,1.0,0.0,5.0,6.0,1.0,5.0,0.0,1.0,0.0,0.0,0.0,6.0,0.0,0.0,realm-java
41831,2015-07-29 03:24:53,beeender,"Issue first reported on SO http://stackoverflow.com/questions/31583359/realm-and-runtime-getruntime-exit0 .

When calling `Runtime.getRuntime().exit(0);` with Realm opened before, finalizer won't stop running. Lock is waiting forever, process cannot exist.

Below code to reproduce this error:

```
Realm realm = Realm.getInstance(this);
realm.close();
Runtime.getRuntime().exit(0);
```

And in the logcat, infinity logs comes as below:

```
D/dalvikvm( 5070): GC_EXPLICIT freed 0K, 6% free 6379K/6716K, paused 2ms+3ms, total 19ms
D/dalvikvm( 5070): GC_EXPLICIT freed 0K, 6% free 6379K/6716K, paused 2ms+2ms, total 19ms
D/dalvikvm( 5070): GC_EXPLICIT freed 0K, 6% free 6379K/6716K, paused 2ms+2ms, total 18ms
```
",2015-08-04 11:48:08,"[{'commitHash': 'eb51da9dfb1f663099f10902dc7f3772c8547881', 'commitGHEventType': 'referenced', 'commitUser': 'beeender', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': '9e35ebbe618363fb4005464dde5695f432986b31', 'commitGHEventType': 'referenced', 'commitUser': 'beeender', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': 'd8f3452518fc190757019c74cbe7c7ac9375acff', 'commitGHEventType': 'referenced', 'commitUser': 'beeender', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': '20f62322b978e4ff5145044b0a11644fbaeb7b2e', 'commitGHEventType': 'referenced', 'commitUser': 'beeender', 'commitParents': [], 'nameRev': '', 'commitMessage': '', 'commitDateTime': '', 'authoredDateTime': '', 'commitGitStats': [], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': ''}, {'commitHash': 'b97d3243556d796dc81622283ed0518ec1eb456a', 'commitGHEventType': 'referenced', 'commitUser': 'beeender', 'commitParents': ['1dce294563a16db694329162d33635acc462efcb'], 'nameRev': 'b97d3243556d796dc81622283ed0518ec1eb456a tags/v0.82.1~2^2~2', 'commitMessage': 'Init testing framework for multi processes\n\n1. Create a service in a standalone process.\n2. Make helpers to play with the service for testing.\n3. Watch dog support for the testing.\n4. Test case to reproduce #1328, Process is deadly locked when exit(0)\n    called.\n', 'commitDateTime': '2015-08-04 13:20:27', 'authoredDateTime': '2015-07-15 15:14:15', 'commitGitStats': [{'filePath': 'realm/src/androidTest/AndroidManifest.xml', 'insertions': 17, 'deletions': 0, 'lines': 17}, {'filePath': 'realm/src/androidTest/java/io/realm/RealmInterprocessTest.java', 'insertions': 290, 'deletions': 0, 'lines': 290}, {'filePath': 'realm/src/androidTest/java/io/realm/services/RemoteProcessService.java', 'insertions': 129, 'deletions': 0, 'lines': 129}], 'commitSpoonAstDiffStats': [], 'spoonStatsSkippedReason': 'tooManyChanges'}, {'commitHash': '346ac3dc5ecac6a72b73c165da522d058d3f3bb8', 'commitGHEventType': 'closed', 'commitUser': 'beeender', 'commitParents': ['b97d3243556d796dc81622283ed0518ec1eb456a'], 'nameRev': '346ac3dc5ecac6a72b73c165da522d058d3f3bb8 tags/v0.82.1~2^2~1', 'commitMessage': 'Remove broken shutdown hook.\n\nTo fix #1328\n\nWhen System.exit(0) called, shutdown hook will be run. There is no\nguarantee in this case GC will collect everything. So a dead lock will\nhappen here.\n\nRemove this part of code since the core side can ensure the integrity of\ndb file when other resource is not released.\n', 'commitDateTime': '2015-08-04 13:20:27', 'authoredDateTime': '2015-07-29 18:24:28', 'commitGitStats': [{'filePath': 'realm/src/main/java/io/realm/internal/RealmCore.java', 'insertions': 0, 'deletions': 30, 'lines': 30}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'RealmCore.java', 'spoonMethods': [{'spoonMethodName': 'io.realm.internal.RealmCore.gcGuaranteed()', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'io.realm.internal.RealmCore.gcOnExit()', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'io.realm.internal.RealmCore.init()', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'io.realm.internal.RealmCore.loadLibrary()', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/realm/realm-java/issues/1328,6.000277777777778,['T-Bug'],Dead lock happens when calling  Runtime.getRuntime().exit(0);,1.0,"['io.realm.internal.RealmCore.loadLibrary()', 'io.realm.internal.RealmCore.gcOnExit()', 'io.realm.internal.RealmCore.init()', 'io.realm.internal.RealmCore.gcGuaranteed()']","['b97d3243556d796dc81622283ed0518ec1eb456a', '346ac3dc5ecac6a72b73c165da522d058d3f3bb8']",,['realm/src/main/java/io/realm/internal'],0.0,30.0,30.0,1.0,0.0,4.0,4.0,0.0,0.0,4.0,1.0,0.0,0.0,0.0,4.0,0.0,0.0,realm-java
42331,2018-08-15 14:18:55,nicktming,"<!--

-->

### Expected behavior
**RLock.lock() does not return anything, is that mean a thread can always get the lock ?**

> After i saw the source code of lock funcation, it calls lockInterruptibly and catch the InterruptedException, it means that when a thread is interrupted by other thread, it will stop waiting and return from this lock function.

**so I tried a example in such way:**
> There are two threads in my example, thread-1 gets the lock, and thread-2 is waiting for the lock, then main thread interrupts thread-2, and in this way, thread-2 actually does not get the lock, but thread-2 still stops waiting and return from lock() funcation and continues to do the business which should be locked.

**should i use the interruption status of current thread to check if the current thread has got the lock?**


Thank you very much!

### Actual behavior
```
import org.redisson.Redisson;
import org.redisson.api.RLock;
import org.redisson.config.Config;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;

public class TestDistributedRedisLock {

    private static CountDownLatch finish = new CountDownLatch(2);
    private static final String KEY = ""testlock"";
    private static Config config;
    private static Redisson redisson;
    static {
        config = new Config();
        config.useSingleServer().setAddress(""127.0.0.1:6379"");
        redisson = (Redisson)Redisson.create(config);
    }

    public static void main(String[] args) {
        Thread thread_1 = new LockWithoutBoolean(""thread-1"");
        Thread thread_2 = new LockWithoutBoolean(""thread-2"");
        thread_1.start();
        try {
            TimeUnit.SECONDS.sleep(10); // let thread-1 get the lock
            thread_2.start();
            TimeUnit.SECONDS.sleep(10); // let thread_2 waiting for the lock
            thread_2.interrupt(); // interrupte the thread-2
            finish.await();
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        } finally {
            redisson.shutdown();
        }
    }

    static class LockWithoutBoolean extends Thread {
        private String name;

        public LockWithoutBoolean(String name) {
            super(name);
        }

        public void run() {
            RLock lock = redisson.getLock(KEY);
            lock.lock(10, TimeUnit.MINUTES);
            System.out.println(Thread.currentThread().getName() + "" gets lock. and interrupt: "" + Thread.currentThread().isInterrupted());
            try {
                TimeUnit.MINUTES.sleep(1);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
            } finally {
                try {
                    lock.unlock();
                } finally {
                    finish.countDown();
                }
            }
            System.out.println(Thread.currentThread().getName() + "" ends."");
        }
    }
}
```

### Steps to reproduce or test case
> **output:**
```
thread-1 gets lock. and interrupt: false
thread-2 gets lock. and interrupt: true
Exception in thread ""thread-2"" java.lang.IllegalMonitorStateException: attempt to unlock lock, not locked by current thread by node id: 965ec21d-aa56-4057-a0e9-a29acacf35b1 thread-id: 21
	at org.redisson.RedissonLock.unlock(RedissonLock.java:353)
	at com.example.TestDistributedRedisLock$LockWithoutBoolean.run(TestDistributedRedisLock.java:55)
thread-1 ends.
```
### Redis version
```4.0.6```
### Redisson version
```
<dependency>
            <groupId>org.redisson</groupId>
            <artifactId>redisson</artifactId>
            <version>2.7.0</version>
        </dependency>
```
### Redisson configuration
```
no other configuration
```",2019-04-02 12:46:41,"[{'commitHash': 'c67c333fa1f3e083914c2d55bd86cca91b831774', 'commitGHEventType': 'referenced', 'commitUser': 'mrniko', 'commitParents': ['e4fe2105df7002e7fba68f66a2b582d8b62de11c'], 'nameRev': 'c67c333fa1f3e083914c2d55bd86cca91b831774 tags/redisson-3.10.6~8', 'commitMessage': 'Fixed - RLock.lock method can be interrupted with Thread.interrupt method. #1593\n', 'commitDateTime': '2019-04-02 15:46:33', 'authoredDateTime': '2019-04-02 15:46:33', 'commitGitStats': [{'filePath': 'redisson/src/main/java/org/redisson/RedissonLock.java', 'insertions': 22, 'deletions': 7, 'lines': 29}, {'filePath': 'redisson/src/test/java/org/redisson/RedissonLockTest.java', 'insertions': 46, 'deletions': 0, 'lines': 46}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'RedissonLock.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.RedissonLock.lockInterruptibly(long,java.util.concurrent.TimeUnit)', 'TOT': 8, 'UPD': 1, 'INS': 1, 'MOV': 5, 'DEL': 1}, {'spoonMethodName': 'org.redisson.RedissonLock.lockInterruptibly()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.RedissonLock.lock()', 'TOT': 3, 'UPD': 1, 'INS': 1, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'org.redisson.RedissonLock.lock(long,java.util.concurrent.TimeUnit)', 'TOT': 3, 'UPD': 1, 'INS': 1, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'org.redisson.RedissonLock.lock(long,java.util.concurrent.TimeUnit,boolean)', 'TOT': 2, 'UPD': 0, 'INS': 2, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'RedissonLockTest.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.RedissonLockTest.LockWithoutBoolean', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.RedissonLockTest.testLockUninterruptibly()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/redisson/redisson/issues/1593,229.00027777777777,['bug'],problem about RLock.lock()  interruptedException ?,1.0,"['org.redisson.RedissonLock.lock(long,java.util.concurrent.TimeUnit)', 'org.redisson.RedissonLock.lockInterruptibly(long,java.util.concurrent.TimeUnit)', 'org.redisson.RedissonLock.lock(long,java.util.concurrent.TimeUnit,boolean)', 'org.redisson.RedissonLock.lock()', 'org.redisson.RedissonLock.lockInterruptibly()']",['c67c333fa1f3e083914c2d55bd86cca91b831774'],,['redisson/src/main/java/org/redisson'],22.0,7.0,29.0,1.0,4.0,5.0,17.0,5.0,5.0,3.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,redisson
42342,2018-07-12 14:44:01,orchapod,"Hello!

﻿I've found something that I believe is a bug in Redisson's [RReadWriteLock](https://static.javadoc.io/org.redisson/redisson/3.4.3/org/redisson/api/RReadWriteLock.html) implementation where multiple ReadLocks seemed to become or at least behaved like WriteLock when it tried to lock on a lockpoint that another WriteLock has already acquired a lock, then released. Because of that, ReadLocks are not sharing the lockpoint like it should and are taking turns in locking the lockpoint. Theoretically, this will have a performance impact on applications that expected Redisson to have quicker locking mechanism based on inclusive locking mechanism but under the hood, it is not inclusive and those applications will spend some time to wait for locks to complete.

I have tested this with Java's [ReentrantReadWriteLock](https://docs.oracle.com/javase/7/docs/api/index.html?java/util/concurrent/locks/ReentrantLock.html) and it has worked exactly what I'd expected it to be.

**Note:** I have only tested this on a single Redis server. I did not test this on clustered Redis servers.

### Expected behavior

1. Writer Thread locks
2. Reader Thread 1 fails to lock and waits
3. Reader Thread 2 fails to lock and waits
4. Reader Thread 3 fails to lock and waits
5. Writer Thread unlocks
6. Read Thread 1 locks
7. Read Thread 2 locks
8. Read Thread 3 locks
9. Read Thread 1 unlocks
10. Read Thread 2 unlocks
11. Read Thread 3 unlocks

This behavior matches Java's [ReentrantReadWriteLock](https://docs.oracle.com/javase/7/docs/api/index.html?java/util/concurrent/locks/ReentrantLock.html) behavior exactly.

### Actual behavior
1. Writer Thread locks
2. Reader Thread 1 fails to lock and waits
3. Reader Thread 2 fails to lock and waits
4. Reader Thread 3 fails to lock and waits
5. Writer Thread unlocks
6. Read Thread 1 locks
7. Read Thread 2 fails to lock and waits
8. Read Thread 3 fails to lock and waits
9. Read Thread 1 unlocks
10. Read Thread 2 locks
11. Read Thread 3 fails to lock and waits
12. Read Thread 2 unlocks
13. Read Thread 3 locks
14. Read Thread 3 unlocks

This behavior does not match Java's [ReentrantReadWriteLock](https://docs.oracle.com/javase/7/docs/api/index.html?java/util/concurrent/locks/ReentrantLock.html) behavior exactly.

### Steps to reproduce or test case
I have written a test code for you to review, download, and test. 

The link to my code is here:
https://github.com/orchapod/redisson-lock-test

The test code includes two test suites that runs a control test using Java's [ReentrantReadWriteLock](https://docs.oracle.com/javase/7/docs/api/index.html?java/util/concurrent/locks/ReentrantLock.html) and an experiment test with Redisson's [RReadWriteLock](https://static.javadoc.io/org.redisson/redisson/3.4.3/org/redisson/api/RReadWriteLock.html).

Each test suites has two test cases where one of the test does multiple ReadLocks locks on the lockpoint before WriteLock locks on it, and another one tests multiple ReadLocks locking on the lockpoint after WriteLock has locked on it. The failure on the latter test on Redisson's [RReadWriteLock](https://static.javadoc.io/org.redisson/redisson/3.4.3/org/redisson/api/RReadWriteLock.html) is what has prompted me to open this issue ticket. Java's [ReentrantReadWriteLock](https://docs.oracle.com/javase/7/docs/api/index.html?java/util/concurrent/locks/ReentrantLock.html) passed that test.

Overview:
* RedissonLockTest
  * `testReadLockBeforeWriteLock` - PASS
  * `testReadLockAfterWriteLock` - **FAIL**
* ReentrantLockTest
  * `testReadLockBeforeWriteLock` - PASS
  * `testReadLockAfterWriteLock` - PASS

To run the test, run `mvn test` with Maven to test the code.

### Redis version
4.0.9

### Redisson version
3.7.3

### Redisson configuration
Single server with default configuration created by Redisson's Config class.",2018-09-21 15:17:09,"[{'commitHash': 'f3e3b56ea9d314ffbf4651c7e131843f1e4b2b7c', 'commitGHEventType': 'referenced', 'commitUser': 'mrniko', 'commitParents': ['862b689f9ca8c4ce1cea1a4954fecd258f3c78b9'], 'nameRev': 'f3e3b56ea9d314ffbf4651c7e131843f1e4b2b7c tags/redisson-2.13.2~33', 'commitMessage': ""Fixed - read locks aren't acquire lock at the same moment when write released. #1542\n"", 'commitDateTime': '2018-09-21 18:20:29', 'authoredDateTime': '2018-09-21 18:20:29', 'commitGitStats': [{'filePath': 'redisson/src/main/java/org/redisson/RedissonWriteLock.java', 'insertions': 2, 'deletions': 2, 'lines': 4}, {'filePath': 'redisson/src/main/java/org/redisson/pubsub/LockPubSub.java', 'insertions': 11, 'deletions': 0, 'lines': 11}, {'filePath': 'redisson/src/test/java/org/redisson/RedissonReadWriteLockTest.java', 'insertions': 26, 'deletions': 0, 'lines': 26}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'RedissonWriteLock.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.RedissonWriteLock.unlockInnerAsync(long)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.RedissonWriteLock.forceUnlockAsync()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'LockPubSub.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.pubsub.LockPubSub.onMessage(org.redisson.RedissonLockEntry,java.lang.Long)', 'TOT': 4, 'UPD': 0, 'INS': 1, 'MOV': 2, 'DEL': 1}, {'spoonMethodName': 'org.redisson.pubsub.LockPubSub', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'RedissonReadWriteLockTest.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.RedissonReadWriteLockTest.testWriteReadReentrancy()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.RedissonReadWriteLockTest.testWriteRead()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/redisson/redisson/issues/1542,71.00027777777778,['bug'],Multiple ReadLocks are not sharing locks properly after WriteLock releases,1.0,"['org.redisson.RedissonWriteLock.forceUnlockAsync()', 'org.redisson.pubsub.LockPubSub', 'org.redisson.RedissonWriteLock.unlockInnerAsync(long)', 'org.redisson.pubsub.LockPubSub.onMessage(org.redisson.RedissonLockEntry,java.lang.Long)']",['f3e3b56ea9d314ffbf4651c7e131843f1e4b2b7c'],,"['redisson/src/main/java/org/redisson', 'redisson/src/main/java/org/redisson/pubsub']",13.0,2.0,15.0,2.0,2.0,4.0,7.0,2.0,2.0,1.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,redisson
42362,2018-05-28 14:22:44,shivendra-kumar,"Hi,
While running a load test on jmeter of 200 users with ramp up time of 10 seconds and number of puts per user equal to 10, threads are getting  stuck. In this test we are using RTransaction and doing load testing on a single key by putting random values for a single key. 
here is the stack trace:

	at sun.misc.Unsafe.park(Native Method)
          
            	-  waiting on java.util.concurrent.CountDownLatch$Sync@b55ea542
          
            	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
          
            	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:847)
          
            	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1008)
          
            	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1315)
          
            	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:242)
          
            	at org.redisson.command.CommandAsyncService.get(CommandAsyncService.java:152)
          
            	at org.redisson.RedissonObject.get(RedissonObject.java:74)
          
            	at org.redisson.RedissonMap.fastPut(RedissonMap.java:922)
          
            	at com.nucleus.controller.Controller1.sameKeyPut(Controller1.java:237)

### Steps to reproduce or test case

	public void sameKeyPut(){
		
		RTransaction transaction = redisson.createTransaction(options);
		map=transaction.getMap(""put_test"");
		map.fastPut(1, 1);
		transaction.commit();
		}
### Redis version
4.0.9
### Redisson version
3.7.0
### Redisson configuration

	
		Config config = new Config();
		config.useSingleServer().setAddress(""redis://10.0.50.128:6379"");

		config.useSingleServer().setRetryAttempts(3);
		config.useSingleServer().setConnectionPoolSize(500);
		config.useSingleServer().setRetryInterval(2000);
		config.useSingleServer().setConnectTimeout(20000);
		redisson = (RedissonClient) Redisson.create(config);
		
		   
                            TransactionOptions  options = TransactionOptions.defaults()
				
				.syncSlavesTimeout(5, TimeUnit.SECONDS)
				.responseTimeout(3, TimeUnit.SECONDS)
				.retryInterval(5, TimeUnit.SECONDS)
				.retryAttempts(3)
				.timeout(10, TimeUnit.SECONDS);
		
		",2018-06-12 11:51:06,"[{'commitHash': '8d347ad22be930b9a92fd55724a5c90440fa0a3c', 'commitGHEventType': 'referenced', 'commitUser': 'mrniko', 'commitParents': ['c3cdb4f5a8d1988c577c113fd7230c5d4d4c18ff'], 'nameRev': '8d347ad22be930b9a92fd55724a5c90440fa0a3c tags/redisson-2.12.1~3', 'commitMessage': 'Fixed - methods belongs to transactional objects get blocked at high concurrency. #1459\n', 'commitDateTime': '2018-06-01 10:38:43', 'authoredDateTime': '2018-06-01 10:38:43', 'commitGitStats': [{'filePath': 'redisson/src/main/java/org/redisson/pubsub/LockPubSub.java', 'insertions': 6, 'deletions': 17, 'lines': 23}, {'filePath': 'redisson/src/main/java/org/redisson/pubsub/SemaphorePubSub.java', 'insertions': 6, 'deletions': 17, 'lines': 23}, {'filePath': 'redisson/src/test/java/org/redisson/transaction/RedissonBaseTransactionalMapTest.java', 'insertions': 23, 'deletions': 1, 'lines': 24}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'LockPubSub.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.pubsub.LockPubSub.onMessage(org.redisson.RedissonLockEntry,java.lang.Long)', 'TOT': 9, 'UPD': 2, 'INS': 1, 'MOV': 3, 'DEL': 3}]}, {'spoonFilePath': 'SemaphorePubSub.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.pubsub.SemaphorePubSub.onMessage(org.redisson.RedissonLockEntry,java.lang.Long)', 'TOT': 9, 'UPD': 2, 'INS': 1, 'MOV': 3, 'DEL': 3}]}, {'spoonFilePath': 'RedissonBaseTransactionalMapTest.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.transaction.RedissonBaseTransactionalMapTest.testFastPut()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': 'eb36207f93add5faa4b2b58677efa1f19cfab426', 'commitGHEventType': 'referenced', 'commitUser': 'mrniko', 'commitParents': ['ba1430061142518e850bf81c0ed0c04219267d77'], 'nameRev': 'eb36207f93add5faa4b2b58677efa1f19cfab426 tags/redisson-2.12.2~16', 'commitMessage': 'Fixed - methods belongs to transactional objects get blocked at high concurrency. #1459\n', 'commitDateTime': '2018-06-04 12:08:51', 'authoredDateTime': '2018-06-04 12:08:51', 'commitGitStats': [{'filePath': 'redisson/src/main/java/org/redisson/RedissonLock.java', 'insertions': 58, 'deletions': 66, 'lines': 124}, {'filePath': 'redisson/src/main/java/org/redisson/RedissonPermitExpirableSemaphore.java', 'insertions': 69, 'deletions': 75, 'lines': 144}, {'filePath': 'redisson/src/main/java/org/redisson/RedissonSemaphore.java', 'insertions': 42, 'deletions': 48, 'lines': 90}, {'filePath': 'redisson/src/main/java/org/redisson/pubsub/LockPubSub.java', 'insertions': 2, 'deletions': 6, 'lines': 8}, {'filePath': 'redisson/src/main/java/org/redisson/pubsub/SemaphorePubSub.java', 'insertions': 2, 'deletions': 6, 'lines': 8}, {'filePath': 'redisson/src/test/java/org/redisson/transaction/RedissonBaseTransactionalMapTest.java', 'insertions': 3, 'deletions': 3, 'lines': 6}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'RedissonLock.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.RedissonLock.lockAsync(long,java.util.concurrent.TimeUnit,org.redisson.api.RFuture,org.redisson.misc.RPromise,long).7.operationComplete(io.netty.util.concurrent.Future).2.run(io.netty.util.Timeout)', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'org.redisson.RedissonLock.lockAsync(long,java.util.concurrent.TimeUnit,org.redisson.api.RFuture,org.redisson.misc.RPromise,long).7.operationComplete(io.netty.util.concurrent.Future)', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'org.redisson.RedissonLock.tryLockAsync(java.util.concurrent.atomic.AtomicLong,long,java.util.concurrent.TimeUnit,org.redisson.api.RFuture,org.redisson.misc.RPromise,long).10.operationComplete(io.netty.util.concurrent.Future).2.run(io.netty.util.Timeout)', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'org.redisson.RedissonLock.tryLockAsync(java.util.concurrent.atomic.AtomicLong,long,java.util.concurrent.TimeUnit,org.redisson.api.RFuture,org.redisson.misc.RPromise,long).10.operationComplete(io.netty.util.concurrent.Future)', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}]}, {'spoonFilePath': 'RedissonPermitExpirableSemaphore.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.RedissonPermitExpirableSemaphore.tryAcquireAsync(java.util.concurrent.atomic.AtomicLong,int,org.redisson.api.RFuture,org.redisson.misc.RPromise,long,java.util.concurrent.TimeUnit).2.operationComplete(io.netty.util.concurrent.Future).3.run(io.netty.util.Timeout)', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'org.redisson.RedissonPermitExpirableSemaphore.tryAcquireAsync(java.util.concurrent.atomic.AtomicLong,int,org.redisson.api.RFuture,org.redisson.misc.RPromise,long,java.util.concurrent.TimeUnit).2.operationComplete(io.netty.util.concurrent.Future)', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'org.redisson.RedissonPermitExpirableSemaphore.acquireAsync(int,org.redisson.api.RFuture,org.redisson.misc.RPromise,long,java.util.concurrent.TimeUnit).3.operationComplete(io.netty.util.concurrent.Future)', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}]}, {'spoonFilePath': 'RedissonSemaphore.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.RedissonSemaphore.tryAcquireAsync(java.util.concurrent.atomic.AtomicLong,int,org.redisson.api.RFuture,org.redisson.misc.RPromise).2.operationComplete(io.netty.util.concurrent.Future).2.run(io.netty.util.Timeout)', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'org.redisson.RedissonSemaphore.tryAcquireAsync(java.util.concurrent.atomic.AtomicLong,int,org.redisson.api.RFuture,org.redisson.misc.RPromise).2.operationComplete(io.netty.util.concurrent.Future)', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}, {'spoonMethodName': 'org.redisson.RedissonSemaphore.acquireAsync(int,org.redisson.api.RFuture,org.redisson.misc.RPromise).3.operationComplete(io.netty.util.concurrent.Future)', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 1}]}, {'spoonFilePath': 'LockPubSub.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.pubsub.LockPubSub.onMessage(org.redisson.RedissonLockEntry,java.lang.Long)', 'TOT': 7, 'UPD': 1, 'INS': 1, 'MOV': 3, 'DEL': 2}]}, {'spoonFilePath': 'SemaphorePubSub.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.pubsub.SemaphorePubSub.onMessage(org.redisson.RedissonLockEntry,java.lang.Long)', 'TOT': 7, 'UPD': 1, 'INS': 1, 'MOV': 3, 'DEL': 2}]}, {'spoonFilePath': 'RedissonBaseTransactionalMapTest.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.transaction.RedissonBaseTransactionalMapTest.testFastPut()', 'TOT': 3, 'UPD': 3, 'INS': 0, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '6c8f04b0f101acd4940155270758ed41f9ca4645', 'commitGHEventType': 'referenced', 'commitUser': 'mrniko', 'commitParents': ['cfb37545e1c5eb9219c33166480cc81914947e7a'], 'nameRev': '6c8f04b0f101acd4940155270758ed41f9ca4645 tags/redisson-2.12.2~10', 'commitMessage': 'Fixed - ConcurrentModificationException in RTransaction object. #1459\n', 'commitDateTime': '2018-06-12 14:37:07', 'authoredDateTime': '2018-06-12 14:37:07', 'commitGitStats': [{'filePath': 'redisson/src/main/java/org/redisson/cache/LocalCachedMapDisabledKey.java', 'insertions': 3, 'deletions': 1, 'lines': 4}, {'filePath': 'redisson/src/main/java/org/redisson/cache/LocalCachedMapEnable.java', 'insertions': 3, 'deletions': 1, 'lines': 4}, {'filePath': 'redisson/src/main/java/org/redisson/transaction/RedissonTransaction.java', 'insertions': 3, 'deletions': 1, 'lines': 4}, {'filePath': 'redisson/src/test/java/org/redisson/transaction/RedissonBaseTransactionalMapTest.java', 'insertions': 1, 'deletions': 1, 'lines': 2}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'LocalCachedMapDisabledKey.java', 'spoonMethods': []}, {'spoonFilePath': 'LocalCachedMapEnable.java', 'spoonMethods': []}, {'spoonFilePath': 'RedissonTransaction.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.transaction.RedissonTransaction', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.transaction.RedissonTransaction.checkTimeout()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'RedissonBaseTransactionalMapTest.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.transaction.RedissonBaseTransactionalMapTest.testFastPut()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/redisson/redisson/issues/1459,14.000277777777777,['bug'],CommandAsyncService gets blocked at high concurrency [without OutOfMemoryError] ,2.0,"['org.redisson.RedissonPermitExpirableSemaphore.tryAcquireAsync(java.util.concurrent.atomic.AtomicLong,int,org.redisson.api.RFuture,org.redisson.misc.RPromise,long,java.util.concurrent.TimeUnit).2.operationComplete(io.netty.util.concurrent.Future).3.run(io.netty.util.Timeout)', 'org.redisson.RedissonPermitExpirableSemaphore.acquireAsync(int,org.redisson.api.RFuture,org.redisson.misc.RPromise,long,java.util.concurrent.TimeUnit).3.operationComplete(io.netty.util.concurrent.Future)', 'org.redisson.transaction.RedissonTransaction', 'org.redisson.RedissonLock.tryLockAsync(java.util.concurrent.atomic.AtomicLong,long,java.util.concurrent.TimeUnit,org.redisson.api.RFuture,org.redisson.misc.RPromise,long).10.operationComplete(io.netty.util.concurrent.Future)', 'org.redisson.transaction.RedissonTransaction.checkTimeout()', 'org.redisson.RedissonPermitExpirableSemaphore.tryAcquireAsync(java.util.concurrent.atomic.AtomicLong,int,org.redisson.api.RFuture,org.redisson.misc.RPromise,long,java.util.concurrent.TimeUnit).2.operationComplete(io.netty.util.concurrent.Future)', 'org.redisson.RedissonLock.tryLockAsync(java.util.concurrent.atomic.AtomicLong,long,java.util.concurrent.TimeUnit,org.redisson.api.RFuture,org.redisson.misc.RPromise,long).10.operationComplete(io.netty.util.concurrent.Future).2.run(io.netty.util.Timeout)', 'org.redisson.RedissonLock.lockAsync(long,java.util.concurrent.TimeUnit,org.redisson.api.RFuture,org.redisson.misc.RPromise,long).7.operationComplete(io.netty.util.concurrent.Future)', 'org.redisson.RedissonSemaphore.tryAcquireAsync(java.util.concurrent.atomic.AtomicLong,int,org.redisson.api.RFuture,org.redisson.misc.RPromise).2.operationComplete(io.netty.util.concurrent.Future).2.run(io.netty.util.Timeout)', 'org.redisson.RedissonSemaphore.acquireAsync(int,org.redisson.api.RFuture,org.redisson.misc.RPromise).3.operationComplete(io.netty.util.concurrent.Future)', 'org.redisson.RedissonLock.lockAsync(long,java.util.concurrent.TimeUnit,org.redisson.api.RFuture,org.redisson.misc.RPromise,long).7.operationComplete(io.netty.util.concurrent.Future).2.run(io.netty.util.Timeout)', 'org.redisson.RedissonSemaphore.tryAcquireAsync(java.util.concurrent.atomic.AtomicLong,int,org.redisson.api.RFuture,org.redisson.misc.RPromise).2.operationComplete(io.netty.util.concurrent.Future)', 'org.redisson.pubsub.SemaphorePubSub.onMessage(org.redisson.RedissonLockEntry,java.lang.Long)', 'org.redisson.pubsub.LockPubSub.onMessage(org.redisson.RedissonLockEntry,java.lang.Long)']","['eb36207f93add5faa4b2b58677efa1f19cfab426', '6c8f04b0f101acd4940155270758ed41f9ca4645']",,"['redisson/src/main/java/org/redisson/cache', 'redisson/src/main/java/org/redisson', 'redisson/src/main/java/org/redisson/transaction', 'redisson/src/main/java/org/redisson/pubsub']",182.0,204.0,386.0,8.0,3.0,14.0,36.0,16.0,3.0,14.0,6.0,0.0,0.0,0.0,1.0,0.0,0.0,redisson
42368,2018-05-10 21:25:44,techthumb,"﻿### Expected behavior
Concurrent calls to RemoteService should not result in an exception

### Actual behavior
Concurrent calls to RemoteService should result in an exception

### Steps to reproduce or test case

```
public interface IService {
  Boolean addToThreadUnsafeList(Integer element);

  List<Integer> getList();

  void clear();
}


public class ServiceImpl implements IService {
  private List<Integer> list;

  public ServiceImpl() {
    list = new ArrayList<>();
  }

  public Boolean addToThreadUnsafeList(Integer element) {
    System.out.println(element);
    return list.add(element);
  }

  @Override
  public List<Integer> getList() {
    return list;
  }

  @Override
  public void clear() {
    list.clear();
  }

}


  private void parallelTestUsingExecutorServiceAndPoolSizeOf(int poolSize) {
    ExecutorService executorService = Executors.newFixedThreadPool(poolSize);
    IService service = redissonClient.getRemoteService().get(IService.class);

    List<Future<Boolean>> futures = new ArrayList<>();

    for (int i = 0; i < iterations; i++) {
      final Integer element = i;
      futures.add(executorService.submit(() -> service.addToThreadUnsafeList(element)));
    }

    while (!futures.stream().allMatch(Future::isDone)) {}

    try {
      Thread.sleep(10000);
    } catch (InterruptedException e) {
      throw new RuntimeException(e);
    }
    assertThat(service.getList()).hasSize(iterations);
  }

```
### Redis version
4.0.9

### Redisson version
3.6.5

### Redisson configuration
```
config.useSingleServer().setAddress(""redis://127.0.0.1:6379"");
```",2018-05-11 10:04:58,"[{'commitHash': '6786a22b943879306ba6d8816af6c67b76cc4185', 'commitGHEventType': 'referenced', 'commitUser': 'mrniko', 'commitParents': ['28b0c49acbb2fb1e32612a043e3a0b2d5013aadc'], 'nameRev': '6786a22b943879306ba6d8816af6c67b76cc4185 tags/redisson-2.12.0~7', 'commitMessage': ""Fixed - RemoteService sync invocations aren't thread safe. #1433\n"", 'commitDateTime': '2018-05-11 15:05:57', 'authoredDateTime': '2018-05-11 15:05:57', 'commitGitStats': [{'filePath': 'redisson/src/main/java/org/redisson/BaseRemoteService.java', 'insertions': 46, 'deletions': 33, 'lines': 79}, {'filePath': 'redisson/src/main/java/org/redisson/remote/RemoteServiceResponse.java', 'insertions': 1, 'deletions': 0, 'lines': 1}, {'filePath': 'redisson/src/test/java/org/redisson/RedissonRemoteServiceTest.java', 'insertions': 37, 'deletions': 0, 'lines': 37}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'BaseRemoteService.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.BaseRemoteService.sync(java.lang.Class,org.redisson.api.RemoteInvocationOptions).7.invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])', 'TOT': 45, 'UPD': 10, 'INS': 8, 'MOV': 26, 'DEL': 1}, {'spoonMethodName': 'org.redisson.BaseRemoteService.tryPollAckAgain(org.redisson.api.RemoteInvocationOptions,org.redisson.api.RBlockingQueue,java.lang.String)', 'TOT': 22, 'UPD': 8, 'INS': 0, 'MOV': 11, 'DEL': 3}, {'spoonMethodName': 'org.redisson.BaseRemoteService.sync(java.lang.Class,org.redisson.api.RemoteInvocationOptions)', 'TOT': 11, 'UPD': 0, 'INS': 1, 'MOV': 9, 'DEL': 1}]}, {'spoonFilePath': 'RemoteServiceResponse.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.remote.RemoteServiceResponse.getId()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'RedissonRemoteServiceTest.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.RedissonRemoteServiceTest.testConcurrentInvocations()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/redisson/redisson/issues/1433,0.0002777777777777778,['bug'],Concurrent calls to RemoteService should not result in an exception,1.0,"['org.redisson.BaseRemoteService.tryPollAckAgain(org.redisson.api.RemoteInvocationOptions,org.redisson.api.RBlockingQueue,java.lang.String)', 'org.redisson.BaseRemoteService.sync(java.lang.Class,org.redisson.api.RemoteInvocationOptions)', 'org.redisson.remote.RemoteServiceResponse.getId()', 'org.redisson.BaseRemoteService.sync(java.lang.Class,org.redisson.api.RemoteInvocationOptions).7.invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])']",['6786a22b943879306ba6d8816af6c67b76cc4185'],,"['redisson/src/main/java/org/redisson', 'redisson/src/main/java/org/redisson/remote']",47.0,33.0,80.0,2.0,18.0,4.0,79.0,46.0,10.0,5.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,redisson
42409,2018-01-30 07:41:52,Yipei,"Hi there,

We hit an issue in one of our server using Redission as Redis Client. We do have a single thread executor handling subscribe/unsubscribe for multiple channels. But last week, we found this got blocked on one server . Below is the thread print

""pool-12-thread-1"" #67 prio=5 os_prio=0 tid=0x00007f9f6c9e1000 nid=0x5091 waiting on condition [0x00007f9f6a2f6000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000006cc5406a8> (a java.util.concurrent.CountDownLatch$Sync)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
	at org.redisson.pubsub.AsyncSemaphore.acquireUninterruptibly(AsyncSemaphore.java:49)
	at org.redisson.RedissonTopic.removeAllListeners(RedissonTopic.java:89)

The org.redisson.RedissonTopic.removeAllListeners is unable to respond request, and blocked in the acquireUninterruptibly FOREVER. I tried to dig into logs, there was one exception thrown before this in CommandAsyncService.syncSubscription line 125 ""Subscribe timeout 9500ms"". This is possibly related since this was the only ""Subscribe timeout"" message I saw in last 30 days logs and then this issue happened. But I still don't find prove on this. Looks like this is a rare case since we just hit once. I tried to reproduce locally but with no luck.

I think the removeAllListeners should give an option to let caller pass in a timeout. But this is also not going to resolve the root cause. I am wondering if other people have observed this issue before. Please advise if you have any ideas on this, thanks.




  



",2018-02-13 11:41:30,"[{'commitHash': '707b419d97c20e0d86df8712ba05ed19edf628b3', 'commitGHEventType': 'referenced', 'commitUser': 'mrniko', 'commitParents': ['ed247c0ce7eb0608a2ecc807fa31c74e91aea6f5'], 'nameRev': '707b419d97c20e0d86df8712ba05ed19edf628b3 tags/redisson-2.11.1~10', 'commitMessage': 'Fixed - org.redisson.RedissonTopic.removeAllListeners got blocked on invocation. #1268\n', 'commitDateTime': '2018-02-12 07:25:15', 'authoredDateTime': '2018-02-12 07:25:15', 'commitGitStats': [{'filePath': 'redisson/src/main/java/org/redisson/RedissonPatternTopic.java', 'insertions': 13, 'deletions': 3, 'lines': 16}, {'filePath': 'redisson/src/main/java/org/redisson/RedissonTopic.java', 'insertions': 13, 'deletions': 3, 'lines': 16}, {'filePath': 'redisson/src/main/java/org/redisson/pubsub/AsyncSemaphore.java', 'insertions': 16, 'deletions': 4, 'lines': 20}, {'filePath': 'redisson/src/main/java/org/redisson/reactive/RedissonPatternTopicReactive.java', 'insertions': 11, 'deletions': 1, 'lines': 12}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'RedissonPatternTopic.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.RedissonPatternTopic.removeListener(int)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.RedissonPatternTopic.removeAllListeners()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.RedissonPatternTopic.removeListener(org.redisson.api.listener.PatternMessageListener)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.RedissonPatternTopic.acquire(org.redisson.pubsub.AsyncSemaphore)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'RedissonTopic.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.RedissonTopic.removeAllListeners()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.RedissonTopic.removeListener(org.redisson.api.listener.MessageListener)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.RedissonTopic.removeListener(int)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.RedissonTopic.acquire(org.redisson.pubsub.AsyncSemaphore)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'AsyncSemaphore.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.pubsub.AsyncSemaphore.acquireUninterruptibly()', 'TOT': 5, 'UPD': 2, 'INS': 0, 'MOV': 2, 'DEL': 1}, {'spoonMethodName': 'org.redisson.pubsub.AsyncSemaphore.tryAcquire(long)', 'TOT': 6, 'UPD': 0, 'INS': 6, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'RedissonPatternTopicReactive.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.reactive.RedissonPatternTopicReactive.removeListener(int)', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.reactive.RedissonPatternTopicReactive.acquire(org.redisson.pubsub.AsyncSemaphore)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': 'a481259aa5c83d37e3e54c3e6d2fee1b99f27bb1', 'commitGHEventType': 'referenced', 'commitUser': 'mrniko', 'commitParents': ['707b419d97c20e0d86df8712ba05ed19edf628b3'], 'nameRev': 'a481259aa5c83d37e3e54c3e6d2fee1b99f27bb1 tags/redisson-2.11.1~9', 'commitMessage': 'Fixed - possible pubsub listeners leak.  #1268\n', 'commitDateTime': '2018-02-12 08:32:29', 'authoredDateTime': '2018-02-12 08:32:29', 'commitGitStats': [{'filePath': 'redisson/src/main/java/org/redisson/connection/MasterSlaveConnectionManager.java', 'insertions': 30, 'deletions': 17, 'lines': 47}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'MasterSlaveConnectionManager.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(org.redisson.client.codec.Codec,java.lang.String,org.redisson.client.RedisPubSubListener[])', 'TOT': 11, 'UPD': 1, 'INS': 1, 'MOV': 7, 'DEL': 2}, {'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(org.redisson.client.codec.Codec,java.lang.String,org.redisson.misc.RPromise,org.redisson.client.protocol.pubsub.PubSubType,org.redisson.pubsub.AsyncSemaphore,org.redisson.client.RedisPubSubListener[])', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.psubscribe(java.lang.String,org.redisson.client.codec.Codec,org.redisson.client.RedisPubSubListener[]).2.run()', 'TOT': 7, 'UPD': 2, 'INS': 0, 'MOV': 5, 'DEL': 0}, {'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(org.redisson.client.codec.Codec,java.lang.String,org.redisson.client.RedisPubSubListener[]).3', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(org.redisson.client.codec.Codec,java.lang.String,org.redisson.misc.RPromise,org.redisson.client.protocol.pubsub.PubSubType,org.redisson.pubsub.AsyncSemaphore,org.redisson.client.RedisPubSubListener[]).4', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(java.lang.String,org.redisson.misc.RPromise,org.redisson.client.protocol.pubsub.PubSubType,org.redisson.pubsub.AsyncSemaphore,org.redisson.connection.PubSubConnectionEntry,org.redisson.client.RedisPubSubListener[]).5', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.connect(org.redisson.client.codec.Codec,java.lang.String,org.redisson.misc.RPromise,org.redisson.client.protocol.pubsub.PubSubType,org.redisson.pubsub.AsyncSemaphore,org.redisson.client.RedisPubSubListener[]).6', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.unsubscribe(java.lang.String,org.redisson.pubsub.AsyncSemaphore).7', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.punsubscribe(java.lang.String,org.redisson.pubsub.AsyncSemaphore).9', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.unsubscribe(java.lang.String,boolean).8', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.punsubscribe(java.lang.String,boolean).10', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(org.redisson.client.codec.Codec,java.lang.String,org.redisson.client.RedisPubSubListener[]).3.run()', 'TOT': 8, 'UPD': 1, 'INS': 0, 'MOV': 5, 'DEL': 2}, {'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(org.redisson.client.codec.Codec,java.lang.String,org.redisson.misc.RPromise,org.redisson.client.protocol.pubsub.PubSubType,org.redisson.pubsub.AsyncSemaphore,org.redisson.client.RedisPubSubListener[]).4.run()', 'TOT': 3, 'UPD': 3, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.connect(org.redisson.client.codec.Codec,java.lang.String,org.redisson.misc.RPromise,org.redisson.client.protocol.pubsub.PubSubType,org.redisson.pubsub.AsyncSemaphore,org.redisson.client.RedisPubSubListener[]).6.operationComplete(io.netty.util.concurrent.Future)', 'TOT': 3, 'UPD': 3, 'INS': 0, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.psubscribe(java.lang.String,org.redisson.client.codec.Codec,org.redisson.client.RedisPubSubListener[])', 'TOT': 9, 'UPD': 0, 'INS': 2, 'MOV': 4, 'DEL': 3}, {'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(org.redisson.client.protocol.pubsub.PubSubType,org.redisson.client.codec.Codec,java.lang.String,org.redisson.client.RedisPubSubListener[])', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.psubscribe(java.lang.String,org.redisson.client.codec.Codec,org.redisson.pubsub.AsyncSemaphore,org.redisson.client.RedisPubSubListener[])', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(org.redisson.client.protocol.pubsub.PubSubType,org.redisson.client.codec.Codec,java.lang.String,org.redisson.client.RedisPubSubListener[]).2.run()', 'TOT': 2, 'UPD': 0, 'INS': 2, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(java.lang.String,org.redisson.misc.RPromise,org.redisson.client.protocol.pubsub.PubSubType,org.redisson.pubsub.AsyncSemaphore,org.redisson.connection.PubSubConnectionEntry,org.redisson.client.RedisPubSubListener[]).4.operationComplete(io.netty.util.concurrent.Future)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(org.redisson.client.codec.Codec,java.lang.String,org.redisson.misc.RPromise,org.redisson.client.protocol.pubsub.PubSubType,org.redisson.pubsub.AsyncSemaphore,org.redisson.client.RedisPubSubListener[]).3.run()', 'TOT': 2, 'UPD': 0, 'INS': 2, 'MOV': 0, 'DEL': 0}, {'spoonMethodName': 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(java.lang.String,org.redisson.misc.RPromise,org.redisson.client.protocol.pubsub.PubSubType,org.redisson.pubsub.AsyncSemaphore,org.redisson.connection.PubSubConnectionEntry,org.redisson.client.RedisPubSubListener[]).5.operationComplete(io.netty.util.concurrent.Future)', 'TOT': 2, 'UPD': 0, 'INS': 0, 'MOV': 2, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/redisson/redisson/issues/1268,14.000277777777777,['bug'],org.redisson.RedissonTopic.removeAllListeners got blocked on CountDownLatch.await,2.0,"['org.redisson.RedissonTopic.removeAllListeners()', 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(java.lang.String,org.redisson.misc.RPromise,org.redisson.client.protocol.pubsub.PubSubType,org.redisson.pubsub.AsyncSemaphore,org.redisson.connection.PubSubConnectionEntry,org.redisson.client.RedisPubSubListener[]).5.operationComplete(io.netty.util.concurrent.Future)', 'org.redisson.reactive.RedissonPatternTopicReactive.removeListener(int)', 'org.redisson.reactive.RedissonPatternTopicReactive.acquire(org.redisson.pubsub.AsyncSemaphore)', 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(org.redisson.client.codec.Codec,java.lang.String,org.redisson.client.RedisPubSubListener[]).3.run()', 'org.redisson.RedissonPatternTopic.acquire(org.redisson.pubsub.AsyncSemaphore)', 'org.redisson.connection.MasterSlaveConnectionManager.punsubscribe(java.lang.String,org.redisson.pubsub.AsyncSemaphore).9', 'org.redisson.connection.MasterSlaveConnectionManager.connect(org.redisson.client.codec.Codec,java.lang.String,org.redisson.misc.RPromise,org.redisson.client.protocol.pubsub.PubSubType,org.redisson.pubsub.AsyncSemaphore,org.redisson.client.RedisPubSubListener[]).6', 'org.redisson.pubsub.AsyncSemaphore.tryAcquire(long)', 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(java.lang.String,org.redisson.misc.RPromise,org.redisson.client.protocol.pubsub.PubSubType,org.redisson.pubsub.AsyncSemaphore,org.redisson.connection.PubSubConnectionEntry,org.redisson.client.RedisPubSubListener[]).4.operationComplete(io.netty.util.concurrent.Future)', 'org.redisson.connection.MasterSlaveConnectionManager.psubscribe(java.lang.String,org.redisson.client.codec.Codec,org.redisson.pubsub.AsyncSemaphore,org.redisson.client.RedisPubSubListener[])', 'org.redisson.RedissonTopic.removeListener(org.redisson.api.listener.MessageListener)', 'org.redisson.connection.MasterSlaveConnectionManager.unsubscribe(java.lang.String,boolean).8', 'org.redisson.RedissonPatternTopic.removeListener(org.redisson.api.listener.PatternMessageListener)', 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(org.redisson.client.protocol.pubsub.PubSubType,org.redisson.client.codec.Codec,java.lang.String,org.redisson.client.RedisPubSubListener[])', 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(org.redisson.client.codec.Codec,java.lang.String,org.redisson.misc.RPromise,org.redisson.client.protocol.pubsub.PubSubType,org.redisson.pubsub.AsyncSemaphore,org.redisson.client.RedisPubSubListener[])', 'org.redisson.connection.MasterSlaveConnectionManager.connect(org.redisson.client.codec.Codec,java.lang.String,org.redisson.misc.RPromise,org.redisson.client.protocol.pubsub.PubSubType,org.redisson.pubsub.AsyncSemaphore,org.redisson.client.RedisPubSubListener[]).6.operationComplete(io.netty.util.concurrent.Future)', 'org.redisson.pubsub.AsyncSemaphore.acquireUninterruptibly()', 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(org.redisson.client.codec.Codec,java.lang.String,org.redisson.client.RedisPubSubListener[])', 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(org.redisson.client.codec.Codec,java.lang.String,org.redisson.misc.RPromise,org.redisson.client.protocol.pubsub.PubSubType,org.redisson.pubsub.AsyncSemaphore,org.redisson.client.RedisPubSubListener[]).4.run()', 'org.redisson.connection.MasterSlaveConnectionManager.unsubscribe(java.lang.String,org.redisson.pubsub.AsyncSemaphore).7', 'org.redisson.connection.MasterSlaveConnectionManager.psubscribe(java.lang.String,org.redisson.client.codec.Codec,org.redisson.client.RedisPubSubListener[]).2.run()', 'org.redisson.RedissonTopic.acquire(org.redisson.pubsub.AsyncSemaphore)', 'org.redisson.RedissonPatternTopic.removeAllListeners()', 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(java.lang.String,org.redisson.misc.RPromise,org.redisson.client.protocol.pubsub.PubSubType,org.redisson.pubsub.AsyncSemaphore,org.redisson.connection.PubSubConnectionEntry,org.redisson.client.RedisPubSubListener[]).5', 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(org.redisson.client.codec.Codec,java.lang.String,org.redisson.client.RedisPubSubListener[]).3', 'org.redisson.connection.MasterSlaveConnectionManager.psubscribe(java.lang.String,org.redisson.client.codec.Codec,org.redisson.client.RedisPubSubListener[])', 'org.redisson.RedissonPatternTopic.removeListener(int)', 'org.redisson.RedissonTopic.removeListener(int)', 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(org.redisson.client.codec.Codec,java.lang.String,org.redisson.misc.RPromise,org.redisson.client.protocol.pubsub.PubSubType,org.redisson.pubsub.AsyncSemaphore,org.redisson.client.RedisPubSubListener[]).3.run()', 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(org.redisson.client.protocol.pubsub.PubSubType,org.redisson.client.codec.Codec,java.lang.String,org.redisson.client.RedisPubSubListener[]).2.run()', 'org.redisson.connection.MasterSlaveConnectionManager.punsubscribe(java.lang.String,boolean).10', 'org.redisson.connection.MasterSlaveConnectionManager.subscribe(org.redisson.client.codec.Codec,java.lang.String,org.redisson.misc.RPromise,org.redisson.client.protocol.pubsub.PubSubType,org.redisson.pubsub.AsyncSemaphore,org.redisson.client.RedisPubSubListener[]).4']","['707b419d97c20e0d86df8712ba05ed19edf628b3', 'a481259aa5c83d37e3e54c3e6d2fee1b99f27bb1']",,"['redisson/src/main/java/org/redisson', 'redisson/src/main/java/org/redisson/pubsub', 'redisson/src/main/java/org/redisson/connection', 'redisson/src/main/java/org/redisson/reactive']",83.0,28.0,111.0,5.0,28.0,33.0,80.0,25.0,19.0,8.0,5.0,0.0,0.0,0.0,0.0,0.0,0.0,redisson
42424,2017-11-29 21:04:07,bhoover10001,"It looks like the second time you try to get a key that was null through a map loader, the loader will simply hang.

I was able to reproduce this in RedissonMapCacheTest.java with the following code

    @Test
    public void testMapLoaderGetMulipleNulls() {
        Map<String, String> cache = new HashMap<String, String>();
        cache.put(""1"", ""11"");
        cache.put(""2"", ""22"");
        cache.put(""3"", ""33"");
        
        RMap<String, String> map = getLoaderTestMap(""test"", cache);
        assertThat(map.get(""0"")).isNull();
        assertThat(map.get(""1"")).isEqualTo(""11"");
        assertThat(map.get(""0"")).isNull(); // This line will never return anything and the test will hang
    }

    // This code will work fine, because the keys without values are different.
    @Test
    public void testMapLoaderGetMulipleNullsDifferentKeys() {
        Map<String, String> cache = new HashMap<String, String>();
        cache.put(""1"", ""11"");
        cache.put(""2"", ""22"");
        cache.put(""3"", ""33"");
        
        RMap<String, String> map = getLoaderTestMap(""test"", cache);
        assertThat(map.get(""0"")).isNull();
        assertThat(map.get(""1"")).isEqualTo(""11"");
        assertThat(map.get(""-1"")).isNull(); 
    }

I'm running Windows 10 with Redis Version 3.2.100 64 bit version.  This is happening on Redisson 3.5.5 and the current master branch.",2017-11-30 09:34:50,"[{'commitHash': 'a98118221ff2270cf29fc320c505da0cbe8a554a', 'commitGHEventType': 'referenced', 'commitUser': 'mrniko', 'commitParents': ['353f9aa24df14eaa6999c0b4527da8260543d670'], 'nameRev': 'a98118221ff2270cf29fc320c505da0cbe8a554a tags/redisson-2.10.6~6', 'commitMessage': 'Fixed MapLoader hangs if loaded value is null. #1170\n', 'commitDateTime': '2017-11-30 12:39:52', 'authoredDateTime': '2017-11-30 12:39:52', 'commitGitStats': [{'filePath': 'redisson/src/main/java/org/redisson/RedissonMap.java', 'insertions': 1, 'deletions': 1, 'lines': 2}, {'filePath': 'redisson/src/test/java/org/redisson/BaseMapTest.java', 'insertions': 13, 'deletions': 0, 'lines': 13}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'RedissonMap.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.RedissonMap.loadValue(java.lang.Object,org.redisson.misc.RPromise,org.redisson.api.RLock,long).19.run()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'BaseMapTest.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.BaseMapTest.testMapLoaderGetMulipleNulls()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/redisson/redisson/issues/1170,0.0002777777777777778,['bug'],"MapLoader seems to hang if you try to check the same null key, more than once",1.0,"['org.redisson.RedissonMap.loadValue(java.lang.Object,org.redisson.misc.RPromise,org.redisson.api.RLock,long).19.run()']",['a98118221ff2270cf29fc320c505da0cbe8a554a'],,['redisson/src/main/java/org/redisson'],1.0,1.0,2.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,redisson
42443,2017-09-18 14:38:59,mrniko,,2017-09-18 14:40:46,"[{'commitHash': 'f30351f94b032658dfcc3b071b2db610f1d1b5c6', 'commitGHEventType': 'referenced', 'commitUser': 'mrniko', 'commitParents': ['9e4767690cd5a5b309c7062949dd8a24e775b6cd'], 'nameRev': 'f30351f94b032658dfcc3b071b2db610f1d1b5c6 tags/redisson-2.10.4~15', 'commitMessage': 'Fixed possible race-condition during write operation cancellation.  #1061\n', 'commitDateTime': '2017-09-18 17:43:37', 'authoredDateTime': '2017-09-18 17:43:37', 'commitGitStats': [{'filePath': 'redisson/src/main/java/org/redisson/client/handler/CommandBatchEncoder.java', 'insertions': 13, 'deletions': 1, 'lines': 14}, {'filePath': 'redisson/src/main/java/org/redisson/client/handler/CommandEncoder.java', 'insertions': 6, 'deletions': 0, 'lines': 6}, {'filePath': 'redisson/src/test/java/org/redisson/RedissonBatchTest.java', 'insertions': 13, 'deletions': 1, 'lines': 14}, {'filePath': 'redisson/src/test/java/org/redisson/RedissonMapTest.java', 'insertions': 12, 'deletions': 0, 'lines': 12}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'CommandBatchEncoder.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.client.handler.CommandBatchEncoder.write(io.netty.channel.ChannelHandlerContext,java.lang.Object,io.netty.channel.ChannelPromise)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'CommandEncoder.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.client.handler.CommandEncoder.write(io.netty.channel.ChannelHandlerContext,java.lang.Object,io.netty.channel.ChannelPromise)', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'RedissonBatchTest.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.RedissonBatchTest.testWriteTimeout()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}, {'spoonFilePath': 'RedissonMapTest.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.RedissonMapTest.testWriteTimeout()', 'TOT': 1, 'UPD': 0, 'INS': 1, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/redisson/redisson/issues/1061,0.0002777777777777778,['bug'],Possible race-condition during write operation cancellation,1.0,"['org.redisson.client.handler.CommandEncoder.write(io.netty.channel.ChannelHandlerContext,java.lang.Object,io.netty.channel.ChannelPromise)', 'org.redisson.client.handler.CommandBatchEncoder.write(io.netty.channel.ChannelHandlerContext,java.lang.Object,io.netty.channel.ChannelPromise)']",['f30351f94b032658dfcc3b071b2db610f1d1b5c6'],,['redisson/src/main/java/org/redisson/client/handler'],19.0,1.0,20.0,2.0,0.0,2.0,2.0,0.0,2.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,redisson
42627,2014-12-17 21:06:55,alycecil,"RedissonList iterator as it tries to keep ""up to date"" with data has a race condition in which if between the .hasNext() and the .next() call the set is emptied the list will throw NoSuchElementException.

---
## Unit Test

private static final String LIST_NAME = ""TEST_LIST"";
private static boolean failed;

```
@Ignore
class Delete implements Runnable {

    public void run() {
        // remove one item
        RList<Object> list = redisson.getList(LIST_NAME);
        if (!list.isEmpty()) {
            Iterator<Object> iter = list.iterator();
            if (iter.hasNext()) {
                iter.next();
                iter.remove();
            }
        }
    }
}

@Ignore
class Add implements Runnable {
    int i = 0;

    public void run() {
        // add items up to place up to one in list
        RList<Object> list = redisson.getList(LIST_NAME);
        if (list.isEmpty()) {
            list.add(""Entry:"" + (i++));

        }
    }
}

@Ignore
class Loop implements Runnable {
    public void run() {
        try {
            RList<Object> list = redisson.getList(LIST_NAME);
            if (!list.isEmpty()) {
                // implicit iterator
                for (@SuppressWarnings(""unused"")
                Object o : list) {
                }
            }
        } catch (Exception e) {
            RedissonListTest.failed = true;
            e.printStackTrace();
        }
    }
}

class ThousandAndOneThread extends Thread {
    public ThousandAndOneThread(Runnable r) {
        this.r = r;
    }

    boolean dead = false;
    int nightsLeft = 1001;
    Runnable r;

    public void run() {
        // while shahrazad still is alive
        while (!dead && nightsLeft > 0) {
            nightsLeft--;

            // do runnable
            r.run();

            // take a nap
            try {
                Thread.sleep(3);
            } catch (InterruptedException e) {
                e.printStackTrace();
                break;
            }
        }

        dead = true;
    }

    // Kill shahrazad
    public void kill() {
        dead = true;
    }

    // if i am done
    public boolean isDone() {
        return dead;
    }
}

@Test
public void testIteratorAfterDelete() {
    for (int i = 0; i < 4; i++) {
        // make runnables
        Delete d = new Delete();
        Add a = new Add();
        Loop l = new Loop();

        // make threads
        ThousandAndOneThread aT = new ThousandAndOneThread(a);
        ThousandAndOneThread dT = new ThousandAndOneThread(l);
        ThousandAndOneThread lT = new ThousandAndOneThread(d);

        // start threads
        aT.start();
        dT.start();
        lT.start();

        // monitor threads
        while (true) {
            if (failed) {
                fail(""Exception occured, in child thread"");
            }
            if (lT.isDone() || dT.isDone() || aT.isDone()) {
                System.out.println(""All done!"" + i);
                aT.kill();
                dT.kill();
                lT.kill();
                break;
            }
            try {
                Thread.sleep(10);
            } catch (InterruptedException e) {
                e.printStackTrace();
                aT.kill();
                dT.kill();
                lT.kill();
                fail(""fail on "" + i);
            }
        }
    }
}
```

---
## Additional Information

This also effects RedissonSet, but much less likely to hit the race condition. It causes RedissonSet line 96 to throw a null pointer exception. 

This is akin to issue #104 
",2014-12-18 18:49:33,"[{'commitHash': '983c34faddb513e090f5456e86bea1e301bafe41', 'commitGHEventType': 'referenced', 'commitUser': 'mrniko', 'commitParents': ['1b3adef3a11636f49745defcaf42074276103190'], 'nameRev': '983c34faddb513e090f5456e86bea1e301bafe41 tags/redisson-1.2.0~18', 'commitMessage': 'RedissonList iterator race conditions #106\n', 'commitDateTime': '2014-12-18 20:37:01', 'authoredDateTime': '2014-12-18 20:37:01', 'commitGitStats': [{'filePath': 'src/main/java/org/redisson/RedissonList.java', 'insertions': 34, 'deletions': 11, 'lines': 45}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'RedissonList.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.RedissonList.get(int)', 'TOT': 4, 'UPD': 1, 'INS': 1, 'MOV': 2, 'DEL': 0}, {'spoonMethodName': 'org.redisson.RedissonList.listIterator(int).13.hasNext()', 'TOT': 13, 'UPD': 6, 'INS': 2, 'MOV': 4, 'DEL': 1}, {'spoonMethodName': 'org.redisson.RedissonList.listIterator(int).13.hasPrevious()', 'TOT': 12, 'UPD': 5, 'INS': 3, 'MOV': 3, 'DEL': 1}, {'spoonMethodName': 'org.redisson.RedissonList.listIterator(int).13.remove()', 'TOT': 4, 'UPD': 1, 'INS': 2, 'MOV': 1, 'DEL': 0}, {'spoonMethodName': 'org.redisson.RedissonList.listIterator(int).13.next()', 'TOT': 8, 'UPD': 0, 'INS': 4, 'MOV': 3, 'DEL': 1}, {'spoonMethodName': 'org.redisson.RedissonList.listIterator(int).13.previous()', 'TOT': 10, 'UPD': 0, 'INS': 4, 'MOV': 4, 'DEL': 2}, {'spoonMethodName': 'org.redisson.RedissonList.listIterator(int).13', 'TOT': 3, 'UPD': 0, 'INS': 3, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}, {'commitHash': '5018a3c31cc2dd10b7bb5d812f75f72548e233ec', 'commitGHEventType': 'referenced', 'commitUser': 'mrniko', 'commitParents': ['4ffd260b6c42ee31116621286c9368a1d828a27f'], 'nameRev': '5018a3c31cc2dd10b7bb5d812f75f72548e233ec tags/redisson-2.1.2~8', 'commitMessage': 'BlockingQueue.peek race-condition fixed. #106\n', 'commitDateTime': '2015-09-03 14:25:51', 'authoredDateTime': '2015-09-03 14:25:51', 'commitGitStats': [{'filePath': 'src/main/java/org/redisson/RedissonQueue.java', 'insertions': 1, 'deletions': 4, 'lines': 5}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'RedissonQueue.java', 'spoonMethods': [{'spoonMethodName': 'org.redisson.RedissonQueue.peek()', 'TOT': 2, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 1}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/redisson/redisson/issues/106,0.0002777777777777778,['bug'],RedissonList's Iterator Race Conditions,2.0,"['org.redisson.RedissonList.listIterator(int).13.hasPrevious()', 'org.redisson.RedissonQueue.peek()', 'org.redisson.RedissonList.listIterator(int).13', 'org.redisson.RedissonList.listIterator(int).13.next()', 'org.redisson.RedissonList.listIterator(int).13.remove()', 'org.redisson.RedissonList.get(int)', 'org.redisson.RedissonList.listIterator(int).13.hasNext()', 'org.redisson.RedissonList.listIterator(int).13.previous()']","['983c34faddb513e090f5456e86bea1e301bafe41', '5018a3c31cc2dd10b7bb5d812f75f72548e233ec']",,['src/main/java/org/redisson'],35.0,15.0,50.0,2.0,14.0,8.0,56.0,17.0,19.0,6.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,redisson
42822,2016-10-28 07:42:58,thboileau,"Encountered Deadlock in the org.restlet.data.MediaType class while comparing the parameters:
https://github.com/restlet/restlet-framework-java/blob/2.3/modules/org.restlet/src/org/restlet/data/MediaType.java#L965

Noticed that two threads where comparing the two same media types but in distinct order. The deadlock is caused by the fact that WrapperList relies on the Vector class and that each thread locks one list of parameters and wait for the other when comparing it.
",2016-12-10 15:03:35,"[{'commitHash': '6c056c3409d24c5ac6920d2010f8a14e6e87fc91', 'commitGHEventType': 'referenced', 'commitUser': 'thboileau', 'commitParents': ['0e0a2a48b83db6a8b205741a0702dfad7c9fd9fe'], 'nameRev': '6c056c3409d24c5ac6920d2010f8a14e6e87fc91 tags/2.3.8~1', 'commitMessage': 'Fixed potential dead lock when compating two instances of WrapperList. Issue #1243. Contributed by Tim Peierls.\n', 'commitDateTime': '2016-12-10 16:02:36', 'authoredDateTime': '2016-12-10 16:02:36', 'commitGitStats': [{'filePath': 'build/tmpl/text/changes.txt', 'insertions': 2, 'deletions': 0, 'lines': 2}, {'filePath': 'modules/org.restlet/src/org/restlet/util/WrapperList.java', 'insertions': 13, 'deletions': 5, 'lines': 18}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'WrapperList.java', 'spoonMethods': [{'spoonMethodName': 'org.restlet.util.WrapperList.equals(java.lang.Object)', 'TOT': 10, 'UPD': 2, 'INS': 5, 'MOV': 2, 'DEL': 1}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/restlet/restlet-framework-java/issues/1243,43.000277777777775,"['Priority: high', 'State: analysis', 'Type: bug']",Concurrency issue when comparing MediaType#parameters,1.0,['org.restlet.util.WrapperList.equals(java.lang.Object)'],['6c056c3409d24c5ac6920d2010f8a14e6e87fc91'],,['modules/org.restlet/src/org/restlet/util'],13.0,5.0,18.0,1.0,2.0,1.0,10.0,2.0,5.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,restlet-framework-java
43575,2013-08-26 16:16:07,roundhill,"I've been in Montana with EDGE connection only, and when I start the app it doesn't show the notes from the db straight away. It takes a few seconds and then it shows up. If I put the phone in airplane mode it will load the list instantly.
",2013-08-29 19:03:37,"[{'commitHash': '1771d47ce1590fe920a794bdc526cc894520c912', 'commitGHEventType': 'closed', 'commitUser': 'roundhill', 'commitParents': ['b7f135b9800b9adc335a120c5b306b5fc158213e'], 'nameRev': '1771d47ce1590fe920a794bdc526cc894520c912 tags/1.0.0~61', 'commitMessage': 'Run the refreshListTask in parallel to fix thread block on startup. See http://developer.android.com/reference/android/os/AsyncTask.html#executeOnExecutor(java.util.concurrent.Executor, Params...)\n\nFixes #109.\n', 'commitDateTime': '2013-08-29 13:03:35', 'authoredDateTime': '2013-08-29 13:03:35', 'commitGitStats': [{'filePath': 'Simplenote/src/main/java/com/automattic/simplenote/NoteListFragment.java', 'insertions': 1, 'deletions': 1, 'lines': 2}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'NoteListFragment.java', 'spoonMethods': [{'spoonMethodName': 'com.automattic.simplenote.NoteListFragment.refreshList()', 'TOT': 1, 'UPD': 1, 'INS': 0, 'MOV': 0, 'DEL': 0}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/Automattic/simplenote-android/issues/109,3.000277777777778,['bug'],No notes shown for a while when starting app on slow connection.,1.0,['com.automattic.simplenote.NoteListFragment.refreshList()'],['1771d47ce1590fe920a794bdc526cc894520c912'],,['Simplenote/src/main/java/com/automattic/simplenote'],1.0,1.0,2.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,simplenote-android
44236,2017-01-12 00:09:37,minborg,"A connection that resulted in a connection error was made. Then:
<pre>
java.util.concurrent.CompletionException: java.lang.IllegalStateException: Called endChange before beginChange
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1629)
	at java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1618)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: java.lang.IllegalStateException: Called endChange before beginChange
	at javafx.collections.ListChangeBuilder.endChange(ListChangeBuilder.java:538)
	at javafx.collections.ObservableListBase.endChange(ObservableListBase.java:205)
	at javafx.collections.ModifiableObservableListBase.add(ModifiableObservableListBase.java:155)
	at java.util.AbstractList.add(AbstractList.java:108)
	at com.speedment.tool.core.internal.component.UserInterfaceComponentImpl.showNotification(UserInterfaceComponentImpl.java:669)
	at com.speedment.tool.core.internal.component.UserInterfaceComponentImpl.showNotification(UserInterfaceComponentImpl.java:664)
	at com.speedment.tool.core.internal.component.UserInterfaceComponentImpl.showNotification(UserInterfaceComponentImpl.java:659)
	at com.speedment.tool.core.internal.component.UserInterfaceComponentImpl.lambda$start$2(UserInterfaceComponentImpl.java:179)
	at com.speedment.common.logger.internal.AbstractLogger.lambda$log$5(AbstractLogger.java:455)
	at java.util.concurrent.ConcurrentHashMap$KeySetView.forEach(ConcurrentHashMap.java:4649)
	at java.util.Collections$SetFromMap.forEach(Collections.java:5476)
	at com.speedment.common.logger.internal.AbstractLogger.log(AbstractLogger.java:455)
	at com.speedment.common.logger.internal.AbstractLogger.log(AbstractLogger.java:422)
	at com.speedment.common.logger.internal.AbstractLogger.error(AbstractLogger.java:343)
	at com.speedment.common.logger.internal.SystemOutLogger.error(SystemOutLogger.java:25)
	at com.speedment.runtime.core.internal.component.ConnectionPoolComponentImpl.newConnection(ConnectionPoolComponentImpl.java:122)
	at com.speedment.runtime.core.internal.component.ConnectionPoolComponentImpl.getConnection(ConnectionPoolComponentImpl.java:95)
	at com.speedment.runtime.core.internal.component.ConnectionPoolComponentImpl.getConnection(ConnectionPoolComponentImpl.java:79)
	at com.speedment.runtime.core.internal.db.AbstractDbmsMetadataHandler.getConnection(AbstractDbmsMetadataHandler.java:779)
	at com.speedment.runtime.core.internal.db.AbstractDbmsMetadataHandler.lambda$readSchemaMetadata$4(AbstractDbmsMetadataHandler.java:171)
	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)
	... 5 more
</pre>",2017-01-26 21:06:25,"[{'commitHash': 'a1c86b00043e6336bed2a93b68fa74ffb0d0b3c3', 'commitGHEventType': 'closed', 'commitUser': 'minborg', 'commitParents': ['4b03fc6dc8bea714d3c82556693bd733db0d710e'], 'nameRev': 'a1c86b00043e6336bed2a93b68fa74ffb0d0b3c3 tags/3.0.2~46', 'commitMessage': 'tool-core: Fix #299 make sure on UI thread before modifying observ list\n\n', 'commitDateTime': '2017-01-11 16:48:22', 'authoredDateTime': '2017-01-11 16:48:22', 'commitGitStats': [{'filePath': 'build-parent/maven-plugin/src/main/java/com/speedment/maven/typemapper/Mapping.java', 'insertions': 1, 'deletions': 1, 'lines': 2}, {'filePath': 'tool-parent/tool-core/src/main/java/com/speedment/tool/core/internal/component/UserInterfaceComponentImpl.java', 'insertions': 3, 'deletions': 1, 'lines': 4}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'Mapping.java', 'spoonMethods': []}, {'spoonFilePath': 'UserInterfaceComponentImpl.java', 'spoonMethods': [{'spoonMethodName': 'com.speedment.tool.core.internal.component.UserInterfaceComponentImpl.showNotification(java.lang.String,de.jensd.fx.glyphs.fontawesome.FontAwesomeIcon,com.speedment.tool.core.brand.Palette,java.lang.Runnable)', 'TOT': 6, 'UPD': 1, 'INS': 1, 'MOV': 2, 'DEL': 2}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/speedment/speedment/issues/299,14.000277777777777,"['bug', 'fixed']",Error when outputting to log,1.0,"['com.speedment.tool.core.internal.component.UserInterfaceComponentImpl.showNotification(java.lang.String,de.jensd.fx.glyphs.fontawesome.FontAwesomeIcon,com.speedment.tool.core.brand.Palette,java.lang.Runnable)']",['a1c86b00043e6336bed2a93b68fa74ffb0d0b3c3'],,"['tool-parent/tool-core/src/main/java/com/speedment/tool/core/internal', 'build-parent/maven-plugin/src/main/java/com/speedment/maven/typemapper']",4.0,2.0,6.0,2.0,1.0,1.0,6.0,2.0,1.0,2.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,speedment
44269,2016-08-18 21:06:24,minborg,"Module Common - Lazy, LazyIntegerTest.testConcurrency()
Now marked with @Ignore
",2017-03-01 05:17:30,"[{'commitHash': '898e45c954ea31b4e4340bd16c540259d1369b96', 'commitGHEventType': 'referenced', 'commitUser': 'minborg', 'commitParents': ['c47b4cb6103ec4e401ad4d29793946827c46d2f0'], 'nameRev': '898e45c954ea31b4e4340bd16c540259d1369b96 tags/3.0.4~19', 'commitMessage': 'lazy: Fix race condition in LazyInt and the likes. Fix #234\n\n', 'commitDateTime': '2017-02-28 21:17:11', 'authoredDateTime': '2017-02-28 21:17:11', 'commitGitStats': [{'filePath': 'common-parent/common-all/pom.xml', 'insertions': 1, 'deletions': 1, 'lines': 2}, {'filePath': 'common-parent/lazy/pom.xml', 'insertions': 2, 'deletions': 2, 'lines': 4}, {'filePath': 'common-parent/lazy/src/main/java/com/speedment/common/lazy/LazyDouble.java', 'insertions': 1, 'deletions': 3, 'lines': 4}, {'filePath': 'common-parent/lazy/src/main/java/com/speedment/common/lazy/LazyInt.java', 'insertions': 1, 'deletions': 3, 'lines': 4}, {'filePath': 'common-parent/lazy/src/main/java/com/speedment/common/lazy/LazyLong.java', 'insertions': 1, 'deletions': 3, 'lines': 4}, {'filePath': 'common-parent/lazy/src/test/java/com/speedment/common/lazy/LazyIntegerTest.java', 'insertions': 7, 'deletions': 9, 'lines': 16}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'LazyDouble.java', 'spoonMethods': [{'spoonMethodName': 'com.speedment.common.lazy.LazyDouble.getOrCompute(java.util.function.DoubleSupplier)', 'TOT': 3, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 2}]}, {'spoonFilePath': 'LazyInt.java', 'spoonMethods': [{'spoonMethodName': 'com.speedment.common.lazy.LazyInt.getOrCompute(java.util.function.IntSupplier)', 'TOT': 3, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 2}]}, {'spoonFilePath': 'LazyLong.java', 'spoonMethods': [{'spoonMethodName': 'com.speedment.common.lazy.LazyLong.getOrCompute(java.util.function.LongSupplier)', 'TOT': 3, 'UPD': 0, 'INS': 0, 'MOV': 1, 'DEL': 2}]}, {'spoonFilePath': 'LazyIntegerTest.java', 'spoonMethods': [{'spoonMethodName': 'com.speedment.common.lazy.LazyIntegerTest.testConcurrency()', 'TOT': 3, 'UPD': 2, 'INS': 0, 'MOV': 0, 'DEL': 1}, {'spoonMethodName': 'com.speedment.common.lazy.LazyIntegerTest.makeFromThread(java.lang.Thread)', 'TOT': 1, 'UPD': 0, 'INS': 0, 'MOV': 0, 'DEL': 1}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/speedment/speedment/issues/234,194.00027777777777,"['bug', 'fixed']",Spurious fail in test,1.0,"['com.speedment.common.lazy.LazyInt.getOrCompute(java.util.function.IntSupplier)', 'com.speedment.common.lazy.LazyDouble.getOrCompute(java.util.function.DoubleSupplier)', 'com.speedment.common.lazy.LazyLong.getOrCompute(java.util.function.LongSupplier)']",['898e45c954ea31b4e4340bd16c540259d1369b96'],,['common-parent/lazy/src/main/java/com/speedment/common/lazy'],3.0,9.0,12.0,3.0,0.0,3.0,9.0,3.0,0.0,6.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,speedment
46513,2017-10-02 18:47:29,wilkinsona,"```
Full thread dump OpenJDK 64-Bit Server VM (25.141-b15 mixed mode):

""Attach Listener"" #41 daemon prio=9 os_prio=0 tid=0x00007f7d8c002000 nid=0xa331 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Thread-9"" #37 prio=5 os_prio=0 tid=0x00007f7d6c1e6800 nid=0x1fb waiting for monitor entry [0x00007f7da5cf6000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.springframework.context.support.AbstractApplicationContext$1.run(AbstractApplicationContext.java:933)
	- waiting to lock <0x00000000836b91b8> (a java.lang.Object)

""SIGTERM handler"" #40 daemon prio=9 os_prio=0 tid=0x00007f7d8c001000 nid=0x1fa in Object.wait() [0x00007f7da680a000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000db4526b0> (a org.springframework.context.support.AbstractApplicationContext$1)
	at java.lang.Thread.join(Thread.java:1252)
	- locked <0x00000000db4526b0> (a org.springframework.context.support.AbstractApplicationContext$1)
	at java.lang.Thread.join(Thread.java:1326)
	at java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:106)
	at java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:46)
	at java.lang.Shutdown.runHooks(Shutdown.java:123)
	at java.lang.Shutdown.sequence(Shutdown.java:167)
	at java.lang.Shutdown.exit(Shutdown.java:212)
	- locked <0x00000000837edf28> (a java.lang.Class for java.lang.Shutdown)
	at java.lang.Terminator$1.handle(Terminator.java:52)
	at sun.misc.Signal$1.run(Signal.java:212)
	at java.lang.Thread.run(Thread.java:748)

""Thread-6"" #20 prio=5 os_prio=0 tid=0x00007f7d5c002000 nid=0x1c1 in Object.wait() [0x00007f7da732f000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000da13f968> (a java.lang.Thread)
	at java.lang.Thread.join(Thread.java:1252)
	- locked <0x00000000da13f968> (a java.lang.Thread)
	at java.lang.Thread.join(Thread.java:1326)
	at org.springframework.boot.devtools.filewatch.FileSystemWatcher.stopAfter(FileSystemWatcher.java:194)
	- locked <0x00000000da0b6d18> (a java.lang.Object)
	at org.springframework.boot.devtools.filewatch.FileSystemWatcher.stop(FileSystemWatcher.java:177)
	at org.springframework.boot.devtools.classpath.ClassPathFileSystemWatcher.destroy(ClassPathFileSystemWatcher.java:95)
	at org.springframework.beans.factory.support.DisposableBeanAdapter.destroy(DisposableBeanAdapter.java:256)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroyBean(DefaultSingletonBeanRegistry.java:576)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroySingleton(DefaultSingletonBeanRegistry.java:552)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.destroySingleton(DefaultListableBeanFactory.java:953)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroySingletons(DefaultSingletonBeanRegistry.java:521)
	at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.destroySingletons(FactoryBeanRegistrySupport.java:227)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.destroySingletons(DefaultListableBeanFactory.java:960)
	at org.springframework.context.support.AbstractApplicationContext.destroyBeans(AbstractApplicationContext.java:1035)
	at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:1011)
	at org.springframework.context.support.AbstractApplicationContext.close(AbstractApplicationContext.java:963)
	- locked <0x00000000836b91b8> (a java.lang.Object)
	at org.springframework.boot.devtools.restart.Restarter.stop(Restarter.java:311)
	at org.springframework.boot.devtools.restart.Restarter.lambda$restart$1(Restarter.java:250)
	at org.springframework.boot.devtools.restart.Restarter$$Lambda$199/713682444.call(Unknown Source)
	at org.springframework.boot.devtools.restart.Restarter$LeakSafeThread.run(Restarter.java:617)

""DestroyJavaVM"" #38 prio=5 os_prio=0 tid=0x00007f7dc800c800 nid=0x184 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""http-nio-auto-1-35746-AsyncTimeout"" #36 daemon prio=5 os_prio=0 tid=0x00007f7d789d3800 nid=0x1bb waiting on condition [0x00007f7d3befd000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.coyote.AbstractProtocol$AsyncTimeout.run(AbstractProtocol.java:1211)
	at java.lang.Thread.run(Thread.java:748)

""http-nio-auto-1-Acceptor-0"" #35 daemon prio=5 os_prio=0 tid=0x00007f7d789d2000 nid=0x1ba runnable [0x00007f7d3bffe000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:422)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:250)
	- locked <0x00000000db2d9e68> (a java.lang.Object)
	at org.apache.tomcat.util.net.NioEndpoint$Acceptor.run(NioEndpoint.java:455)
	at java.lang.Thread.run(Thread.java:748)

""http-nio-auto-1-ClientPoller-1"" #34 daemon prio=5 os_prio=0 tid=0x00007f7d789cf800 nid=0x1b9 runnable [0x00007f7da414a000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
	- locked <0x00000000db381498> (a sun.nio.ch.Util$3)
	- locked <0x00000000db381488> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000db381370> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
	at org.apache.tomcat.util.net.NioEndpoint$Poller.run(NioEndpoint.java:793)
	at java.lang.Thread.run(Thread.java:748)

""http-nio-auto-1-ClientPoller-0"" #33 daemon prio=5 os_prio=0 tid=0x00007f7d789b5800 nid=0x1b8 runnable [0x00007f7da424b000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
	- locked <0x00000000db370b58> (a sun.nio.ch.Util$3)
	- locked <0x00000000db370b48> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000db370a30> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
	at org.apache.tomcat.util.net.NioEndpoint$Poller.run(NioEndpoint.java:793)
	at java.lang.Thread.run(Thread.java:748)

""http-nio-auto-1-exec-10"" #32 daemon prio=5 os_prio=0 tid=0x00007f7d78999800 nid=0x1b7 waiting on condition [0x00007f7da434c000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000db3100f8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:103)
	at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:31)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.lang.Thread.run(Thread.java:748)

""http-nio-auto-1-exec-9"" #31 daemon prio=5 os_prio=0 tid=0x00007f7d78997800 nid=0x1b6 waiting on condition [0x00007f7da444d000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000db3100f8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:103)
	at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:31)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.lang.Thread.run(Thread.java:748)

""http-nio-auto-1-exec-8"" #30 daemon prio=5 os_prio=0 tid=0x00007f7d78995800 nid=0x1b5 waiting on condition [0x00007f7da454e000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000db3100f8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:103)
	at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:31)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.lang.Thread.run(Thread.java:748)

""http-nio-auto-1-exec-7"" #29 daemon prio=5 os_prio=0 tid=0x00007f7d78993800 nid=0x1b4 waiting on condition [0x00007f7da464f000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000db3100f8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:103)
	at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:31)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.lang.Thread.run(Thread.java:748)

""http-nio-auto-1-exec-6"" #28 daemon prio=5 os_prio=0 tid=0x00007f7d78991800 nid=0x1b3 waiting on condition [0x00007f7da4750000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000db3100f8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:103)
	at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:31)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.lang.Thread.run(Thread.java:748)

""http-nio-auto-1-exec-5"" #27 daemon prio=5 os_prio=0 tid=0x00007f7d7898f800 nid=0x1b2 waiting on condition [0x00007f7da4851000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000db3100f8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:103)
	at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:31)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.lang.Thread.run(Thread.java:748)

""http-nio-auto-1-exec-4"" #26 daemon prio=5 os_prio=0 tid=0x00007f7d7898d800 nid=0x1b1 waiting on condition [0x00007f7da4952000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000db3100f8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:103)
	at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:31)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.lang.Thread.run(Thread.java:748)

""http-nio-auto-1-exec-3"" #25 daemon prio=5 os_prio=0 tid=0x00007f7d7898b800 nid=0x1b0 waiting on condition [0x00007f7da4a53000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000db3100f8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:103)
	at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:31)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.lang.Thread.run(Thread.java:748)

""http-nio-auto-1-exec-2"" #24 daemon prio=5 os_prio=0 tid=0x00007f7d7898a000 nid=0x1af waiting on condition [0x00007f7da4b54000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000db3100f8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:103)
	at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:31)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.lang.Thread.run(Thread.java:748)

""http-nio-auto-1-exec-1"" #23 daemon prio=5 os_prio=0 tid=0x00007f7d78988000 nid=0x1ae waiting on condition [0x00007f7da4c55000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000db3100f8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:103)
	at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:31)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.lang.Thread.run(Thread.java:748)

""NioBlockingSelector.BlockPoller-1"" #22 daemon prio=5 os_prio=0 tid=0x00007f7d7896a800 nid=0x1ad runnable [0x00007f7da4d56000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
	- locked <0x00000000db2db078> (a sun.nio.ch.Util$3)
	- locked <0x00000000db2daff0> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000db2dae70> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
	at org.apache.tomcat.util.net.NioBlockingSelector$BlockPoller.run(NioBlockingSelector.java:339)

""Live Reload Server"" #21 daemon prio=5 os_prio=0 tid=0x00007f7d7895c000 nid=0x1ac runnable [0x00007f7da4e57000]
   java.lang.Thread.State: RUNNABLE
	at java.net.PlainSocketImpl.socketAccept(Native Method)
	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)
	at java.net.ServerSocket.implAccept(ServerSocket.java:545)
	at java.net.ServerSocket.accept(ServerSocket.java:513)
	at org.springframework.boot.devtools.livereload.LiveReloadServer.acceptConnections(LiveReloadServer.java:146)
	at org.springframework.boot.devtools.livereload.LiveReloadServer$$Lambda$178/353302609.run(Unknown Source)
	at java.lang.Thread.run(Thread.java:748)

""File Watcher"" #19 daemon prio=5 os_prio=0 tid=0x00007f7d78950800 nid=0x1aa waiting for monitor entry [0x00007f7da4f58000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.springframework.boot.devtools.filewatch.FileSystemWatcher.stopAfter(FileSystemWatcher.java:186)
	- waiting to lock <0x00000000da0b6d18> (a java.lang.Object)
	at org.springframework.boot.devtools.filewatch.FileSystemWatcher.stop(FileSystemWatcher.java:177)
	at org.springframework.boot.devtools.classpath.ClassPathFileChangeListener.publishEvent(ClassPathFileChangeListener.java:70)
	at org.springframework.boot.devtools.classpath.ClassPathFileChangeListener.onChange(ClassPathFileChangeListener.java:64)
	at org.springframework.boot.devtools.filewatch.FileSystemWatcher$Watcher.fireListeners(FileSystemWatcher.java:305)
	at org.springframework.boot.devtools.filewatch.FileSystemWatcher$Watcher.updateSnapshots(FileSystemWatcher.java:298)
	at org.springframework.boot.devtools.filewatch.FileSystemWatcher$Watcher.scan(FileSystemWatcher.java:258)
	at org.springframework.boot.devtools.filewatch.FileSystemWatcher$Watcher.run(FileSystemWatcher.java:238)
	at java.lang.Thread.run(Thread.java:748)

""container-0"" #18 prio=5 os_prio=0 tid=0x00007f7d788c4800 nid=0x1a6 waiting on condition [0x00007f7da5259000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.catalina.core.StandardServer.await(StandardServer.java:427)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer$1.run(TomcatWebServer.java:170)

""ContainerBackgroundProcessor[StandardEngine[Tomcat]]"" #17 daemon prio=5 os_prio=0 tid=0x00007f7d788c2000 nid=0x1a4 waiting on condition [0x00007f7da535a000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.run(ContainerBase.java:1355)
	at java.lang.Thread.run(Thread.java:748)

""Service Thread"" #8 daemon prio=9 os_prio=0 tid=0x00007f7dc80bd000 nid=0x190 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C1 CompilerThread2"" #7 daemon prio=9 os_prio=0 tid=0x00007f7dc80b5800 nid=0x18f waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread1"" #6 daemon prio=9 os_prio=0 tid=0x00007f7dc80b4000 nid=0x18e waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread0"" #5 daemon prio=9 os_prio=0 tid=0x00007f7dc80b1000 nid=0x18d waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Signal Dispatcher"" #4 daemon prio=9 os_prio=0 tid=0x00007f7dc80af000 nid=0x18c runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007f7dc8086800 nid=0x18b in Object.wait() [0x00007f7da7cfb000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x000000008362dd98> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
	- locked <0x000000008362dd98> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)
	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)

""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007f7dc8081800 nid=0x18a in Object.wait() [0x00007f7da7dfc000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x000000008362dd00> (a java.lang.ref.Reference$Lock)
	at java.lang.Object.wait(Object.java:502)
	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
	- locked <0x000000008362dd00> (a java.lang.ref.Reference$Lock)
	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)

""VM Thread"" os_prio=0 tid=0x00007f7dc807a000 nid=0x189 runnable 

""GC task thread#0 (ParallelGC)"" os_prio=0 tid=0x00007f7dc8022000 nid=0x185 runnable 

""GC task thread#1 (ParallelGC)"" os_prio=0 tid=0x00007f7dc8023800 nid=0x186 runnable 

""GC task thread#2 (ParallelGC)"" os_prio=0 tid=0x00007f7dc8025800 nid=0x187 runnable 

""GC task thread#3 (ParallelGC)"" os_prio=0 tid=0x00007f7dc8027000 nid=0x188 runnable 

""VM Periodic Task Thread"" os_prio=0 tid=0x00007f7dc80bf800 nid=0x191 waiting on condition 

JNI global references: 694
```

The two interesting threads are:

```
""File Watcher"" #19 daemon prio=5 os_prio=0 tid=0x00007f7d78950800 nid=0x1aa waiting for monitor entry [0x00007f7da4f58000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.springframework.boot.devtools.filewatch.FileSystemWatcher.stopAfter(FileSystemWatcher.java:186)
	- waiting to lock <0x00000000da0b6d18> (a java.lang.Object)
	at org.springframework.boot.devtools.filewatch.FileSystemWatcher.stop(FileSystemWatcher.java:177)
	at org.springframework.boot.devtools.classpath.ClassPathFileChangeListener.publishEvent(ClassPathFileChangeListener.java:70)
	at org.springframework.boot.devtools.classpath.ClassPathFileChangeListener.onChange(ClassPathFileChangeListener.java:64)
	at org.springframework.boot.devtools.filewatch.FileSystemWatcher$Watcher.fireListeners(FileSystemWatcher.java:305)
	at org.springframework.boot.devtools.filewatch.FileSystemWatcher$Watcher.updateSnapshots(FileSystemWatcher.java:298)
	at org.springframework.boot.devtools.filewatch.FileSystemWatcher$Watcher.scan(FileSystemWatcher.java:258)
	at org.springframework.boot.devtools.filewatch.FileSystemWatcher$Watcher.run(FileSystemWatcher.java:238)
	at java.lang.Thread.run(Thread.java:748)
```

And:

```
""Thread-6"" #20 prio=5 os_prio=0 tid=0x00007f7d5c002000 nid=0x1c1 in Object.wait() [0x00007f7da732f000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000da13f968> (a java.lang.Thread)
	at java.lang.Thread.join(Thread.java:1252)
	- locked <0x00000000da13f968> (a java.lang.Thread)
	at java.lang.Thread.join(Thread.java:1326)
	at org.springframework.boot.devtools.filewatch.FileSystemWatcher.stopAfter(FileSystemWatcher.java:194)
	- locked <0x00000000da0b6d18> (a java.lang.Object)
	at org.springframework.boot.devtools.filewatch.FileSystemWatcher.stop(FileSystemWatcher.java:177)
	at org.springframework.boot.devtools.classpath.ClassPathFileSystemWatcher.destroy(ClassPathFileSystemWatcher.java:95)
	at org.springframework.beans.factory.support.DisposableBeanAdapter.destroy(DisposableBeanAdapter.java:256)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroyBean(DefaultSingletonBeanRegistry.java:576)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroySingleton(DefaultSingletonBeanRegistry.java:552)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.destroySingleton(DefaultListableBeanFactory.java:953)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroySingletons(DefaultSingletonBeanRegistry.java:521)
	at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.destroySingletons(FactoryBeanRegistrySupport.java:227)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.destroySingletons(DefaultListableBeanFactory.java:960)
	at org.springframework.context.support.AbstractApplicationContext.destroyBeans(AbstractApplicationContext.java:1035)
	at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:1011)
	at org.springframework.context.support.AbstractApplicationContext.close(AbstractApplicationContext.java:963)
	- locked <0x00000000836b91b8> (a java.lang.Object)
	at org.springframework.boot.devtools.restart.Restarter.stop(Restarter.java:311)
	at org.springframework.boot.devtools.restart.Restarter.lambda$restart$1(Restarter.java:250)
	at org.springframework.boot.devtools.restart.Restarter$$Lambda$199/713682444.call(Unknown Source)
	at org.springframework.boot.devtools.restart.Restarter$LeakSafeThread.run(Restarter.java:617)
```",2017-10-02 19:10:20,"[{'commitHash': '71c15cb65e93eec439d3f86ab4b22986b64f4a6b', 'commitGHEventType': 'closed', 'commitUser': 'wilkinsona', 'commitParents': ['205c25bf0f5b84cfe757ae33625a2bf3b4ad42f2'], 'nameRev': '71c15cb65e93eec439d3f86ab4b22986b64f4a6b tags/v2.0.0.M5~7^2~5', 'commitMessage': ""Avoid possible livelock when stopping FileSystemWatcher in parallel\n\nPreviously, if the file watcher thread tried to stop the\nFileSystemWatcher when another thread was already stopping it a\nlivelock could occur. The livelock occurred because the file watcher\nthread would attempt to lock a monitor that was being held by a thread\nthat had joined the file watcher thread and was waiting for it to die.\n\nThis commit avoid the livelock by narrowing the synchronization that's\nused when stopping the FileSystemWatcher. The monitor is used to\nobtain a reference to the file watcher thread in a thread-safe manner,\nbut it is released prior to joining the file watcher thread and\nwaiting for it to die. This will allow a parallel attempt by the\nfile watcher thread to stop itself to succeed.\n\nCloses gh-10496\n"", 'commitDateTime': '2017-10-02 20:03:17', 'authoredDateTime': '2017-10-02 20:03:17', 'commitGitStats': [{'filePath': 'spring-boot-devtools/src/main/java/org/springframework/boot/devtools/filewatch/FileSystemWatcher.java', 'insertions': 12, 'deletions': 11, 'lines': 23}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'FileSystemWatcher.java', 'spoonMethods': [{'spoonMethodName': 'org.springframework.boot.devtools.filewatch.FileSystemWatcher.stopAfter(int)', 'TOT': 9, 'UPD': 0, 'INS': 2, 'MOV': 6, 'DEL': 1}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/spring-projects/spring-boot/issues/10496,0.0002777777777777778,['type: bug'],DevTools can live lock during stop/restart,1.0,['org.springframework.boot.devtools.filewatch.FileSystemWatcher.stopAfter(int)'],['71c15cb65e93eec439d3f86ab4b22986b64f4a6b'],,['spring-boot-devtools/src/main/java/org/springframework/boot/devtools/filewatch'],12.0,11.0,23.0,1.0,0.0,1.0,9.0,6.0,2.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,spring-boot
47363,2014-06-09 18:23:46,wilkinsona,"If you run an app and then kill it with `CTRL-C` it may report an NPE during shutdown. Here's an example from using `mvn spring-boot:run`:

```
[ERROR] Failed to execute goal org.springframework.boot:spring-boot-maven-plugin:1.1.0.BUILD-SNAPSHOT:run (default-cli) on project spring-boot-sample-ws: Could not exec java: NullPointerException -> [Help 1]

org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.springframework.boot:spring-boot-maven-plugin:1.1.0.BUILD-SNAPSHOT:run (default-cli) on project spring-boot-sample-ws: Could not exec java
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:216)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:108)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:76)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:116)
    at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:361)
    at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
    at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
    at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:213)
    at org.apache.maven.cli.MavenCli.main(MavenCli.java:157)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.plugin.MojoExecutionException: Could not exec java
    at org.springframework.boot.maven.RunMojo.run(RunMojo.java:172)
    at org.springframework.boot.maven.RunMojo.execute(RunMojo.java:134)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:133)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
    ... 19 more
Caused by: java.lang.NullPointerException
    at org.springframework.boot.loader.tools.RunProcess.run(RunProcess.java:78)
    at org.springframework.boot.loader.tools.RunProcess.run(RunProcess.java:52)
    at org.springframework.boot.maven.RunMojo.run(RunMojo.java:168)
    ... 22 more
```

The problem is that there's a race between the signal handler setting `this.process` to `null` on one thread while another thread in `run()` that was blocked on `this.process.waitFor()` then tries to call `this.process.exitValue()`. If the signal handling thread gets in first and manages to null out `this.process` the call to `exitValue()` will NPE.

The fix is to remove the call to `this.process.exitValue()` from `run()` and use the value returned from `this.process.waitFor()` instead.
",2014-06-09 20:42:48,"[{'commitHash': '02de6e35316f5325c49e398744408e9ea6a39d92', 'commitGHEventType': 'closed', 'commitUser': 'wilkinsona', 'commitParents': ['0b7836b447e7d09ecbdc049deddb2709b618d995'], 'nameRev': '02de6e35316f5325c49e398744408e9ea6a39d92 tags/v1.1.0.RELEASE~20', 'commitMessage': 'Remove signal handling race condition from RunProcess\n\nFixes #1061\n', 'commitDateTime': '2014-06-09 21:29:33', 'authoredDateTime': '2014-06-09 21:29:33', 'commitGitStats': [{'filePath': 'spring-boot-tools/spring-boot-loader-tools/src/main/java/org/springframework/boot/loader/tools/RunProcess.java', 'insertions': 6, 'deletions': 9, 'lines': 15}], 'commitSpoonAstDiffStats': [{'spoonFilePath': 'RunProcess.java', 'spoonMethods': [{'spoonMethodName': 'org.springframework.boot.loader.tools.RunProcess.run(java.util.Collection)', 'TOT': 9, 'UPD': 1, 'INS': 3, 'MOV': 2, 'DEL': 3}]}], 'spoonStatsSkippedReason': ''}]",https://github.com/spring-projects/spring-boot/issues/1061,0.0002777777777777778,['type: bug'],RunProcess signal handling isn't thread-safe,1.0,['org.springframework.boot.loader.tools.RunProcess.run(java.util.Collection)'],['02de6e35316f5325c49e398744408e9ea6a39d92'],,['spring-boot-tools/spring-boot-loader-tools/src/main/java/org/springframework/boot/loader/tools'],6.0,9.0,15.0,1.0,1.0,1.0,9.0,2.0,3.0,3.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,spring-boot
