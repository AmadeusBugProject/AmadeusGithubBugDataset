{"bugLabels": ["Bug"], "projectName": "apache/druid", "issues": [{"user": "leedohyun", "commits": {}, "labels": ["Area - SQL", "Bug"], "created": "2020-06-23 22:37:37", "title": "Druid SQL multi-lingual decoded problem", "url": "https://github.com/apache/druid/issues/10064", "closed": "2020-06-25 02:58:35", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "Hi! I'm using 0.18.1.\r\n\r\nWhen I executed sql with Korean characters, I got the empty result.\r\nSo, I debugged the broker code and there was the problem as follows.\r\n[https://issues.apache.org/jira/browse/CALCITE-2704](https://issues.apache.org/jira/browse/CALCITE-2704)\r\n\r\nWe need to bump to avatica-0.17.0.\r\nI'd like to ask you to review.\r\n"}, {"user": "JackDavidson", "commits": {"33a37d85d7ae3155e5f5d38814feade8e08e1d71": {"commitGHEventType": "referenced", "commitUser": "ccaominh"}}, "labels": ["Area - Batch Ingestion", "Bug"], "title": "index_parallel with single_dim partitionSpec type generating just one file/segment", "created": "2020-06-20 00:48:28", "closed": "2020-06-30 00:49:53", "spoonStatsSummary": {}, "statsSkippedReason": "", "filteredCommits": [], "url": "https://github.com/apache/druid/issues/10057", "filteredCommitsReason": {"multipleIssueFixes": 0, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 1, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 10.000277777777777, "commitsDetails": [{"commitGitStats": [], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "", "authoredDateTime": "", "commitMessage": "", "commitUser": "ccaominh", "commitDateTime": "", "commitParents": [], "commitGHEventType": "referenced", "nameRev": "", "commitHash": "33a37d85d7ae3155e5f5d38814feade8e08e1d71"}], "body": "### Affected Version\r\nDruid Built From Source as of Fri Jun 5, 2020\r\n\r\n### Description\r\n\r\nWe are trying to create new druid ingestion specs to pull from s3 directly via index_parallel rather than hadoop. The outputs simply don't seem to be partitioned though.\r\n\r\nTo make it easy to reproduce, here is a simple config that shows the issue:\r\n\r\n```\r\n{\r\n  \"spec\": {\r\n    \"type\": \"index_parallel\",\r\n    \"ioConfig\": {\r\n      \"type\": \"index_parallel\",\r\n      \"inputSource\": {\r\n        \"type\": \"http\",\r\n        \"uris\": [\r\n          \"https://druid.apache.org/data/wikipedia.json.gz\",\r\n          \"https://druid.apache.org/data/wikipedia.json.gz\"\r\n        ]\r\n      },\r\n      \"inputFormat\": {\r\n        \"type\": \"json\"\r\n      }\r\n    },\r\n    \"tuningConfig\": {\r\n      \"type\": \"index_parallel\",\r\n      \"partitionsSpec\": {\r\n        \"type\": \"single_dim\",\r\n        \"partitionDimension\": \"channel\",\r\n        \"maxRowsPerSegment\": 2000\r\n      },\r\n      \"forceGuaranteedRollup\": true,\r\n      \"maxNumConcurrentSubTasks\": 4\r\n    },\r\n    \"dataSchema\": {\r\n      \"dataSource\": \"wikipedia-test-partitioned-2\",\r\n      \"granularitySpec\": {\r\n        \"type\": \"uniform\",\r\n        \"segmentGranularity\": \"DAY\",\r\n        \"queryGranularity\": \"HOUR\",\r\n        \"rollup\": true,\r\n        \"intervals\": [\r\n          \"2000-01-01/2030-01-01\"\r\n        ]\r\n      },\r\n      \"timestampSpec\": {\r\n        \"column\": \"timestamp\",\r\n        \"format\": \"iso\"\r\n      },\r\n      \"dimensionsSpec\": {\r\n        \"dimensions\": [\r\n          \"channel\",\r\n          \"cityName\",\r\n          \"comment\",\r\n          \"countryIsoCode\",\r\n          \"countryName\",\r\n          \"diffUrl\",\r\n          \"flags\",\r\n          \"isAnonymous\",\r\n          \"isMinor\",\r\n          \"isNew\",\r\n          \"isRobot\",\r\n          \"isUnpatrolled\",\r\n          \"metroCode\",\r\n          \"namespace\",\r\n          \"page\",\r\n          \"regionIsoCode\",\r\n          \"regionName\",\r\n          \"user\"\r\n        ]\r\n      },\r\n      \"metricsSpec\": [\r\n        {\r\n          \"name\": \"count\",\r\n          \"type\": \"count\"\r\n        },\r\n        {\r\n          \"name\": \"sum_added\",\r\n          \"type\": \"longSum\",\r\n          \"fieldName\": \"added\"\r\n        },\r\n        {\r\n          \"name\": \"sum_commentLength\",\r\n          \"type\": \"longSum\",\r\n          \"fieldName\": \"commentLength\"\r\n        },\r\n        {\r\n          \"name\": \"sum_deleted\",\r\n          \"type\": \"longSum\",\r\n          \"fieldName\": \"deleted\"\r\n        },\r\n        {\r\n          \"name\": \"sum_delta\",\r\n          \"type\": \"longSum\",\r\n          \"fieldName\": \"delta\"\r\n        },\r\n        {\r\n          \"name\": \"sum_deltaBucket\",\r\n          \"type\": \"longSum\",\r\n          \"fieldName\": \"deltaBucket\"\r\n        }\r\n      ]\r\n    }\r\n  },\r\n  \"type\": \"index_parallel\"\r\n}\r\n```\r\n\r\nSince maxRowsPerSegment is 2,000 and there are 24,000 rows in the dateset, I was expecting many partitions.\r\n\r\nI made sure to set two files so that it could be parallelized, since I saw some comments about needing to set maxNumConcurrentSubTasks and figured this might also apply to needing multiple input files.\r\n\r\nOf course the data that I have is much larger, coming out to a few GB, but I'm seeing the exact same issue.\r\n\r\nI have tried both setting targetRowsPerSegment and maxRowsPerSegment, and neither work.\r\n"}, {"user": "suneet-s", "commits": {}, "labels": ["Area - SQL", "Bug"], "created": "2020-06-03 06:42:08", "title": "SQL SELECT with IN filter containing null value throws ISE", "url": "https://github.com/apache/druid/issues/9977", "closed": "2020-06-17 08:12:42", "ttf": 14.000277777777777, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.19\r\n\r\n### Description\r\n\r\nThe following query \r\n\r\n```\r\nselect * from druid.foo where dim2 in ('a', null)\r\n```\r\n\r\nfails to plan with this exception\r\n\r\n```\r\nUnknown exception / Cannot build plan for query: select * from druid.foo where dim2 in ('a', null) / org.apache.druid.java.util.common.ISE\r\n```\r\n\r\nThis query should be able to use an in dim filter with a null value, but the Calcite planning phase fails with this error\r\n\r\n```\r\nMissing conversion is LogicalSort[convention: NONE -> DRUID]\r\nThere is 1 empty subset: rel#4087:Subset#5.DRUID.[], the relevant part of the original plan is as follows\r\n4085:LogicalSort(fetch=[100:BIGINT])\r\n  4083:LogicalProject(subset=[rel#4084:Subset#4.NONE.[]], __time=[$0], dim1=[$1], dim2=[$2], dim3=[$3], m1=[$4], m2=[$5])\r\n    4081:LogicalJoin(subset=[rel#4082:Subset#3.NONE.[]], condition=[=($2, $6)], joinType=[inner])\r\n      4050:LogicalTableScan(subset=[rel#4077:Subset#0.NONE.[]], table=[[druid, foo]])\r\n      4051:LogicalValues(subset=[rel#4098:Subset#1.NONE.[]], tuples=[[{ 'a' }, { null }]])\r\n```"}, {"user": "suneet-s", "commits": {}, "labels": ["Area - Null Handling", "Area - SQL", "Bug"], "created": "2020-06-03 06:20:16", "title": "join condition on column = null does not work as expected in non sql compatible mode", "url": "https://github.com/apache/druid/issues/9976", "closed": "2020-06-03 21:38:17", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.19\r\n\r\n### Description\r\n\r\nDruid does not support joining against a condition that evaluates against a constant - see #9941, however if you write a SQL query where the join condition is of the type `key = NULL`, Calcite optimizes this condition to false. This is correct in SQL compatible mode, but in non sql compatible mode (default) this condition should be evaluated to `key = ''` which is not supported.\r\n\r\nNOTE: queries with the shape `table1 inner join table2 on table1.key IS NULL` and `table1 inner join table2 on table1.key = ''` do not plan as expected.\r\n\r\nThe following CalciteQueryTest shows the issue.\r\n```\r\n @Test\r\n  @Parameters(source = QueryContextForJoinProvider.class)\r\n  public void testSelectOnLookupUsingInnerJoinOnDimEqualsNullOperator(Map<String, Object> queryContext) throws Exception\r\n  {\r\n    testQuery(\r\n        \"SELECT dim2, lookyloo.k\\n\"\r\n        + \"FROM foo INNER JOIN lookup.lookyloo ON foo.dim2 = null\\n\",\r\n        queryContext,\r\n        ImmutableList.of(\r\n            newScanQueryBuilder()\r\n                .dataSource(\r\n                    join(\r\n                        new TableDataSource(CalciteTests.DATASOURCE1),\r\n                        new LookupDataSource(\"lookyloo\"),\r\n                        \"j0.\",\r\n                        equalsCondition(DruidExpression.fromColumn(\"dim2\"), DruidExpression.fromColumn(\"j0.k\")),\r\n                        JoinType.INNER\r\n                    )\r\n                )\r\n                .intervals(querySegmentSpec(Filtration.eternity()))\r\n                .columns(\"dim2\", \"j0.k\", \"j0.v\")\r\n                .context(queryContext)\r\n                .build()\r\n        ),\r\n        NullHandling.sqlCompatible() ?\r\n        ImmutableList.of() :\r\n        ImmutableList.of(\r\n            new Object[]{\"\", \"a\"},\r\n            new Object[]{\"\", \"abc\"},\r\n            new Object[]{\"\", \"nosuchkey\"},\r\n            new Object[]{\"\", \"6\"}\r\n        )\r\n    );\r\n  }\r\n```\r\n"}, {"user": "jihoonson", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug"], "created": "2020-06-03 01:19:59", "title": "Segments created by an overwriting task with dynamic partitioning doesn't atomically overshadow old segments in the overlapping time chunk", "url": "https://github.com/apache/druid/issues/9972", "closed": "2020-06-13 04:39:38", "ttf": 10.000277777777777, "commitsDetails": [], "body": "### Affected Version\r\n\r\nAll versions\r\n\r\n### Description\r\n\r\nThe overwriting task such as the compaction task with the dynamic partitioning creates segments with `NumberedShardSpec`. This `NumberedShardSpec` has the `partitions` field indicating the core partition size. However, it is always set to 0 for batch tasks. Since the core partition size is 0, the segment update in the broker/coordinator server view is not guaranteed to be done atomically even in the overlapping time chunk. Instead, a new segment will overshadow immediately when a first historical announces a new segment. As a result, some queries can return less results than normal until all new segments are available in historicals.\r\n\r\nThis issue doesn't exist with hash or range partitioning."}, {"user": "vogievetsky", "commits": {}, "labels": ["Area - Querying", "Bug"], "created": "2020-05-21 04:31:23", "title": "Stack overflow with SELECT ARRAY ['Hello', NULL]", "url": "https://github.com/apache/druid/issues/9906", "closed": "2020-07-02 00:48:10", "ttf": 41.000277777777775, "commitsDetails": [], "body": "### Affected Version\r\n\r\nDruid 0.18.0\r\n\r\n### Description\r\n\r\nThe query `SELECT ARRAY ['Hello', NULL]` returns a (HTML formatted) stack overflow\r\n\r\n![image](https://user-images.githubusercontent.com/177816/82523420-0aef5e80-9ae1-11ea-9eff-7295a4825fb9.png)\r\n\r\nNote that `SELECT ARRAY [NULL]` works fine\r\n\r\nThe error (truncated)\r\n\r\n```\r\n<html>\r\n<head>\r\n<meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\"/>\r\n<title>Error 500 Server Error</title>\r\n</head>\r\n<body><h2>HTTP ERROR 500</h2>\r\n<p>Problem accessing /druid/v2/sql. Reason:\r\n<pre>    Server Error</pre></p><h3>Caused by:</h3><pre>javax.servlet.ServletException: java.lang.StackOverflowError\r\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:420)\r\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)\r\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)\r\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\r\n\tat com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:286)\r\n\tat com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:276)\r\n\tat com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:181)\r\n\tat com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)\r\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:120)\r\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:135)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\r\n\tat org.apache.druid.server.security.PreResponseAuthorizationCheckFilter.doFilter(PreResponseAuthorizationCheckFilter.java:82)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\r\n\tat org.apache.druid.server.security.AllowOptionsResourceFilter.doFilter(AllowOptionsResourceFilter.java:75)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\r\n\tat org.apache.druid.server.security.AllowAllAuthenticator$1.doFilter(AllowAllAuthenticator.java:84)\r\n\tat org.apache.druid.server.security.AuthenticationWrappingFilter.doFilter(AuthenticationWrappingFilter.java:59)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\r\n\tat org.apache.druid.server.security.SecuritySanityCheckFilter.doFilter(SecuritySanityCheckFilter.java:86)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\r\n\tat org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:740)\r\n\tat org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:61)\r\n\tat org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:174)\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\r\n\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\r\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\r\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\r\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\r\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\r\n\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\r\n\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.StackOverflowError\r\n\tat java.util.regex.Pattern.error(Pattern.java:1969)\r\n\tat java.util.regex.Pattern.&lt;init&gt;(Pattern.java:1354)\r\n\tat java.util.regex.Pattern.compile(Pattern.java:1054)\r\n\tat java.lang.String.replace(String.java:2239)\r\n\tat org.apache.calcite.sql.SqlDialect.quoteStringLiteral(SqlDialect.java:430)\r\n\tat org.apache.calcite.util.NlsString.asSql(NlsString.java:231)\r\n\tat org.apache.calcite.util.NlsString.asSql(NlsString.java:214)\r\n\tat org.apache.calcite.rex.RexLiteral.appendAsJava(RexLiteral.java:597)\r\n\tat org.apache.calcite.rex.RexLiteral.toJavaString(RexLiteral.java:385)\r\n\tat org.apache.calcite.rex.RexLiteral.computeDigest(RexLiteral.java:272)\r\n\tat org.apache.calcite.rex.RexLiteral.&lt;init&gt;(RexLiteral.java:219)\r\n\tat org.apache.calcite.rex.RexBuilder.makeLiteral(RexBuilder.java:954)\r\n\tat org.apache.calcite.rex.RexBuilder.makeCharLiteral(RexBuilder.java:1137)\r\n\tat org.apache.calcite.rex.RexBuilder.makeLiteral(RexBuilder.java:1367)\r\n\tat org.apache.calcite.rex.RexBuilder.makeLiteral(RexBuilder.java:1359)\r\n\tat org.apache.calcite.rex.RexBuilder.makeLiteral(RexBuilder.java:1443)\r\n\tat org.apache.druid.sql.calcite.planner.DruidRexExecutor.reduce(DruidRexExecutor.java:132)\r\n\tat org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressionsInternal(ReduceExpressionsRule.java:695)\r\n\tat org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressions(ReduceExpressionsRule.java:616)\r\n\tat org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:301)\r\n\tat org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:319)\r\n\tat org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:560)\r\n\tat org.apache.calcite.plan.hep.HepPlanner.depthFirstApply(HepPlanner.java:374)\r\n\tat org.apache.calcite.plan.hep.HepPlanner.depthFirstApply(HepPlanner.java:386)\r\n\tat org.apache.calcite.plan.hep.HepPlanner.depthFirstApply(HepPlanner.java:386)\r\n\tat org.apache.calcite.plan.hep.HepPlanner.depthFirstApply(HepPlanner.java:386)\r\n\tat org.apache.calcite.plan.hep.HepPlanner.depthFirstApply(HepPlanner.java:386)\r\n\tat org.apache.calcite.plan.hep.HepPlanner.depthFirstApply(HepPlanner.java:386)\r\n\tat org.apache.calcite.plan.hep.HepPlanner.depthFirstApply(HepPlanner.java:386)\r\n\tat org.apache.calcite.plan.hep.HepPlanner.depthFirstApply(HepPlanner.java:386)\r\n\tat org.apache.calcite.plan.hep.HepPlanner.depthFirstApply(HepPlanner.java:386)\r\n\tat org.apache.calcite.plan.hep.HepPlanner.depthFirstApply(HepPlanner.java:386)\r\n\tat org.apache.calcite.plan.hep.HepPlanner.depthFirstApply(HepPlanner.java:386)\r\n\tat org.apache.calcite.plan.hep.HepPlanner.depthFirstApply(HepPlanner.java:386)\r\n\tat org.apache.calcite.plan.hep.HepPlanner.depthFirstApply(HepPlanner.java:386)\r\n\tat org.apache.calcite.plan.hep.HepPlanner.depthFirstApply(HepPlanner.java:386)\r\n\tat org.apache.calcite.plan.hep.HepPlanner.depthFirstApply(HepPlanner.java:386)\r\n\tat org.apache.calcite.plan.hep.HepPlanner.depthFirstApply(HepPlanner.java:386)\r\n\tat org.apache.calcite.plan.hep.HepPlanner.depthFirstApply(HepPlanner.java:386)\r\n\tat org.apache.calcite.plan.hep.HepPlanner.depthFirstApply(HepPlanner.java:386)\r\n        ...\r\n```"}, {"user": "yuanlihan", "commits": {}, "labels": ["Bug"], "created": "2020-05-21 02:23:27", "title": "Fail to compact overlapping segments", "url": "https://github.com/apache/druid/issues/9904", "closed": "2020-06-08 16:54:40", "ttf": 18.00027777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\nFound in 0.16.x and 0.18.x.\r\n\r\n### Description\r\n\r\nCompaction task will create segments with `VersionedIntervalTimeline` conflicting when compacting overlapping segments without explicitly specifying segment granularity. Coordinator will constantly fails to poll latest segment records from metadata db when there is an error caused by `VersionedIntervalTimeline` conflicting. And therefore the indexing tasks will constantly fail on segment publishing stage.\r\n\r\n### Bug duplication\r\n1. create a segment with `DAY` segment granularity.\r\n```json\r\n\r\n{\r\n    \"type\": \"index\",\r\n    \"ioConfig\": {\r\n        \"type\": \"index\",\r\n        \"firehose\": {\r\n            \"type\": \"inline\",\r\n            \"data\": \"{\\\"timestamp\\\":\\\"2020-05-18T00:00:00.000Z\\\",\\\"col\\\":\\\"col_1\\\",\\\"val\\\":1}\\n{\\\"timestamp\\\":\\\"2020-05-18T01:00:00.000Z\\\",\\\"col\\\":\\\"col_2\\\",\\\"val\\\":2}\\n{\\\"timestamp\\\":\\\"2020-05-18T02:00:00.000Z\\\",\\\"col\\\":\\\"col_3\\\",\\\"val\\\":3}\\n{\\\"timestamp\\\":\\\"2020-05-18T03:00:00.000Z\\\",\\\"col\\\":\\\"col_4\\\",\\\"val\\\":4}\\n{\\\"timestamp\\\":\\\"2020-05-18T04:00:00.000Z\\\",\\\"col\\\":\\\"col_5\\\",\\\"val\\\":5}\\n{\\\"timestamp\\\":\\\"2020-05-18T05:00:00.000Z\\\",\\\"col\\\":\\\"col_6\\\",\\\"val\\\":6}\\n{\\\"timestamp\\\":\\\"2020-05-18T06:00:00.000Z\\\",\\\"col\\\":\\\"col_7\\\",\\\"val\\\":7}\\n{\\\"timestamp\\\":\\\"2020-05-18T07:00:00.000Z\\\",\\\"col\\\":\\\"col_8\\\",\\\"val\\\":8}\\n{\\\"timestamp\\\":\\\"2020-05-18T08:00:00.000Z\\\",\\\"col\\\":\\\"col_9\\\",\\\"val\\\":9}\\n{\\\"timestamp\\\":\\\"2020-05-18T09:00:00.000Z\\\",\\\"col\\\":\\\"col_10\\\",\\\"val\\\":10}\\n{\\\"timestamp\\\":\\\"2020-05-18T10:00:00.000Z\\\",\\\"col\\\":\\\"col_11\\\",\\\"val\\\":11}\\n{\\\"timestamp\\\":\\\"2020-05-18T11:00:00.000Z\\\",\\\"col\\\":\\\"col_12\\\",\\\"val\\\":12}\\n{\\\"timestamp\\\":\\\"2020-05-18T12:00:00.000Z\\\",\\\"col\\\":\\\"col_13\\\",\\\"val\\\":13}\\n{\\\"timestamp\\\":\\\"2020-05-18T13:00:00.000Z\\\",\\\"col\\\":\\\"col_14\\\",\\\"val\\\":14}\\n{\\\"timestamp\\\":\\\"2020-05-18T14:00:00.000Z\\\",\\\"col\\\":\\\"col_15\\\",\\\"val\\\":15}\\n{\\\"timestamp\\\":\\\"2020-05-18T15:00:00.000Z\\\",\\\"col\\\":\\\"col_16\\\",\\\"val\\\":16}\\n{\\\"timestamp\\\":\\\"2020-05-18T16:00:00.000Z\\\",\\\"col\\\":\\\"col_17\\\",\\\"val\\\":17}\\n{\\\"timestamp\\\":\\\"2020-05-18T17:00:00.000Z\\\",\\\"col\\\":\\\"col_18\\\",\\\"val\\\":18}\\n{\\\"timestamp\\\":\\\"2020-05-18T18:00:00.000Z\\\",\\\"col\\\":\\\"col_19\\\",\\\"val\\\":19}\\n{\\\"timestamp\\\":\\\"2020-05-18T19:00:00.000Z\\\",\\\"col\\\":\\\"col_20\\\",\\\"val\\\":20}\\n{\\\"timestamp\\\":\\\"2020-05-18T20:00:00.000Z\\\",\\\"col\\\":\\\"col_21\\\",\\\"val\\\":21}\\n{\\\"timestamp\\\":\\\"2020-05-18T21:00:00.000Z\\\",\\\"col\\\":\\\"col_22\\\",\\\"val\\\":22}\\n{\\\"timestamp\\\":\\\"2020-05-18T22:00:00.000Z\\\",\\\"col\\\":\\\"col_23\\\",\\\"val\\\":23}\\n{\\\"timestamp\\\":\\\"2020-05-18T23:00:00.000Z\\\",\\\"col\\\":\\\"col_24\\\",\\\"val\\\":24}\"\r\n        }\r\n    },\r\n    \"tuningConfig\": {\r\n        \"type\": \"index\"\r\n    },\r\n    \"dataSchema\": {\r\n        \"dataSource\": \"inline_data\",\r\n        \"granularitySpec\": {\r\n            \"type\": \"uniform\",\r\n            \"segmentGranularity\": \"DAY\",\r\n            \"queryGranularity\": \"NONE\",\r\n            \"rollup\": false\r\n        },\r\n        \"parser\": {\r\n            \"type\": \"string\",\r\n            \"parseSpec\": {\r\n                \"format\": \"json\",\r\n                \"timestampSpec\": {\r\n                    \"column\": \"timestamp\",\r\n                    \"format\": \"iso\"\r\n                },\r\n                \"dimensionsSpec\": {\r\n                    \"dimensions\": [\r\n                        \"col\",\r\n                        {\r\n                            \"type\": \"long\",\r\n                            \"name\": \"val\"\r\n                        }\r\n                    ]\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n2. create a segment with `HOUR` segment granularity that overlaps the previous segment with `DAY` segment granularity.\r\n```json\r\n\r\n{\r\n    \"type\": \"index\",\r\n    \"ioConfig\": {\r\n        \"type\": \"index\",\r\n        \"firehose\": {\r\n            \"type\": \"inline\",\r\n            \"data\": \"{\\\"timestamp\\\":\\\"2020-05-18T20:00:00.000Z\\\",\\\"col\\\":\\\"col_21\\\",\\\"val\\\":-21}\"\r\n        }\r\n    },\r\n    \"tuningConfig\": {\r\n        \"type\": \"index\"\r\n    },\r\n    \"dataSchema\": {\r\n        \"dataSource\": \"inline_data\",\r\n        \"granularitySpec\": {\r\n            \"type\": \"uniform\",\r\n            \"segmentGranularity\": \"HOUR\",\r\n            \"queryGranularity\": \"NONE\",\r\n            \"rollup\": false\r\n        },\r\n        \"parser\": {\r\n            \"type\": \"string\",\r\n            \"parseSpec\": {\r\n                \"format\": \"json\",\r\n                \"timestampSpec\": {\r\n                    \"column\": \"timestamp\",\r\n                    \"format\": \"iso\"\r\n                },\r\n                \"dimensionsSpec\": {\r\n                    \"dimensions\": [\r\n                        \"col\",\r\n                        {\r\n                            \"type\": \"long\",\r\n                            \"name\": \"val\"\r\n                        }\r\n                    ]\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n3. now we have two overlapping segments:\r\n    - `inline_data_2020-05-18T00:00:00.000Z_2020-05-19T00:00:00.000Z_version_1`\r\n    - `inline_data_2020-05-18T20:00:00.000Z_2020-05-18T21:00:00.000Z_version_2`\r\n\r\nthen submit a compaction task to compact these two overlapping segments.\r\n```json\r\n{\r\n    \"type\": \"compact\",\r\n    \"dataSource\": \"inline_data\",\r\n    \"interval\": \"2020-05-18/2020-05-19\"\r\n}\r\n```\r\n4. when the compaction task finishing, coordinator will constantly get the following error.\r\n```\r\n2020-05-17T20:15:20,174 ERROR [org.apache.druid.metadata.SqlSegmentsMetadataManager-Exec--0] org.apache.druid.metadata.SqlSegmentsMetadataManager - Uncaught exception in class org.apache.druid.metadata.SqlSegmentsMetadataManager's polling thread: {class=org.apache.druid.metadata.SqlSegmentsMetadataManager, exceptionType=class org.apache.druid.java.util.common.UOE, exceptionMessage=Cannot add overlapping segments [2020-05-15T05:00:00.000Z/2020-05-15T06:00:00.000Z and 2020-05-15T00:00:00.000Z/2020-05-16T00:00:00.000Z] with the same version [2020-05-17T12:14:26.722Z]}\r\norg.apache.druid.java.util.common.UOE: Cannot add overlapping segments [2020-05-15T05:00:00.000Z/2020-05-15T06:00:00.000Z and 2020-05-15T00:00:00.000Z/2020-05-16T00:00:00.000Z] with the same version [2020-05-17T12:14:26.722Z]\r\n        at org.apache.druid.timeline.VersionedIntervalTimeline.addAtKey(VersionedIntervalTimeline.java:638) ~[druid-core-0.18.0.jar:0.18.1-SNAPSHOT]\r\n        at org.apache.druid.timeline.VersionedIntervalTimeline.add(VersionedIntervalTimeline.java:542) ~[druid-core-0.18.0.jar:0.18.1-SNAPSHOT]\r\n        at org.apache.druid.timeline.VersionedIntervalTimeline.addAll(VersionedIntervalTimeline.java:226) ~[druid-core-0.18.0.jar:0.18.1-SNAPSHOT]\r\n        at org.apache.druid.timeline.VersionedIntervalTimeline.addSegments(VersionedIntervalTimeline.java:119) ~[druid-core-0.18.0.jar:0.18.1-SNAPSHOT]\r\n        at org.apache.druid.timeline.VersionedIntervalTimeline.forSegments(VersionedIntervalTimeline.java:86) ~[druid-core-0.18.0.jar:0.18.1-SNAPSHOT]\r\n        at org.apache.druid.timeline.VersionedIntervalTimeline.forSegments(VersionedIntervalTimeline.java:79) ~[druid-core-0.18.0.jar:0.18.1-SNAPSHOT]\r\n        at org.apache.druid.client.DataSourcesSnapshot.lambda$new$3(DataSourcesSnapshot.java:85) ~[druid-server-0.18.0.jar:0.18.1-SNAPSHOT]\r\n        at org.apache.druid.utils.CollectionUtils.lambda$mapValues$0(CollectionUtils.java:101) ~[druid-core-0.18.0.jar:0.18.1-SNAPSHOT]\r\n        at java.util.HashMap.forEach(HashMap.java:1289) ~[?:1.8.0_252]\r\n        at org.apache.druid.utils.CollectionUtils.mapValues(CollectionUtils.java:101) ~[druid-core-0.18.0.jar:0.18.1-SNAPSHOT]\r\n        at org.apache.druid.client.DataSourcesSnapshot.<init>(DataSourcesSnapshot.java:83) ~[druid-server-0.18.0.jar:0.18.1-SNAPSHOT]\r\n        at org.apache.druid.client.DataSourcesSnapshot.fromUsedSegments(DataSourcesSnapshot.java:55) ~[druid-server-0.18.0.jar:0.18.1-SNAPSHOT]\r\n        at org.apache.druid.metadata.SqlSegmentsMetadataManager.doPoll(SqlSegmentsMetadataManager.java:967) ~[druid-server-0.18.0.jar:0.18.1-SNAPSHOT]\r\n        at org.apache.druid.metadata.SqlSegmentsMetadataManager.poll(SqlSegmentsMetadataManager.java:899) ~[druid-server-0.18.0.jar:0.18.1-SNAPSHOT]\r\n        at org.apache.druid.metadata.SqlSegmentsMetadataManager.lambda$createPollTaskForStartOrder$0(SqlSegmentsMetadataManager.java:323) ~[druid-server-0.18.0.jar:0.18.1-SNAPSHOT]\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_252]\r\n        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [?:1.8.0_252]\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_252]\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [?:1.8.0_252]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_252]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_252]\r\n        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_252]\r\n```\r\n5. you may wonder what happened if submitting a compaction task with explicit segment granularity, like:\r\n```json\r\n{\r\n    \"type\": \"compact\",\r\n    \"dataSource\": \"inline_data\",\r\n    \"interval\": \"2020-05-18/2020-05-19\",\r\n    \"segmentGranularity\": \"DAY\"\r\n}\r\n```\r\nthen this task will fail with the following error\r\n```\r\n21T02:04:43.735Z', groupId='compact_inline_data_llmogppk_2020-05-21T02:04:43.735Z', taskResource=TaskResource{availabilityGroup='compact_inline_data_llmogppk_2020-05-21T02:04:43.735Z', requiredCapacity=1}, dataSource='inline_data', context={forceTimeChunkLock=true}}]\r\njava.lang.IllegalStateException: QueryableIndexes are not sorted! Interval[2020-05-18T20:00:00.000Z/2020-05-18T21:00:00.000Z] of segment[inline_data_2020-05-18T20:00:00.000Z_2020-05-18T21:00:00.000Z_2020-05-21T01:58:00.951Z] is laster than interval[2020-05-18T00:00:00.000Z/2020-05-19T00:00:00.000Z] of segment[inline_data_2020-05-18T00:00:00.000Z_2020-05-19T00:00:00.000Z_2020-05-21T01:57:38.882Z]\r\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:200) ~[guava-16.0.1.jar:?]\r\n\tat org.apache.druid.indexing.common.task.CompactionTask.createDimensionsSpec(CompactionTask.java:718) ~[classes/:?]\r\n\tat org.apache.druid.indexing.common.task.CompactionTask.createDataSchema(CompactionTask.java:664) ~[classes/:?]\r\n\tat org.apache.druid.indexing.common.task.CompactionTask.createIngestionSchema(CompactionTask.java:559) ~[classes/:?]\r\n\tat org.apache.druid.indexing.common.task.CompactionTask.runTask(CompactionTask.java:374) ~[classes/:?]\r\n\tat org.apache.druid.indexing.common.task.AbstractBatchIndexTask.run(AbstractBatchIndexTask.java:123) ~[classes/:?]\r\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:421) [classes/:?]\r\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:393) [classes/:?]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_212]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_212]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_212]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_212]\r\n```\r\n\r\n\r\n\r\n"}, {"user": "wjhypo", "commits": {}, "labels": ["Bug"], "created": "2020-05-15 21:16:22", "title": "Broker select custom priority NPE", "url": "https://github.com/apache/druid/issues/9876", "closed": "2020-06-27 00:28:55", "ttf": 42.000277777777775, "commitsDetails": [], "body": "### Affected Version\r\nMaster branch\r\n\r\n### Description\r\n1) When `druid.broker.select.tier` is set to `custom` and `druid.broker.select.tier.custom.priorities` is omitted, we get NPE when broker updates server view, e.g, when there's segment balance in historical nodes.\r\n2) When `druid.broker.select.tier` is set to `custom` and `druid.broker.select.tier.custom.priorities` doesn't explicitly list all the priorities of existing tiers, we get NPE same as above.\r\n\r\n```\r\nException in thread \"BrokerServerView-1985\" java.lang.NullPointerException\r\n        at java.util.Comparator.lambda$comparingInt$7b0bb60$1(Comparator.java:490)\r\n        at it.unimi.dsi.fastutil.ints.IntComparators$1.compare(IntComparators.java:86)\r\n        at it.unimi.dsi.fastutil.ints.Int2ObjectRBTreeMap.compare(Int2ObjectRBTreeMap.java:205)\r\n        at it.unimi.dsi.fastutil.ints.Int2ObjectRBTreeMap.findKey(Int2ObjectRBTreeMap.java:219)\r\n        at it.unimi.dsi.fastutil.ints.Int2ObjectRBTreeMap.get(Int2ObjectRBTreeMap.java:931)\r\n        at it.unimi.dsi.fastutil.ints.Int2ObjectMap.computeIfAbsent(Int2ObjectMap.java:380)\r\n        at org.apache.druid.client.selector.ServerSelector.addServerAndUpdateSegment(ServerSelector.java:72)\r\n```"}, {"user": "mghosh4", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug"], "created": "2020-05-08 23:05:27", "title": "Empty partitionDimension in Native Ingestion leads to lesser data rollup compared to explicit partitionDimension", "url": "https://github.com/apache/druid/issues/9846", "closed": "2020-06-05 19:42:43", "ttf": 27.00027777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\nDruid v0.17.0 and v0.18.0\r\n\r\n### Description\r\n\r\nWhen you run a native ingestion spec without explicitly specifying partitionDimension, the data rollup observed is less than when you specify it explicitly. This behavior is contradictory to that observed in Hadoop and mentioned in the documentation.\r\n\r\n### Steps to Reproduce\r\n\r\nTo generate data you can use this python script:\r\n\r\n``` python\r\nimport json\r\nimport time\r\nimport random\r\nimport sys\r\nn = 10\r\nif len(sys.argv) == 2:\r\n  n = int(sys.argv[1])\r\nUNITS_IN_HOUR = 3600\r\nnow = int(time.time())\r\nnow -= now % UNITS_IN_HOUR\r\nfor i in range(n):\r\n  obj = {\"time\" : now + random.randrange(UNITS_IN_HOUR)}\r\n  for k in range(1, 7):\r\n    obj[\"v\" + str(k)] = i % (10 ** k)\r\n  print(json.dumps(obj))\r\n```\r\nCreate 3 files with n = 1000000\r\nIngest this data using the following spec\r\n```\r\n{\r\n  \"type\": \"index_parallel\",\r\n  \"spec\": {\r\n    \"ioConfig\": {\r\n      \"type\": \"index_parallel\",\r\n      \"inputSource\": {\r\n        \"type\": \"local\",\r\n        \"filter\": \"*\",\r\n        \"baseDir\": \"/path/to/data/\"\r\n      },\r\n      \"inputFormat\": {\r\n        \"type\": \"json\"\r\n      }\r\n    },\r\n    \"tuningConfig\": {\r\n      \"type\": \"index_parallel\",\r\n      \"partitionsSpec\": {\r\n        \"type\": \"hashed\",\r\n        \"numShards\": 2,\r\n        \"partitionDimensions\": [ \"v1\" ]\r\n      },\r\n      \"forceGuaranteedRollup\": true,\r\n      \"maxNumConcurrentSubTasks\": 10\r\n    },\r\n    \"dataSchema\": {\r\n      \"dataSource\": \"1m_json_data\",\r\n      \"granularitySpec\": {\r\n        \"type\": \"uniform\",\r\n        \"queryGranularity\": \"HOUR\",\r\n        \"rollup\": true,\r\n        \"intervals\": [\r\n          <change this range based on your time>\r\n        ],\r\n        \"segmentGranularity\": \"DAY\"\r\n      },\r\n      \"timestampSpec\": {\r\n        \"column\": \"time\",\r\n        \"format\": \"posix\"\r\n      },\r\n      \"dimensionsSpec\": {\r\n        \"dimensions\": [\r\n          {\r\n            \"type\": \"long\",\r\n            \"name\": \"v1\"\r\n          }\r\n        ]\r\n      },\r\n      \"metricsSpec\": [\r\n        {\r\n          \"name\": \"count\",\r\n          \"type\": \"count\"\r\n        },\r\n        {\r\n          \"name\": \"sum_v2\",\r\n          \"type\": \"longSum\",\r\n          \"fieldName\": \"v2\"\r\n        },\r\n        {\r\n          \"name\": \"sum_v3\",\r\n          \"type\": \"longSum\",\r\n          \"fieldName\": \"v3\"\r\n        },\r\n        {\r\n          \"name\": \"sum_v4\",\r\n          \"type\": \"longSum\",\r\n          \"fieldName\": \"v4\"\r\n        },\r\n        {\r\n          \"name\": \"sum_v5\",\r\n          \"type\": \"longSum\",\r\n          \"fieldName\": \"v5\"\r\n        },\r\n        {\r\n          \"name\": \"sum_v6\",\r\n          \"type\": \"longSum\",\r\n          \"fieldName\": \"v6\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\nYou will find there are 10 rows being created once the ingestion complete.\r\nNow remove the line `\"partitionDimensions\": [ \"v1\" ]`\r\nYou will find 20 rows getting created\r\n\r\n### Problem Diagnosis\r\n\r\nThe problem arises in this piece of code: `src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java`\r\n\r\n``` java\r\n  @VisibleForTesting\r\n  List<Object> getGroupKey(final long timestamp, final InputRow inputRow)\r\n  {\r\n    if (partitionDimensions.isEmpty()) {\r\n      return Rows.toGroupKey(timestamp, inputRow);\r\n    } else {\r\n      return Lists.transform(partitionDimensions, inputRow::getDimension);\r\n    }\r\n  }\r\n```\r\nIf `partitionDimension` is empty then we use `timestamp` as well as the `dimensions` to hash while we ignore the `timestamp` when we have an explicit `partitionDimension` present. I am not completely sure why that is the case but we can discuss.\r\n\r\nThat being said I found out this piece of code is also used for hadoop ingestion where we do not see this problem. The difference is in Hadoop we pass a constant timestamp by using something like this `rollupGran.bucketStart(inputRow.getTimestamp()).getMillis()`. This is missing in the native ingestion code path.\r\n\r\n### Next Steps\r\n\r\nI have both types of fixes implemented and tested. Hoping to hear your thoughts on this before moving ahead.\r\n"}, {"user": "erkdursun", "commits": {}, "labels": ["Area - Querying", "Bug"], "created": "2020-04-29 19:18:03", "title": "Aggregation functions are not working properly when using join feature", "url": "https://github.com/apache/druid/issues/9792", "closed": "2020-06-19 04:32:30", "ttf": 50.000277777777775, "commitsDetails": [], "body": "Aggregation functions are not working properly when using join feature\r\n\r\n### Affected Version\r\n0.18.0\r\n### Description\r\nI have a query to find duration of event which returns 1 record in total\r\n\r\n![image](https://user-images.githubusercontent.com/35697140/80636547-57a5c380-8a66-11ea-9c1a-de85530d5ed3.png)\r\n\r\nWhen I try to **count the results with query** it returns **6 records** and if I try to sum all duration for one key it returns some dummy record which should be 86341\r\n\r\n![image](https://user-images.githubusercontent.com/35697140/80636836-b9fec400-8a66-11ea-8f27-6436810c98f7.png)\r\n\r\n![image](https://user-images.githubusercontent.com/35697140/80636900-d7cc2900-8a66-11ea-8ddd-200e111c1f10.png)\r\n\r\n"}, {"user": "scrawfor", "commits": {}, "labels": ["Bug"], "created": "2020-04-21 18:26:46", "title": "HLLSketchMerge aggregator failing for some metrics after upgrade to v0.18", "url": "https://github.com/apache/druid/issues/9736", "closed": "2020-05-16 21:09:24", "ttf": 25.00027777777778, "commitsDetails": [], "body": "Please provide a detailed title (e.g. \"Broker crashes when using TopN query with Bound filter\" instead of just \"Broker crashes\").\r\n\r\n### Affected Version\r\n\r\nv0.18 (upgraded from 0.16.0)\r\n\r\n### Description\r\n\r\nThe HLLSketchMerge aggregator is failing for some of our metrics after upgrading to druid 0.18.0.  Reverting back to 0.16.0 fixes the issue. I have isolated specific segments where the issue occurs, moved those segments back to our 0.16 historical and have been successfully able to query the same metric.\r\n\r\nRe-indexing data does not seem to fix the issue.\r\n\r\n### Error Message.\r\n```json\r\n{\r\n  \"error\": \"Unknown exception\",\r\n  \"errorMessage\": \"java.util.concurrent.ExecutionException: java.lang.RuntimeException: org.apache.datasketches.SketchesArgumentException: Incomming sketch is corrupted, Rebuild_CurMin_Num_KxQ flag is set.\",\r\n  \"errorClass\": \"java.lang.RuntimeException\",\r\n  \"host\": null\r\n}\r\n```"}, {"user": "jihoonson", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2020-04-21 01:28:15", "title": "NPE when using AvroStreamInputRowParser and Kafka", "url": "https://github.com/apache/druid/issues/9728", "closed": "2020-04-24 17:22:52", "ttf": 3.000277777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.18.0\r\n\r\n### Description\r\n\r\nThis bug was added in #9625. Stack trace is\r\n\r\n```java.lang.NullPointerException: inputFormatat com.google.common.base.Preconditions.checkNotNull(Preconditions.java:229) ~[guava-16.0.1.jar:?]at org.apache.druid.indexing.seekablestream.SettableByteEntityReader.<init>(SettableByteEntityReader.java:57) ~[druid-indexing-service-0.18.0-iap3.jar:0.18.0-iap3]at org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner.runInternal(SeekableStreamIndexTaskRunner.java:379) ~[druid-indexing-service-0.18.0-iap3.jar:0.18.0-iap3]at org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner.run(SeekableStreamIndexTaskRunner.java:276) [druid-indexing-service-0.18.0-iap3.jar:0.18.0-iap3]at org.apache.druid.indexing.seekablestream.SeekableStreamIndexTask.run(SeekableStreamIndexTask.java:164) [druid-indexing-service-0.18.0-iap3.jar:0.18.0-iap3]at org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:421) [druid-indexing-service-0.18.0-iap3.jar:0.18.0-iap3]at org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:393) [druid-indexing-service-0.18.0-iap3.jar:0.18.0-iap3]at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_242]at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_242]at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_242]at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]```"}, {"user": "jihoonson", "commits": {}, "labels": ["Bug"], "created": "2020-04-15 01:55:07", "title": "DruidLeaderClient.goAsync() is broken", "url": "https://github.com/apache/druid/issues/9701", "closed": "2020-04-15 03:42:56", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.18.0\r\n\r\n### Description\r\n\r\nThis bug seems to be introduced after https://github.com/apache/druid/pull/9481. Before this PR, `DruidLeaderClient` used to use `ServerDiscoverySelector` to find the leader. Since only the leader Overlord and Coordinator announce themselves, `ServerDiscoverySelector` could return the current leader to `DruidLeaderClient`. But now, `DruidLeaderClient` picks up the first node returned from `DruidNodeDiscovery` which knows all announced nodes. Since `DruidLeaderClient.goAsync()` doesn't follow redirection (https://github.com/apache/druid/issues/8716), some methods of the system table which use `goAsync()` API returns an error with the Temporary Redirect status."}, {"user": "clintropolis", "commits": {}, "labels": ["Area - Querying", "Bug"], "created": "2020-04-14 00:41:20", "title": "TopN and GroupBy aggregating on Scan subqueries with multi-value columns fail", "url": "https://github.com/apache/druid/issues/9697", "closed": "2020-04-22 01:44:27", "ttf": 8.000277777777777, "commitsDetails": [], "body": "Queries with scan subqueries which select multi-value dimension do not work correctly fro TopN and Group By queries.\r\n\r\n### Affected Version\r\n\r\n0.18.0\r\n\r\n### Description\r\nA side-effect of the joins work in 0.18, an `InlineDataSource` was added which allows subqueries on arbitrary query types instead of just being a special group by mode that only allows other group by queries to be the subqueries. This means that scan queries can now be subqueries, which means the broker sometimes now has to handle aggregating on multi-value dimensions if the scan subquery selected them. However, it is ill prepared, and is making assumptions that the absence of column capabilities means that it can assume there are no multi-value dimensions, which is no longer true.\r\n\r\ngroup by error:\r\n```\r\njava.lang.IllegalStateException: Not supported for multi-value dimensions\r\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:176) ~[guava-16.0.1.jar:?]\r\n\tat org.apache.druid.query.groupby.epinephelinae.column.DictionaryBuildingStringGroupByColumnSelectorStrategy.getOnlyValue(DictionaryBuildingStringGroupByColumnSelectorStrategy.java:112) ~[druid-processing-0.18.0-iap-preview8-SNAPSHOT.jar:0.18.0-iap-preview8-SNAPSHOT]\r\n\tat org.apache.druid.query.groupby.epinephelinae.GroupByQueryEngineV2$HashAggregateIterator.aggregateSingleValueDims(GroupByQueryEngineV2.java:669) ~[druid-processing-0.18.0-iap-preview8-SNAPSHOT.jar:0.18.0-iap-preview8-SNAPSHOT]\r\n\tat org.apache.druid.query.groupby.epinephelinae.GroupByQueryEngineV2$GroupByEngineIterator.initNewDelegate(GroupByQueryEngineV2.java:457) ~[druid-processing-0.18.0-iap-preview8-SNAPSHOT.jar:0.18.0-iap-preview8-SNAPSHOT]\r\n\tat org.apache.druid.query.groupby.epinephelinae.GroupByQueryEngineV2$GroupByEngineIterator.hasNext(GroupByQueryEngineV2.java:511) ~[druid-processing-0.18.0-iap-preview8-SNAPSHOT.jar:0.18.0-iap-preview8-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.BaseSequence.accumulate(BaseSequence.java:43) ~[druid-core-0.18.0-iap-preview8-SNAPSHOT.jar:0.18.0-iap-preview8-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.ConcatSequence.lambda$accumulate$0(ConcatSequence.java:41) ~[druid-core-0.18.0-iap-preview8-SNAPSHOT.jar:0.18.0-iap-preview8-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:40) ~[druid-core-0.18.0-iap-preview8-SNAPSHOT.jar:0.18.0-iap-preview8-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.BaseSequence.accumulate(BaseSequence.java:44) ~[druid-core-0.18.0-iap-preview8-SNAPSHOT.jar:0.18.0-iap-preview8-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.MappedSequence.accumulate(MappedSequence.java:43) ~[druid-core-0.18.0-iap-preview8-SNAPSHOT.jar:0.18.0-iap-preview8-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.ConcatSequence.accumulate(ConcatSequence.java:41) ~[druid-core-0.18.0-iap-preview8-SNAPSHOT.jar:0.18.0-iap-preview8-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.WrappingSequence$1.get(WrappingSequence.java:50) ~[druid-core-0.18.0-iap-preview8-SNAPSHOT.jar:0.18.0-iap-preview8-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.SequenceWrapper.wrap(SequenceWrapper.java:55) ~[druid-core-0.18.0-iap-preview8-SNAPSHOT.jar:0.18.0-iap-preview8-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.WrappingSequence.accumulate(WrappingSequence.java:45) ~[druid-core-0.18.0-iap-preview8-SNAPSHOT.jar:0.18.0-iap-preview8-SNAPSHOT]\r\n\tat org.apache.druid.query.groupby.epinephelinae.GroupByMergingQueryRunnerV2$1$1$1.call(GroupByMergingQueryRunnerV2.java:246) [druid-processing-0.18.0-iap-preview8-SNAPSHOT.jar:0.18.0-iap-preview8-SNAPSHOT]\r\n\tat org.apache.druid.query.groupby.epinephelinae.GroupByMergingQueryRunnerV2$1$1$1.call(GroupByMergingQueryRunnerV2.java:233) [druid-processing-0.18.0-iap-preview8-SNAPSHOT.jar:0.18.0-iap-preview8-SNAPSHOT]\r\n...\r\n```\r\n\r\ntopn error:\r\n```\r\n2020-04-13T16:08:11,837 ERROR [main] org.apache.druid.query.ChainedExecutionQueryRunner - Exception with one of the sequences!\r\njava.lang.UnsupportedOperationException: Cannot operate on a dimension with no dictionary\r\n\tat org.apache.druid.query.topn.PooledTopNAlgorithm.makeInitParams(PooledTopNAlgorithm.java:222) ~[classes/:?]\r\n\tat org.apache.druid.query.topn.PooledTopNAlgorithm.makeInitParams(PooledTopNAlgorithm.java:53) ~[classes/:?]\r\n\tat org.apache.druid.query.topn.TopNMapFn.apply(TopNMapFn.java:62) ~[classes/:?]\r\n\tat org.apache.druid.query.topn.TopNQueryEngine.lambda$query$0(TopNQueryEngine.java:91) ~[classes/:?]\r\n...\r\n```\r\n"}, {"user": "vogievetsky", "commits": {}, "labels": ["Area - Batch Ingestion", "Area - Web Console", "Bug"], "created": "2020-04-09 22:41:08", "title": "Sampler with Inline input source swallows up columns that are all `null`", "url": "https://github.com/apache/druid/issues/9658", "closed": "2020-05-09 04:53:40", "ttf": 29.00027777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.17.0\r\n\r\n### Description\r\n\r\nWhen posting to the sampler with an inline input source that has a column that is all `null` that column would not be returned as part of the parsed fields without explicit dimensions being set.\r\n\r\nTry sending:\r\n\r\n```json\r\n{\r\n  \"type\": \"index\",\r\n  \"spec\": {\r\n    \"ioConfig\": {\r\n      \"type\": \"index\",\r\n      \"inputSource\": {\r\n        \"type\": \"inline\",\r\n        \"data\": \"{\\\"make\\\":\\\"Honda\\\",\\\"model\\\":\\\"Odyssey\\\",\\\"a\\\":null}\\n{\\\"make\\\":\\\"Tesla\\\",\\\"model\\\":\\\"Cybertruck\\\",\\\"a\\\":null}\"\r\n      },\r\n      \"inputFormat\": {\r\n        \"type\": \"json\"\r\n      }\r\n    },\r\n    \"dataSchema\": {\r\n      \"dataSource\": \"sample\",\r\n      \"timestampSpec\": {\r\n        \"column\": \"!!!_no_such_column_!!!\",\r\n        \"missingValue\": \"1970-01-01T00:00:00Z\"\r\n      },\r\n      \"dimensionsSpec\": {}\r\n    },\r\n    \"type\": \"index\",\r\n    \"tuningConfig\": {\r\n      \"type\": \"index\"\r\n    }\r\n  },\r\n  \"samplerConfig\": {\r\n    \"numRows\": 500,\r\n    \"timeoutMs\": 15000\r\n  }\r\n}\r\n```\r\n\r\nto:\r\n\r\n`/druid/indexer/v1/sampler`\r\n\r\nYou get back:\r\n\r\n```json\r\n{\r\n  \"numRowsRead\": 2,\r\n  \"numRowsIndexed\": 2,\r\n  \"data\": [\r\n    {\r\n      \"input\": {\r\n        \"make\": \"Honda\",\r\n        \"model\": \"Odyssey\",\r\n        \"a\": null\r\n      },\r\n      \"parsed\": {\r\n        \"__time\": 0,\r\n        \"model\": \"Odyssey\",\r\n        \"make\": \"Honda\"\r\n      }\r\n    },\r\n    {\r\n      \"input\": {\r\n        \"make\": \"Tesla\",\r\n        \"model\": \"Cybertruck\",\r\n        \"a\": null\r\n      },\r\n      \"parsed\": {\r\n        \"__time\": 0,\r\n        \"model\": \"Cybertruck\",\r\n        \"make\": \"Tesla\"\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nNotice how the `a` column is in not returned as part of a the parsed section.\r\n\r\nThis creates issues with columns that happen to be all `null` in the sample (of 20) that is used by the console.\r\n\r\nJust to be clear: I would expect the column to show up in the `parsed` section even if all the values for it are `null`"}, {"user": "gianm", "commits": {"75c543b50f40d7b6739ff0d6e10a81315be29736": {"commitGHEventType": "referenced", "commitUser": "gianm"}, "95a4d2adde64572b57b744ab36a54672d7f265ed": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Area - SQL", "Bug"], "spoonStatsSummary": {}, "title": "SQL: Excessive time and memory use while planning joins on subqueries", "numCommits": 0, "created": "2020-04-08 10:11:56", "closed": "2020-04-09 23:21:44", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "filteredCommits": [], "url": "https://github.com/apache/druid/issues/9646", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 1.0002777777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "sql/src/test/java/org/apache/druid/sql/calcite/CalciteQueryTest.java", "deletions": 8, "insertions": 400, "lines": 408}, {"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/planner/Rules.java", "deletions": 22, "insertions": 18, "lines": 40}, {"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/rel/DruidQueryRel.java", "deletions": 6, "insertions": 0, "lines": 6}, {"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/rel/DruidRel.java", "deletions": 7, "insertions": 0, "lines": 7}, {"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/rel/CostEstimates.java", "deletions": 4, "insertions": 10, "lines": 14}, {"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/rel/DruidOuterQueryRel.java", "deletions": 6, "insertions": 0, "lines": 6}, {"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/rel/DruidUnionRel.java", "deletions": 6, "insertions": 0, "lines": 6}, {"filePath": "core/src/main/java/org/apache/druid/math/expr/ExprEval.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/rule/DruidJoinRule.java", "deletions": 26, "insertions": 268, "lines": 294}, {"filePath": "core/src/test/java/org/apache/druid/math/expr/FunctionTest.java", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/rel/DruidQuery.java", "deletions": 2, "insertions": 1, "lines": 3}, {"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/rel/DruidJoinQueryRel.java", "deletions": 55, "insertions": 35, "lines": 90}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "tooManyChanges", "authoredDateTime": "2020-04-09 16:21:43", "commitMessage": "SQL: More straightforward handling of join planning. (#9648)\n\n* SQL: More straightforward handling of join planning.\r\n\r\nTwo changes that simplify how joins are planned:\r\n\r\n1) Stop using JoinProjectTransposeRule as a way of guiding subquery\r\nremoval. Instead, add logic to DruidJoinRule that identifies removable\r\nsubqueries and removes them at the point of creating a DruidJoinQueryRel.\r\nThis approach reduces the size of the planning space and allows the\r\nplanner to complete quickly.\r\n\r\n2) Remove rules that reorder joins. Not because of an impact on the\r\nplanning time (it seems minimal), but because the decisions that the\r\nplanner was making in the new tests were sometimes worse than the\r\nuser-provided order. I think we'll need to go with the user-provided\r\norder for now, and revisit reordering when we can add more smarts to\r\nthe cost estimator.\r\n\r\nA third change updates numeric ExprEval classes to store their\r\nvalue as a boxed type that corresponds to what it is supposed to be.\r\nThis is useful because it affects the behavior of \"asString\", and\r\nis included in this patch because it is needed for the new test\r\n\"testInnerJoinTwoLookupsToTableUsingNumericColumnInReverse\". This\r\ntest relies on CAST('6', 'DOUBLE') stringifying to \"6.0\" like an\r\nactual double would.\r\n\r\nFixes #9646.\r\n\r\n* Fix comments.\r\n\r\n* Fix tests.", "commitUser": "gianm", "commitDateTime": "2020-04-09 16:21:43", "commitParents": ["eb45981b606a4f95f35745ccb8955672b2e70d90"], "commitGHEventType": "referenced", "nameRev": "75c543b50f40d7b6739ff0d6e10a81315be29736 master~168", "commitHash": "75c543b50f40d7b6739ff0d6e10a81315be29736"}, {"commitGitStats": [{"filePath": "sql/src/test/java/org/apache/druid/sql/calcite/CalciteQueryTest.java", "deletions": 8, "insertions": 400, "lines": 408}, {"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/planner/Rules.java", "deletions": 22, "insertions": 18, "lines": 40}, {"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/rel/DruidQueryRel.java", "deletions": 6, "insertions": 0, "lines": 6}, {"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/rel/DruidRel.java", "deletions": 7, "insertions": 0, "lines": 7}, {"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/rel/CostEstimates.java", "deletions": 4, "insertions": 10, "lines": 14}, {"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/rel/DruidOuterQueryRel.java", "deletions": 6, "insertions": 0, "lines": 6}, {"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/rel/DruidUnionRel.java", "deletions": 6, "insertions": 0, "lines": 6}, {"filePath": "core/src/main/java/org/apache/druid/math/expr/ExprEval.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/rule/DruidJoinRule.java", "deletions": 26, "insertions": 268, "lines": 294}, {"filePath": "core/src/test/java/org/apache/druid/math/expr/FunctionTest.java", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/rel/DruidQuery.java", "deletions": 2, "insertions": 1, "lines": 3}, {"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/rel/DruidJoinQueryRel.java", "deletions": 55, "insertions": 35, "lines": 90}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "tooManyChanges", "authoredDateTime": "2020-04-10 09:06:05", "commitMessage": "SQL: More straightforward handling of join planning. (#9648) (#9659)\n\n* SQL: More straightforward handling of join planning.\r\n\r\nTwo changes that simplify how joins are planned:\r\n\r\n1) Stop using JoinProjectTransposeRule as a way of guiding subquery\r\nremoval. Instead, add logic to DruidJoinRule that identifies removable\r\nsubqueries and removes them at the point of creating a DruidJoinQueryRel.\r\nThis approach reduces the size of the planning space and allows the\r\nplanner to complete quickly.\r\n\r\n2) Remove rules that reorder joins. Not because of an impact on the\r\nplanning time (it seems minimal), but because the decisions that the\r\nplanner was making in the new tests were sometimes worse than the\r\nuser-provided order. I think we'll need to go with the user-provided\r\norder for now, and revisit reordering when we can add more smarts to\r\nthe cost estimator.\r\n\r\nA third change updates numeric ExprEval classes to store their\r\nvalue as a boxed type that corresponds to what it is supposed to be.\r\nThis is useful because it affects the behavior of \"asString\", and\r\nis included in this patch because it is needed for the new test\r\n\"testInnerJoinTwoLookupsToTableUsingNumericColumnInReverse\". This\r\ntest relies on CAST('6', 'DOUBLE') stringifying to \"6.0\" like an\r\nactual double would.\r\n\r\nFixes #9646.\r\n\r\n* Fix comments.\r\n\r\n* Fix tests.", "commitUser": "fjy", "commitDateTime": "2020-04-10 09:06:05", "commitParents": ["0f25367b517063879f3659b0bf220c2a29ba1863"], "commitGHEventType": "referenced", "nameRev": "95a4d2adde64572b57b744ab36a54672d7f265ed tags/druid-0.18.0-rc1~16", "commitHash": "95a4d2adde64572b57b744ab36a54672d7f265ed"}], "body": "These queries will blow up the planner's rule queue and make it take forever (and also use a lot of memory).\r\n\r\n```sql\r\nSELECT *\r\n  FROM foo\r\n  WHERE dim1 IN (SELECT NULL FROM lookup.lookyloo);\r\n\r\nSELECT l.k, l.v, SUM(f.m1), SUM(nf.m1)\r\n  FROM lookup.lookyloo l\r\n  INNER JOIN druid.foo f ON f.dim1 = l.k\r\n  INNER JOIN druid.numfoo nf ON nf.dim1 = l.k\r\n  GROUP BY 1, 2 ORDER BY 2;\r\n\r\nSELECT COUNT(*)\r\n  FROM foo, lookup.lookyloo l, numfoo\r\n  WHERE foo.cnt = l.k AND l.k = numfoo.cnt;\r\n\r\nSELECT COUNT(*)\r\n  FROM foo\r\n  INNER JOIN lookup.lookyloo l1 ON l1.k = foo.m1\r\n  INNER JOIN lookup.lookyloo l2 ON l2.k = l1.k;\r\n```\r\n\r\nSomething in common with all of them is type mismatches in the join condition yielding implicit casts. Removing JoinProjectTransposeRule calms down the rule queue and the planner is able to complete in a reasonable amount of time. This sort of makes sense, since the implicit casts would generate a Project beneath the Join, and the purpose of JoinProjectTransposeRule is to try to swap them.\r\n\r\nHowever, this rule is important because it helps us avoid subqueries. So we'll need another solution to replace it."}, {"user": "suneet-s", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug"], "created": "2020-03-31 06:41:53", "title": "Can not add a column when re-indexing a datasource using DruidInputSource", "url": "https://github.com/apache/druid/issues/9593", "closed": "2020-04-03 00:32:32", "ttf": 2.000277777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.18\r\n\r\n### Description\r\n\r\nSee the following ingestionSpec that can be used in Integration tests. The transformed column \"newPage\" does not show up in the dimensions for the new re-indexed dataSource. To work around this, I fall back to use the ingestSegmentFirehose instead\r\n```\r\n{\r\n    \"type\": \"index\",\r\n    \"spec\": {\r\n        \"ioConfig\": {\r\n            \"type\": \"index\",\r\n            \"inputSource\": {\r\n                \"type\": \"druid\",\r\n                \"dataSource\": \"%%DATASOURCE%%\",\r\n                \"interval\": \"2013-08-31/2013-09-01\"\r\n            }\r\n        },\r\n        \"tuningConfig\": {\r\n            \"type\": \"index\",\r\n            \"partitionsSpec\": {\r\n                \"type\": \"dynamic\"\r\n            }\r\n        },\r\n        \"dataSchema\": {\r\n            \"dataSource\": \"%%REINDEX_DATASOURCE%%\",\r\n            \"granularitySpec\": {\r\n                \"type\": \"uniform\",\r\n                \"queryGranularity\": \"SECOND\",\r\n                \"segmentGranularity\": \"DAY\"\r\n            },\r\n            \"timestampSpec\": {\r\n                \"column\": \"__time\",\r\n                \"format\": \"iso\"\r\n            },\r\n            \"dimensionsSpec\": {\r\n                \"dimensions\": [\r\n                    {\"type\": \"string\", \"name\": \"language\", \"createBitmapIndex\": false},\r\n                    \"user\",\r\n                    \"unpatrolled\",\r\n                    \"page\",\r\n                    \"newPage\",\r\n                    \"anonymous\",\r\n                    \"namespace\",\r\n                    \"country\",\r\n                    \"region\",\r\n                    \"city\"\r\n                ],\r\n                \"dimensionExclusions\" : [\"robot\", \"continent\"]\r\n            },\r\n            \"transformSpec\": {\r\n                \"transforms\": [\r\n                    {\r\n                        \"type\": \"expression\",\r\n                        \"name\": \"newPage\",\r\n                        \"expression\": \"\\\"page\\\"\"\r\n                    },\r\n                    {\r\n                        \"type\": \"expression\",\r\n                        \"name\": \"one-plus-triple-added\",\r\n                        \"expression\": \"\\\"triple-added\\\" + 1\"\r\n                    },\r\n                    {\r\n                        \"type\": \"expression\",\r\n                        \"name\": \"double-deleted\",\r\n                        \"expression\": \"deleted * 2\"\r\n                    }\r\n                ]\r\n            },\r\n            \"metricsSpec\": [\r\n                {\r\n                    \"type\": \"doubleSum\",\r\n                    \"name\": \"added\",\r\n                    \"fieldName\": \"added\"\r\n                },\r\n                {\r\n                    \"type\": \"doubleSum\",\r\n                    \"name\": \"triple-added\",\r\n                    \"fieldName\": \"triple-added\"\r\n                },\r\n                {\r\n                    \"type\": \"doubleSum\",\r\n                    \"name\": \"one-plus-triple-added\",\r\n                    \"fieldName\": \"one-plus-triple-added\"\r\n                },\r\n                {\r\n                    \"type\": \"doubleSum\",\r\n                    \"name\": \"deleted\",\r\n                    \"fieldName\": \"deleted\"\r\n                },\r\n                {\r\n                    \"type\": \"doubleSum\",\r\n                    \"name\": \"double-deleted\",\r\n                    \"fieldName\": \"double-deleted\"\r\n                },\r\n                {\r\n                    \"type\": \"doubleSum\",\r\n                    \"name\": \"delta\",\r\n                    \"fieldName\": \"delta\"\r\n                }\r\n            ]\r\n        }\r\n    }\r\n}\r\n```"}, {"user": "suneet-s", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug"], "created": "2020-03-31 06:12:38", "title": "Re-indexing a DruidInputSource with explicit dimensions includes all dimensions from original datasource", "url": "https://github.com/apache/druid/issues/9592", "closed": "2020-04-09 14:38:04", "ttf": 9.000277777777777, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.18\r\n\r\n### Description\r\n\r\nIf you attempt to re-index a druid datasource using a Druid InputSource, and you explicitly set the columns in the dimension spec, the re-index job will ingest all columns. To exclude columns, they need to be manually added to the dimensionsExclusion field\r\n\r\nThis ingestion spec used in integration tests re-produces the issue. The field \"robot\" is included in the re-indexed datasource even though it's not explicitly specified in the ingestionSpec\r\n```\r\n{\r\n    \"type\": \"index\",\r\n    \"spec\": {\r\n        \"ioConfig\": {\r\n            \"type\": \"index\",\r\n            \"inputSource\": {\r\n                \"type\": \"druid\",\r\n                \"dataSource\": \"%%DATASOURCE%%\",\r\n                \"interval\": \"2013-08-31/2013-09-01\"\r\n            }\r\n        },\r\n        \"tuningConfig\": {\r\n            \"type\": \"index\",\r\n            \"partitionsSpec\": {\r\n                \"type\": \"dynamic\"\r\n            }\r\n        },\r\n        \"dataSchema\": {\r\n            \"dataSource\": \"%%REINDEX_DATASOURCE%%\",\r\n            \"granularitySpec\": {\r\n                \"type\": \"uniform\",\r\n                \"queryGranularity\": \"SECOND\",\r\n                \"segmentGranularity\": \"DAY\"\r\n            },\r\n            \"timestampSpec\": {\r\n                \"column\": \"__time\",\r\n                \"format\": \"iso\"\r\n            },\r\n            \"dimensionsSpec\": {\r\n                \"dimensions\": [\r\n                    \"page\",\r\n                    {\"type\": \"string\", \"name\": \"language\", \"createBitmapIndex\": false},\r\n                    \"user\",\r\n                    \"unpatrolled\",\r\n                    \"newPage\",\r\n                    \"anonymous\",\r\n                    \"namespace\",\r\n                    \"country\",\r\n                    \"region\",\r\n                    \"city\"\r\n                ]\r\n            },\r\n            \"transformSpec\": {\r\n                \"transforms\": [\r\n                    {\r\n                        \"type\": \"expression\",\r\n                        \"name\": \"newPage\",\r\n                        \"expression\": \"page\"\r\n                    },\r\n                    {\r\n                        \"type\": \"expression\",\r\n                        \"name\": \"one-plus-triple-added\",\r\n                        \"expression\": \"\\\"triple-added\\\" + 1\"\r\n                    },\r\n                    {\r\n                        \"type\": \"expression\",\r\n                        \"name\": \"double-deleted\",\r\n                        \"expression\": \"deleted * 2\"\r\n                    }\r\n                ]\r\n            },\r\n            \"metricsSpec\": [\r\n                {\r\n                    \"type\": \"doubleSum\",\r\n                    \"name\": \"added\",\r\n                    \"fieldName\": \"added\"\r\n                },\r\n                {\r\n                    \"type\": \"doubleSum\",\r\n                    \"name\": \"triple-added\",\r\n                    \"fieldName\": \"triple-added\"\r\n                },\r\n                {\r\n                    \"type\": \"doubleSum\",\r\n                    \"name\": \"one-plus-triple-added\",\r\n                    \"fieldName\": \"one-plus-triple-added\"\r\n                },\r\n                {\r\n                    \"type\": \"doubleSum\",\r\n                    \"name\": \"deleted\",\r\n                    \"fieldName\": \"deleted\"\r\n                },\r\n                {\r\n                    \"type\": \"doubleSum\",\r\n                    \"name\": \"double-deleted\",\r\n                    \"fieldName\": \"double-deleted\"\r\n                },\r\n                {\r\n                    \"type\": \"doubleSum\",\r\n                    \"name\": \"delta\",\r\n                    \"fieldName\": \"delta\"\r\n                }\r\n            ]\r\n        }\r\n    }\r\n}\r\n```"}, {"user": "samarthjain", "commits": {}, "labels": ["Bug", "Compatibility"], "created": "2020-02-27 00:00:05", "title": "Unknown complex types break Druid SQL", "url": "https://github.com/apache/druid/issues/9421", "closed": "2020-06-30 21:06:18", "ttf": 124.00027777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.16.0\r\n\r\n### Description\r\n\r\nAfter upgrading our clusters to 0.16.0, we noticed that the Druid SQL wasn't working because the metadata updates in DruidSchema were failing because of unknown aggregators in the segment metadata.\r\n\r\nTurns out, in 0.16.0, I made a backward incompatible change in t-digest aggregator extension where I removed the aggregator types `buildTDigestSketch` and `mergeTDigestSketch` and replaced it with a single aggregator `tDigestSketch`. \r\n\r\nWe had some users ingest their data using the `buildTDigestSketch` aggregator. However, when the 0.16.0 cluster was deployed, it wasn't able to deserialize the aggregator metadata as they were removed. This caused problems for the Druid SQL metadata cache as SegmentMetadataRequests started failing which in turn completely broke the Druid SQL functionality on the cluster. \r\n\r\nI think we should fix this and prevent future cases of someone making incompatible changes, especially in extensions-contrib modules, from breaking a core Druid functionality."}, {"user": "sascha-coenen", "commits": {}, "labels": ["Bug", "Performance"], "created": "2020-02-26 22:00:53", "title": "SQL: severe performance degradation if multi-valued dimension gets combined with a LOOKUP and NVL function", "url": "https://github.com/apache/druid/issues/9414", "closed": "2020-03-06 20:53:33", "ttf": 8.000277777777777, "commitsDetails": [], "body": "### Affected Version\r\n0.16.0 and 0.17.0\r\n\r\n### Description\r\nWe often use the combination of NULLIF() and LOOKUP() functions in combination. This works fine for normal dimension columns, but when using a multi-valued dimension, then the following combination of NULLIF and LOOKUP leads to extremely high segment scan times of 30 seconds or even up to several minutes:\r\n\r\n```\r\nSELECT\r\n   NULLIF(LOOKUP(adTypeIdPubRequest, 'adtype_by_adtypeid'), 'fallback') AS adFormat,\r\n   SUM(\"count\") AS cnt\r\nFROM \"supply-activities\" \r\nWHERE  (__time >= timestamp'2020-01-20 00:00:00' AND __time < timestamp '2020-01-20 01:00:00')\r\nGROUP BY 1\r\nORDER BY 2\r\nLIMIT 10\r\n```\r\n\r\nIn contrast, the following query which is merely dropping the surrounding NULLIF() function is executing at normal speed:\r\n\r\n```\r\nSELECT\r\n   LOOKUP(adTypeIdPubRequest, 'adtype_by_adtypeid') AS adFormat,\r\n   SUM(\"count\") AS cnt\r\nFROM \"supply-activities\" \r\nWHERE  (__time >= timestamp'2020-01-20 00:00:00' AND __time < timestamp '2020-01-20 01:00:00')\r\nGROUP BY 1\r\nORDER BY 2\r\n```\r\n\r\nUsing NULLIF in combination with LOOKUP on normal dinensions does NOT lead to a performance issue.\r\nPrior to Druid 0.16.0 we did not notice any performance issue but cannot guarentee that it was introduced with 0.16 because we might have simply overlooked it in prior releases.\r\nWith 0.16.0 and also 0.17.0 the performance issue exists. We tested with SQL compatible null handling turned on and off. Performance is bad in both cases. As mentioned, the segment scan times are off the charts.\r\n\r\n--\r\n\r\nI could not demonstrate the issue using the wikipedia dataset because it doesn't seem to contain any multi-valued dimensions. Perhaps it would be good to have a normative dataset which showcases all field types such that it can be used for problem reports that can be reproduced?\r\n\r\n"}, {"user": "sascha-coenen", "commits": {}, "labels": ["Area - SQL", "Bug"], "created": "2020-02-26 21:34:37", "title": "Regression: SQL query with multiple similar CASE clauses cannot be translated to correct native json format", "url": "https://github.com/apache/druid/issues/9412", "closed": "2020-03-10 00:55:30", "ttf": 12.000277777777777, "commitsDetails": [], "body": "### Affected Version\r\nv0.17.0\r\nThis is a serious regression bug for us. Queries like the following used to work in Druid 0.16.0 and are broken in 0.17.0\r\nWe used Druid 0.17.0 with SQL compatible null handling DISABLED.\r\n\r\n### Description\r\n\r\nThe following query produces a rule error in the SQL engine:\r\n```\r\nSELECT \r\n\tCASE cityName WHEN NULL THEN FALSE ELSE TRUE END AS col_a,\r\n\tCASE countryName WHEN NULL THEN FALSE ELSE TRUE END AS col_b\r\nFROM wikipedia\r\nGROUP BY 1, 2\r\n```\r\n\r\nthe error is:\r\n\r\n```\r\nUnknown exception / Error while applying rule DruidQueryRule(AGGREGATE), args [rel#592:LogicalAggregate.NONE.[](input=RelSubset#591,group={0, 1}), rel#599:DruidQueryRel.NONE.[](query={\"queryType\":\"scan\",\"dataSource\":{\"type\":\"table\",\"name\":\"wikipedia\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"virtualColumns\":[{\"type\":\"expression\",\"name\":\"v0\",\"expression\":\"1\",\"outputType\":\"LONG\"}],\"resultFormat\":\"compactedList\",\"batchSize\":20480,\"limit\":9223372036854775807,\"order\":\"none\",\"filter\":null,\"columns\":[\"v0\"],\"legacy\":false,\"context\":{\"forceLimitPushDown\":\"false\",\"sqlOuterLimit\":100,\"sqlQueryId\":\"f83cf8c0-1856-4dc9-aeb6-fc487b92750f\",\"useApproximateTopN\":\"false\",\"vectorize\":\"false\"},\"descending\":false,\"granularity\":{\"type\":\"all\"}},signature={v0:LONG, v0:LONG})] / java.lang.RuntimeException\r\n```\r\n\r\nFollowing is a comparison between what a similar query got translated to in Druid 0.16.0 vs now\r\n\r\nin 0.16.0:\r\n```\r\n{\r\n  \"queryType\": \"groupBy\",\r\n...\r\n  \"virtualColumns\": [\r\n    {\r\n      \"type\": \"expression\",\r\n      \"name\": \"v0\",\r\n      \"expression\": \"case_searched((\\\"integrationType\\\" == null),0,1)\",\r\n      \"outputType\": \"LONG\"\r\n    },\r\n    {\r\n      \"type\": \"expression\",\r\n      \"name\": \"v1\",\r\n      \"expression\": \"case_searched((\\\"integrationVersion\\\" == null),0,1)\",\r\n      \"outputType\": \"LONG\"\r\n    }\r\n  ],\r\n  \"dimensions\": [\r\n    {\r\n      \"type\": \"default\",\r\n      \"dimension\": \"v0\",\r\n      \"outputName\": \"v0\",\r\n      \"outputType\": \"LONG\"\r\n    },\r\n    {\r\n      \"type\": \"default\",\r\n      \"dimension\": \"v1\",\r\n      \"outputName\": \"v1\",\r\n      \"outputType\": \"LONG\"\r\n    }\r\n  ],\r\n...\r\n}\r\n```\r\n\r\n\r\nIn v 0.17.0, there seem to be several things that compound:\r\na) the query is rewritten into a scan query although it augh to be a groupby query\r\nb) the main issue seems to be that instead of recognizing the two CASE statements as being different and generating different logical names \"v0\" and \"v1\" for them, the two statements are being given the same name \"v0\" which is then consecutively leading to an illegal query syntax.\r\nc) instead of faithful CASE expressions, a virtual column expression gets generated that is just a constant number \"1\" \r\n\r\n```\r\n{\r\n   \"queryType\":\"scan\",\r\n...\r\n   \"virtualColumns\":[\r\n      {\r\n         \"type\":\"expression\",\r\n         \"name\":\"v0\",\r\n         \"expression\":\"1\",\r\n         \"outputType\":\"LONG\"\r\n      }\r\n   ],\r\n   \"columns\":[\r\n      \"v0\"\r\n   ],\r\n...\r\n}\r\n```\r\n\r\n"}, {"user": "acdn-mpreston", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2020-02-21 16:05:56", "title": "0.17.0 Kafka Supervisor becomes \"unhealthy\" due to missing inputFormat when Overlord is restarted", "url": "https://github.com/apache/druid/issues/9391", "closed": "2020-04-03 18:00:55", "ttf": 42.000277777777775, "commitsDetails": [], "body": "\r\n### Affected Version\r\n\r\n0.17.0\r\n\r\nThe Druid version where the problem was encountered.\r\n\r\n0.17.0\r\n\r\n### Description\r\n\r\nWhen provisioning a Kafka Supervisor in druid 0.17.0, a payload using the inputFormat property in the ioConfig will be accepted and the supervisor will work properly until you restart the Overlord service. When the Overlord service is restarted, it will fail to create tasks due to there no longer being any \"inputFormat\" data in the ioConfig.\r\n\r\nThis is due to the way the json marshalling is handled in the configuration persistence. When you issue a command to create a supervisor with an ioConfig like this:\r\n\r\n\"ioConfig\": {\r\n        \"topic\": \"some-cool-topic\",\r\n        \"inputFormat\": {\r\n            \"type\": \"json\"\r\n        },\r\n        \"replicas\": 1,\r\n        \"taskCount\": 2,\r\n        \"taskDuration\": \"PT1800S\",\r\n        \"consumerProperties\": {\r\n            \"bootstrap.servers\": \"kafka:9092\"\r\n        },\r\n        \"pollTimeout\": 100,\r\n        \"startDelay\": \"PT5S\",\r\n        \"period\": \"PT30S\",\r\n        \"useEarliestOffset\": true,\r\n        \"completionTimeout\": \"PT3600S\",\r\n        \"lateMessageRejectionPeriod\": null,\r\n        \"earlyMessageRejectionPeriod\": \"PT3600S\",\r\n        \"stream\": \"npav-ts-metrics\",\r\n        \"useEarliestSequenceNumber\": true\r\n    },\r\n\r\nIt will be accepted, but anytime you try to retrieve this supervisor, you will get back an ioConfig that looks like this:\r\n\r\n\"ioConfig\": {\r\n    \"topic\": \"some-cool-topic\",\r\n    \"replicas\": 1,\r\n    \"taskCount\": 2,\r\n    \"taskDuration\": \"PT1800S\",\r\n    \"consumerProperties\": {\r\n      \"bootstrap.servers\": \"kafka:9092\"\r\n    },\r\n    \"pollTimeout\": 100,\r\n    \"startDelay\": \"PT5S\",\r\n    \"period\": \"PT30S\",\r\n    \"useEarliestOffset\": true,\r\n    \"completionTimeout\": \"PT3600S\",\r\n    \"lateMessageRejectionPeriod\": null,\r\n    \"earlyMessageRejectionPeriod\": \"PT3600S\",\r\n    \"lateMessageRejectionStartDateTime\": null,\r\n    \"stream\": \"npav-ts-metrics\",\r\n    \"useEarliestSequenceNumber\": true,\r\n    \"givenInputFormat\": {\r\n      \"type\": \"json\",\r\n      \"flattenSpec\": {\r\n        \"useFieldDiscovery\": true,\r\n        \"fields\": []\r\n      },\r\n      \"featureSpec\": {}\r\n    }\r\n  },\r\n\r\nNotice that the inputFormat is gone and givenInputFormat is there instead. This supervisor will work properly until you restart the overlord, at which point the supervisor ioConfig will contain the following:\r\n\r\n\"ioConfig\": {\r\n    \"topic\": \"some-cool-topic\",\r\n    \"replicas\": 1,\r\n    \"taskCount\": 2,\r\n    \"taskDuration\": \"PT1800S\",\r\n    \"consumerProperties\": {\r\n      \"bootstrap.servers\": \"kafka:9092\"\r\n    },\r\n    \"pollTimeout\": 100,\r\n    \"startDelay\": \"PT5S\",\r\n    \"period\": \"PT30S\",\r\n    \"useEarliestOffset\": true,\r\n    \"completionTimeout\": \"PT3600S\",\r\n    \"lateMessageRejectionPeriod\": null,\r\n    \"earlyMessageRejectionPeriod\": \"PT3600S\",\r\n    \"lateMessageRejectionStartDateTime\": null,\r\n    \"stream\": \"npav-ts-metrics\",\r\n    \"useEarliestSequenceNumber\": true,\r\n    \"givenInputFormat\": null\r\n  },\r\n\r\nNotice that there is no longer a valid inputFormat, nor is there a valid givenInputFormat. When in this state, the supervisor is unable to create new ingestion tasks."}, {"user": "saulfrank", "commits": {}, "labels": ["Bug"], "created": "2020-02-13 18:12:44", "title": "SQL firehose error", "url": "https://github.com/apache/druid/issues/9359", "closed": "2020-02-15 01:45:13", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "SQL firehose error with org.apache.druid.segment.realtime.firehose.SqlFirehoseFactory cannot be cast to org.apache.druid.data.input.FiniteFirehoseFactory\r\n\r\n### Affected Version\r\n\r\n0.17.0\r\n\r\n### Description\r\n\r\nRan this command (spec below):\r\nbin/post-index-task --file postgresql-test.json --url http://localhost:8081\r\n\r\nGot this error:\r\n```\r\n2020-02-13T17:52:53,628 ERROR [task-runner-0-priority-0] org.apache.druid.indexing.common.task.IndexTask - Encountered exception in NOT_STARTED.\r\njava.lang.ClassCastException: org.apache.druid.segment.realtime.firehose.SqlFirehoseFactory cannot be cast to org.apache.druid.data.input.FiniteFirehoseFactory\r\n\tat org.apache.druid.indexing.common.task.IndexTask$IndexIOConfig.getNonNullInputSource(IndexTask.java:1148) ~[druid-indexing-service-0.17.0.jar:0.17.0]\r\n\tat org.apache.druid.indexing.common.task.IndexTask.runTask(IndexTask.java:477) [druid-indexing-service-0.17.0.jar:0.17.0]\r\n\tat org.apache.druid.indexing.common.task.AbstractBatchIndexTask.run(AbstractBatchIndexTask.java:138) [druid-indexing-service-0.17.0.jar:0.17.0]\r\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:419) [druid-indexing-service-0.17.0.jar:0.17.0]\r\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:391) [druid-indexing-service-0.17.0.jar:0.17.0]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_212]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_212]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_212]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_212]\r\n```\r\n\r\nAll db connections were correct and networking was all working OK.\r\n\r\n**Using this spec**\r\n\r\n```\r\n{\r\n  \"type\": \"index_parallel\",\r\n  \"spec\": {\r\n    \"dataSchema\": {\r\n      \"dataSource\": \"some_datasource\",\r\n      \"parser\": {\r\n        \"parseSpec\": {\r\n          \"format\": \"timeAndDims\",\r\n          \"dimensionsSpec\": {\r\n            \"dimensionExclusions\": [],\r\n            \"dimensions\": [\r\n        \"dim1\",\r\n        \"dim2\"\r\n            ]\r\n          },\r\n          \"timestampSpec\": {\r\n            \"format\": \"auto\",\r\n            \"column\": \"ts\"\r\n          }\r\n        }\r\n      },\r\n      \"metricsSpec\": [],\r\n      \"granularitySpec\": {\r\n        \"type\": \"uniform\",\r\n        \"segmentGranularity\": \"DAY\",\r\n        \"queryGranularity\": {\r\n          \"type\": \"none\"\r\n        },\r\n        \"rollup\": false,\r\n        \"intervals\": null\r\n      },\r\n      \"transformSpec\": {\r\n        \"filter\": null,\r\n        \"transforms\": []\r\n      }\r\n    },\r\n    \"ioConfig\": {\r\n      \"type\": \"index_parallel\",\r\n      \"firehose\": {\r\n        \"type\": \"sql\",\r\n        \"database\": {\r\n          \"type\": \"postgresql\",\r\n          \"connectorConfig\": {\r\n            \"connectURI\": \"jdbc:postgresql://<location>:5432/db\",\r\n            \"user\": \"user\",\r\n            \"password\": \"password\"\r\n          }\r\n        },\r\n        \"sqls\": [\r\n          \"SELECT * FROM some_table\"\r\n        ]\r\n      }\r\n    },\r\n    \"tuningconfig\": {\r\n      \"type\": \"index_parallel\"\r\n    }\r\n  }\r\n}\r\n```\r\n"}, {"user": "jihoonson", "commits": {}, "labels": ["Bug", "Discuss"], "created": "2020-02-11 19:28:12", "title": "Broken feature: appending linearly partitioned segments into a hash partitioned datasource", "url": "https://github.com/apache/druid/issues/9352", "closed": "2020-06-25 20:37:32", "ttf": 135.00027777777777, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.16, 0.17, master\r\n\r\n### Description\r\n\r\nBefore 0.16, Druid used to allow you to create a datasource with the `HashedPartitionsSpec` and then run a task that appends to the datasource with a linear partitioning (using `maxRowsPerSegment`). This was possible because the segments created with `HashedPartitionsSpec` have the `HashBasedNumberedShardSpec` which extends `NumberedShardSpec` which in turn is used for linearly partitioned segments (see https://github.com/apache/druid/blob/0.15.1-incubating/server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java#L691-L700). \r\n\r\nThis feature was broken in #7547 and it is supposed to be a bug. However, I'm wondering we really want to support this in the future because of the below reasons.\r\n\r\n- Allowing mixed partitioning methods for one datasource is confusing and not very useful.\r\n- This feature introduces an ambiguous concept of the \"core partitions\". Only the hash partitioned datasource has the core partitions which is the set of segments created by the initial task. All segments in the core partitions should have the same `HashBasedNumberedShardSpec`, but other segments should have the `NumberedShardSpec`. In the timeline management, a hash partitioned datasource is regarded as visible in brokers if and only if all segments in the core partitions become available in historicals no matter how many segments are left in the non-core partitions. I think this concept is not that useful but makes things complicated.\r\n- This feature allows you to append only _linearly_ partitioned segments to a _hash_ partitioned datasource. Other combinations or directions are not allowed.\r\n- Finally, https://github.com/apache/druid/issues/9241 was recently proposed which seems more promising.\r\n\r\nI would like to promote https://github.com/apache/druid/issues/9241 rather than fixing this bug. Welcome any thoughts."}, {"user": "jon-wei", "commits": {}, "labels": ["Area - Querying", "Bug"], "created": "2020-02-06 22:24:20", "title": "JoinFilterAnalyzer.splitFilter should check for duplicate/shadowing prefixes in clauses", "url": "https://github.com/apache/druid/issues/9329", "closed": "2020-02-26 02:17:24", "ttf": 19.00027777777778, "commitsDetails": [], "body": "This issue describes follow on work to #9301 \r\n\r\n`JoinFilterAnalyzer.splitFilter` (and possibly elsewhere) should check that the prefixes from the joinable clauses do not duplicate or shadow each other. (https://github.com/apache/druid/pull/9301#discussion_r375009323)\r\n"}, {"user": "jon-wei", "commits": {}, "labels": ["Area - Querying", "Bug"], "created": "2020-02-06 21:57:17", "title": "Join query exception when one RHS column is joined to two different columns", "url": "https://github.com/apache/druid/issues/9327", "closed": "2020-02-17 18:54:05", "ttf": 10.000277777777777, "commitsDetails": [], "body": "If a join query has an equicondition where one RHS column is joined to two different columns, e.g.\r\n\r\n```\r\nr1.regionIsoCode == regionIsoCode && r1.regionIsoCode == countryIsoCode\r\n```\r\n\r\nThe `Equality` object for one of the conditions will currently have a `rightColumn` identifier where an extra `_0` suffix is appended to the rhs column name (`regionIsoCode_0` in this example), causing the query to fail with an error like:\r\n\r\n```\r\nCannot build hash-join matcher on non-key-based condition: Equality{leftExpr=countryIsoCode, rightColumn='regionIsoCode_0`\r\n```"}, {"user": "gianm", "commits": {"07a91f90226da1eb56255b6a4e8f77cf5fd8d659": {"commitGHEventType": "referenced", "commitUser": "ccaominh"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "YieldingSequenceBase \"accumulate\" method returns too early sometimes", "numCommits": 0, "created": "2020-01-30 06:22:05", "closed": "2020-01-30 20:01:18", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/9291", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 0.0002777777777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "core/src/test/java/org/apache/druid/java/util/common/guava/SequenceTestHelper.java", "deletions": 3, "insertions": 1, "lines": 4}, {"filePath": "core/src/main/java/org/apache/druid/java/util/common/guava/LimitedSequence.java", "deletions": 6, "insertions": 4, "lines": 10}, {"filePath": "core/src/main/java/org/apache/druid/java/util/common/guava/YieldingSequenceBase.java", "deletions": 0, "insertions": 3, "lines": 3}, {"filePath": "core/src/test/java/org/apache/druid/java/util/common/guava/LimitedSequenceTest.java", "deletions": 11, "insertions": 66, "lines": 77}], "commitSpoonAstDiffStats": [{"spoonFilePath": "LimitedSequenceTest.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 3, "spoonMethodName": "org.apache.druid.java.util.common.guava.LimitedSequenceTest.testTwo()", "MOV": 1, "TOT": 6}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.java.util.common.guava.LimitedSequenceTest", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 3, "spoonMethodName": "org.apache.druid.java.util.common.guava.LimitedSequenceTest.testSanityAccumulate()", "MOV": 2, "TOT": 7}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.java.util.common.guava.LimitedSequenceTest.testWithYieldingSequence()", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 3, "spoonMethodName": "org.apache.druid.java.util.common.guava.LimitedSequenceTest.testOne()", "MOV": 1, "TOT": 6}]}, {"spoonFilePath": "LimitedSequence.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.java.util.common.guava.LimitedSequence.LimitedYieldingAccumulator", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "YieldingSequenceBase.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.java.util.common.guava.YieldingSequenceBase.accumulate(java.lang.Object,org.apache.druid.java.util.common.guava.Accumulator)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "SequenceTestHelper.java", "spoonMethods": [{"INS": 0, "UPD": 4, "DEL": 0, "spoonMethodName": "org.apache.druid.java.util.common.guava.SequenceTestHelper.testYield(java.lang.String,int,org.apache.druid.java.util.common.guava.Sequence,java.util.List)", "MOV": 0, "TOT": 4}, {"INS": 0, "UPD": 5, "DEL": 0, "spoonMethodName": "org.apache.druid.java.util.common.guava.SequenceTestHelper.testClosed(java.util.concurrent.atomic.AtomicInteger,org.apache.druid.java.util.common.guava.Sequence)", "MOV": 0, "TOT": 5}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.java.util.common.guava.SequenceTestHelper.testAccumulation(java.lang.String,org.apache.druid.java.util.common.guava.Sequence,java.util.List)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.java.util.common.guava.SequenceTestHelper.testYield(java.lang.String,int,org.apache.druid.java.util.common.guava.Sequence,java.util.List).1.accumulate(java.lang.Integer,java.lang.Integer)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.java.util.common.guava.SequenceTestHelper.testAccumulation(java.lang.String,org.apache.druid.java.util.common.guava.Sequence,java.util.List).2.accumulate(java.lang.Integer,java.lang.Integer)", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2020-01-30 12:01:18", "commitMessage": "Fix early return from YieldingSequenceBase#accumulate. (#9293)\n\nFixes #9291.", "commitUser": "ccaominh", "commitDateTime": "2020-01-30 12:01:18", "commitParents": ["6b44d4aa804997a47545b70ae44e83d3882bf819"], "commitGHEventType": "referenced", "nameRev": "07a91f90226da1eb56255b6a4e8f77cf5fd8d659 tags/druid-0.18.0-rc1~179", "commitHash": "07a91f90226da1eb56255b6a4e8f77cf5fd8d659"}], "body": "Specifically, when the Yielder returned by `toYielder(initValue, accumulator)` yields on its own, before `accumulator.accumulate` tells it to."}, {"user": "leventov", "commits": {"d268ff729754288132a84fa1bb9b949d1fc5b824": {"commitGHEventType": "referenced", "commitUser": "jihoonson"}}, "changesInPackagesGIT": [], "labels": ["Area - Automation/Static Analysis", "Bug", "Contributions Welcome", "Starter"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "Prohibit assigning ScheduledExecutorService into ExecutorService", "numCommits": 0, "created": "2020-01-29 16:38:51", "closed": "2020-02-12 03:05:49", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/9286", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 13.000277777777777, "commitsDetails": [{"commitGitStats": [{"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java", "deletions": 2, "insertions": 1, "lines": 3}, {"filePath": ".idea/inspectionProfiles/Druid.xml", "deletions": 0, "insertions": 5, "lines": 5}, {"filePath": "extensions-core/druid-basic-security/src/main/java/org/apache/druid/security/basic/CommonCacheNotifier.java", "deletions": 1, "insertions": 1, "lines": 2}], "commitSpoonAstDiffStats": [{"spoonFilePath": "CommonCacheNotifier.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.security.basic.CommonCacheNotifier", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "DruidSchema.java", "spoonMethods": [{"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.schema.DruidSchema", "MOV": 0, "TOT": 2}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2020-02-12 04:05:48", "commitMessage": "Use ExecutorService instead of ScheduledExecutorService where necessary (#9325)\n\n* Use ExecutorService instead of ScheduledExecutorService where necessary - #9286\r\n\r\n* Added inspection rule to prohibit ScheduledExecutorService assignment to ExecutorService\r\n", "commitUser": "jihoonson", "commitDateTime": "2020-02-11 19:05:48", "commitParents": ["5c202343c96e59694808b7c232d85c72704b9926"], "commitGHEventType": "referenced", "nameRev": "d268ff729754288132a84fa1bb9b949d1fc5b824 tags/druid-0.18.0-rc1~140", "commitHash": "d268ff729754288132a84fa1bb9b949d1fc5b824"}], "body": "It's needless, a simple ExecutorService should be created instead.\r\n\r\nThis Structural Search pattern finds two occurrences in the codebase: \r\n\r\n```\r\n      <searchConfiguration name=\"Create a simple ExecutorService (not scheduled)\" text=\"$x$ = $y$;\" recursive=\"true\" caseInsensitive=\"true\" type=\"JAVA\" pattern_context=\"default\">\r\n        <constraint name=\"__context__\" within=\"\" contains=\"\" />\r\n        <constraint name=\"x\" nameOfExprType=\"java\\.util\\.concurrent\\.ExecutorService\" expressionTypes=\"java.util.concurrent.ExecutorService\" within=\"\" contains=\"\" />\r\n        <constraint name=\"y\" nameOfExprType=\"java\\.util\\.concurrent\\.ScheduledExecutorService\" expressionTypes=\"java.util.concurrent.ScheduledExecutorService\" exprTypeWithinHierarchy=\"true\" within=\"\" contains=\"\" />\r\n      </searchConfiguration>\r\n```\r\n\r\nWorth experimenting with variable or field declarations, too."}, {"user": "ceastman-ibm", "commits": {}, "labels": ["Bug", "Contributions Welcome", "Ease of Use", "Starter"], "created": "2020-01-29 00:03:01", "title": "0.17.0 reports Druid requires Java 8", "url": "https://github.com/apache/druid/issues/9276", "closed": "2020-02-19 21:44:01", "ttf": 21.00027777777778, "commitsDetails": [], "body": "Please provide a detailed title (e.g. \"Broker crashes when using TopN query with Bound filter\" instead of just \"Broker crashes\").\r\n\r\n### Affected Version\r\n\r\n0.17.0\r\n\r\n### Description\r\n\r\nPlease include as much detailed information about the problem as possible.\r\n- Cluster size\r\n- Configurations in use\r\n- Steps to reproduce the problem\r\n- The error message or stack traces encountered. Providing more context, such as nearby log messages or even entire logs, can be helpful.\r\n- Any debugging that you have already done\r\n\r\nCarls-MBP:druid carleastman$ kubectl logs druid-789d7d4496-c2g5f\r\nsh: 1: source: not found\r\nDruid requires Java 8. Make sure that \"java\" is installed and on your PATH.\r\n\r\nIf you believe this check is in error, you can skip it using an\r\nenvironment variable:\r\n\r\n  export DRUID_SKIP_JAVA_CHECK=1\r\n\r\nOtherwise, install Java 8 and try again.\r\n\r\nThis script searches for Java 8 in 3 locations in the following\r\norder\r\n  * DRUID_JAVA_HOME\r\n  * JAVA_HOME\r\n  * java (installed on PATH)\r\nCarls-MBP:druid carleastman$ kubectl exec -it druid-7649d9d7dd-9hbjg bash\r\n\r\nroot@druid-7649d9d7dd-9hbjg:/# java -version\r\nopenjdk version \"1.8.0_242\"\r\nOpenJDK Runtime Environment (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08)\r\nOpenJDK 64-Bit Server VM (build 25.242-b08, mixed mode)"}, {"user": "maytasm", "commits": {}, "labels": ["Bug"], "created": "2020-01-18 02:40:57", "title": "Auto Compaction does not work when coordinator is running on 0.17.0 and MiddleManager is running on 0.16.0", "url": "https://github.com/apache/druid/issues/9220", "closed": "2020-01-21 21:56:55", "ttf": 3.000277777777778, "commitsDetails": [], "body": "Auto Compaction does not work when coordinator is running on 0.17.0 and MiddleManager is running on 0.16.0\r\n\r\n### Affected Version\r\n\r\ncoordinator is running on 0.17.0 and MiddleManager is running on 0.16.0\r\n\r\n### Description\r\n\r\nAuto Compaction does not work when coordinator is running on 0.17.0 and MiddleManager is running on 0.16.0.\r\nThis is due to incompatibility when deserializing the json ingestion spec that is sent to MiddleManager from coordinator. This will automatically fix itself (since auto compaction is auto run) after coordinator upgraded to 0.17.0\r\n\r\n`2020-01-18T02:02:34,669 ERROR [TaskMonitorCache-0] org.apache.curator.framework.recipes.cache.PathChildrenCache - \r\ncom.fasterxml.jackson.databind.exc.InvalidTypeIdException: Could not resolve type id 'index' as a subtype of `org.apache.druid.indexing.common.task.batch.parallel.ParallelIndexTuningConfig`: known type ids = [index_parallel, realtime] (for POJO property 'tuningConfig')\r\n at [Source: (byte[])\"{\"type\":\"compact\",\"id\":\"compact_auto-compact-test-new-4_2020-01-18T02:02:34.633Z\",\"resource\":{\"availabilityGroup\":\"compact_auto-compact-test-new-4_2020-01-18T02:02:34.633Z\",\"requiredCapacity\":1},\"dataSource\":\"auto-compact-test-new-4\",\"interval\":null,\"segments\":[{\"dataSource\":\"auto-compact-test-new-4\",\"interval\":\"2015-09-12T00:00:00.000Z/2015-09-13T00:00:00.000Z\",\"version\":\"2020-01-18T01:33:43.101Z\",\"loadSpec\":{\"type\":\"local\",\"path\":\"/tmp/rolling-upgrade-test/var/druid/segments/auto-compact-test-\"[truncated 30732 bytes]; line: 1, column: 30439] (through reference chain: org.apache.druid.indexing.common.task.CompactionTask[\"tuningConfig\"])\r\n        at com.fasterxml.jackson.databind.exc.InvalidTypeIdException.from(InvalidTypeIdException.java:43) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at com.fasterxml.jackson.databind.DeserializationContext.invalidTypeIdException(DeserializationContext.java:1758) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at com.fasterxml.jackson.databind.DeserializationContext.handleUnknownTypeId(DeserializationContext.java:1265) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at com.fasterxml.jackson.databind.jsontype.impl.TypeDeserializerBase._handleUnknownTypeId(TypeDeserializerBase.java:290) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at com.fasterxml.jackson.databind.jsontype.impl.TypeDeserializerBase._findDeserializer(TypeDeserializerBase.java:162) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:113) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:97) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeWithType(BeanDeserializerBase.java:1178) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:527) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:528) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:417) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1287) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:326) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:194) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:161) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:130) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:97) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:254) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at com.fasterxml.jackson.databind.deser.impl.TypeWrappedDeserializer.deserialize(TypeWrappedDeserializer.java:68) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4202) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3266) ~[jackson-databind-2.10.1.jar:2.10.1]\r\n        at org.apache.druid.indexing.worker.WorkerTaskMonitor$1.childEvent(WorkerTaskMonitor.java:165) ~[druid-indexing-service-0.17.0-SNAPSHOT.jar:0.17.0-SNAPSHOT]\r\n        at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:538) [curator-recipes-4.1.0.jar:4.1.0]\r\n        at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:532) [curator-recipes-4.1.0.jar:4.1.0]\r\n        at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:93) [curator-framework-4.1.0.jar:4.1.0]\r\n        at org.apache.curator.shaded.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:435) [curator-client-4.1.0.jar:?]\r\n        at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:85) [curator-framework-4.1.0.jar:4.1.0]\r\n        at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:530) [curator-recipes-4.1.0.jar:4.1.0]\r\n        at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35) [curator-recipes-4.1.0.jar:4.1.0]\r\n        at org.apache.curator.framework.recipes.cache.PathChildrenCache$9.run(PathChildrenCache.java:808) [curator-recipes-4.1.0.jar:4.1.0]\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_232]\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_232]\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_232]\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_232]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_232]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_232]\r\n        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_232]`"}, {"user": "lrosenman", "commits": {}, "labels": ["Bug"], "created": "2020-01-17 22:35:09", "title": "azure extensions: task log kill crashes with \"Not Implemented\".", "url": "https://github.com/apache/druid/issues/9212", "closed": "2020-03-31 17:07:40", "ttf": 73.00027777777778, "commitsDetails": [], "body": "azure extensions: task log kill crashes with \"Not Implemented\"\r\n\r\n### Affected Version\r\n0.16.0-incubating\r\n\r\n### Description\r\nI have a small 3-master, 2-query, 2-storage node cluster and need to be able to shrink the druid_tasks table.  \r\n\r\nI added the following to my overlord configuration:\r\n>  \r\n    # remove old logs\r\n    druid.indexer.logs.kill.enabled=true\r\n    # after: (in ms).\r\n    # 14 days * 24 hours * 60 minutes * 60 seconds * 1000 ms\r\n    druid.indexer.logs.kill.durationToRetain=1209600000`\r\n\r\nand when the kill task ran, I got:\r\n\r\n> 2020-01-17T22:04:38,578 ERROR [Overlord-Helper-Manager-Exec--0] org.apache.druid.indexing.overlord.helpers.TaskLogAutoCleaner - Failed to clean-up the task logs\r\njava.lang.UnsupportedOperationException: not implemented\r\n        at org.apache.druid.storage.azure.AzureTaskLogs.killOlderThan(AzureTaskLogs.java:159) ~[?:?]\r\n        at org.apache.druid.indexing.overlord.helpers.TaskLogAutoCleaner$1.run(TaskLogAutoCleaner.java:75) [druid-indexing-service-0.16.0-incubating.jar:0.16.0-incubating]\r\n        at org.apache.druid.java.util.common.concurrent.ScheduledExecutors$1.call(ScheduledExecutors.java:55) [druid-core-0.16.0-incubating.jar:0.16.0-incubating]\r\n        at org.apache.druid.java.util.common.concurrent.ScheduledExecutors$1.call(ScheduledExecutors.java:51) [druid-core-0.16.0-incubating.jar:0.16.0-incubating]\r\n        at org.apache.druid.java.util.common.concurrent.ScheduledExecutors$2.run(ScheduledExecutors.java:92) [druid-core-0.16.0-incubating.jar:0.16.0-incubating]\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_232]\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_232]\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_232]\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_232]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_232]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_232]\r\n        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_232]\r\n\r\nIs there a plan to implement this?\r\n"}, {"user": "jihoonson", "commits": {}, "labels": ["Bug"], "created": "2020-01-15 01:19:26", "title": "DelimitedInputFormat doesn't support the multi-character delimiter which is supported by DelimitedParser", "url": "https://github.com/apache/druid/issues/9188", "closed": "2020-01-18 02:40:29", "ttf": 3.000277777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.17.0 and master\r\n\r\n### Description\r\n\r\nThe behavior of `DelimitedInputFormat` is different from the `DelimitedParser` in supporting multi-character delimiter. It currently uses the first character as its delimiter if the given delimiter has multiple characters. This behavior should be fixed to match with the behavior of `DelimitedParser`."}, {"user": "jihoonson", "commits": {}, "labels": ["Bug"], "created": "2020-01-13 22:14:30", "title": "The TSV input format doesn't respect \"delimiter\" property", "url": "https://github.com/apache/druid/issues/9177", "closed": "2020-01-18 02:39:57", "ttf": 4.000277777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.17.0 and master branches\r\n\r\n### Description\r\n\r\nThe `delimiter` property is missing in the serialized JSON of the `DelimitedInputFormat`."}, {"user": "ccaominh", "commits": {}, "labels": ["Bug"], "created": "2020-01-13 22:12:45", "title": "hdfs-storage extension is missing org.codehaus.jackson.map.ObjectMapper", "url": "https://github.com/apache/druid/issues/9176", "closed": "2020-01-14 17:50:46", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\nmaster and 0.17.0\r\n\r\n### Description\r\n\r\n`extensions-core/hdfs-storage/pom.xml` currently excludes `jackson-mapper-asl` to remove a security vulnerability, but `jackson-mapper-asl` is needed to provide `org.codehaus.jackson.map.ObjectMapper`:\r\n\r\n```\r\n2020-01-13T16:00:06,327 WARN [qtp415793386-164] org.apache.druid.segment.indexing.DataSchema - No granularitySpec has been specified. Using UniformGranularitySpec as default.\r\n2020-01-13T16:00:06,327 WARN [qtp415793386-164] org.apache.druid.segment.indexing.DataSchema - No metricsSpec has been specified. Are you sure this is what you want?\r\n2020-01-13T16:00:06,356 INFO [qtp415793386-164] org.apache.hadoop.hdfs.DFSClient - Created HDFS_DELEGATION_TOKEN token 161877450 for druid on ha-hdfs:titan\r\n2020-01-13T16:00:06,393 ERROR [qtp415793386-164] com.sun.jersey.spi.container.ContainerResponse - The exception contained within MappableContainerException could not be mapped to a response, re-throwing to the HTTP container\r\njava.lang.NoClassDefFoundError: org/codehaus/jackson/map/ObjectMapper\r\n        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.doDelegationTokenOperation(DelegationTokenAuthenticator.java:320) ~[?:?]\r\n        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.getDelegationToken(DelegationTokenAuthenticator.java:182) ~[?:?]\r\n        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.getDelegationToken(DelegationTokenAuthenticatedURL.java:382) ~[?:?]\r\n        at org.apache.hadoop.crypto.key.kms.KMSClientProvider$4.run(KMSClientProvider.java:1029) ~[?:?]\r\n        at org.apache.hadoop.crypto.key.kms.KMSClientProvider$4.run(KMSClientProvider.java:1023) ~[?:?]\r\n        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_222]\r\n        at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_222]\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844) ~[?:?]\r\n        at org.apache.hadoop.crypto.key.kms.KMSClientProvider.addDelegationTokens(KMSClientProvider.java:1023 ~[?:?]\r\n        at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$1.call(LoadBalancingKMSClientProvider.java:193) ~[?:?]\r\n        at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$1.call(LoadBalancingKMSClientProvider.java:190) ~[?:?]\r\n        at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.doOp(LoadBalancingKMSClientProvider.java:123) ~[?:?]\r\n        at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.addDelegationTokens(LoadBalancingKMSClientProvider.java:190) ~[?:?]\r\n        at org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension.addDelegationTokens(KeyProviderDelegationTokenExtension.java:110) ~[?:?]\r\n        at org.apache.hadoop.hdfs.HdfsKMSUtil.addDelegationTokensForKeyProvider(HdfsKMSUtil.java:83) ~[?:?]\r\n        at org.apache.hadoop.hdfs.DistributedFileSystem.addDelegationTokens(DistributedFileSystem.java:2516) ~[?:?]\r\n        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:143) ~[?:?]\r\n        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:102) ~[?:?]\r\n        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:81) ~[?:?]\r\n        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:249) ~[?:?]\r\n        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:393) ~[?:?]\r\n        at org.apache.druid.inputsource.hdfs.HdfsInputSource.getPaths(HdfsInputSource.java:113) ~[?:?]\r\n        at org.apache.druid.inputsource.hdfs.HdfsInputSource.cachePathsIfNeeded(HdfsInputSource.java:199) ~[?:?]\r\n        at org.apache.druid.inputsource.hdfs.HdfsInputSource.createSplits(HdfsInputSource.java:173) ~[?:?]\r\n        at org.apache.druid.inputsource.hdfs.HdfsInputSource.formattableReader(HdfsInputSource.java:155) ~[?:?]\r\n        at org.apache.druid.data.input.AbstractInputSource.reader(AbstractInputSource.java:42) ~[druid-core-0.17.0-incubating-iap-preview4.jar:0.17.0-incubating-iap-preview4]\r\n```"}, {"user": "jihoonson", "commits": {}, "labels": ["Bug"], "created": "2020-01-09 00:47:02", "title": "CSV and TSV complains about missing findColumnsFromHeader or hasHeaderRow which are optional", "url": "https://github.com/apache/druid/issues/9156", "closed": "2020-01-17 23:35:15", "ttf": 8.000277777777777, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.17.0 branch\r\n\r\n### Description\r\n\r\nHere's the stack trace.\r\n\r\n```\r\norg.apache.druid.java.util.common.IAE: At least one of [Property{name='hasHeaderRow', value=null}, Property{name='findColumnsFromHeader', value=null}] must be present\r\n\r\n\tat org.apache.druid.indexer.Checks.checkOneNotNullOrEmpty(Checks.java:44)\r\n\tat org.apache.druid.data.input.impl.DelimitedInputFormat.<init>(DelimitedInputFormat.java:100)\r\n\tat org.apache.druid.data.input.impl.CsvInputFormat.<init>(CsvInputFormat.java:39)\r\n```\r\n\r\n`findColumnsFromHeader` should have different default values depending on `columns`. If `columns` is null, there should be no default value for `findColumnsFromHeader` so that users can be notified if `columns` is missing by mistake. If `columns` is not null, `findColumnsFromHeader` should be defaulted to false."}, {"user": "jihoonson", "commits": {}, "labels": ["Area - Web Console", "Bug"], "created": "2020-01-08 19:17:43", "title": "Missing \"spec\" in the batch ingestion spec created by the web console", "url": "https://github.com/apache/druid/issues/9144", "closed": "2020-03-12 21:21:24", "ttf": 64.00027777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.17.0 branch\r\n\r\n### Description\r\n\r\nThe below is an ingestion spec created by the web console. `dataSchema`, `ioConfig`, and `tuningConfig` should be in `spec` which is missing. But if you submit this ingestion spec, the `spec` is properly inserted and the task succeeds.\r\n\r\n```json\r\n{\r\n  \"type\": \"index_parallel\",\r\n  \"ioConfig\": {\r\n    \"type\": \"index_parallel\",\r\n    \"inputSource\": {\r\n      \"type\": \"http\",\r\n      \"uris\": [\r\n        \"https://static.imply.io/data/wikipedia.json.gz\"\r\n      ]\r\n    },\r\n    \"inputFormat\": {\r\n      \"type\": \"json\"\r\n    }\r\n  },\r\n  \"tuningConfig\": {\r\n    \"type\": \"index_parallel\",\r\n    \"partitionsSpec\": {\r\n      \"type\": \"dynamic\"\r\n    }\r\n  },\r\n  \"dataSchema\": {\r\n    \"dataSource\": \"wikipedia\",\r\n    \"granularitySpec\": {\r\n      \"type\": \"uniform\",\r\n      \"segmentGranularity\": \"DAY\",\r\n      \"queryGranularity\": \"HOUR\",\r\n      \"rollup\": true\r\n    },\r\n    \"timestampSpec\": {\r\n      \"column\": \"timestamp\",\r\n      \"format\": \"iso\"\r\n    },\r\n    \"dimensionsSpec\": {\r\n      \"dimensions\": [\r\n        \"channel\",\r\n        \"cityName\",\r\n        \"comment\",\r\n        \"countryIsoCode\",\r\n        \"countryName\",\r\n        \"diffUrl\",\r\n        \"flags\",\r\n        \"isAnonymous\",\r\n        \"isMinor\",\r\n        \"isNew\",\r\n        \"isRobot\",\r\n        \"isUnpatrolled\",\r\n        \"namespace\",\r\n        \"page\",\r\n        \"regionIsoCode\",\r\n        \"regionName\",\r\n        \"user\"\r\n      ]\r\n    },\r\n    \"metricsSpec\": [\r\n      {\r\n        \"name\": \"count\",\r\n        \"type\": \"count\"\r\n      },\r\n      {\r\n        \"name\": \"sum_added\",\r\n        \"type\": \"longSum\",\r\n        \"fieldName\": \"added\"\r\n      },\r\n      {\r\n        \"name\": \"sum_commentLength\",\r\n        \"type\": \"longSum\",\r\n        \"fieldName\": \"commentLength\"\r\n      },\r\n      {\r\n        \"name\": \"sum_deleted\",\r\n        \"type\": \"longSum\",\r\n        \"fieldName\": \"deleted\"\r\n      },\r\n      {\r\n        \"name\": \"sum_delta\",\r\n        \"type\": \"longSum\",\r\n        \"fieldName\": \"delta\"\r\n      },\r\n      {\r\n        \"name\": \"sum_deltaBucket\",\r\n        \"type\": \"longSum\",\r\n        \"fieldName\": \"deltaBucket\"\r\n      }\r\n    ]\r\n  }\r\n}\r\n```"}, {"user": "jon-wei", "commits": {}, "labels": ["Bug"], "created": "2020-01-07 23:59:15", "title": "NPE with SQL-compatible null handling in LongSumAggregator compare", "url": "https://github.com/apache/druid/issues/9140", "closed": "2020-01-10 21:49:06", "ttf": 2.000277777777778, "commitsDetails": [], "body": "I observed the following NPE when running a TopN query with `druid.generic.useDefaultValueForNull=false`, when ordering by a long column that contains nulls:\r\n\r\n```\r\njava.lang.NullPointerException: null\r\n\tat org.apache.druid.query.aggregation.LongSumAggregator$1.compare(LongSumAggregator.java:36) ~[druid-processing-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.query.topn.TopNNumericResultBuilder.lambda$new$0(TopNNumericResultBuilder.java:96) ~[druid-processing-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat java.util.PriorityQueue.siftUpUsingComparator(PriorityQueue.java:670) ~[?:1.8.0_202]\r\n\tat java.util.PriorityQueue.siftUp(PriorityQueue.java:646) ~[?:1.8.0_202]\r\n\tat java.util.PriorityQueue.offer(PriorityQueue.java:345) ~[?:1.8.0_202]\r\n\tat java.util.PriorityQueue.add(PriorityQueue.java:322) ~[?:1.8.0_202]\r\n\tat org.apache.druid.query.topn.TopNNumericResultBuilder.addEntry(TopNNumericResultBuilder.java:178) ~[druid-processing-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.query.topn.TopNNumericResultBuilder.addEntry(TopNNumericResultBuilder.java:43) ~[druid-processing-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.query.topn.PooledTopNAlgorithm.updateResults(PooledTopNAlgorithm.java:745) ~[druid-processing-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.query.topn.PooledTopNAlgorithm.updateResults(PooledTopNAlgorithm.java:53) ~[druid-processing-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.query.topn.BaseTopNAlgorithm.runWithCardinalityKnown(BaseTopNAlgorithm.java:122) ~[druid-processing-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.query.topn.BaseTopNAlgorithm.run(BaseTopNAlgorithm.java:83) ~[druid-processing-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.query.topn.TopNMapFn.apply(TopNMapFn.java:67) ~[druid-processing-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.query.topn.TopNQueryEngine$1.apply(TopNQueryEngine.java:97) ~[druid-processing-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.query.topn.TopNQueryEngine$1.apply(TopNQueryEngine.java:90) ~[druid-processing-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:40) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.FilteringAccumulator.accumulate(FilteringAccumulator.java:41) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:40) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.BaseSequence.accumulate(BaseSequence.java:44) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.MappedSequence.accumulate(MappedSequence.java:43) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.WrappingSequence$1.get(WrappingSequence.java:50) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.SequenceWrapper.wrap(SequenceWrapper.java:55) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.WrappingSequence.accumulate(WrappingSequence.java:45) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.FilteredSequence.accumulate(FilteredSequence.java:45) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.MappedSequence.accumulate(MappedSequence.java:43) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.FilteredSequence.accumulate(FilteredSequence.java:45) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.WrappingSequence$1.get(WrappingSequence.java:50) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.SequenceWrapper.wrap(SequenceWrapper.java:55) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.WrappingSequence.accumulate(WrappingSequence.java:45) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.LazySequence.accumulate(LazySequence.java:40) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.WrappingSequence$1.get(WrappingSequence.java:50) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.SequenceWrapper.wrap(SequenceWrapper.java:55) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.WrappingSequence.accumulate(WrappingSequence.java:45) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.LazySequence.accumulate(LazySequence.java:40) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.WrappingSequence$1.get(WrappingSequence.java:50) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.SequenceWrapper.wrap(SequenceWrapper.java:55) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.WrappingSequence.accumulate(WrappingSequence.java:45) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.query.spec.SpecificSegmentQueryRunner$1.accumulate(SpecificSegmentQueryRunner.java:79) ~[druid-processing-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.WrappingSequence$1.get(WrappingSequence.java:50) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.query.spec.SpecificSegmentQueryRunner.doNamed(SpecificSegmentQueryRunner.java:163) ~[druid-processing-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.query.spec.SpecificSegmentQueryRunner.access$100(SpecificSegmentQueryRunner.java:42) ~[druid-processing-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.query.spec.SpecificSegmentQueryRunner$2.wrap(SpecificSegmentQueryRunner.java:145) ~[druid-processing-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.WrappingSequence.accumulate(WrappingSequence.java:45) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.WrappingSequence$1.get(WrappingSequence.java:50) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.query.CPUTimeMetricQueryRunner$1.wrap(CPUTimeMetricQueryRunner.java:74) ~[druid-processing-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.WrappingSequence.accumulate(WrappingSequence.java:45) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.Sequence.toList(Sequence.java:85) ~[druid-core-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.query.ChainedExecutionQueryRunner$1$1.call(ChainedExecutionQueryRunner.java:124) ~[druid-processing-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat org.apache.druid.query.ChainedExecutionQueryRunner$1$1.call(ChainedExecutionQueryRunner.java:114) ~[druid-processing-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_202]\r\n\tat org.apache.druid.query.PrioritizedListenableFutureTask.run(PrioritizedExecutorService.java:247) ~[druid-processing-0.18.0-SNAPSHOT.jar:0.18.0-SNAPSHOT]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_202]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_202]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_202]\r\n```"}, {"user": "jihoonson", "commits": {}, "labels": ["Bug"], "created": "2020-01-06 05:35:14", "title": "NPE while reading a list containing nulls in Orc files", "url": "https://github.com/apache/druid/issues/9131", "closed": "2020-01-08 21:40:24", "ttf": 2.000277777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\nmaster and 0.17.0 branches\r\n\r\n### Description\r\n\r\n`OrcStructConverter.convertPrimitive()` assumes `field` is not null, but `convertList()` can pass null. Here is a stack trace.\r\n\r\n```\r\n2020-01-06T05:30:09,717 ERROR [task-runner-0-priority-0] org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner - Exception while running task[AbstractTask{id='single_phase_sub_task_s3-orc_odngeokb_2020-01-06T05:29:59.197Z', groupId='index_parallel_s3-orc_dbhjmago_2020-01-06T05:29:50.303Z', taskResource=TaskResource{availabilityGroup='single_phase_sub_task_s3-orc_odngeokb_2020-01-06T05:29:59.197Z', requiredCapacity=1}, dataSource='s3-orc', context={forceTimeChunkLock=true}}]\r\njava.lang.NullPointerException: null\r\n\tat org.apache.druid.data.input.orc.OrcStructConverter.convertPrimitive(OrcStructConverter.java:110) ~[?:?]\r\n\tat org.apache.druid.data.input.orc.OrcStructConverter.lambda$convertList$0(OrcStructConverter.java:61) ~[?:?]\r\n\tat java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_212]\r\n\tat java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ~[?:1.8.0_212]\r\n\tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_212]\r\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_212]\r\n\tat java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ~[?:1.8.0_212]\r\n\tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_212]\r\n\tat java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ~[?:1.8.0_212]\r\n\tat org.apache.druid.data.input.orc.OrcStructConverter.convertList(OrcStructConverter.java:62) ~[?:?]\r\n\tat org.apache.druid.data.input.orc.OrcStructConverter.convertField(OrcStructConverter.java:213) ~[?:?]\r\n\tat org.apache.druid.data.input.orc.OrcStructConverter.convertRootField(OrcStructConverter.java:172) ~[?:?]\r\n\tat org.apache.druid.data.input.orc.OrcStructFlattenerMaker.getRootField(OrcStructFlattenerMaker.java:74) ~[?:?]\r\n\tat org.apache.druid.data.input.orc.OrcStructFlattenerMaker.getRootField(OrcStructFlattenerMaker.java:39) ~[?:?]\r\n\tat org.apache.druid.java.util.common.parsers.ObjectFlatteners$1$1.get(ObjectFlatteners.java:118) ~[druid-core-0.17.0-SNAPSHOT.jar:0.17.0-SNAPSHOT]\r\n\tat java.util.Collections$UnmodifiableMap.get(Collections.java:1454) ~[?:1.8.0_212]\r\n\tat org.apache.druid.data.input.MapBasedRow.getRaw(MapBasedRow.java:87) ~[druid-core-0.17.0-SNAPSHOT.jar:0.17.0-SNAPSHOT]\r\n\tat org.apache.druid.segment.incremental.IncrementalIndex.toIncrementalIndexRow(IncrementalIndex.java:672) ~[druid-processing-0.17.0-SNAPSHOT.jar:0.17.0-SNAPSHOT]\r\n\tat org.apache.druid.segment.incremental.IncrementalIndex.add(IncrementalIndex.java:606) ~[druid-processing-0.17.0-SNAPSHOT.jar:0.17.0-SNAPSHOT]\r\n\tat org.apache.druid.segment.realtime.plumber.Sink.add(Sink.java:210) ~[druid-server-0.17.0-SNAPSHOT.jar:0.17.0-SNAPSHOT]\r\n\tat org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.add(AppenderatorImpl.java:259) ~[druid-server-0.17.0-SNAPSHOT.jar:0.17.0-SNAPSHOT]\r\n\tat org.apache.druid.segment.realtime.appenderator.BaseAppenderatorDriver.append(BaseAppenderatorDriver.java:408) ~[druid-server-0.17.0-SNAPSHOT.jar:0.17.0-SNAPSHOT]\r\n\tat org.apache.druid.segment.realtime.appenderator.BatchAppenderatorDriver.add(BatchAppenderatorDriver.java:113) ~[druid-server-0.17.0-SNAPSHOT.jar:0.17.0-SNAPSHOT]\r\n\tat org.apache.druid.indexing.common.task.batch.parallel.SinglePhaseSubTask.generateAndPushSegments(SinglePhaseSubTask.java:476) ~[druid-indexing-service-0.17.0-SNAPSHOT.jar:0.17.0-SNAPSHOT]\r\n\tat org.apache.druid.indexing.common.task.batch.parallel.SinglePhaseSubTask.runTask(SinglePhaseSubTask.java:225) ~[druid-indexing-service-0.17.0-SNAPSHOT.jar:0.17.0-SNAPSHOT]\r\n\tat org.apache.druid.indexing.common.task.AbstractBatchIndexTask.run(AbstractBatchIndexTask.java:138) ~[druid-indexing-service-0.17.0-SNAPSHOT.jar:0.17.0-SNAPSHOT]\r\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:419) [druid-indexing-service-0.17.0-SNAPSHOT.jar:0.17.0-SNAPSHOT]\r\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:391) [druid-indexing-service-0.17.0-SNAPSHOT.jar:0.17.0-SNAPSHOT]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_212]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_212]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_212]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_212]\r\n```"}, {"user": "leventov", "commits": {}, "labels": ["Bug", "Contributions Welcome", "Starter"], "created": "2019-12-29 09:01:58", "title": "Fix concurrency of ComplexMetrics", "url": "https://github.com/apache/druid/issues/9107", "closed": "2020-01-15 14:19:46", "ttf": 17.00027777777778, "commitsDetails": [], "body": "A possible solution:\r\n 1. Make `COMPLEX_SERIALIZERS` a `ConcurrentHashMap`\r\n 2. Convert the whole body of `registerSerde()` method as a single `COMPLEX_SERIALIZERS.compute()` operation\r\n\r\nAlternatively, both `getSerdeForType()` and `registerSerde()` methods could be made `synchronized`."}, {"user": "gianm", "commits": {"18eb456fe6190e6db5d8e76324cc7b83cc4e0a35": {"commitGHEventType": "referenced", "commitUser": "gianm"}, "0680a8d2b0d7b7afcdb4bd310d7844829ef099af": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Area - Batch Ingestion", "Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "S3: Infinite loop when listing prefixes that need multiple batches", "numCommits": 0, "created": "2019-12-25 06:00:16", "closed": "2020-01-01 00:06:50", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/9097", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 6.000277777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/ObjectSummaryIteratorTest.java", "deletions": 0, "insertions": 239, "lines": 239}, {"filePath": "extensions-core/s3-extensions/src/test/java/org/apache/druid/data/input/s3/S3InputSourceTest.java", "deletions": 40, "insertions": 74, "lines": 114}, {"filePath": "extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/ObjectSummaryIterator.java", "deletions": 0, "insertions": 173, "lines": 173}, {"filePath": "extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/S3Utils.java", "deletions": 164, "insertions": 13, "lines": 177}, {"filePath": "extensions-core/s3-extensions/src/main/java/org/apache/druid/firehose/s3/StaticS3FirehoseFactory.java", "deletions": 44, "insertions": 8, "lines": 52}, {"filePath": "extensions-core/s3-extensions/src/main/java/org/apache/druid/data/input/s3/S3InputSource.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/S3TimestampVersionedDataFinder.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "extensions-core/s3-extensions/pom.xml", "deletions": 0, "insertions": 5, "lines": 5}], "commitSpoonAstDiffStats": [{"spoonFilePath": "ObjectSummaryIteratorTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.storage.s3.ObjectSummaryIteratorTest", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "S3TimestampVersionedDataFinder.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.storage.s3.S3TimestampVersionedDataFinder.getLatestVersion(java.net.URI,java.util.regex.Pattern)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "S3InputSourceTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.expectListObjectsAndThrowAccessDenied(java.net.URI)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 3, "DEL": 2, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.addExpectedPrefixObjects(java.net.URI,java.util.List)", "MOV": 2, "TOT": 7}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.addExpectedGetObjectMock(java.net.URI)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 3, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.testWithPrefixesWhereOneIsUrisAndNoListPermissionSplit()", "MOV": 4, "TOT": 7}, {"INS": 1, "UPD": 4, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.testCompressedReader()", "MOV": 1, "TOT": 6}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.testSerdeWithInvalidArgs()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 10, "DEL": 1, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.addExpectedNonPrefixObjectsWithNoListPermission()", "MOV": 11, "TOT": 22}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.matchListObjectsRequest(java.net.URI)", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.expectListObjects(java.net.URI,java.util.List)", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.addExpectedGetCompressedObjectMock(java.net.URI)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 2, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.testWithPrefixesSplit()", "MOV": 0, "TOT": 3}, {"INS": 1, "UPD": 4, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.testReader()", "MOV": 1, "TOT": 6}]}, {"spoonFilePath": "ObjectSummaryIterator.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.storage.s3.ObjectSummaryIterator", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "StaticS3FirehoseFactory.java", "spoonMethods": [{"INS": 3, "UPD": 2, "DEL": 5, "spoonMethodName": "org.apache.druid.firehose.s3.StaticS3FirehoseFactory.initObjects()", "MOV": 6, "TOT": 16}]}, {"spoonFilePath": "S3Utils.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.storage.s3.S3Utils.lazyFetchingObjectSummariesIterator(org.apache.druid.storage.s3.ServerSideEncryptingAmazonS3,java.util.Iterator,int)", "MOV": 3, "TOT": 4}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.storage.s3.S3Utils.isDirectoryPlaceholder(java.lang.String,com.amazonaws.services.s3.model.ObjectMetadata)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.storage.s3.S3Utils", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 2, "DEL": 4, "spoonMethodName": "org.apache.druid.storage.s3.S3Utils.objectSummaryIterator(org.apache.druid.storage.s3.ServerSideEncryptingAmazonS3,java.net.URI,int)", "MOV": 0, "TOT": 6}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.storage.s3.S3Utils.objectSummaryIterator(org.apache.druid.storage.s3.ServerSideEncryptingAmazonS3,java.lang.Iterable,int)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "S3InputSource.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSource.getIterableObjectsFromPrefixes()", "MOV": 1, "TOT": 2}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2019-12-31 19:06:49", "commitMessage": "S3: Improvements to prefix listing (including fix for an infinite loop) (#9098)\n\n* S3: Improvements to prefix listing (including fix for an infinite loop)\r\n\r\n1) Fixes #9097, an infinite loop that occurs when more than one batch\r\n   of objects is retrieved during a prefix listing.\r\n\r\n2) Removes the Access Denied fallback code added in #4444. I don't think\r\n   the behavior is reasonable: its purpose is to fall back from a prefix\r\n   listing to a single-object access, but it's only activated when the\r\n   end user supplied a prefix, so it would be better to simply fail, so\r\n   the end user knows that their request for a prefix-based load is not\r\n   going to work. Presumably the end user can switch from supplying\r\n   'prefixes' to supplying 'uris' if desired.\r\n\r\n3) Filters out directory placeholders when walking prefixes.\r\n\r\n4) Splits LazyObjectSummariesIterator into its own class and adds tests.\r\n\r\n* Adjust S3InputSourceTest.\r\n\r\n* Changes from review.\r\n\r\n* Include hamcrest-core.\r\n", "commitUser": "gianm", "commitDateTime": "2019-12-31 19:06:49", "commitParents": ["dec619ebf4972fb59257b31649a4d416319b00a2"], "commitGHEventType": "referenced", "nameRev": "18eb456fe6190e6db5d8e76324cc7b83cc4e0a35 tags/druid-0.18.0-rc1~247", "commitHash": "18eb456fe6190e6db5d8e76324cc7b83cc4e0a35"}, {"commitGitStats": [{"filePath": "extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/ObjectSummaryIteratorTest.java", "deletions": 0, "insertions": 239, "lines": 239}, {"filePath": "extensions-core/s3-extensions/src/test/java/org/apache/druid/data/input/s3/S3InputSourceTest.java", "deletions": 40, "insertions": 74, "lines": 114}, {"filePath": "extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/ObjectSummaryIterator.java", "deletions": 0, "insertions": 173, "lines": 173}, {"filePath": "extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/S3Utils.java", "deletions": 164, "insertions": 13, "lines": 177}, {"filePath": "extensions-core/s3-extensions/src/main/java/org/apache/druid/firehose/s3/StaticS3FirehoseFactory.java", "deletions": 44, "insertions": 8, "lines": 52}, {"filePath": "extensions-core/s3-extensions/src/main/java/org/apache/druid/data/input/s3/S3InputSource.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/S3TimestampVersionedDataFinder.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "extensions-core/s3-extensions/pom.xml", "deletions": 0, "insertions": 5, "lines": 5}], "commitSpoonAstDiffStats": [{"spoonFilePath": "ObjectSummaryIteratorTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.storage.s3.ObjectSummaryIteratorTest", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "S3TimestampVersionedDataFinder.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.storage.s3.S3TimestampVersionedDataFinder.getLatestVersion(java.net.URI,java.util.regex.Pattern)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "S3InputSourceTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.expectListObjectsAndThrowAccessDenied(java.net.URI)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 3, "DEL": 2, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.addExpectedPrefixObjects(java.net.URI,java.util.List)", "MOV": 2, "TOT": 7}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.addExpectedGetObjectMock(java.net.URI)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 3, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.testWithPrefixesWhereOneIsUrisAndNoListPermissionSplit()", "MOV": 4, "TOT": 7}, {"INS": 1, "UPD": 4, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.testCompressedReader()", "MOV": 1, "TOT": 6}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.testSerdeWithInvalidArgs()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 10, "DEL": 1, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.addExpectedNonPrefixObjectsWithNoListPermission()", "MOV": 11, "TOT": 22}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.matchListObjectsRequest(java.net.URI)", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.expectListObjects(java.net.URI,java.util.List)", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.addExpectedGetCompressedObjectMock(java.net.URI)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 2, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.testWithPrefixesSplit()", "MOV": 0, "TOT": 3}, {"INS": 1, "UPD": 4, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSourceTest.testReader()", "MOV": 1, "TOT": 6}]}, {"spoonFilePath": "ObjectSummaryIterator.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.storage.s3.ObjectSummaryIterator", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "StaticS3FirehoseFactory.java", "spoonMethods": [{"INS": 3, "UPD": 2, "DEL": 5, "spoonMethodName": "org.apache.druid.firehose.s3.StaticS3FirehoseFactory.initObjects()", "MOV": 6, "TOT": 16}]}, {"spoonFilePath": "S3Utils.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.storage.s3.S3Utils.lazyFetchingObjectSummariesIterator(org.apache.druid.storage.s3.ServerSideEncryptingAmazonS3,java.util.Iterator,int)", "MOV": 3, "TOT": 4}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.storage.s3.S3Utils.isDirectoryPlaceholder(java.lang.String,com.amazonaws.services.s3.model.ObjectMetadata)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.storage.s3.S3Utils", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 2, "DEL": 4, "spoonMethodName": "org.apache.druid.storage.s3.S3Utils.objectSummaryIterator(org.apache.druid.storage.s3.ServerSideEncryptingAmazonS3,java.net.URI,int)", "MOV": 0, "TOT": 6}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.storage.s3.S3Utils.objectSummaryIterator(org.apache.druid.storage.s3.ServerSideEncryptingAmazonS3,java.lang.Iterable,int)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "S3InputSource.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.data.input.s3.S3InputSource.getIterableObjectsFromPrefixes()", "MOV": 1, "TOT": 2}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2020-01-03 19:39:27", "commitMessage": "S3: Improvements to prefix listing (including fix for an infinite loop) (#9098) (#9118)\n\n* S3: Improvements to prefix listing (including fix for an infinite loop)\r\n\r\n1) Fixes #9097, an infinite loop that occurs when more than one batch\r\n   of objects is retrieved during a prefix listing.\r\n\r\n2) Removes the Access Denied fallback code added in #4444. I don't think\r\n   the behavior is reasonable: its purpose is to fall back from a prefix\r\n   listing to a single-object access, but it's only activated when the\r\n   end user supplied a prefix, so it would be better to simply fail, so\r\n   the end user knows that their request for a prefix-based load is not\r\n   going to work. Presumably the end user can switch from supplying\r\n   'prefixes' to supplying 'uris' if desired.\r\n\r\n3) Filters out directory placeholders when walking prefixes.\r\n\r\n4) Splits LazyObjectSummariesIterator into its own class and adds tests.\r\n\r\n* Adjust S3InputSourceTest.\r\n\r\n* Changes from review.\r\n\r\n* Include hamcrest-core.", "commitUser": "fjy", "commitDateTime": "2020-01-03 16:39:27", "commitParents": ["08cace906d84ecbd77481301cad0e84d927e1844"], "commitGHEventType": "referenced", "nameRev": "0680a8d2b0d7b7afcdb4bd310d7844829ef099af tags/druid-0.17.0-rc1~31", "commitHash": "0680a8d2b0d7b7afcdb4bd310d7844829ef099af"}], "body": "`S3Utils.lazyFetchingObjectSummariesIterator` and `S3Utils.objectSummaryIterator` (used for listing prefixes by S3InputSource and StaticS3FirehoseFactory) both have a bug where they do something like this:\r\n\r\n```java\r\nif (result.isTruncated()) {\r\n  request.setContinuationToken(result.getContinuationToken());\r\n}\r\n```\r\n\r\ni.e. they make follow-on requests based on the continuation token of the prior result. But this is wrong, since they should do it based on the _next_ continuation token: `result.getNextContinuationToken()`. This leads to an infinite loop if more than one batch is needed."}, {"user": "samarthjain", "commits": {}, "labels": ["Bug"], "created": "2019-12-18 21:21:48", "title": "Fix tdigest aggregators to handle null values and enable them to be used for rollup during ingestion", "url": "https://github.com/apache/druid/issues/9069", "closed": "2019-12-24 01:49:07", "ttf": 5.000277777777778, "commitsDetails": [], "body": "### Affected Version\r\n0.16.0\r\nThe Druid version where the problem was encountered.\r\n0.16.0\r\n### Description\r\nDuring our local testing I ran into an issue where the t-digest aggregators throw an NPE when a null value is encountered. Additionally, while doing the ingestion, it turned out that the aggregators need to implement the `makeAggregateCombiner` interface. While this worked fine in 0.12.0, something has changed from then which makes this an explicit requirement. \r\n\r\n"}, {"user": "clintropolis", "commits": {}, "labels": ["Area - Null Handling", "Bug"], "created": "2019-12-17 23:45:24", "title": "vectorized query engine numeric filter matchers do not correctly handle matching numeric null values", "url": "https://github.com/apache/druid/issues/9062", "closed": "2019-12-20 21:15:48", "ttf": 2.000277777777778, "commitsDetails": [], "body": "\r\n### Affected Version\r\n\r\n0.16.0+\r\n\r\n### Description\r\n\r\nThis functionality was not implemented, https://github.com/apache/incubator-druid/blob/master/processing/src/main/java/org/apache/druid/query/filter/vector/LongVectorValueMatcher.java#L44"}, {"user": "gianm", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug"], "created": "2019-12-13 01:24:26", "title": "GenericIndexedWriter can fail when writing large values into large columns", "url": "https://github.com/apache/druid/issues/9027", "closed": "2019-12-13 23:33:15", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "In GenericIndexedWriter, for each value written:\r\n\r\n1. A value is written to `valuesOut`.\r\n2. The current offset in `valuesOut` (the _end_ of the just-written value) is written into the header as an int (with a checked cast).\r\n3. `getSerializedSize()`, which includes `valuesOut` and some other stuff, is called to see if it is larger than the `fileSizeLimit`, which is very close to the max int value. If so, the writer will transition from single-file to multi-file mode.\r\n\r\nThis works great if `valuesOut` grows slowly enough that \"other stuff\" in `getSerializedSize()`, plus the slack space between `fileSizeLimit` and `Integer.MAX_VALUE`, is larger than any one particular addition to `valuesOut`. Otherwise, the checked cast in (2) will fail."}, {"user": "jihoonson", "commits": {}, "labels": ["Bug"], "created": "2019-12-09 21:38:44", "title": "Hadoop task fails because of duplicate injection of Configuration", "url": "https://github.com/apache/druid/issues/9004", "closed": "2019-12-12 01:30:45", "ttf": 2.000277777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\nCurrent master\r\n\r\n### Description\r\n\r\nHere is the error.\r\n\r\n```\r\n2019-12-09T15:38:33,215 ERROR [task-runner-0-priority-0] org.apache.druid.indexing.common.task.HadoopIndexTask - Got invocation target exception in run(), cause: \r\njava.lang.ExceptionInInitializerError: null\r\n\tat org.apache.druid.indexing.common.task.HadoopIndexTask$HadoopDetermineConfigInnerProcessingRunner.runTask(HadoopIndexTask.java:639) ~[druid-indexing-service-0.17.0-incubating-iap-preview1.jar:0.17.0-incubating-iap-preview1]\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_181]\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_181]\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_181]\r\n\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_181]\r\n\tat org.apache.druid.indexing.common.task.HadoopIndexTask.runInternal(HadoopIndexTask.java:347) ~[druid-indexing-service-0.17.0-incubating-iap-preview1.jar:0.17.0-incubating-iap-preview1]\r\n\tat org.apache.druid.indexing.common.task.HadoopIndexTask.runTask(HadoopIndexTask.java:281) [druid-indexing-service-0.17.0-incubating-iap-preview1.jar:0.17.0-incubating-iap-preview1]\r\n\tat org.apache.druid.indexing.common.task.AbstractBatchIndexTask.run(AbstractBatchIndexTask.java:138) [druid-indexing-service-0.17.0-incubating-iap-preview1.jar:0.17.0-incubating-iap-preview1]\r\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:419) [druid-indexing-service-0.17.0-incubating-iap-preview1.jar:0.17.0-incubating-iap-preview1]\r\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:391) [druid-indexing-service-0.17.0-incubating-iap-preview1.jar:0.17.0-incubating-iap-preview1]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_181]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_181]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_181]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]\r\nCaused by: com.google.inject.CreationException: Unable to create injector, see the following errors:\r\n\r\n1) A binding to org.apache.hadoop.conf.Configuration was already configured at org.apache.druid.storage.hdfs.HdfsStorageDruidModule.configure(HdfsStorageDruidModule.java:111) (via modules: com.google.inject.util.Modules$OverrideModule -> org.apache.druid.storage.hdfs.HdfsStorageDruidModule).\r\n  at org.apache.druid.data.input.parquet.ParquetExtensionsModule.configure(ParquetExtensionsModule.java:101) (via modules: com.google.inject.util.Modules$OverrideModule -> org.apache.druid.data.input.parquet.ParquetExtensionsModule)\r\n\r\n1 error\r\n\tat com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:470) ~[guice-4.1.0.jar:?]\r\n\tat com.google.inject.internal.InternalInjectorCreator.initializeStatically(InternalInjectorCreator.java:155) ~[guice-4.1.0.jar:?]\r\n\tat com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:107) ~[guice-4.1.0.jar:?]\r\n\tat com.google.inject.Guice.createInjector(Guice.java:99) ~[guice-4.1.0.jar:?]\r\n\tat com.google.inject.Guice.createInjector(Guice.java:73) ~[guice-4.1.0.jar:?]\r\n\tat com.google.inject.Guice.createInjector(Guice.java:62) ~[guice-4.1.0.jar:?]\r\n\tat org.apache.druid.initialization.Initialization.makeInjectorWithModules(Initialization.java:431) ~[druid-server-0.17.0-incubating-iap-preview1.jar:0.17.0-incubating-iap-preview1]\r\n\tat org.apache.druid.indexer.HadoopDruidIndexerConfig.<clinit>(HadoopDruidIndexerConfig.java:101) ~[druid-indexing-hadoop-0.17.0-incubating-iap-preview1.jar:0.17.0-incubating-iap-preview1]\r\n\t... 14 more\r\n```"}, {"user": "jihoonson", "commits": {}, "labels": ["Bug"], "created": "2019-12-09 19:55:04", "title": "Broken master because of duplicate injection", "url": "https://github.com/apache/druid/issues/9002", "closed": "2019-12-11 23:56:37", "ttf": 2.000277777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\nCurrent master\r\n\r\n### Description\r\n\r\nHere is the error on start up. Maybe it only happens with coordinator + overlord mode.\r\n\r\n```\r\n1) A binding to org.apache.druid.discovery.NodeRole annotated with interface org.apache.druid.guice.annotations.Self was already configured at org.apache.druid.cli.CliCoordinator$1.configure(CliCoordinator.java:242) (via modules: com.google.inject.util.Modules$Override\r\nModule -> com.google.inject.util.Modules$OverrideModule -> org.apache.druid.cli.CliCoordinator$1).\r\n  at org.apache.druid.cli.CliOverlord$1.configure(CliOverlord.java:251) (via modules: com.google.inject.util.Modules$OverrideModule -> com.google.inject.util.Modules$OverrideModule -> org.apache.druid.cli.CliOverlord$1)\r\n\r\n1 error\r\n        at org.apache.druid.cli.GuiceRunnable.makeInjector(GuiceRunnable.java:72)\r\n        at org.apache.druid.cli.ServerRunnable.run(ServerRunnable.java:56)\r\n        at org.apache.druid.cli.Main.main(Main.java:113)\r\nCaused by: com.google.inject.CreationException: Unable to create injector, see the following errors:\r\n\r\n1) A binding to org.apache.druid.discovery.NodeRole annotated with interface org.apache.druid.guice.annotations.Self was already configured at org.apache.druid.cli.CliCoordinator$1.configure(CliCoordinator.java:242) (via modules: com.google.inject.util.Modules$Override\r\nModule -> com.google.inject.util.Modules$OverrideModule -> org.apache.druid.cli.CliCoordinator$1).\r\n  at org.apache.druid.cli.CliOverlord$1.configure(CliOverlord.java:251) (via modules: com.google.inject.util.Modules$OverrideModule -> com.google.inject.util.Modules$OverrideModule -> org.apache.druid.cli.CliOverlord$1)\r\n\r\n1 error\r\n        at com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:470)\r\n        at com.google.inject.internal.InternalInjectorCreator.initializeStatically(InternalInjectorCreator.java:155)\r\n        at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:107)\r\n        at com.google.inject.Guice.createInjector(Guice.java:99)\r\n        at com.google.inject.Guice.createInjector(Guice.java:73)\r\n        at com.google.inject.Guice.createInjector(Guice.java:62)\r\n        at org.apache.druid.initialization.Initialization.makeInjectorWithModules(Initialization.java:431)\r\n        at org.apache.druid.cli.GuiceRunnable.makeInjector(GuiceRunnable.java:69)\r\n        ... 2 more\r\n```"}, {"user": "leventov", "commits": {}, "labels": ["Bug"], "created": "2019-11-14 10:20:22", "title": "splitHintSpec is not added to ClientCompactQueryTuningConfig's equals(), hashCode(), and toString()", "url": "https://github.com/apache/druid/issues/8866", "closed": "2019-12-16 22:33:01", "ttf": 32.000277777777775, "commitsDetails": [], "body": "This bug is introduced in #8570, see [diff](https://github.com/apache/incubator-druid/commit/30c15900bec7f4df7a72088ec076223839da258f#diff-d961bf5891534d69c4b45d0c1f69431f).\r\n\r\nFYI @jihoonson "}, {"user": "glasser", "commits": {}, "labels": ["Area - Web Console", "Bug"], "created": "2019-11-11 20:34:13", "title": "web console: Load rule includeFuture flag is not visible or editable", "url": "https://github.com/apache/druid/issues/8855", "closed": "2019-11-13 04:34:31", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\nTested on 0.15.1. From source examination, still problematic on 0.16/master.\r\n\r\n### Description\r\n\r\nhttps://github.com/apache/incubator-druid/pull/6414 added an includeFuture flag to loadByPeriod and dropByPeriod rules, but the web console doesn't let you see or edit this flag."}, {"user": "vogievetsky", "commits": {}, "labels": ["Area - Batch Ingestion", "Area - Web Console", "Bug"], "created": "2019-11-08 02:02:42", "title": "Sampler swallows up `null`s in CSV", "url": "https://github.com/apache/druid/issues/8845", "closed": "2019-11-19 21:59:45", "ttf": 11.000277777777777, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.16.0 and probably 0.15.0\r\n\r\n### Description\r\n\r\nWhen parsing a CSV in the sampler it totally ignores columns that should return as `null` or `\"\"`\r\n\r\nRequest:\r\n\r\nPOST `/druid/indexer/v1/sampler`\r\n\r\nPayload:\r\n\r\n```json\r\n{\r\n  \"type\": \"index\",\r\n  \"spec\": {\r\n    \"type\": \"index\",\r\n    \"ioConfig\": {\r\n      \"type\": \"index\",\r\n      \"firehose\": {\r\n        \"type\": \"inline\",\r\n        \"data\": \"Make,Model,Color,Year\\nHonda,,,2009\\nVW,,,2010\"\r\n      }\r\n    },\r\n    \"dataSchema\": {\r\n      \"dataSource\": \"sample\",\r\n      \"parser\": {\r\n        \"type\": \"string\",\r\n        \"parseSpec\": {\r\n          \"format\": \"csv\",\r\n          \"timestampSpec\": {\r\n            \"column\": \"!!!_no_such_column_!!!\",\r\n            \"missingValue\": \"2010-01-01T00:00:00Z\"\r\n          },\r\n          \"dimensionsSpec\": {},\r\n          \"hasHeaderRow\": true\r\n        }\r\n      }\r\n    }\r\n  },\r\n  \"samplerConfig\": {\r\n    \"numRows\": 500,\r\n    \"timeoutMs\": 15000,\r\n    \"cacheKey\": \"4cadbef182774626b2f2e397ec28e5f8\"\r\n  }\r\n}\r\n```\r\n\r\nResult:\r\n\r\n```json\r\n{\r\n  \"cacheKey\": \"4cadbef182774626b2f2e397ec28e5f8\",\r\n  \"numRowsRead\": 2,\r\n  \"numRowsIndexed\": 2,\r\n  \"data\": [\r\n    {\r\n      \"raw\": \"Honda,,,2009\",\r\n      \"parsed\": {\r\n        \"__time\": 1262304000000,\r\n        \"Year\": \"2009\",\r\n        \"Make\": \"Honda\"\r\n      }\r\n    },\r\n    {\r\n      \"raw\": \"VW,,,2010\",\r\n      \"parsed\": {\r\n        \"__time\": 1262304000000,\r\n        \"Year\": \"2010\",\r\n        \"Make\": \"VW\"\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nWhere are `Model` / `Color` ?"}, {"user": "sixtus", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug"], "created": "2019-11-07 11:12:00", "title": "index_hadoop tasks fail on wrong file format when run inside indexer", "url": "https://github.com/apache/druid/issues/8840", "closed": "2019-12-18 20:13:20", "ttf": 41.000277777777775, "commitsDetails": [], "body": "we have seen this before, it's not replacing `:` and thus HDFS refuses the file as it's illegal.\r\n\r\nI guess yet another thing fixed in peon but not in indexer?\r\n\r\n```java.lang.IllegalArgumentException: Pathname /druid/indexer/foo/2019-10-29T00:00:00.000Z_2019-10-30T00:00:00.000Z/2019-11-06T19:57:56.216Z/29/index.zip.0 from hdfs://us2/druid/indexer/foo/2019-10-29T00:00:00.000Z_2019-10-30T00:00:00.000Z/2019-11-06T19:57:56.216Z/29/index.zip.0 is not a valid DFS filename.```"}, {"user": "jihoonson", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2019-11-07 02:44:47", "title": "Kafka indexing task can fail during rolling update", "url": "https://github.com/apache/druid/issues/8837", "closed": "2019-11-19 21:11:32", "ttf": 12.000277777777777, "commitsDetails": [], "body": "### Affected Version\r\n\r\nWhen updating 0.15 or earlier -> 0.16\r\n\r\n### Description\r\n\r\nhttps://github.com/apache/incubator-druid/pull/7319 has removed an unused parameter, but it can cause task failures during rolling update with the following error.\r\n\r\n```\r\n2019-11-07T00:23:20,740 WARN [task-runner-0-priority-0] org.apache.druid.indexing.common.actions.RemoteTaskActionClient - Exception submitting action for task[index_kafka_clarity-cloud0_7c42ed11c91cf72_ghfhppbk]\r\norg.apache.druid.java.util.common.IOE: Error with status[400 Bad Request] and message[{\"error\":\"Instantiation of [simple type, class org.apache.druid.indexing.common.actions.CheckPointDataSourceMetadataAction] value failed: currentCheckPoint (through reference chain: org.apache.druid.indexing.common.actions.TaskActionHolder[\\\"action\\\"])\"}]. Check overlord logs for details.\r\n\tat org.apache.druid.indexing.common.actions.RemoteTaskActionClient.submit(RemoteTaskActionClient.java:94) [druid-indexing-service-0.16.0-incubating-iap3.jar:0.16.0-incubating-iap3]\r\n\tat org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner.runInternal(SeekableStreamIndexTaskRunner.java:736) [druid-indexing-service-0.16.0-incubating-iap3.jar:0.16.0-incubating-iap3]\r\n\tat org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner.run(SeekableStreamIndexTaskRunner.java:259) [druid-indexing-service-0.16.0-incubating-iap3.jar:0.16.0-incubating-iap3]\r\n\tat org.apache.druid.indexing.seekablestream.SeekableStreamIndexTask.run(SeekableStreamIndexTask.java:177) [druid-indexing-service-0.16.0-incubating-iap3.jar:0.16.0-incubating-iap3]\r\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:419) [druid-indexing-service-0.16.0-incubating-iap3.jar:0.16.0-incubating-iap3]\r\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:391) [druid-indexing-service-0.16.0-incubating-iap3.jar:0.16.0-incubating-iap3]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_212]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_212]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_212]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_212]\r\n```\r\n\r\nTo fix this issue, `CheckPointDataSourceMetadataAction` can have a method as below to add the missing property in its serialized JSON.\r\n\r\n```java\r\n  // For backwards compatibility\r\n  @Deprecated\r\n  @JsonProperty\r\n  public SeekableStreamDataSourceMetadata getCurrentCheckPoint()\r\n  {\r\n    return checkpointMetadata;\r\n  }\r\n```"}, {"user": "jihoonson", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug"], "created": "2019-11-06 22:27:05", "title": "Parallel indexing task fails during the rolling update", "url": "https://github.com/apache/druid/issues/8836", "closed": "2019-11-20 03:29:26", "ttf": 13.000277777777777, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.16\r\n\r\n### Description\r\n\r\nThere are roughly two types of rolling update. One is stopping MMs immediately and restarting them with a newer version. Another is spawning a new instance for a new MM, waiting for an old MM to finish all running tasks and stopping it. Parallel indexing task fails with the below message during the second type of rolling update.\r\n\r\n```\r\njava.lang.RuntimeException: org.apache.druid.java.util.common.ISE: Failed to post task[AbstractTask{id='single_phase_sub_task_exchange_report_sampled_2019-11-06T21:25:33.976Z', groupId='index_parallel_exchange_report_sampled_2019-11-06T21:25:26.427Z', taskResource=TaskResource{availabilityGroup='single_phase_sub_task_exchange_report_sampled_2019-11-06T21:25:33.976Z', requiredCapacity=1}, dataSource='exchange_report_sampled', context={}}] with error[{\"error\":\"Could not resolve type id 'single_phase_sub_task' into a subtype of [simple type, class org.apache.druid.indexing.common.task.Task]: known type ids = [Task, archive, compact, index, index_hadoop, index_kafka, index_kinesis, index_parallel, index_realtime, index_realtime_appenderator, index_sub, kill, move, noop, restore]\\n at [Source: HttpInputOverHTTP@43b1d6e5[c=4286,q=0,[0]=null,s=STREAM]; line: 1, column: 2]\"}].\r\n```\r\n\r\nThis is because one of the sub task type name has been changed in https://github.com/apache/incubator-druid/pull/8257. Note we should be able to upgrade the overload last.\r\n\r\nI think the supervisor task can first try to issue a sub task with the new type name, and then retry with the old name if the first try fails with the above error."}, {"user": "abhishekrb19", "commits": {}, "labels": ["Area - Web Console", "Bug"], "created": "2019-10-30 20:05:15", "title": "Druid Console SQL query page crashes for a timeseries query issued with grandTotal query", "url": "https://github.com/apache/druid/issues/8791", "closed": "2019-10-31 02:37:54", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "The Druid Console SQL query page crashes for a timeseries query issued with `grandTotal` query param set to `true` in the query param.\r\n\r\n### Affected Version\r\n0.16.0\r\n\r\n### Description\r\n- MicroQuickStart setup running locally on OSX\r\n- Issue a `timeseries` query, with `\"grandTotal\" : true` set in the query context param in the UI\r\n\r\nThe same query issued directly to the Druid Broker via a `curl` command with the same query params set works:\r\n```json\r\n[\r\n  {\r\n    \"EXPR$0\": \"2019-10-29T21:30:00.000Z\",\r\n    \"EXPR$1\": 112\r\n  },\r\n  {\r\n    \"EXPR$0\": \"2019-10-29T21:33:00.000Z\",\r\n    \"EXPR$1\": 106\r\n  },\r\n  {\r\n    \"EXPR$0\": \"2019-10-29T21:36:00.000Z\",\r\n    \"EXPR$1\": 106\r\n  },\r\n  {\r\n    \"EXPR$0\": \"2019-10-29T21:39:00.000Z\",\r\n    \"EXPR$1\": 123\r\n  },\r\n  {\r\n    \"EXPR$0\": \"2019-10-29T21:42:00.000Z\",\r\n    \"EXPR$1\": 115\r\n  },\r\n  {\r\n    \"EXPR$0\": \"2019-10-29T21:45:00.000Z\",\r\n    \"EXPR$1\": 9\r\n  },\r\n  {}\r\n]\r\n```\r\n\r\n**I suspect that the last row returned (empty in this case) is not handled correctly causing the UI screen to crash/blank out.** \r\n\r\nFurther, setting `\"grandTotal\" : false` doesn't cause the page to crash.\r\n\r\nPlease see screenshots attached:\r\n\r\n_Setting query param:_\r\n<img width=\"497\" alt=\"Screen Shot 2019-10-30 at 12 47 14 PM\" src=\"https://user-images.githubusercontent.com/8687261/67894231-5de9d000-fb15-11e9-9ad0-ad82bf9eb87d.png\">\r\n\r\n_Network/console tab for inspection:_\r\n<img width=\"1431\" alt=\"Screen Shot 2019-10-30 at 12 45 12 PM\" src=\"https://user-images.githubusercontent.com/8687261/67894230-5de9d000-fb15-11e9-8d64-d19467ae9027.png\">\r\n\r\n_BOOM - page crashes:_\r\n<img width=\"1023\" alt=\"Screen Shot 2019-10-30 at 12 42 50 PM\" src=\"https://user-images.githubusercontent.com/8687261/67894229-5de9d000-fb15-11e9-9aba-ea247fedf7b4.png\">\r\n\r\n\r\n\r\n"}, {"user": "vogievetsky", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug"], "created": "2019-10-30 03:48:31", "title": "stringFirst/stringLast do not work correctly at ingest time", "url": "https://github.com/apache/druid/issues/8784", "closed": "2019-11-07 19:17:58", "ttf": 8.000277777777777, "commitsDetails": [], "body": "### Affected Version\r\n\r\nDruid 0.16.0\r\n\r\n### Description\r\n\r\nIt might be related to https://github.com/apache/incubator-druid/issues/7243 I am not sure\r\n\r\nIngested data with this spec in a quickstart cluster:\r\n\r\n```json\r\n{\r\n  \"type\": \"index_parallel\",\r\n  \"ioConfig\": {\r\n    \"type\": \"index_parallel\",\r\n    \"firehose\": {\r\n      \"type\": \"http\",\r\n      \"uris\": [\r\n        \"https://druid.apache.org/data/wikipedia.json.gz\"\r\n      ]\r\n    }\r\n  },\r\n  \"tuningConfig\": {\r\n    \"type\": \"index_parallel\"\r\n  },\r\n  \"dataSchema\": {\r\n    \"dataSource\": \"wikipedia-test\",\r\n    \"granularitySpec\": {\r\n      \"type\": \"uniform\",\r\n      \"segmentGranularity\": \"DAY\",\r\n      \"queryGranularity\": \"HOUR\",\r\n      \"rollup\": true\r\n    },\r\n    \"parser\": {\r\n      \"type\": \"string\",\r\n      \"parseSpec\": {\r\n        \"format\": \"json\",\r\n        \"timestampSpec\": {\r\n          \"column\": \"timestamp\",\r\n          \"format\": \"iso\"\r\n        },\r\n        \"dimensionsSpec\": {\r\n          \"dimensions\": [\r\n            \"channel\",\r\n            \"cityName\",\r\n            \"comment\",\r\n            \"countryIsoCode\",\r\n            \"countryName\",\r\n            \"flags\",\r\n            \"isAnonymous\",\r\n            \"isMinor\",\r\n            \"isNew\",\r\n            \"isRobot\",\r\n            \"isUnpatrolled\",\r\n            \"namespace\",\r\n            \"page\",\r\n            \"regionIsoCode\",\r\n            \"regionName\"\r\n          ]\r\n        }\r\n      }\r\n    },\r\n    \"metricsSpec\": [\r\n      {\r\n        \"name\": \"count\",\r\n        \"type\": \"count\"\r\n      },\r\n      {\r\n        \"name\": \"sum_added\",\r\n        \"type\": \"longSum\",\r\n        \"fieldName\": \"added\"\r\n      },\r\n      {\r\n        \"name\": \"sum_commentLength\",\r\n        \"type\": \"longSum\",\r\n        \"fieldName\": \"commentLength\"\r\n      },\r\n      {\r\n        \"name\": \"sum_deleted\",\r\n        \"type\": \"longSum\",\r\n        \"fieldName\": \"deleted\"\r\n      },\r\n      {\r\n        \"name\": \"sum_metroCode\",\r\n        \"type\": \"longSum\",\r\n        \"fieldName\": \"metroCode\"\r\n      },\r\n      {\r\n        \"name\": \"sum_deltaBucket\",\r\n        \"type\": \"longSum\",\r\n        \"fieldName\": \"deltaBucket\"\r\n      },\r\n      {\r\n        \"name\": \"sum_delta\",\r\n        \"type\": \"longSum\",\r\n        \"fieldName\": \"delta\"\r\n      },\r\n      {\r\n        \"name\": \"test\",\r\n        \"type\": \"stringFirst\",\r\n        \"fieldName\": \"user\"\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\nAnd got:\r\n\r\n![image](https://user-images.githubusercontent.com/177816/67827409-2df6ea00-fa8d-11e9-917c-37c526f0c931.png)\r\n\r\nAlso that column's data type gets reported as `OTHER`\r\n\r\n![image](https://user-images.githubusercontent.com/177816/67827454-4b2bb880-fa8d-11e9-8f2f-ac443028b1ff.png)\r\n\r\n"}, {"user": "v-vishwa", "commits": {}, "labels": ["Area - SQL", "Bug"], "created": "2019-10-29 07:51:27", "title": "Limit clause in the Inner query can cause NullPointerException ", "url": "https://github.com/apache/druid/issues/8774", "closed": "2019-12-17 18:21:13", "ttf": 49.000277777777775, "commitsDetails": [], "body": "If we run below query it fails with `UnknownException/NullPointerException` in the UI\r\n```\r\nSELECT COUNT(*) AS \"count\",\"colB\"\r\n   FROM (\r\n        SELECT \"channel\" AS \"colB\"\r\n        FROM (\r\n            SELECT *\r\n                FROM \"wikiticker-Hour\"\r\n           limit 100     \r\n        ) tmpA\r\n        ) tmpA\r\n       \r\n   GROUP BY colB\r\n\r\n```\r\n\r\n**BROKER log show below trace:**\r\n```\r\njava.lang.NullPointerException\r\n\tat org.apache.druid.sql.calcite.rel.QueryMaker.findInnerMostQuery(QueryMaker.java:122) ~[druid-sql-0.16.0-incubating.jar:0.16.0-incubating]\r\n\tat org.apache.druid.sql.calcite.rel.QueryMaker.runQuery(QueryMaker.java:98) ~[druid-sql-0.16.0-incubating.jar:0.16.0-incubating]\r\n\tat org.apache.druid.sql.calcite.rel.DruidOuterQueryRel.runQuery(DruidOuterQueryRel.java:96) ~[druid-sql-0.16.0-incubating.jar:0.16.0-incubating]\r\n\tat org.apache.druid.sql.calcite.planner.DruidPlanner$1.get(DruidPlanner.java:138) ~[druid-sql-0.16.0-incubating.jar:0.16.0-incubating]\r\n\tat org.apache.druid.sql.calcite.planner.DruidPlanner$1.get(DruidPlanner.java:133) ~[druid-sql-0.16.0-incubating.jar:0.16.0-incubating]\r\n\tat org.apache.druid.sql.calcite.planner.PlannerResult.run(PlannerResult.java:54) ~[druid-sql-0.16.0-incubating.jar:0.16.0-incubating]\r\n\tat org.apache.druid.sql.SqlLifecycle.execute(SqlLifecycle.java:230) ~[druid-sql-0.16.0-incubating.jar:0.16.0-incubating]\r\n\tat org.apache.druid.sql.http.SqlResource.doPost(SqlResource.java:108) [druid-sql-0.16.0-incubating.jar:0.16.0-incubating]\r\n\tat sun.reflect.GeneratedMethodAccessor67.invoke(Unknown Source) ~[?:?]\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_211]\r\n\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_211]\r\n\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75) [jersey-server-1.19.3.jar:1.19.3]\r\n```\r\n\r\n**What is causing this :**\r\n\r\nhttps://github.com/apache/incubator-druid/blob/0.16.0-incubating/sql/src/main/java/org/apache/druid/sql/calcite/rel/QueryMaker.java\r\n\r\n```\r\n  private Query findInnerMostQuery(Query outerQuery)\r\n  {\r\n    Query query = outerQuery;\r\n #Line 122 while (query.getDataSource() instanceof QueryDataSource) {\r\n      query = ((QueryDataSource) query.getDataSource()).getQuery();\r\n    }\r\n    return query;\r\n  }\r\n```\r\n\r\n`It seems it is not able to compute the inner query when limit is placed into the inner queries`\r\n\r\n### Affected Version\r\n\r\n`Druid 0.16`\r\n\r\n### Description\r\n`as described above `\r\n\r\n**WorkAround (actually not a workaround, but something which works ):** \r\nIf we remove the LIMIT clause from the query it works. i.e Below query works without any issue.\r\n\r\n```\r\nSELECT COUNT(*) AS \"count\",\"colB\"\r\n   FROM (\r\n        SELECT \"channel\" AS \"colB\"\r\n        FROM (\r\n            SELECT *\r\n                FROM \"wikiticker-Hour\"    \r\n        ) tmpA\r\n        ) tmpA\r\n       \r\n   GROUP BY colB\r\n```\r\n\r\n"}, {"user": "JasonChoi27", "commits": {}, "labels": ["Bug"], "created": "2019-10-28 12:15:46", "title": "Failed to publish segments when all kafka records of previous task were filtered out by transformSpec", "url": "https://github.com/apache/druid/issues/8765", "closed": "2019-12-19 03:45:12", "ttf": 51.000277777777775, "commitsDetails": [], "body": "\r\n### Affected Version\r\n\r\nmaster\r\n\r\n### Description\r\n\r\n#### Ingest Schema\r\n\r\n```json\r\n{\r\n  \"type\":\"kafka\",\r\n  \"dataSchema\":{\r\n      \"dataSource\":\"growth_short_links\",\r\n      \"parser\":{\r\n          ...\r\n      },\r\n      \"metricsSpec\":[\r\n          ...\r\n      ],\r\n      \"granularitySpec\":{\r\n          ...\r\n      },\r\n      \"transformSpec\":{\r\n          \"filter\":{\r\n              \"type\":\"in\",\r\n              \"dimension\":\"appId\",\r\n              \"values\":[\r\n                  \"5d30228e45a44304f83c8f94\",\r\n                  \"5d3022a94b7215a8ca5f2f59\",\r\n                  \"5d3022b14b7215a8ca5f2f5a\"\r\n              ]\r\n          }\r\n      }\r\n  },\r\n  \"tuningConfig\":{\r\n       ...\r\n  },\r\n  \"ioConfig\":{\r\n      ...\r\n  }\r\n}\r\n```\r\n\r\n\r\n\r\n#### Reproduce\r\n\r\n- From the beginning of the first task, make sure all records be filtered out from the condition at `transformSpec` and wait for the task duration over. The task will finish sucessfully.\r\n- After the second task be submitted, make sure some records do not be filtered out from the condition, and this task will fail before publishes segments with the exception as below.\r\n\r\n\r\n\r\n#### Exception\r\n\r\n```\r\n2019-10-28T19:02:48,360 ERROR [publish-0] org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner - Error while publishing segments for sequenceNumber[SequenceMetadata{sequenceId=0, sequenceName='index_kafka_growth_short_links_149a86e99f195dd_0', assignments=[], startOffsets={0=1997372995}, exclusiveStartPartitions=[], endOffsets={0=1997606975}, sentinel=false, checkpointed=true}]\r\norg.apache.druid.java.util.common.ISE: Failed to publish segments because of [java.lang.RuntimeException: Aborting transaction!].\r\n\tat org.apache.druid.segment.realtime.appenderator.BaseAppenderatorDriver.lambda$publishInBackground$8(BaseAppenderatorDriver.java:605) ~[druid-server-0.16.0-incubating.jar:0.16.0-incubating]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_144]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_144]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]\r\n```\r\n\r\n\r\n\r\nThe first task just checkpointed metadata and did not save it into db becasue no segment should be published. After the first task finished, the offset in db still be like `1995845112`  and the `partitionSequenceNumber` of the second task was bigger than `1995845112` because the first one has ingested some records.\r\n\r\nWhen the second task was ready to published segment, it sent a `SegmentTransactionalInsertAction` to overlord to save metadata into db. Overlord would firstly compare the metadata in db with this commit metadata, but unfortunately, it failed. Log at overlord like,\r\n\r\n```\r\n2019-10-28T19:02:48,264 ERROR [qtp577628476-146] org.apache.druid.metadata.IndexerSQLMetadataStorageCoordinator - Not updating metadata, existing state[KafkaDataSourceMetadata{SeekableStreamStartSequenceNumbers=SeekableStreamEndSequenceNumbers{stream='logs.creditcard.ofa.analytics.event', partitionSequenceNumberMap={0=1995845112}}}] in metadata store doesn't match to the new start state[KafkaDataSourceMetadata{SeekableStreamStartSequenceNumbers=SeekableStreamStartSequenceNumbers{stream='logs.creditcard.ofa.analytics.event', partitionSequenceNumberMap={0=1997372995}, exclusivePartitions=[]}}].\r\n```\r\n\r\nOf cause the task will be recovered by next submission, and starts ingesting data from the offset saved in db. But i think it would be better to make kafka offset promoted no matter the task has segment to publish or not.\r\n"}, {"user": "leventov", "commits": {}, "labels": ["Area - Testing", "Bug", "Contributions Welcome", "Flaky test", "Starter"], "created": "2019-10-27 09:32:57", "title": "Fix a concurrency bug in InMemoryAppender", "url": "https://github.com/apache/druid/issues/8759", "closed": "2019-11-07 18:11:40", "ttf": 11.000277777777777, "commitsDetails": [], "body": "https://github.com/apache/incubator-druid/blob/1b9d4ce8114683d95e8b073ebc2b9ba0b5b61399/core/src/test/java/org/apache/druid/testing/junit/LoggerCaptureRule.java#L97\r\n\r\nViolates this checklist item: https://github.com/code-review-checklists/java-concurrency#unsafe-concurrent-iteration\r\n\r\nLeads to real problems in tests, e. g.:\r\n```\r\n[ERROR] Tests run: 7, Failures: 0, Errors: 1, Skipped: 1, Time elapsed: 0.3 s <<< FAILURE! - in org.apache.druid.curator.CuratorModuleTest\r\n[ERROR] exitsJvmWhenMaxRetriesExceeded(org.apache.druid.curator.CuratorModuleTest)  Time elapsed: 0.047 s  <<< ERROR!\r\njava.util.ConcurrentModificationException\r\n\tat java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909)\r\n\tat java.util.ArrayList$Itr.next(ArrayList.java:859)\r\n\tat java.util.AbstractCollection.toString(AbstractCollection.java:461)\r\n\tat java.util.Collections$UnmodifiableCollection.toString(Collections.java:1037)\r\n\tat java.lang.String.valueOf(String.java:2994)\r\n\tat java.lang.StringBuilder.append(StringBuilder.java:131)\r\n\tat org.apache.druid.curator.CuratorModuleTest.exitsJvmWhenMaxRetriesExceeded(CuratorModuleTest.java:150)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.contrib.java.lang.system.ExpectedSystemExit$1.evaluate(ExpectedSystemExit.java:130)\r\n\tat org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)\r\n\tat org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)\r\n\tat org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:363)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\r\n```"}, {"user": "Murthy7587", "commits": {}, "labels": ["Area - Web Console", "Bug"], "created": "2019-10-26 06:56:33", "title": "Unified Console not working on IE11", "url": "https://github.com/apache/druid/issues/8755", "closed": "2019-11-01 04:03:05", "ttf": 5.000277777777778, "commitsDetails": [], "body": "Trying to launch Unified console on IE 11 browser, getting an error on console:\r\n\"Exception Thrown and not caught\".\r\n![UnifiedConsole](https://user-images.githubusercontent.com/26702748/67615550-7b542c80-f7eb-11e9-9cab-183f0a721bec.JPG)\r\n"}, {"user": "sixtus", "commits": {}, "labels": ["Bug"], "created": "2019-10-22 03:20:31", "title": "druid indexer running Kafka indexing not seen as dataSource", "url": "https://github.com/apache/druid/issues/8710", "closed": "2019-10-23 22:52:58", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "I just spent 2.5h figuring out why I am not seeing any realtime data using the new indexer nodes.\r\n\r\nAnd here it is:\r\n\r\n```druid-broker[10249]: WARN [CuratorDruidNodeDiscoveryProvider-ListenerExecutor] org.apache.druid.discovery.DruidNodeDiscoveryProvider$ServiceDruidNodeDiscovery - Node[DiscoveryDruidNode{druidNode=DruidNode{serviceName='druid/indexer', host='indexer.node', bindOnHost=false, port=-1, plaintextPort=8091, enablePlaintextPort=true, tlsPort=-1, enableTlsPort=false}, nodeType='INDEXER', services={workerNodeService=WorkerNodeService{ip='...', capacity=64, version='1'}, lookupNodeService=LookupNodeService{lookupTier='__default'}}}] discovered but doesn't have service[dataNodeService]. Ignored.```\r\n\r\n### Affected Version\r\n\r\n0.16\r\n\r\n### Description\r\n\r\nBrokers (and coordinators for UI) ignore the new indexer nodes when using http segment announcement (didn't try batch).\r\n\r\nI had to revert to middle manager. It was the feature I upgraded for... oh well, shouldn't be hard to fix."}, {"user": "liutang123", "commits": {"063811710effe2e9ff9a78364bf7e3ea102ec14a": {"commitGHEventType": "referenced", "commitUser": "jihoonson"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "KIS task fail when set segmentGranularity with timezone", "numCommits": 0, "created": "2019-10-17 07:43:11", "closed": "2020-02-26 21:21:00", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/8690", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 132.00027777777777, "commitsDetails": [{"commitGitStats": [{"filePath": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "deletions": 2, "insertions": 4, "lines": 6}, {"filePath": "indexing-service/src/main/java/org/apache/druid/indexing/common/actions/SegmentAllocateAction.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "indexing-service/src/test/java/org/apache/druid/indexing/common/actions/SegmentAllocateActionTest.java", "deletions": 0, "insertions": 29, "lines": 29}], "commitSpoonAstDiffStats": [{"spoonFilePath": "SegmentAllocateAction.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.common.actions.SegmentAllocateAction.perform(org.apache.druid.indexing.common.task.Task,org.apache.druid.indexing.common.actions.TaskActionToolbox)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "IndexerSQLMetadataStorageCoordinator.java", "spoonMethods": [{"INS": 1, "UPD": 2, "DEL": 0, "spoonMethodName": "org.apache.druid.metadata.IndexerSQLMetadataStorageCoordinator.allocatePendingSegment(java.lang.String,java.lang.String,java.lang.String,org.joda.time.Interval,org.apache.druid.timeline.partition.PartialShardSpec,java.lang.String,boolean)", "MOV": 0, "TOT": 3}]}, {"spoonFilePath": "SegmentAllocateActionTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.common.actions.SegmentAllocateActionTest.testSameIntervalWithSegmentGranularity()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2020-02-27 05:21:00", "commitMessage": "#8690 use utc interval when create pedding segments (#9142)\n\nCo-authored-by: Gian Merlino <gianmerlino@gmail.com>\r\n", "commitUser": "jihoonson", "commitDateTime": "2020-02-26 13:20:59", "commitParents": ["b924161086ec460ba1da9b5f61a7f50f9664dc9b"], "commitGHEventType": "referenced", "nameRev": "063811710effe2e9ff9a78364bf7e3ea102ec14a tags/druid-0.18.0-rc1~103", "commitHash": "063811710effe2e9ff9a78364bf7e3ea102ec14a"}], "body": "### Affected Version\r\n0.15.1-incubating\r\n\r\n### Description\r\nWhen submit a KIS Supervisor with segmentGranularity configured timezone, fowllow error will occurred.\r\nIn task log:\r\n```SeekableStreamIndexTaskRunner - Encountered exception in run() before persisting.\r\norg.apache.druid.java.util.common.ISE: Could not allocate segment for row with timestamp[2019-10-16T20:12:11.000Z]\r\n\tat org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner.runInternal(SeekableStreamIndexTaskRunner.java:605) [druid-indexing-service-0.15.1-incubating.jar:0.15.1-incubating]\r\n\tat org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner.run(SeekableStreamIndexTaskRunner.java:246) [druid-indexing-service-0.15.1-incubating.jar:0.15.1-incubating]\r\n\tat org.apache.druid.indexing.seekablestream.SeekableStreamIndexTask.run(SeekableStreamIndexTask.java:167) [druid-indexing-service-0.15.1-incubating.jar:0.15.1-incubating]\r\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:419) [druid-indexing-service-0.15.1-incubating.jar:0.15.1-incubating]\r\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:391) [druid-indexing-service-0.15.1-incubating.jar:0.15.1-incubating]\r\n```\r\nIn Overlord log:\r\n```\r\nLocalTaskActionClient - Performing action for task[index_kafka_bug_demo_57ebb7db959865d_bebmfiod]: SegmentAllocateAction{dataSource='bug_demo', timestamp=2019-10-16T20:12:11.000Z, queryGranularity={type=period, period=PT1M, timeZone=Asia/Shanghai, origin=null}, preferredSegmentGranularity={type=period, period=PT1H, timeZone=Asia/Shanghai, origin=null}, sequenceName='index_kafka_bug_demo_57ebb7db959865d_0', previousSegmentId='null', skipSegmentLineageCheck='true'}\r\nIndexerSQLMetadataStorageCoordinator - Cannot allocate new segment for dataSource[bug_demo], interval[2019-10-17T04:00:00.000+08:00/2019-10-17T05:00:00.000+08:00], maxVersion[2019-10-16T12:08:55.897Z]: conflicting segment[bug_demo_2019-10-16T20:00:00.000Z_2019-10-16T21:00:00.000Z_2019-10-16T12:08:55.897Z]\r\n```\r\nFollow config will reproduce the problem:\r\n```\r\n{\r\n   \"type\": \"kafka\",\r\n   \"dataSchema\": {\r\n     \"dataSource\": \"bug_demo\",\r\n     \"parser\": {\r\n             \"type\": \"string\",\r\n             \"parseSpec\": {\r\n               \"format\": \"json\",\r\n               \"timestampSpec\": {\r\n                 \"column\": \"metricTime\",\r\n                 \"format\": \"yyyy-MM-dd HH:mm:ss\"\r\n               },\r\n               \"dimensionsSpec\": {\r\n                 \"dimensions\": [\r\n                   \"activity\"\r\n                 ],\r\n                 \"dimensionExclusions\": [],\r\n                 \"spatialDimensions\": []\r\n               }\r\n             }\r\n           },\r\n           \"metricsSpec\": [\r\n             {\r\n               \"fieldName\": \"\",\r\n               \"type\": \"count\",\r\n               \"name\": \"numEvents\"\r\n             }\r\n           ],\r\n     \"granularitySpec\": {\r\n             \"type\": \"uniform\",\r\n             \"segmentGranularity\": {\r\n               \"type\": \"period\",\r\n               \"period\": \"PT1H\",\r\n               \"timeZone\": \"Asia/Shanghai\"\r\n             },\r\n             \"queryGranularity\": {\r\n               \"type\": \"period\",\r\n               \"period\": \"PT1M\",\r\n               \"timeZone\": \"Asia/Shanghai\"\r\n             }\r\n           }\r\n   },\r\n   \"tuningConfig\": {\r\n     \"type\": \"kafka\",\r\n     \"maxRowsPerSegment\": 5000000\r\n   },\r\n   \"ioConfig\": {\r\n     \"topic\": \"kafkaTopic\",\r\n     \"consumerProperties\": {\r\n       XXXXXXXXXXXXXXXXX............\r\n     },\r\n     \"taskCount\": 1,\r\n     \"replicas\": 1,\r\n     \"taskDuration\": \"PT10M\"\r\n  }\r\n }\r\n```\r\nIn the metadata database, the peddingSegments table stores segmentId are as follows:\r\n```\r\n*************************** 1. row ***************************\r\n                        id: bug_demo_2019-10-17T04:00:00.000+08:00_2019-10-17T05:00:00.000+08:00_2019-10-16T12:08:55.897Z\r\n                dataSource: bug_demo\r\n              created_date: 2019-10-16T12:08:55.938Z\r\n                     start: 2019-10-17T04:00:00.000+08:00\r\n                       end: 2019-10-17T05:00:00.000+08:00\r\n             sequence_name: index_kafka_gh2_test_liulijia_kis_origin_57b06c0cbaa83ea_0\r\n```\r\n\r\n1. KIS task A start\r\n2. KIS task A allocate pedding segment [dataSource: bug_demo, start: timeA, end: timeB] with segmentGranularity PT1H and time zone Asia/Shanghai.\r\n3. Overlord receives request, and create a new pendding segment with  Asia/Shanghai timezone and insert it into metadata table.\r\nsee [IndexerSQLMetadataStorageCoordinator.java#L720](https://github.com/apache/incubator-druid/blob/75527f09cddd5160e27d3af373c6a2bc656b6e7f/server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java#L720) and [IndexerSQLMetadataStorageCoordinator.java#L627](https://github.com/apache/incubator-druid/blob/75527f09cddd5160e27d3af373c6a2bc656b6e7f/server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java#L627)\r\n4. KIS task A receive SegmentIdWithShardSpec, but its interval will deserialized as UTC, see [JodaStuff.java#L75](https://github.com/apache/incubator-druid/blob/75527f09cddd5160e27d3af373c6a2bc656b6e7f/processing/src/main/java/org/apache/druid/jackson/JodaStuff.java#L75) \r\n5. KIS task A create segment bug_demo_timeA_timeB_versionS. timeA and timeB are UTC.\r\n6. KIS task A stop after taskDuration reached.\r\n7. KIS task B start\r\n8. KIS task B allocate pedding segment [dataSource: bug_demo, start: timeA, end: timeB] with segmentGranularity PT1H and time zone Asia/Shanghai.\r\n9. **bug 1** Overlord can not find segment bug_demo_timeA_timeB_versionS.\r\n10. **bug 2** Overlord get pedding segment bug_demo_timeA_timeB_versionS but deserialized its interval as UTC, see [IndexerSQLMetadataStorageCoordinator.java#L163](https://github.com/apache/incubator-druid/blob/75527f09cddd5160e27d3af373c6a2bc656b6e7f/server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java#L163). Comparision of the two segments are not equal, see [IndexerSQLMetadataStorageCoordinator.java#L721](https://github.com/apache/incubator-druid/blob/75527f09cddd5160e27d3af373c6a2bc656b6e7f/server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java#L721)\r\n\r\nSo, Should we unify the pending segment into UTC?"}, {"user": "vogievetsky", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug", "Starter"], "created": "2019-10-15 03:11:42", "title": "InlineFirehose does not work with index_parallel ingestion", "url": "https://github.com/apache/druid/issues/8673", "closed": "2019-10-17 02:28:55", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "In Druid 0.16.0\r\n\r\nAs the title says, if you try to submit a task like:\r\n\r\n```json\r\n{\r\n  \"type\": \"index_parallel\",\r\n  \"spec\": {\r\n    \"type\": \"index_parallel\",\r\n    \"ioConfig\": {\r\n      \"type\": \"index_parallel\",\r\n      \"firehose\": {\r\n        \"type\": \"inline\",\r\n        \"data\": \"{\\\"name\\\":\\\"Vadim\\\"}\"\r\n      }\r\n    },\r\n    \"tuningConfig\": {\r\n      \"type\": \"index_parallel\"\r\n    },\r\n    \"dataSchema\": {\r\n      \"dataSource\": \"some_data\",\r\n      \"granularitySpec\": {\r\n        \"type\": \"uniform\",\r\n        \"queryGranularity\": \"HOUR\",\r\n        \"rollup\": true,\r\n        \"segmentGranularity\": \"DAY\"\r\n      },\r\n      \"parser\": {\r\n        \"type\": \"string\",\r\n        \"parseSpec\": {\r\n          \"format\": \"json\",\r\n          \"timestampSpec\": {\r\n            \"column\": \"!!!_no_such_column_!!!\",\r\n            \"missingValue\": \"2010-01-01T00:00:00Z\"\r\n          },\r\n          \"dimensionsSpec\": {\r\n            \"dimensions\": [\r\n              \"name\"\r\n            ]\r\n          }\r\n        }\r\n      },\r\n      \"metricsSpec\": [\r\n        {\r\n          \"name\": \"count\",\r\n          \"type\": \"count\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nYou get an error of:\r\n\r\n```\r\n{\"error\":\"Instantiation of [simple type, class org.apache.druid.indexing.common.task.batch.parallel.ParallelIndexSupervisorTask] value failed: [InlineFirehoseFactory] should implement FiniteFirehoseFactory\"}\r\n```\r\n\r\nHowever changing the ingestion to just `index`:\r\n\r\n```json\r\n{\r\n  \"type\": \"index\",\r\n  \"spec\": {\r\n    \"type\": \"index\",\r\n    \"ioConfig\": {\r\n      \"type\": \"index\",\r\n      \"firehose\": {\r\n        \"type\": \"inline\",\r\n        \"data\": \"{\\\"name\\\":\\\"Vadim\\\"}\"\r\n      }\r\n    },\r\n    \"tuningConfig\": {\r\n      \"type\": \"index\"\r\n    },\r\n    \"dataSchema\": {\r\n      \"dataSource\": \"some_data\",\r\n      \"granularitySpec\": {\r\n        \"type\": \"uniform\",\r\n        \"queryGranularity\": \"HOUR\",\r\n        \"rollup\": true,\r\n        \"segmentGranularity\": \"DAY\"\r\n      },\r\n      \"parser\": {\r\n        \"type\": \"string\",\r\n        \"parseSpec\": {\r\n          \"format\": \"json\",\r\n          \"timestampSpec\": {\r\n            \"column\": \"!!!_no_such_column_!!!\",\r\n            \"missingValue\": \"2010-01-01T00:00:00Z\"\r\n          },\r\n          \"dimensionsSpec\": {\r\n            \"dimensions\": [\r\n              \"name\"\r\n            ]\r\n          }\r\n        }\r\n      },\r\n      \"metricsSpec\": [\r\n        {\r\n          \"name\": \"count\",\r\n          \"type\": \"count\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nMakes it all work"}, {"user": "Laboltus", "commits": {}, "labels": ["Bug"], "created": "2019-10-13 15:14:21", "title": "Historicals can't work with multiple druid.segmentCache.locations", "url": "https://github.com/apache/druid/issues/8667", "closed": "2019-10-14 02:29:11", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "Historicals can't work with multiple druid.segmentCache.locations after upgrade from 0.12.3 to 0.16.0\r\n\r\n### Affected Version\r\n\r\nAt least 0.16.0\r\n\r\n### Description\r\n\r\nWe use from 3 to 6 volumes on historicals with such configs:\r\ndruid.segmentCache.locations=[{\"path\": \"/storage1\", \"maxSize\": 1360000000000},{\"path\": \"/storage2\\\", \"maxSize\": 1360000000000},{\"path\": \"/storage3\", \"maxSize\": 1360000000000}]\r\n\r\nAfter the upgrade I see that the usage of /storage1 is growing, but all other locations do not change."}, {"user": "himanshug", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug", "Design Review", "Proposal"], "created": "2019-10-02 23:29:17", "title": "Proposal: concept of supervisor type task slots for resolving parallel task deadlocks", "url": "https://github.com/apache/druid/issues/8622", "closed": "2019-10-18 20:09:42", "ttf": 15.000277777777777, "commitsDetails": [], "body": "### Motivation\r\n\r\n`ParallelIndexSupervisorTask` is a supervisor style task that delegates work to one or more spawned subtasks. Since both, supervisor task and subtasks, are using same task slot pool, there is possibility of a deadlock.\r\nFor example, say a druid cluster has 4 task slots and 4 `ParallelIndexSupervisorTask` tasks are submitted simultaneously and started running on available 4 task slots. Subtasks spawned by the supervisor tasks would never be able to run and supervisor tasks would keep on waiting.\r\n\r\nIt is also discussed in https://github.com/apache/incubator-druid/issues/8061#issuecomment-537231350 .\r\n\r\n### Proposed changes\r\n\r\nAdd a method `boolean Task.isSupervisor()` to `Task` interface which returns `true` if the task is a supervisor task that spawns subtasks to delegate work. `ParallelIndexSupervisorTask` would return `true` while all other current task impls would return `false`.\r\n\r\nAdd a `druid.worker.supervisorCapacity` configuration on middleManagers, which designates available slots to run supervisor tasks. This config is similar to `druid.worker.capacity` which designates available non-supervisor task slots.\r\n\r\n[Http]RemoteTaskRunner code would be updated to recognize that supervisor tasks consume slot from `supervisorCapacity` and not `capacity` .\r\n\r\n### Rationale\r\n\r\nA potential alternative is to use #7066 to send all supervisor tasks to a dedicated set of middleManagers which only get supervisor tasks.\r\n\r\n### Operational impact\r\n\r\nUser of `ParallelIndexSupervisorTask` would need to set property `druid.worker.supervisorCapacity` on middleManagers.\r\n\r\n### Test plan (optional)\r\n\r\nwill run it on a staging cluster.\r\n\r\n### Future work (optional)\r\n\r\nfor reliability: supervisor tasks could be treated further specially be imposing a \"always restartable\" restriction on them and also not failing them if middleManager running them crashed .\r\n"}, {"user": "jihoonson", "commits": {}, "labels": ["Bug"], "created": "2019-10-01 14:21:23", "title": "The same storage location can be picked up over again with RoundRobinStorageLocationSelectorStrategy", "url": "https://github.com/apache/druid/issues/8614", "closed": "2019-10-12 01:53:52", "ttf": 10.000277777777777, "commitsDetails": [], "body": "### Affected Version\r\n\r\nmaster\r\n\r\n### Description\r\n\r\n`RoundRobinStorageLocationSelectorStrategy` was introduced in #8038. In the current implementation of this strategy, the iteration state is shared by different threads. When a thread calls `Iterator.next()` for the iterator returned from `RoundRobinStorageLocationSelectorStrategy.getLocations()`, the next thread will get a next storage location in a round robin fashion. However, with this strategy, the same storage location can be potentially picked up over again for the same thread. For example, suppose we have 2 storage locations and 2 threads. After the Thread `T1` can pick up the storage location `L1`, another thread `T2` can pick up the storage location `L2`. This will let `T1` pick up the same location `L1`. This might be fine in general, but it could be problematic if there is something wrong with `L1`. Since `T1` will can pick up the problematic location `L1` over again, it will never finish segment loading and fail.\r\n\r\nFYI @sashidhar."}, {"user": "sebbASF", "commits": {}, "labels": ["Apache", "Area - Documentation", "Bug"], "created": "2019-09-26 14:28:36", "title": "Download page needs to include incubation disclaimer", "url": "https://github.com/apache/druid/issues/8599", "closed": "2019-09-26 23:33:31", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "As the subject says; it's important that downloaders are made aware of the incubation status"}, {"user": "gianm", "commits": {}, "labels": ["Area - Documentation", "Bug"], "created": "2019-09-25 20:39:32", "title": "Docs \"edit\" link is broken", "url": "https://github.com/apache/druid/issues/8592", "closed": "2019-09-26 00:25:40", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "To reproduce, go to https://druid.apache.org/docs/0.16.0-incubating/design/index.html or https://druid.apache.org/docs/latest/design/index.html and click \"edit\". The issue is that the extra `/latest/` or `/0.16.0-incubating/` is being passed along to the GitHub URL, leading to broken links like https://github.com/apache/incubator-druid/edit/master/docs/0.16.0-incubating/design/index.md. I think this is related to the `editUrl` in siteConfig.js (currently set to `https://github.com/apache/incubator-druid/edit/master/docs/`).\r\n\r\nBtw, a minor related thing: it'd be better to use https://github.com/apache/druid rather than https://github.com/apache/incubator-druid. That way, the links still work when we move to a TLP."}, {"user": "leventov", "commits": {}, "labels": ["Area - Dev", "Bug", "Development Blocker"], "created": "2019-09-12 13:08:54", "title": "Cannot run an individual test method from IntelliJ IDEA", "url": "https://github.com/apache/druid/issues/8524", "closed": "2019-09-14 18:56:10", "ttf": 2.000277777777778, "commitsDetails": [], "body": "When trying to run an individual test from IDE (IntelliJ IDEA), I have the following error:\r\n```\r\nError: Could not find or load main class @{jacocoArgLine}\r\n```\r\n\r\nI would say this is a regression in development experience (after #8303, cc @ccaominh) and must be fixed."}, {"user": "jihoonson", "commits": {}, "labels": ["Bug"], "created": "2019-09-06 19:01:45", "title": "Auto compaction can get stuck", "url": "https://github.com/apache/druid/issues/8481", "closed": "2020-04-07 20:05:54", "ttf": 214.00027777777777, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.13.x, 0.14.x, 0.15.x\r\n\r\n### Description\r\n\r\nThe auto compaction can pick up the same interval over again if there are more than 1 segment in that interval after compaction. This was introduced in https://github.com/apache/incubator-druid/pull/6407 and is because the auto compaction is not considering `targetCompactionSizeBytes` or `maxRowsPerSegment` while searching for segments to compact."}, {"user": "jon-wei", "commits": {}, "labels": ["Area - Querying", "Bug"], "created": "2019-08-21 01:29:00", "title": "Integer->Long ClassCastException in TopN query", "url": "https://github.com/apache/druid/issues/8348", "closed": "2019-08-23 19:55:32", "ttf": 2.000277777777778, "commitsDetails": [], "body": "`TopNNumericResultBuilder` can hit a ClassCastException on the broker when a query uses a long dimension, due to Jackson behavior where a number gets deserialized as an integer instead of as a long if it fits in 32 bits.\r\n\r\n```\r\njava.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long\r\n\tat java.lang.Long.compareTo(Long.java:54) ~[?:1.8.0_202]\r\n\tat org.apache.druid.query.topn.TopNNumericResultBuilder$1.compare(TopNNumericResultBuilder.java:67) ~[druid-processing-0.16.0-incubating-SNAPSHOT.jar:0.16.0-incubating-SNAPSHOT]\r\n\tat org.apache.druid.query.topn.TopNNumericResultBuilder$1.compare(TopNNumericResultBuilder.java:52) ~[druid-processing-0.16.0-incubating-SNAPSHOT.jar:0.16.0-incubating-SNAPSHOT]\r\n\tat org.apache.druid.query.topn.TopNNumericResultBuilder.lambda$new$0(TopNNumericResultBuilder.java:99) ~[druid-processing-0.16.0-incubating-SNAPSHOT.jar:0.16.0-incubating-SNAPSHOT]\r\n\tat java.util.PriorityQueue.siftUpUsingComparator(PriorityQueue.java:670) ~[?:1.8.0_202]\r\n\tat java.util.PriorityQueue.siftUp(PriorityQueue.java:646) ~[?:1.8.0_202]\r\n\tat java.util.PriorityQueue.offer(PriorityQueue.java:345) ~[?:1.8.0_202]\r\n\tat java.util.PriorityQueue.add(PriorityQueue.java:322) ~[?:1.8.0_202]\r\n\tat org.apache.druid.query.topn.TopNNumericResultBuilder.addEntry(TopNNumericResultBuilder.java:204) ~[druid-processing-0.16.0-incubating-SNAPSHOT.jar:0.16.0-incubating-SNAPSHOT]\r\n\tat org.apache.druid.query.topn.TopNBinaryFn.apply(TopNBinaryFn.java:132) ~[druid-processing-0.16.0-incubating-SNAPSHOT.jar:0.16.0-incubating-SNAPSHOT]\r\n\tat org.apache.druid.query.topn.TopNBinaryFn.apply(TopNBinaryFn.java:39) ~[druid-processing-0.16.0-incubating-SNAPSHOT.jar:0.16.0-incubating-SNAPSHOT]\r\n\tat org.apache.druid.common.guava.CombiningSequence$CombiningYieldingAccumulator.accumulate(CombiningSequence.java:210) ~[druid-core-0.16.0-incubating-SNAPSHOT.jar:0.16.0-incubating-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.MergeSequence.makeYielder(MergeSequence.java:104) ~[druid-core-0.16.0-incubating-SNAPSHOT.jar:0.16.0-incubating-SNAPSHOT]\r\n```\r\n\r\n\r\nThe following data and specs can be used to reproduce the issue.\r\n\r\nData\r\n```\r\n{\"time\": \"2015-09-12T00:46:58.771Z\", \"id\": 10, \"val\": 5}\r\n{\"time\": \"2015-09-13T00:46:58.771Z\", \"id\": 8147483647, \"val\": 5}\r\n```\r\n\r\nIngest spec\r\n```\r\n{\r\n  \"type\" : \"index\",\r\n  \"spec\" : {\r\n    \"dataSchema\" : {\r\n      \"dataSource\" : \"topnmerge\",\r\n      \"parser\" : {\r\n        \"type\" : \"string\",\r\n        \"parseSpec\" : {\r\n          \"format\" : \"json\",\r\n          \"dimensionsSpec\" : {\r\n            \"dimensions\" : [\r\n              {\r\n                \"type\":\"long\",\r\n                \"name\":\"id\"\r\n              }\r\n            ],\r\n            \"dimensionExclusions\" : []\r\n          },\r\n          \"timestampSpec\" : {\r\n            \"format\" : \"auto\",\r\n            \"column\" : \"time\"\r\n          }\r\n        }\r\n      },\r\n      \"metricsSpec\" : [\r\n        { \"type\" : \"count\", \"name\" : \"count\", \"type\" : \"count\" },\r\n        { \"type\" : \"longSum\", \"name\" : \"val\", \"fieldName\" : \"val\" }\r\n      ],\r\n      \"granularitySpec\" : {\r\n        \"type\" : \"uniform\",\r\n        \"segmentGranularity\" : \"day\",\r\n        \"queryGranularity\" : \"none\",\r\n        \"intervals\" : [\"2015-09-01/2015-09-20\"],\r\n        \"rollup\" : false\r\n      }\r\n    },\r\n    \"ioConfig\" : {\r\n      \"type\" : \"index\",\r\n      \"firehose\" : {\r\n        \"type\" : \"local\",\r\n        \"baseDir\" : \"quickstart/topnmerge/\",\r\n        \"filter\" : \"data.json\"\r\n      },\r\n      \"appendToExisting\" : false\r\n    },\r\n    \"tuningConfig\" : {\r\n      \"type\" : \"index\",\r\n      \"targetPartitionSize\" : null,\r\n      \"maxRowsInMemory\" : 1000,\r\n      \"forceGuaranteedRollup\" : true,\r\n      \"numShards\": 1\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nQuery\r\n```\r\n{\r\n  \"queryType\": \"topN\",\r\n  \"dataSource\": {\r\n    \"type\": \"table\",\r\n    \"name\": \"topnmerge\"\r\n  },\r\n  \"virtualColumns\": [],\r\n  \"dimension\": {\r\n    \"type\": \"default\",\r\n    \"dimension\": \"id\",\r\n    \"outputName\": \"d0\",\r\n    \"outputType\": \"LONG\"\r\n  },\r\n  \"metric\": {\r\n    \"type\": \"numeric\",\r\n    \"metric\": \"a0\"\r\n  },\r\n  \"threshold\": 1000,\r\n  \"intervals\": {\r\n    \"type\": \"intervals\",\r\n    \"intervals\": [\r\n      \"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"\r\n    ]\r\n  },\r\n  \"filter\": null,\r\n  \"granularity\": {\r\n    \"type\": \"all\"\r\n  },\r\n  \"aggregations\": [\r\n    {\r\n      \"type\": \"longSum\",\r\n      \"name\": \"a0\",\r\n      \"fieldName\": \"val\",\r\n      \"expression\": null\r\n    }\r\n  ],\r\n  \"postAggregations\": [],\r\n  \"context\": {\r\n  },\r\n  \"descending\": false\r\n}\r\n```\r\n\r\nWith the examples above, the issue can be reproduced on a quickstart cluster with 2 historicals, by putting each historical in a separate tier and setting load rules such that each tier loads only one day of data (9/12 or 9/13)."}, {"user": "leventov", "commits": {}, "labels": ["Bug"], "created": "2019-08-13 11:37:51", "title": "ReferenceCountedSegment is acquired too many times", "url": "https://github.com/apache/druid/issues/8291", "closed": "2019-08-20 19:01:17", "ttf": 7.000277777777778, "commitsDetails": [], "body": "A error when a timeseries query is sent to Druid with high concurrency.\r\n\r\n### Affected Version\r\n\r\n0.14.0-incubating\r\n\r\n### Description\r\n\r\n```\r\norg.apache.druid.query.ChainedExecutionQueryRunner - Exception with one of the sequences!\r\n\r\njava.lang.IllegalStateException: Attempt to register more than 65535 parties for org.apache.druid.segment.ReferenceCountingSegment$1@50149526[phase = 0 parties = 65535 arrived = 0]\r\n\r\n       at java.util.concurrent.Phaser.doRegister(Phaser.java:438) ~[?:1.8.0_151]\r\n\r\n       at java.util.concurrent.Phaser.register(Phaser.java:591) ~[?:1.8.0_151]\r\n\r\n       at org.apache.druid.segment.ReferenceCountingSegment.increment(ReferenceCountingSegment.java:124) ~[druid-processing-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.segment.realtime.FireHydrant.getIncrementedSegment(FireHydrant.java:76) ~[druid-server-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.segment.realtime.FireHydrant.getAndIncrementSegment(FireHydrant.java:136) ~[druid-server-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.segment.realtime.appenderator.SinkQuerySegmentWalker$3$1.apply(SinkQuerySegmentWalker.java:227) ~[druid-server-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.segment.realtime.appenderator.SinkQuerySegmentWalker$3$1.apply(SinkQuerySegmentWalker.java:214) ~[druid-server-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at com.google.common.collect.Iterators$8.transform(Iterators.java:794) ~[guava-16.0.1.jar:?]\r\n\r\n       at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48) ~[guava-16.0.1.jar:?]\r\n\r\n       at com.google.common.collect.Iterators$3.next(Iterators.java:163) ~[guava-16.0.1.jar:?]\r\n\r\n       at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48) ~[guava-16.0.1.jar:?]\r\n\r\n       at com.google.common.collect.Iterators.addAll(Iterators.java:357) ~[guava-16.0.1.jar:?]\r\n\r\n       at com.google.common.collect.Lists.newArrayList(Lists.java:147) ~[guava-16.0.1.jar:?]\r\n\r\n       at com.google.common.collect.Lists.newArrayList(Lists.java:129) ~[guava-16.0.1.jar:?]\r\n\r\n       at org.apache.druid.query.ChainedExecutionQueryRunner$1.make(ChainedExecutionQueryRunner.java:104) ~[druid-processing-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.java.util.common.guava.BaseSequence.accumulate(BaseSequence.java:42) ~[druid-core-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.java.util.common.guava.LazySequence.accumulate(LazySequence.java:40) ~[druid-core-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.java.util.common.guava.WrappingSequence$1.get(WrappingSequence.java:50) ~[druid-core-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.java.util.common.guava.SequenceWrapper.wrap(SequenceWrapper.java:55) ~[druid-core-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.java.util.common.guava.WrappingSequence.accumulate(WrappingSequence.java:45) ~[druid-core-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.java.util.common.guava.LazySequence.accumulate(LazySequence.java:40) ~[druid-core-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.java.util.common.guava.WrappingSequence$1.get(WrappingSequence.java:50) ~[druid-core-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.java.util.common.guava.SequenceWrapper.wrap(SequenceWrapper.java:55) ~[druid-core-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.java.util.common.guava.WrappingSequence.accumulate(WrappingSequence.java:45) ~[druid-core-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.java.util.common.guava.WrappingSequence$1.get(WrappingSequence.java:50) ~[druid-core-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.query.CPUTimeMetricQueryRunner$1.wrap(CPUTimeMetricQueryRunner.java:74) ~[druid-processing-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.java.util.common.guava.WrappingSequence.accumulate(WrappingSequence.java:45) ~[druid-core-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.query.spec.SpecificSegmentQueryRunner$1.accumulate(SpecificSegmentQueryRunner.java:82) ~[druid-processing-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.java.util.common.guava.WrappingSequence$1.get(WrappingSequence.java:50) ~[druid-core-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.query.spec.SpecificSegmentQueryRunner.doNamed(SpecificSegmentQueryRunner.java:168) ~[druid-processing-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.query.spec.SpecificSegmentQueryRunner.access$100(SpecificSegmentQueryRunner.java:45) ~[druid-processing-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.query.spec.SpecificSegmentQueryRunner$2.wrap(SpecificSegmentQueryRunner.java:148) ~[druid-processing-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.java.util.common.guava.WrappingSequence.accumulate(WrappingSequence.java:45) ~[druid-core-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.java.util.common.guava.Sequence.toList(Sequence.java:76) ~[druid-core-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.query.ChainedExecutionQueryRunner$1$1.call(ChainedExecutionQueryRunner.java:124) [druid-processing-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at org.apache.druid.query.ChainedExecutionQueryRunner$1$1.call(ChainedExecutionQueryRunner.java:114) [druid-processing-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_151]\r\n\r\n       at org.apache.druid.query.PrioritizedListenableFutureTask.run(PrioritizedExecutorService.java:247) [druid-processing-0.14.0-incubating.jar:0.14.0-incubating]\r\n\r\n       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_151]\r\n\r\n       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_151]\r\n\r\n       at java.lang.Thread.run(Thread.java:748) [?:1.8.0_151]\r\n```"}, {"user": "vogievetsky", "commits": {}, "labels": ["Area - Querying", "Area - SQL", "Bug"], "created": "2019-08-08 18:36:00", "title": "LEFT() and RIGHT() functions do not work in DruidSQL", "url": "https://github.com/apache/druid/issues/8266", "closed": "2019-11-21 05:22:56", "ttf": 104.00027777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.15.0, 0.16.0-snapshot\r\n\r\n### Description\r\n\r\nLEFT and RIGHT are documented:\r\n\r\nhttps://druid.apache.org/docs/latest/querying/sql\r\n\r\n![image](https://user-images.githubusercontent.com/177816/62728389-878cd280-b9d0-11e9-8750-1cbc409a9f82.png)\r\n\r\nbut they do not seem to be supported in Calcite:\r\n\r\n![image](https://user-images.githubusercontent.com/177816/62728417-9a9fa280-b9d0-11e9-8fff-e609563964c6.png)\r\n\r\n\r\n"}, {"user": "vogievetsky", "commits": {}, "labels": ["Bug", "Feature/Change Description"], "created": "2019-08-08 18:02:28", "title": "Auto-compaction should forward the maxBytesInMemory setting to the compaction tasks", "url": "https://github.com/apache/druid/issues/8262", "closed": "2019-08-13 21:55:47", "ttf": 5.000277777777778, "commitsDetails": [], "body": "### Description\r\n\r\nRight now maxBytesInMemory is not sent from the auto compaction tuningConfg (https://druid.apache.org/docs/latest/configuration/index.html#compact-task-tuningconfig) to \r\nthe compaction task tuningConfig ( https://druid.apache.org/docs/latest/ingestion/native_tasks.html#tuningconfig )\r\n\r\n### Motivation\r\n\r\nIdeally most of the keys available in the compaction task tuningConfig should be settable from the auto compaction tuningConfig. `maxBytesInMemory` is particularly useful for when you need to tune memory in a specific way (maybe you have a ton of lookups or whatever).\r\n"}, {"user": "sashidhar", "commits": {}, "labels": ["Area - Web Console", "Bug"], "created": "2019-08-07 19:06:18", "title": "[Unified Web Console] EXPLAIN PLAN FOR throws org.apache.calcite.sql.parser.SqlParseException", "url": "https://github.com/apache/druid/issues/8254", "closed": "2019-09-25 21:24:16", "ttf": 49.000277777777775, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.15.0\r\n\r\n### Description\r\n\r\n1. A simple SQL statement like `\"EXPLAIN PLAN FOR SELECT 1 + 1;\"` from the unified web console is resulting in a `org.apache.calcite.sql.parser.SqlParseException`.\r\n\r\n`Unknown exception / Encountered \"( EXPLAIN\" at line 1, column 15. Was expecting one of: <IDENTIFIER> ... <QUOTED_IDENTIFIER> ... <BACK_QUOTED_IDENTIFIER> ... <BRACKET_QUOTED_IDENTIFIER> ... <UNICODE_QUOTED_IDENTIFIER> ... \"LATERAL\" ... \"(\" ... \"(\" \"WITH\" ... \"(\" \"+\" ... \"(\" \"-\" ... \"(\" \"NOT\" ... \"(\" \"EXISTS\" ... \"(\" <UNSIGNED_INTEGER_LITERAL> ... \"(\" <DECIMAL_NUMERIC_LITERAL> ... \"(\" <APPROX_NUMERIC_LITERAL> ... \"(\" <BINARY_STRING_LITERAL> ... \"(\" <PREFIXED_STRING_LITERAL> ... \"(\" <QUOTED_STRING> ... \"(\" <UNICODE_STRING_LITERAL> ... \"(\" \"TRUE\" ... \"(\" \"FALSE\" ... \"(\" \"UNKNOWN\" ... \"(\" \"NULL\" ... \"(\" <LBRACE_D> ... \"(\" <LBRACE_T> ... \"(\" <LBRACE_TS> ... \"(\" \"DATE\" ... \"(\" \"TIME\" ... \"(\" \"TIMESTAMP\" ... \"(\" \"INTERVAL\" ... \"(\" \"?\" ... \"(\" \"CAST\" ... \"(\" \"EXTRACT\" ... \"(\" \"POSITION\" ... \"(\" \"CONVERT\" ... \"(\" \"TRANSLATE\" ... \"(\" \"OVERLAY\" ... \"(\" \"FLOOR\" ... \"(\" \"CEIL\" ... \"(\" \"CEILING\" ... \"(\" \"SUBSTRING\" ... \"(\" \"TRIM\" ... \"(\" \"CLASSIFIER\" ... \"(\" \"MATCH_NUMBER\" ... \"(\" \"RUNNING\" ... \"(\" \"PREV\" ... \"(\" \"NEXT\" ... \"(\" <LBRACE_FN> ... \"(\" \"MULTISET\" ... \"(\" \"ARRAY\" ... \"(\" \"PERIOD\" ... \"(\" \"SPECIFIC\" ... \"(\" <IDENTIFIER> ... \"(\" <QUOTED_IDENTIFIER> ... \"(\" <BACK_QUOTED_IDENTIFIER> ... \"(\" <BRACKET_QUOTED_IDENTIFIER> ... \"(\" <UNICODE_QUOTED_IDENTIFIER> ... \"(\" \"ABS\" ... \"(\" \"AVG\" ... \"(\" \"CARDINALITY\" ... \"(\" \"CHAR_LENGTH\" ... \"(\" \"CHARACTER_LENGTH\" ... \"(\" \"COALESCE\" ... \"(\" \"COLLECT\" ... \"(\" \"COVAR_POP\" ... \"(\" \"COVAR_SAMP\" ... \"(\" \"CUME_DIST\" ... \"(\" \"COUNT\" ... \"(\" \"CURRENT_DATE\" ... \"(\" \"CURRENT_TIME\" ... \"(\" \"CURRENT_TIMESTAMP\" ... \"(\" \"DENSE_RANK\" ... \"(\" \"ELEMENT\" ... \"(\" \"EXP\" ... \"(\" \"FIRST_VALUE\" ... \"(\" \"FUSION\" ... \"(\" \"GROUPING\" ... \"(\" \"HOUR\" ... \"(\" \"LAG\" ... \"(\" \"LEAD\" ... \"(\" \"LAST_VALUE\" ... \"(\" \"LN\" ... \"(\" \"LOCALTIME\" ... \"(\" \"LOCALTIMESTAMP\" ... \"(\" \"LOWER\" ... \"(\" \"MAX\" ... \"(\" \"MIN\" ... \"(\" \"MINUTE\" ... \"(\" \"MOD\" ... \"(\" \"MONTH\" ... \"(\" \"NTH_VALUE\" ... \"(\" \"NTILE\" ... \"(\" \"NULLIF\" ... \"(\" \"OCTET_LENGTH\" ... \"(\" \"PERCENT_RANK\" ... \"(\" \"POWER\" ... \"(\" \"RANK\" ... \"(\" \"REGR_SXX\" ... \"(\" \"REGR_SYY\" ... \"(\" \"ROW_NUMBER\" ... \"(\" \"SECOND\" ... \"(\" \"SQRT\" ... \"(\" \"STDDEV_POP\" ... \"(\" \"STDDEV_SAMP\" ... \"(\" \"SUM\" ... \"(\" \"UPPER\" ... \"(\" \"TRUNCATE\" ... \"(\" \"USER\" ... \"(\" \"VAR_POP\" ... \"(\" \"VAR_SAMP\" ... \"(\" \"YEAR\" ... \"(\" \"CURRENT_CATALOG\" ... \"(\" \"CURRENT_DEFAULT_TRANSFORM_GROUP\" ... \"(\" \"CURRENT_PATH\" ... \"(\" \"CURRENT_ROLE\" ... \"(\" \"CURRENT_SCHEMA\" ... \"(\" \"CURRENT_USER\" ... \"(\" \"SESSION_USER\" ... \"(\" \"SYSTEM_USER\" ... \"(\" \"NEW\" ... \"(\" \"CASE\" ... \"(\" \"CURRENT\" ... \"(\" \"CURSOR\" ... \"(\" \"ROW\" ... \"(\" \"(\" ... \"(\" \"SELECT\" ... \"(\" \"VALUES\" ... \"(\" \"TABLE\" ... \"UNNEST\" ... \"TABLE\" ... / org.apache.calcite.sql.parser.SqlParseException`\r\n\r\nWhile issuing a curl for the same query works and outputs a plan.\r\n\r\n```\r\ncurl -X POST \\\r\n   -H \"Content-Type:application/json\" \\\r\n   -d \\\r\n'{\r\n  \"query\":\"EXPLAIN PLAN FOR SELECT 1+1\"\r\n}' \\\r\n'http://localhost:8888/druid/v2/sql'\r\n```\r\n`[{\"PLAN\":\"BindableValues(tuples=[[{ 2 }]])\\n\"}]`\r\n\r\n=> Inspecting the request payload from the Network activity console, the query issued is \r\n\r\n`\"SELECT * FROM (EXPLAIN PLAN FOR SELECT 1 + 1) LIMIT 5000\"`\r\n\r\n![image](https://user-images.githubusercontent.com/287761/62650289-29ca9d00-b974-11e9-9e1e-46750e4b5ab3.png)\r\n\r\nLooks like the console is modifying the query with the prefix \"SELECT * FROM\" in this case which is resulting in the issue.\r\n\r\ncurl of the modified query `\"SELECT * FROM (EXPLAIN PLAN FOR SELECT 1 + 1) LIMIT 5000\" `fails as expected.\r\n\r\n2. Same behaviour is observed for WITH clause.\r\n\r\n`WITH wikipedia as (SELECT 1 + 1) FOR SELECT * FROM wikipedia LIMIT 1;` throws `org.apache.calcite.sql.parser.SqlParseException` for the same reasons mentioned in 1.\r\n"}, {"user": "ArtyomyuS", "commits": {}, "labels": ["Area - Metrics/Event Emitting", "Bug"], "created": "2019-07-31 06:48:44", "title": "HttpPostEmitter throw Class cast exception when using emitAndReturnBatch", "url": "https://github.com/apache/druid/issues/8204", "closed": "2019-08-01 17:36:24", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "### Affected Version\r\n`0.13.0`, `0.14.2`\r\n\r\n### Description\r\nThe following exception is thrown on broker:\r\n```\r\n2019-07-30T15:55:21,823 WARN [HttpClient-Netty-Worker-9] org.apache.druid.java.util.http.client.NettyHttpClient - [POST http://{instance}:8102/druid/v2/] Exception thrown while processing message, closing channel.\r\njava.lang.ClassCastException: java.lang.Long cannot be cast to org.apache.druid.java.util.emitter.core.Batch\r\n        at org.apache.druid.java.util.emitter.core.HttpPostEmitter.emitAndReturnBatch(HttpPostEmitter.java:262) ~[druid-core-0.14.2-incubating.jar:0.14.2-incubating]\r\n        at org.apache.druid.java.util.emitter.core.HttpPostEmitter.emit(HttpPostEmitter.java:227) ~[druid-core-0.14.2-incubating.jar:0.14.2-incubating]\r\n        at org.apache.druid.java.util.emitter.service.ServiceEmitter.emit(ServiceEmitter.java:67) ~[druid-core-0.14.2-incubating.jar:0.14.2-incubating]\r\n        at org.apache.druid.java.util.emitter.service.ServiceEmitter.emit(ServiceEmitter.java:72) ~[druid-core-0.14.2-incubating.jar:0.14.2-incubating]\r\n        at org.apache.druid.query.DefaultQueryMetrics.emit(DefaultQueryMetrics.java:311) ~[druid-processing-0.14.2-incubating.jar:0.14.2-incubating]\r\n        at org.apache.druid.client.DirectDruidClient$1.handleResponse(DirectDruidClient.java:230) ~[druid-server-0.14.2-incubating.jar:0.14.2-incubating]\r\n        at org.apache.druid.java.util.http.client.NettyHttpClient$1.messageReceived(NettyHttpClient.java:224) [druid-core-0.14.2-incubating.jar:0.14.2-incubating]\r\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.handler.timeout.ReadTimeoutHandler.messageReceived(ReadTimeoutHandler.java:184) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.handler.codec.http.HttpClientCodec.handleUpstream(HttpClientCodec.java:92) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) [netty-3.10.6.Final.jar:?]\r\n        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) [netty-3.10.6.Final.jar:?]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_201]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_201]\r\n        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_201]\r\n```\r\n\r\nWas able to reproduce it on both: `0.13.0` and `0.14.2` versions, however I do see same code in `0.15.0` version.\r\n\r\nI was not able to understand properly how this may happen on a real cluster as the stack trace doesn't show a lot, however spotted the potential cause here: `HttpPostEmitter:308`, while a throwable thrown the `concurrentBatch` is compareAndSet(batch, batch.batchNumber) so that `compareAndSet(Batch.class, Long.class)`.\r\n\r\nWhile checking `HttpPostEmitter#emitAndReturnBatch` method I see there is a condition to check if batch object is instance of `Integer` to try to recover it. I suppose this must be check for `Long.class` as Batch.batchNumber is of type `Long`.\r\n\r\n\r\nI've tried to simulate somehow the behaviour of this flow by forcing `concurrentBatch` as Long type. Example: \r\n```\r\npublic class HttpPostEmitterTest {\r\n\r\n    private static final ObjectMapper objectMapper = new ObjectMapper() {\r\n        @Override\r\n        public byte[] writeValueAsBytes(Object value) {\r\n            return Ints.toByteArray(((IntEvent) value).index);\r\n        }\r\n    };\r\n\r\n    private final MockHttpClient httpClient = new MockHttpClient();\r\n\r\n    @Before\r\n    public void setup() {\r\n        httpClient.setGoHandler(new GoHandler() {\r\n            @Override\r\n            protected ListenableFuture<Response> go(Request request) {\r\n                return GoHandlers.immediateFuture(EmitterTest.okResponse());\r\n            }\r\n        });\r\n    }\r\n\r\n\r\n    @Test(expected = ClassCastException.class)\r\n    @SuppressWarnings(\"unchecked\")\r\n    public void testRecoveryEmitAndReturnBatch() throws InterruptedException, IOException, NoSuchFieldException, IllegalAccessException {\r\n        HttpEmitterConfig config = new HttpEmitterConfig.Builder(\"http://foo.bar\")\r\n                .setFlushMillis(100)\r\n                .setFlushCount(4)\r\n                .setBatchingStrategy(BatchingStrategy.ONLY_EVENTS)\r\n                .setMaxBatchSize(1024 * 1024)\r\n                .setBatchQueueSizeLimit(1000)\r\n                .build();\r\n        final HttpPostEmitter emitter = new HttpPostEmitter(config, httpClient, objectMapper);\r\n        emitter.start();\r\n        // emit first event\r\n        emitter.emitAndReturnBatch(new IntEvent());\r\n        Thread.sleep(1000L);\r\n\r\n        // get concurrentBatch reference and set value to long as if it would fail while\r\n        // HttpPostEmitter#onSealExclusive method invocation.\r\n        Field concurrentBatch = emitter.getClass().getDeclaredField(\"concurrentBatch\");\r\n        concurrentBatch.setAccessible(true);\r\n        ((AtomicReference<Object>) concurrentBatch.get(emitter)).getAndSet(1L);\r\n        // something terrible happened previously so that batch has to recover\r\n\r\n        // emit second event\r\n        emitter.emitAndReturnBatch(new IntEvent()); // will fail with ClassCastException.class here\r\n\r\n        emitter.flush();\r\n        emitter.close();\r\n    }\r\n\r\n}\r\n``` \r\n\r\nI suppose the fix should be relatively simple,  just changing in HttpPostEmitter:254 the following block, from Integer to Long:\r\n```\r\nObject batchObj = concurrentBatch.get();\r\n      if (batchObj instanceof Long) {\r\n        tryRecoverCurrentBatch((Long) batchObj);\r\n        continue;\r\n      }\r\n```\r\nand changing method declaration to consider Long, HttpPostEmitter:346:\r\n```\r\nprivate void tryRecoverCurrentBatch(Long failedBatchNumber){\r\n ...\r\n}\r\n```\r\n\r\nAlso found that @leventov worked on a patch #5386 some time ago to make `onSealExclusive()` more robust in case of OOM Errors, what I notice is that there was no OOM issues found in the logs while class cast exception popup, not sure those are related.\r\n\r\n\r\n"}, {"user": "surekhasaharan", "commits": {}, "labels": ["Area - SQL", "Bug"], "created": "2019-07-24 00:05:40", "title": "sys.segment's is_realtime flag once set, is never unset", "url": "https://github.com/apache/druid/issues/8142", "closed": "2019-08-01 04:26:50", "ttf": 8.000277777777777, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.15\r\n\r\n### Description\r\n\r\nIn the system schema's `sys.segments` table, the `is_realtime` flag is set when a segment is added via `segmentAdded` callback in `DruidSchema`. But it's not unset when a segment is dropped from realtime task and published on historicals. So when `serverSegmentRemoved` is called, the `is_realtime` flag should be unset. \r\nThis was reported here https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/druid-user/t-215-pIJO4/yG9Y5VRLDwAJ\r\n"}, {"user": "pdeva", "commits": {}, "labels": ["Area - Segment Balancing/Coordination", "Bug"], "created": "2019-07-23 18:18:58", "title": "coordinator throwing exception trying to load segments", "url": "https://github.com/apache/druid/issues/8137", "closed": "2019-07-24 17:59:44", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.15\r\n\r\n### Description\r\n\r\nstarted happening all of a sudden. no segments can be loaded in the historical!\r\n\r\n```\r\n2019-07-23 18:17:41,742 ERROR o.a.d.s.c.DruidCoordinator [Coordinator-Exec--0] Caught exception, ignoring so that schedule keeps going.: {class=org.apache.druid.server.coordinator.DruidCoordinator, exceptionType=class java.util.concurrent.RejectedExecutionException, exceptionMessage=Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@57722b5c rejected from java.util.concurrent.ScheduledThreadPoolExecutor@40995ea7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 34991]}\r\njava.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@57722b5c rejected from java.util.concurrent.ScheduledThreadPoolExecutor@40995ea7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 34991]\r\n\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) ~[?:1.8.0_181]\r\n\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) ~[?:1.8.0_181]\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:326) ~[?:1.8.0_181]\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533) ~[?:1.8.0_181]\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor.submit(ScheduledThreadPoolExecutor.java:632) ~[?:1.8.0_181]\r\n\tat org.apache.druid.server.coordinator.CuratorLoadQueuePeon.loadSegment(CuratorLoadQueuePeon.java:181) ~[druid-server-0.15.0-incubating-iap2.jar:0.15.0-incubating-iap2]\r\n\tat org.apache.druid.server.coordinator.rules.LoadRule.assignPrimary(LoadRule.java:216) ~[druid-server-0.15.0-incubating-iap2.jar:0.15.0-incubating-iap2]\r\n\tat org.apache.druid.server.coordinator.rules.LoadRule.assign(LoadRule.java:104) ~[druid-server-0.15.0-incubating-iap2.jar:0.15.0-incubating-iap2]\r\n\tat org.apache.druid.server.coordinator.rules.LoadRule.run(LoadRule.java:77) ~[druid-server-0.15.0-incubating-iap2.jar:0.15.0-incubating-iap2]\r\n\tat org.apache.druid.server.coordinator.helper.DruidCoordinatorRuleRunner.run(DruidCoordinatorRuleRunner.java:122) ~[druid-server-0.15.0-incubating-iap2.jar:0.15.0-incubating-iap2]\r\n\tat org.apache.druid.server.coordinator.DruidCoordinator$CoordinatorRunnable.run(DruidCoordinator.java:716) [druid-server-0.15.0-incubating-iap2.jar:0.15.0-incubating-iap2]\r\n\tat org.apache.druid.server.coordinator.DruidCoordinator$2.call(DruidCoordinator.java:601) [druid-server-0.15.0-incubating-iap2.jar:0.15.0-incubating-iap2]\r\n\tat org.apache.druid.server.coordinator.DruidCoordinator$2.call(DruidCoordinator.java:594) [druid-server-0.15.0-incubating-iap2.jar:0.15.0-incubating-iap2]\r\n\tat org.apache.druid.java.util.common.concurrent.ScheduledExecutors$2.run(ScheduledExecutors.java:92) [druid-core-0.15.0-incubating-iap2.jar:0.15.0-incubating-iap2]\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_181]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_181]\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_181]\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_181]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_181]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_181]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]\r\n```\r\n\r\nalso seeing this:\r\n\r\n```\r\n2019-07-23 18:17:41,733 ERROR o.a.d.s.c.ReplicationThrottler [Coordinator-Exec--0] [_default_tier]: Replicant create queue stuck after 15+ runs!: {class=org.apache.druid.server.coordinator.ReplicationThrottler, segments=[apm-minute_2019-07-23T06:00:00.000Z_2019-07-23T12:00:00.000Z_2019-07-23T05:58:50.795Z ON 192.168.44.101:8080]}\r\n```\r\n"}, {"user": "gianm", "commits": {}, "labels": ["Area - Querying", "Bug"], "created": "2019-07-17 04:16:32", "title": "groupBy with subtotalsSpec doesn't fully group each set", "url": "https://github.com/apache/druid/issues/8091", "closed": "2019-08-06 14:06:29", "ttf": 20.00027777777778, "commitsDetails": [], "body": "These two queries (if posed in GroupByQueryRunnerTest) return different results, but I think they should return the same results. The only difference between query 1 and query 2 is that query 2 has an additional dimension that is _not_ referenced in the subtotalsSpec.\r\n\r\nI believe the issue is that `GroupByStrategyV2.processSubtotalsSpec` calls `mergeResults` on rows returned by `GroupByRowProcessor.getRowsFromGrouper` for each subtotal dimension list, but this isn't enough to fully group them. The result rows from the grouper are sorted based on the original dimension set, and `mergeResults` only merges adjacent rows.\r\n\r\ni.e. Imagine you have two dimensions and they each have values A, B, and C. The original grouper might have rows like this:\r\n\r\nAA\r\nAB\r\nBA\r\nBB\r\n\r\nPassing \"dimsToInclude\" with just the second dim would yield these rows from the Grouper:\r\n\r\nA\r\nB\r\nA\r\nB\r\n\r\nWhich cannot be merged by `mergeResults`.\r\n\r\nI can think of a couple of fixes:\r\n\r\n1. Re-sorting the Grouper rows each time they are pulled out. However, not all Groupers support re-sorting (e.g. SpillingGrouper cannot change its sort order once it has spilled to disk) so it might not always be possible.\r\n2. Using _two_ Groupers (with two merge buffers), one to store the initially grouped rows and one to store subtotal groupings. The act of adding results to the subtotal Grouper will properly and fully group them. Takes more memory but should work in all cases.\r\n\r\nQuery 1:\r\n\r\n```java\r\n    GroupByQuery query = makeQueryBuilder()\r\n        .setDataSource(QueryRunnerTestHelper.dataSource)\r\n        .setQuerySegmentSpec(QueryRunnerTestHelper.firstToThird)\r\n        .setDimensions(\r\n            ImmutableList.of(\r\n                new DefaultDimensionSpec(\"market\", \"market\")\r\n            )\r\n        )\r\n        .setAggregatorSpecs(Collections.singletonList(QueryRunnerTestHelper.rowsCount))\r\n        .setGranularity(QueryRunnerTestHelper.allGran)\r\n        .setSubtotalsSpec(ImmutableList.of(ImmutableList.of(\"market\")))\r\n        .build();\r\n```\r\n\r\nQuery 2:\r\n\r\n```java\r\n    GroupByQuery query = makeQueryBuilder()\r\n        .setDataSource(QueryRunnerTestHelper.dataSource)\r\n        .setQuerySegmentSpec(QueryRunnerTestHelper.firstToThird)\r\n        .setDimensions(\r\n            ImmutableList.of(\r\n                new DefaultDimensionSpec(\"quality\", \"quality\"),\r\n                new DefaultDimensionSpec(\"market\", \"market\")\r\n            )\r\n        )\r\n        .setAggregatorSpecs(Collections.singletonList(QueryRunnerTestHelper.rowsCount))\r\n        .setGranularity(QueryRunnerTestHelper.allGran)\r\n        .setSubtotalsSpec(ImmutableList.of(ImmutableList.of(\"market\")))\r\n        .build();\r\n```\r\n\r\n/cc @himanshug "}, {"user": "trtg", "commits": {}, "labels": ["Area - Web Console", "Bug"], "created": "2019-07-12 21:32:48", "title": "Unified console broken: resizeSensor.js unexpected token { on line 88", "url": "https://github.com/apache/druid/issues/8074", "closed": "2019-07-16 16:22:13", "ttf": 3.000277777777778, "commitsDetails": [], "body": "### Affected Version\r\n0.15.0\r\n\r\n### Description\r\n\r\nThe unified console does not load at all- I get a blank page and the javascript error included at the end of this description appears in the JS console. The unified console worked fine in 0.14.0.\r\n\r\n- 3 R5.2xlarge instances in cluster\r\n- only steps to reproduce- try to load the unified console by going to node:8888\r\n\r\nHere is the error that appears in the JS console:\r\n`\r\nresizeSensor.js:88 Uncaught SyntaxError: Unexpected token {\r\n    at Object../node_modules/@blueprintjs/core/lib/esnext/components/resize-sensor/resizeSensor.js (web-console-0.15.0.js:2323)\r\n    at __webpack_require__ (web-console-0.15.0.js:20)\r\n    at eval (popover.js:15)\r\n    at Module../node_modules/@blueprintjs/core/lib/esnext/components/popover/popover.js (web-console-0.15.0.js:2239)\r\n    at __webpack_require__ (web-console-0.15.0.js:20)\r\n    at eval (contextMenu.js:15)\r\n    at Module../node_modules/@blueprintjs/core/lib/esnext/components/context-menu/contextMenu.js (web-console-0.15.0.js:1771)\r\n    at __webpack_require__ (web-console-0.15.0.js:20)\r\n    at eval (index.js:3)\r\n    at Module../node_modules/@blueprintjs/core/lib/esnext/components/index.js (web-console-0.15.0.js:2083)\r\n`\r\n"}, {"user": "acdn-mpreston", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2019-06-19 13:43:28", "title": "Kafka Indexing breaks due to Mismatched DatSourceMetadata types during InternalReset call", "url": "https://github.com/apache/druid/issues/7926", "closed": "2019-08-22 21:51:26", "ttf": 64.00027777777778, "commitsDetails": [], "body": "\r\n### Affected Version\r\n\r\n0.14.0, 0.14.2, 0.15.0-incubating\r\n\r\n### Description\r\n\r\nThis is pretty hard to reproduce as it seems to be a race condition during unexpected termination of the middle manager process.\r\n\r\nSomething happens, I suspect when middle manager shuts down unexpectedly at a specific yet not currently discovered time, that causes the datasource metadata record to have the wrong type for the sequence stream numbers. When middle manger comes back up and is creating the new indexing tasks, it will fail with the following stack trace:\r\n\r\naod_overlord.1.dcc7ttv9r52t@datahub2-leader-d0xb    | 2019-06-10T14:02:32,642 ERROR [KafkaSupervisor-npav-ts-metrics] org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor - SeekableStreamSupervisor[npav-ts-metrics] failed to handle notice: {class=org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor, exceptionType=class org.apache.druid.java.util.common.IAE, exceptionMessage=Expected instance of org.apache.druid.indexing.seekablestream.SeekableStreamEndSequenceNumbers, got org.apache.druid.indexing.seekablestream.SeekableStreamStartSequenceNumbers, noticeClass=RunNotice}\r\naod_overlord.1.dcc7ttv9r52t@datahub2-leader-d0xb    | org.apache.druid.java.util.common.IAE: Expected instance of org.apache.druid.indexing.seekablestream.SeekableStreamEndSequenceNumbers, got org.apache.druid.indexing.seekablestream.SeekableStreamStartSequenceNumbers\r\naod_overlord.1.dcc7ttv9r52t@datahub2-leader-d0xb    | \tat org.apache.druid.indexing.seekablestream.SeekableStreamEndSequenceNumbers.minus(SeekableStreamEndSequenceNumbers.java:159) ~[druid-indexing-service-0.14.0-incubating.jar:0.14.0-incubating]\r\naod_overlord.1.dcc7ttv9r52t@datahub2-leader-d0xb    | \tat org.apache.druid.indexing.seekablestream.SeekableStreamDataSourceMetadata.minus(SeekableStreamDataSourceMetadata.java:95) ~[druid-indexing-service-0.14.0-incubating.jar:0.14.0-incubating]\r\naod_overlord.1.dcc7ttv9r52t@datahub2-leader-d0xb    | \tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.resetInternal(SeekableStreamSupervisor.java:1147) ~[druid-indexing-service-0.14.0-incubating.jar:0.14.0-incubating]\r\naod_overlord.1.dcc7ttv9r52t@datahub2-leader-d0xb    | \tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.getOffsetFromStorageForPartition(SeekableStreamSupervisor.java:2380) ~[druid-indexing-service-0.14.0-incubating.jar:0.14.0-incubating]\r\naod_overlord.1.dcc7ttv9r52t@datahub2-leader-d0xb    | \tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.generateStartingSequencesForPartitionGroup(SeekableStreamSupervisor.java:2357) ~[druid-indexing-service-0.14.0-incubating.jar:0.14.0-incubating]\r\naod_overlord.1.dcc7ttv9r52t@datahub2-leader-d0xb    | \tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.createNewTasks(SeekableStreamSupervisor.java:2254) ~[druid-indexing-service-0.14.0-incubating.jar:0.14.0-incubating]\r\naod_overlord.1.dcc7ttv9r52t@datahub2-leader-d0xb    | \tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.runInternal(SeekableStreamSupervisor.java:1013) ~[druid-indexing-service-0.14.0-incubating.jar:0.14.0-incubating]\r\naod_overlord.1.dcc7ttv9r52t@datahub2-leader-d0xb    | \tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor$RunNotice.handle(SeekableStreamSupervisor.java:265) ~[druid-indexing-service-0.14.0-incubating.jar:0.14.0-incubating]\r\naod_overlord.1.dcc7ttv9r52t@datahub2-leader-d0xb    | \tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.lambda$tryInit$3(SeekableStreamSupervisor.java:724) ~[druid-indexing-service-0.14.0-incubating.jar:0.14.0-incubating]\r\naod_overlord.1.dcc7ttv9r52t@datahub2-leader-d0xb    | \tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_66-internal]\r\naod_overlord.1.dcc7ttv9r52t@datahub2-leader-d0xb    | \tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_66-internal]\r\naod_overlord.1.dcc7ttv9r52t@datahub2-leader-d0xb    | \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_66-internal]\r\naod_overlord.1.dcc7ttv9r52t@datahub2-leader-d0xb    | \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_66-internal]\r\naod_overlord.1.dcc7ttv9r52t@datahub2-leader-d0xb    | \tat java.lang.Thread.run(Thread.java:745) [?:1.8.0_66-internal]\r\n\r\nThe issue is that the minus operation requires a SeekableStreamSequenceNumber of the same implementation (start or end) to be successful, but the currentMetadata and resetMetadata values are of different types. \r\n\r\nI tried to correct this issue via conversion of the currentMetadata and resetMetadata both to sequence start numbers via the existing helper method 'asStartMetadata' on the SeekableStreamEndSequenceNumber class, but this caused further conversion issues on subsequent indexing tasks as documented in this PR: https://github.com/apache/incubator-druid/pull/7831\r\n\r\nAfter trying some other solutions, it became clear to me that I do not know enough about the way the start and end numbers were being modified to inject a patch using the built in conversion function. Since we are only using Kafka ingestion (not Kinesis), I re-evaluated the SeekableStreamEndSequenceNumber and SeekableStreamStartSequenceNumber classes and realized that they are pretty much the same thing and that only the \"exclusivePartitions' property of the SeekableStreamStartSequenceNumber is used for kinesis streaming. So it seems like both classes are not really required as one class could provide the functionality needed.\r\n\r\nI believe this entire issue could be resolved by consolidating those two classes into one class as I am not certain two separate classes are required to manage the partition offset states for the start and end offsets independently.\r\n\r\n### Current Workaround\r\nI have a custom build where I have commented out the SeekableStreamEndSequenceNumber class and replaced all instances of SeekableStreamEndSequenceNumber with SeekableStreamStartSequenceNumber. Anytime the constructor for SeekableStreamStartSequenceNumber is called, I am using Collections.emptySet() for the third parameter for the exclusivePartitionMap as Kafka indexing does not use this. So far this workaround has \"corrected\" the bad datasource metadata and indexing tasks are all passing successfully. I am currently testing failure scenarios to try and reproduce the original race condition bug to validate this is working.\r\n"}, {"user": "xvrl", "commits": {}, "labels": ["Bug"], "created": "2019-06-13 17:12:38", "title": "Middlemanager fails startup due to corrupt task files", "url": "https://github.com/apache/druid/issues/7886", "closed": "2019-06-18 16:18:44", "ttf": 4.000277777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.15.0-SNAPSHOT (git sha d99f77a01b5f4e0abde0ec85)\r\n\r\n### Description\r\n\r\nA middle-manager shutdown may leave empty task files in \r\n`${druid.indexer.task.baseTaskDir}/completedTasks/`. This may be an issue on it's own, but it could also happen for reasons beyond our control.\r\n\r\nThose empty (corrupt) files cause the middlemanager to fail on a subsequent startup, due to https://github.com/apache/incubator-druid/blob/master/indexing-service/src/main/java/org/apache/druid/indexing/worker/WorkerTaskManager.java#L430 re-throwingh a JsonProcessingException\r\n\r\nThe exception message also looks incorrect, saying the files would be ignored, but instead it causes the entire startup sequence to interrupt, requiring user intervention to remove corrupt files in order to resume startup.\r\n\r\n```2019-06-13T16:39:50,861 ERROR [main] org.apache.druid.cli.CliMiddleManager - Error when starting up.  Failing.\r\njava.lang.reflect.InvocationTargetException\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_212]\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_212]\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_212]\r\n\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_212]\r\n\tat org.apache.druid.java.util.common.lifecycle.Lifecycle$AnnotationBasedHandler.start(Lifecycle.java:443) ~[druid-core-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.lifecycle.Lifecycle.start(Lifecycle.java:339) ~[druid-core-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n\tat org.apache.druid.guice.LifecycleModule$2.start(LifecycleModule.java:140) ~[druid-core-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n\tat org.apache.druid.cli.GuiceRunnable.initLifecycle(GuiceRunnable.java:106) [druid-services-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n\tat org.apache.druid.cli.ServerRunnable.run(ServerRunnable.java:57) [druid-services-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n\tat org.apache.druid.cli.Main.main(Main.java:118) [druid-services-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\nCaused by: org.apache.druid.java.util.common.ISE: Failed to read completed task from disk at [/var/druid/task/completedTasks/index_kafka_metrics_opencensus_ebffe52caf33afb_lcgaddpn]. Ignored.\r\n\tat org.apache.druid.indexing.worker.WorkerTaskManager.initCompletedTasks(WorkerTaskManager.java:430) ~[druid-indexing-service-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n\tat org.apache.druid.indexing.worker.WorkerTaskManager.start(WorkerTaskManager.java:135) ~[druid-indexing-service-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n\tat org.apache.druid.indexing.worker.WorkerTaskMonitor.start(WorkerTaskMonitor.java:94) ~[druid-indexing-service-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n\t... 10 more\r\nCaused by: com.fasterxml.jackson.databind.JsonMappingException: No content to map due to end-of-input\r\n at [Source: var/druid/task/completedTasks/index_kafka_<datasource>_ebffe52caf33afb_lcgaddpn; line: 1, column: 1]\r\n\tat com.fasterxml.jackson.databind.JsonMappingException.from(JsonMappingException.java:148) ~[jackson-databind-2.6.7.jar:2.6.7]\r\n\tat com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:3781) ~[jackson-databind-2.6.7.jar:2.6.7]\r\n\tat com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3721) ~[jackson-databind-2.6.7.jar:2.6.7]\r\n\tat com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2620) ~[jackson-databind-2.6.7.jar:2.6.7]\r\n\tat org.apache.druid.indexing.worker.WorkerTaskManager.initCompletedTasks(WorkerTaskManager.java:421) ~[druid-indexing-service-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n\tat org.apache.druid.indexing.worker.WorkerTaskManager.start(WorkerTaskManager.java:135) ~[druid-indexing-service-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n\tat org.apache.druid.indexing.worker.WorkerTaskMonitor.start(WorkerTaskMonitor.java:94) ~[druid-indexing-service-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n```"}, {"user": "jihoonson", "commits": {}, "labels": ["Area - Web Console", "Bug"], "created": "2019-06-12 01:13:14", "title": "Pagination buttons are hidden in Druid console if web browser window is too small", "url": "https://github.com/apache/druid/issues/7869", "closed": "2019-06-12 21:05:04", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.15.0\r\n\r\n### Description\r\n\r\n![Screen Shot 2019-06-11 at 6 11 46 PM](https://user-images.githubusercontent.com/2322288/59316674-82cbcc80-8c74-11e9-8e00-2fa21db8d140.png)\r\n\r\nThe pagination buttons are not visible as seen above."}, {"user": "CalvinSchulze", "commits": {}, "labels": ["Area - Web Console", "Bug"], "created": "2019-06-06 07:51:07", "title": "No scrolling in Web Interface", "url": "https://github.com/apache/druid/issues/7836", "closed": "2019-09-26 16:08:01", "ttf": 112.00027777777778, "commitsDetails": [], "body": "When I open for example the Dynamic Settings Editor, I can't access all options, because the list is longer than my screen and there's no scroll bar (scrolling with mouse wheel has no effect). Same for the query window. I can't read my 100 entries, because I see only the top 10.\r\n\r\n### Affected Version\r\n\r\n 0.14.2-incubating on Ubuntu 19.04 in Firefox\r\n\r\n### Description\r\n\r\n- Configurations in use: The Quickstart Tutorial config\r\n- Steps to reproduce the problem:\r\n1. Start the server (using the Quickstart Config)\r\n2. Open the Web View\r\n3. Open the Dynamic Settings Assitant\r\n4. Try to see all options\r\n"}, {"user": "c0de0ff", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug", "Feature/Change Description"], "created": "2019-05-28 06:56:06", "title": "kafka indexing service should always use earliest offset for newly discovered topic-partitions instead of useEarliestOffset config", "url": "https://github.com/apache/druid/issues/7776", "closed": "2019-11-18 19:05:32", "ttf": 174.00027777777777, "commitsDetails": [], "body": "### Description\r\n\r\nKafka-indexing-service currently uses `useEarliestOffset` config whenever it can't find any data for a topic-partition. This happens when the supervisor is running for the first time or when there is a new partition for this topic in kafka. The config is also used to reset the offsets ( if `resetOffsetAutomatically` is set to true ).\r\n\r\nIn 2 of the above 3 scenarios, it makes sense to use `useEarliestOffset` config. However, it doesn't seem like indexing service should use this config on newly discovered partitions. If `useEarliestOffset` is set to false then this might result in data loss. In production environment, with large kafka clusters and many long running supervisors, adding new partitions to kafka topics would be a common occurrence and therefore this config must always remain true to avoid any data loss.\r\n\r\nA typical use case in large kafka clusters is to start the new supervisor from latest offset and keep consuming without any data loss ( exactly once ). In order to achieve this currently, we have to start supervisor with `useEarliestOffset` set to false and then wait for it to start running and then set the config back to true to avoid data loss in new partitions. User may also want to reset to latest offsets manually using the reset api, in this case also, he need to remember setting the config back to true which can be error prone.\r\n\r\nThe solution to this might be to not use the config while getting offsets for new partitions ( always use earliest ), however, i am not sure how we can differentiate the 2 events \"new partitions added\" vs \"supervisor first run\".\r\n\r\n### Motivation\r\n\r\n- Currently in order to avoid data loss from new partitions, we must always keep `useEarliestOffset` set to true, which creates the need to manually change the config back and forth in case we want to use the diff option for first-start/reset."}, {"user": "pdeva", "commits": {"04929e480c01a455d86fe5f9dff2201c3f7cc007": {"commitGHEventType": "referenced", "commitUser": "clintropolis"}, "cf09fbf4ce54443c78a40f529b124423ed1c5443": {"commitGHEventType": "referenced", "commitUser": "gianm"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "Regression: Coordinator startup fails with exception", "numCommits": 0, "created": "2019-05-26 03:17:29", "closed": "2019-05-26 22:14:51", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/7762", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 0.0002777777777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "services/src/main/java/org/apache/druid/cli/CliCoordinator.java", "deletions": 2, "insertions": 4, "lines": 6}], "commitSpoonAstDiffStats": [{"spoonFilePath": "CliCoordinator.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.cli.CliCoordinator.getModules()", "MOV": 1, "TOT": 2}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2019-05-27 14:10:06", "commitMessage": "Fix LookupSerdeModule double-binding in Coordinator-as-Overlord mode. (#7765) (#7774)\n\nFixes #7762.", "commitUser": "clintropolis", "commitDateTime": "2019-05-27 14:10:06", "commitParents": ["05d5276b9ef8f65e41d51d5874d147b3e5f23de9"], "commitGHEventType": "referenced", "nameRev": "04929e480c01a455d86fe5f9dff2201c3f7cc007 tags/druid-0.15.0-incubating-rc1~10", "commitHash": "04929e480c01a455d86fe5f9dff2201c3f7cc007"}, {"commitGitStats": [{"filePath": "services/src/main/java/org/apache/druid/cli/CliCoordinator.java", "deletions": 2, "insertions": 4, "lines": 6}], "commitSpoonAstDiffStats": [{"spoonFilePath": "CliCoordinator.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.cli.CliCoordinator.getModules()", "MOV": 1, "TOT": 2}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2019-05-26 15:14:50", "commitMessage": "Fix LookupSerdeModule double-binding in Coordinator-as-Overlord mode. (#7765)\n\nFixes #7762.", "commitUser": "gianm", "commitDateTime": "2019-05-26 15:14:50", "commitParents": ["c97eb93e16d914e3ad54138efe9d3a3990e1a52a"], "commitGHEventType": "referenced", "nameRev": "cf09fbf4ce54443c78a40f529b124423ed1c5443 tags/druid-0.16.0-incubating-rc1~357", "commitHash": "cf09fbf4ce54443c78a40f529b124423ed1c5443"}], "body": "### Affected Version\r\n0.14.0-iap10\r\n\r\n### Last working version\r\n\r\n0.14.0-iap8\r\n\r\n### Description\r\n\r\nNo configuration changes were made when upgrading versions\r\n\r\n```\r\nException in thread \"main\" com.google.inject.CreationException: Unable to create injector, see the following errors:\r\n1) A binding to com.google.common.base.Supplier<org.apache.druid.query.lookup.LookupConfig> was already configured at org.apache.druid.guice.JsonConfigProvider.bind(JsonConfigProvider.java:151) (via modules: com.google.inject.util.Modules$OverrideModule -> com.google.inject.util.Modules$OverrideModule -> org.apache.druid.query.lookup.LookupSerdeModule).\r\n  at org.apache.druid.guice.JsonConfigProvider.bind(JsonConfigProvider.java:151) (via modules: com.google.inject.util.Modules$OverrideModule -> com.google.inject.util.Modules$OverrideModule -> org.apache.druid.query.lookup.LookupSerdeModule)\r\n2) A binding to org.apache.druid.query.lookup.LookupConfig was already configured at org.apache.druid.guice.JsonConfigProvider.bind(JsonConfigProvider.java:152) (via modules: com.google.inject.util.Modules$OverrideModule -> com.google.inject.util.Modules$OverrideModule -> org.apache.druid.query.lookup.LookupSerdeModule).\r\n  at org.apache.druid.guice.JsonConfigProvider.bind(JsonConfigProvider.java:152) (via modules: com.google.inject.util.Modules$OverrideModule -> com.google.inject.util.Modules$OverrideModule -> org.apache.druid.query.lookup.LookupSerdeModule)\r\n2 errors\r\n\tat com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:470)\r\n\tat com.google.inject.internal.InternalInjectorCreator.initializeStatically(InternalInjectorCreator.java:155)\r\n\tat com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:107)\r\n\tat com.google.inject.Guice.createInjector(Guice.java:99)\r\n\tat com.google.inject.Guice.createInjector(Guice.java:73)\r\n\tat com.google.inject.Guice.createInjector(Guice.java:62)\r\n\tat org.apache.druid.initialization.Initialization.makeInjectorWithModules(Initialization.java:422)\r\n\tat org.apache.druid.cli.GuiceRunnable.makeInjector(GuiceRunnable.java:69)\r\n\tat org.apache.druid.cli.ServerRunnable.run(ServerRunnable.java:57)\r\n\tat org.apache.druid.cli.Main.main(Main.java:118)\r\n```\r\n\r\n"}, {"user": "vogievetsky", "commits": {}, "labels": ["Area - Web Console", "Bug"], "created": "2019-05-24 18:06:26", "title": "Web console: the MiddleManagers view should not show an error if everything is ok.", "url": "https://github.com/apache/druid/issues/7753", "closed": "2019-05-29 00:12:51", "ttf": 4.000277777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\nAll versions that have the web console 0.14.0 and above\r\n\r\n### Description\r\n\r\nIf you do not have `druid.indexer.runner.type=remote` set then `/druid/indexer/v1/workers` will fail with an error. The console is currently showing that error even though everything is ok.\r\nIt should instead detect that error condition and hide the entire workers view.\r\n\r\nRelevant report: https://groups.google.com/forum/#!topic/druid-user/rCz3J59yRA4"}, {"user": "a2l007", "commits": {}, "labels": ["Bug"], "created": "2019-05-23 19:54:47", "title": "Indexing tasks containing thetaSketches results in incorrect sketch values", "url": "https://github.com/apache/druid/issues/7741", "closed": "2019-08-23 07:22:41", "ttf": 91.00027777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.13.0-incubating\r\n\r\n### Description\r\n\r\nOne of our clusters running on 0.13.0 is seeing data corruption issues with thetasketches. There are two datasources used in this testcase: basefact and slice. slice is essentially same as basefact except with fewer dimensions, so as to improve the rollup ratio.\r\n**Ingestion Spec for basefact datasource:**\r\n```\r\n{\r\n  \"type\" : \"index_hadoop\",\r\n  \"id\" : \"index_base\",\r\n  \"spec\" : {\r\n    \"dataSchema\" : {\r\n      \"dataSource\" : \"basefact\",\r\n      \"parser\" : {\r\n        \"type\" : \"avro_hadoop\",\r\n        \"parseSpec\" : {\r\n          \"format\" : \"avro\",\r\n          \"timestampSpec\" : {\r\n            \"column\" : \"date\",\r\n            \"format\" : \"yyyyMMdd\"\r\n          },\r\n          \"dimensionsSpec\" : {\r\n            \"dimensions\" : [ \"src_pty_id\" ]\r\n          }\r\n        }\r\n      },\r\n      \"metricsSpec\" : [ {\r\n        \"type\" : \"thetaSketch\",\r\n        \"name\" : \"test_sketch\",\r\n        \"fieldName\" : \"test_sketch\",\r\n        \"size\" : 1048576,\r\n        \"shouldFinalize\" : true,\r\n        \"isInputThetaSketch\" : true,\r\n        \"errorBoundsStdDev\" : null\r\n      } ],\r\n      \"granularitySpec\" : {\r\n        \"type\" : \"uniform\",\r\n        \"segmentGranularity\" : \"DAY\",\r\n        \"queryGranularity\" : {\r\n          \"type\" : \"none\"\r\n        },\r\n        \"rollup\" : true,\r\n        \"intervals\" : [ \"2019-05-22T00:00:00.000Z/2019-05-23T00:00:00.000Z\" ]\r\n      },\r\n      \"transformSpec\" : {\r\n        \"filter\" : null,\r\n        \"transforms\" : [ ]\r\n      }\r\n    },\r\n    \"ioConfig\" : {\r\n      \"type\" : \"hadoop\",\r\n      \"inputSpec\" : {\r\n        \"type\" : \"static\",\r\n        \"paths\" : \"///projects/indexData\",\r\n        \"inputFormat\" : \"org.apache.druid.data.input.avro.AvroValueInputFormat\"\r\n      },\r\n      \"metadataUpdateSpec\" : null,\r\n      \"segmentOutputPath\" : null\r\n    },\r\n    \"tuningConfig\" : {\r\n      \"type\" : \"hadoop\",\r\n      \"workingPath\" : null,\r\n      \"version\" : \"2019-05-23T08:33:19.990Z\",\r\n      \"partitionsSpec\" : {\r\n        \"type\" : \"hashed\",\r\n        \"targetPartitionSize\" : 1050000,\r\n        \"maxPartitionSize\" : 1575000,\r\n        \"assumeGrouped\" : true,\r\n        \"numShards\" : -1,\r\n        \"partitionDimensions\" : [ ]\r\n      },\r\n      \"shardSpecs\" : { },\r\n      \"indexSpec\" : {\r\n        \"bitmap\" : {\r\n          \"type\" : \"concise\"\r\n        },\r\n        \"dimensionCompression\" : \"lz4\",\r\n        \"metricCompression\" : \"lz4\",\r\n        \"longEncoding\" : \"longs\"\r\n      },\r\n      \"maxRowsInMemory\" : 150000,\r\n      \"maxBytesInMemory\" : -1,\r\n      \"leaveIntermediate\" : false,\r\n      \"cleanupOnFailure\" : true,\r\n      \"overwriteFiles\" : false,\r\n      \"ignoreInvalidRows\" : false,\r\n      \"jobProperties\" : {\r\n        \"fs.permissions.umask-mode\" : \"027\"\r\n      },\r\n      \"combineText\" : false,\r\n      \"useCombiner\" : false,\r\n      \"buildV9Directly\" : true,\r\n      \"numBackgroundPersistThreads\" : 0,\r\n      \"forceExtendableShardSpecs\" : false,\r\n      \"useExplicitVersion\" : false,\r\n      \"allowedHadoopPrefix\" : [ ],\r\n      \"logParseExceptions\" : false,\r\n      \"maxParseExceptions\" : 0\r\n    }\r\n  },\r\n  \"hadoopDependencyCoordinates\" : null,\r\n  \"classpathPrefix\" : null,\r\n  \"context\" : { }\r\n}\r\n```\r\n\r\n**Ingestion spec for slice datasource:**\r\n\r\n```\r\n{\r\n  \"type\" : \"index_hadoop\",\r\n  \"id\" : \"index_slice\",\r\n  \"spec\" : {\r\n    \"dataSchema\" : {\r\n      \"dataSource\" : \"slice\",\r\n      \"parser\" : {\r\n        \"type\" : \"avro_hadoop\",\r\n        \"parseSpec\" : {\r\n          \"format\" : \"avro\",\r\n          \"timestampSpec\" : {\r\n            \"column\" : \"date\",\r\n            \"format\" : \"yyyyMMdd\"\r\n          },\r\n          \"dimensionsSpec\" : {\r\n            \"dimensions\" : [ \"src_pty_id\" ]\r\n          }\r\n        }\r\n      },\r\n      \"metricsSpec\" : [ {\r\n      \r\n        \"type\" : \"thetaSketch\",\r\n        \"name\" : \"test_sketch\",\r\n        \"fieldName\" : \"test_sketch\",\r\n        \"size\" : 131072,\r\n        \"shouldFinalize\" : true,\r\n        \"isInputThetaSketch\" : true,\r\n        \"errorBoundsStdDev\" : null\r\n      } ],\r\n      \"granularitySpec\" : {\r\n        \"type\" : \"uniform\",\r\n        \"segmentGranularity\" : \"DAY\",\r\n        \"queryGranularity\" : {\r\n          \"type\" : \"none\"\r\n        },\r\n        \"rollup\" : true,\r\n        \"intervals\" : [ \"2019-05-22T00:00:00.000Z/2019-05-23T00:00:00.000Z\" ]\r\n      },\r\n      \"transformSpec\" : {\r\n        \"filter\" : null,\r\n        \"transforms\" : [ ]\r\n      }\r\n    },\r\n    \"ioConfig\" : {\r\n      \"type\" : \"hadoop\",\r\n      \"inputSpec\" : {\r\n        \"type\" : \"dataSource\",\r\n        \"ingestionSpec\" : {\r\n          \"dataSource\" : \"basefact\",\r\n          \"intervals\" : [ \"2019-05-22T00:00:00Z/P1D\" ]\r\n        }\r\n      },\r\n      \"metadataUpdateSpec\" : null,\r\n      \"segmentOutputPath\" : null\r\n    },\r\n    \"tuningConfig\" : {\r\n      \"type\" : \"hadoop\",\r\n      \"workingPath\" : null,\r\n      \"version\" : \"2019-05-23T08:54:27.227Z\",\r\n      \"partitionsSpec\" : {\r\n        \"type\" : \"hashed\",\r\n        \"targetPartitionSize\" : 133000,\r\n        \"maxPartitionSize\" : 199500,\r\n        \"assumeGrouped\" : true,\r\n        \"numShards\" : -1,\r\n        \"partitionDimensions\" : [ ]\r\n      },\r\n      \"shardSpecs\" : { },\r\n      \"indexSpec\" : {\r\n        \"bitmap\" : {\r\n          \"type\" : \"concise\"\r\n        },\r\n        \"dimensionCompression\" : \"lz4\",\r\n        \"metricCompression\" : \"lz4\",\r\n        \"longEncoding\" : \"longs\"\r\n      },\r\n      \"maxRowsInMemory\" : 10000,\r\n      \"maxBytesInMemory\" : -1,\r\n      \"leaveIntermediate\" : false,\r\n      \"cleanupOnFailure\" : true,\r\n      \"overwriteFiles\" : false,\r\n      \"ignoreInvalidRows\" : false,\r\n      \"jobProperties\" : {\r\n        \"fs.permissions.umask-mode\" : \"027\"\r\n      },\r\n      \"combineText\" : false,\r\n      \"useCombiner\" : false,\r\n      \"buildV9Directly\" : true,\r\n      \"numBackgroundPersistThreads\" : 0,\r\n      \"forceExtendableShardSpecs\" : false,\r\n      \"useExplicitVersion\" : false,\r\n      \"allowedHadoopPrefix\" : [ ],\r\n      \"logParseExceptions\" : false,\r\n      \"maxParseExceptions\" : 0\r\n    }\r\n  },\r\n  \"hadoopDependencyCoordinates\" : null,\r\n  \"classpathPrefix\" : null,\r\n  \"context\" : { }\r\n}\r\n```\r\n\r\nQuerying the basefact datasource provides the result as: `         \"test_sketch\":43672556.4879819` while the slice datasource results in `\"test_sketch\":43676771.06402646`\r\n\r\nStill investigating the issue, but has anyone observed similar behavior?"}, {"user": "leventov", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug"], "created": "2019-05-23 11:35:11", "title": "Dead code in IndexTask.collectIntervalsAndShardSpecs()", "url": "https://github.com/apache/druid/issues/7737", "closed": "2019-05-25 17:12:15", "ttf": 2.000277777777778, "commitsDetails": [], "body": "`thrownAway` and `unparseable` variables are never updated, the logic that depends on these variables to be positive is dead.\r\n\r\n@jihoonson @jon-wei "}, {"user": "gerbal", "commits": {"05d5276b9ef8f65e41d51d5874d147b3e5f23de9": {"commitGHEventType": "referenced", "commitUser": "fjy"}, "53b6467fc83cd4a78d87b5fd1557c84b2a5b2513": {"commitGHEventType": "referenced", "commitUser": "fjy"}, "7ec7257e1dbc095019e20958a48f3aea89632609": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": ["server/src/main/java/org/apache/druid/query/lookup", "services/src/main/java/org/apache/druid/cli"], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 4, "spoonMethodsChanged": 4, "INS": 4, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 4}, "title": "Lookup transform expression not defined during ingest from kafka", "numCommits": 1, "created": "2019-05-21 21:08:41", "closed": "2019-05-23 04:13:58", "gitStatsSummary": {"deletions": 3, "insertions": 14, "lines": 17, "gitFilesChange": 4}, "statsSkippedReason": "", "changesInPackagesSPOON": ["org.apache.druid.cli.CliCoordinator.getModules()", "org.apache.druid.cli.CliMiddleManager.getModules()", "org.apache.druid.cli.CliOverlord.getModules(boolean)", "org.apache.druid.query.lookup.LookupSerdeModule.configure(com.google.inject.Binder)"], "filteredCommits": ["7ec7257e1dbc095019e20958a48f3aea89632609"], "url": "https://github.com/apache/druid/issues/7724", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 1.0002777777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "server/src/main/java/org/apache/druid/query/lookup/LookupSerdeModule.java", "deletions": 2, "insertions": 5, "lines": 7}, {"filePath": "server/src/test/java/org/apache/druid/query/lookup/LookupSerdeModuleTest.java", "deletions": 0, "insertions": 130, "lines": 130}, {"filePath": "services/src/main/java/org/apache/druid/cli/CliMiddleManager.java", "deletions": 1, "insertions": 4, "lines": 5}, {"filePath": "services/src/main/java/org/apache/druid/cli/CliCoordinator.java", "deletions": 0, "insertions": 3, "lines": 3}, {"filePath": "services/src/main/java/org/apache/druid/cli/CliOverlord.java", "deletions": 0, "insertions": 2, "lines": 2}], "commitSpoonAstDiffStats": [{"spoonFilePath": "CliMiddleManager.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.cli.CliMiddleManager.getModules()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "LookupSerdeModuleTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.query.lookup.LookupSerdeModuleTest", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "LookupSerdeModule.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.query.lookup.LookupSerdeModule.configure(com.google.inject.Binder)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "CliCoordinator.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.cli.CliCoordinator.getModules()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "CliOverlord.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.cli.CliOverlord.getModules(boolean)", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2019-05-26 21:55:06", "commitMessage": "Fix lookup serde on node types that don't load lookups. (#7752) (#7767)\n\nThis includes the router, overlord, middleManager, and coordinator.\r\nDoes the following things:\r\n\r\n- Loads LookupSerdeModule on MM, overlord, and coordinator.\r\n- Adds LookupExprMacro to LookupSerdeModule, which allows these node\r\n  types to understand that the 'lookup' function exists.\r\n- Adds a test to make sure that LookupSerdeModule works for virtual\r\n  columns, filters, transforms, and dimension specs.\r\n\r\nThis is implementing the technique discussed on these two issues:\r\n\r\n- https://github.com/apache/incubator-druid/issues/7724#issuecomment-494723333\r\n- https://github.com/apache/incubator-druid/pull/7082#discussion_r264888771", "commitUser": "fjy", "commitDateTime": "2019-05-26 21:55:06", "commitParents": ["a6c43898694da8f5d2900fd36d129c2d0297698a"], "commitGHEventType": "referenced", "nameRev": "05d5276b9ef8f65e41d51d5874d147b3e5f23de9 tags/druid-0.15.0-incubating-rc1~11", "commitHash": "05d5276b9ef8f65e41d51d5874d147b3e5f23de9"}, {"commitGitStats": [{"filePath": "indexing-service/src/main/java/org/apache/druid/indexing/seekablestream/SeekableStreamIndexTask.java", "deletions": 13, "insertions": 19, "lines": 32}, {"filePath": "extensions-core/kafka-indexing-service/src/main/java/org/apache/druid/indexing/kafka/KafkaIndexTask.java", "deletions": 2, "insertions": 4, "lines": 6}, {"filePath": "extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java", "deletions": 16, "insertions": 51, "lines": 67}, {"filePath": "extensions-core/kinesis-indexing-service/src/main/java/org/apache/druid/indexing/kinesis/KinesisIndexTask.java", "deletions": 1, "insertions": 2, "lines": 3}], "commitSpoonAstDiffStats": [{"spoonFilePath": "KafkaIndexTask.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 2, "spoonMethodName": "org.apache.druid.indexing.kafka.KafkaIndexTask.createTaskRunner()", "MOV": 0, "TOT": 4}]}, {"spoonFilePath": "SeekableStreamIndexTask.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.indexing.seekablestream.SeekableStreamIndexTask.stopGracefully(org.apache.druid.indexing.common.config.TaskConfig)", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 6, "DEL": 2, "spoonMethodName": "org.apache.druid.indexing.seekablestream.SeekableStreamIndexTask", "MOV": 3, "TOT": 11}, {"INS": 2, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.indexing.seekablestream.SeekableStreamIndexTask.getQueryRunner(org.apache.druid.query.Query)", "MOV": 1, "TOT": 4}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.indexing.seekablestream.SeekableStreamIndexTask.run(org.apache.druid.indexing.common.TaskToolbox)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.seekablestream.SeekableStreamIndexTask.getAppenderator()", "MOV": 1, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.seekablestream.SeekableStreamIndexTask.getRunner()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "KinesisIndexTask.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.indexing.kinesis.KinesisIndexTask.createTaskRunner()", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "KafkaIndexTaskTest.java", "spoonMethods": [{"INS": 2, "UPD": 3, "DEL": 1, "spoonMethodName": "org.apache.druid.indexing.kafka.KafkaIndexTaskTest", "MOV": 3, "TOT": 9}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.kafka.KafkaIndexTaskTest.testIncrementalHandOffReadsThroughEndOffsets()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.kafka.KafkaIndexTaskTest.cloneDataSchema(org.apache.druid.segment.indexing.DataSchema)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.kafka.KafkaIndexTaskTest.getTaskReportData()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.kafka.KafkaIndexTaskTest.testRunWithPauseAndResume()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 3, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.kafka.KafkaIndexTaskTest.testRunTransactionModeRollback()", "MOV": 0, "TOT": 3}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.kafka.KafkaIndexTaskTest.runTask(org.apache.druid.indexing.common.task.Task)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.kafka.KafkaIndexTaskTest.testSerde()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.kafka.KafkaIndexTaskTest.testRunContextSequenceAheadOfStartingOffsets()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.kafka.KafkaIndexTaskTest.createTask(java.lang.String,org.apache.druid.segment.indexing.DataSchema,org.apache.druid.indexing.kafka.KafkaIndexTaskIOConfig,java.util.Map)", "MOV": 0, "TOT": 2}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2019-05-22 21:13:57", "commitMessage": "SeekableStreamIndexTaskRunner: Lazy init of runner. (#7729)\n\nThe main motivation is that this fixes #7724, by making it so the overlord\r\ndoesn't try to create a task runner and parser when all it really wants to\r\ndo is create a task object and serialize it.", "commitUser": "fjy", "commitDateTime": "2019-05-22 21:13:57", "commitParents": ["ffc2397bcda67c6ba153ef072694d5bd740696f9"], "commitGHEventType": "referenced", "nameRev": "53b6467fc83cd4a78d87b5fd1557c84b2a5b2513 tags/druid-0.16.0-incubating-rc1~382", "commitHash": "53b6467fc83cd4a78d87b5fd1557c84b2a5b2513"}, {"commitGitStats": [{"filePath": "server/src/main/java/org/apache/druid/query/lookup/LookupSerdeModule.java", "deletions": 2, "insertions": 5, "lines": 7}, {"filePath": "server/src/test/java/org/apache/druid/query/lookup/LookupSerdeModuleTest.java", "deletions": 0, "insertions": 130, "lines": 130}, {"filePath": "services/src/main/java/org/apache/druid/cli/CliMiddleManager.java", "deletions": 1, "insertions": 4, "lines": 5}, {"filePath": "services/src/main/java/org/apache/druid/cli/CliCoordinator.java", "deletions": 0, "insertions": 3, "lines": 3}, {"filePath": "services/src/main/java/org/apache/druid/cli/CliOverlord.java", "deletions": 0, "insertions": 2, "lines": 2}], "commitSpoonAstDiffStats": [{"spoonFilePath": "CliMiddleManager.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.cli.CliMiddleManager.getModules()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "LookupSerdeModuleTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.query.lookup.LookupSerdeModuleTest", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "LookupSerdeModule.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.query.lookup.LookupSerdeModule.configure(com.google.inject.Binder)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "CliCoordinator.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.cli.CliCoordinator.getModules()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "CliOverlord.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.cli.CliOverlord.getModules(boolean)", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2019-05-24 12:30:49", "commitMessage": "Fix lookup serde on node types that don't load lookups. (#7752)\n\nThis includes the router, overlord, middleManager, and coordinator.\r\nDoes the following things:\r\n\r\n- Loads LookupSerdeModule on MM, overlord, and coordinator.\r\n- Adds LookupExprMacro to LookupSerdeModule, which allows these node\r\n  types to understand that the 'lookup' function exists.\r\n- Adds a test to make sure that LookupSerdeModule works for virtual\r\n  columns, filters, transforms, and dimension specs.\r\n\r\nThis is implementing the technique discussed on these two issues:\r\n\r\n- https://github.com/apache/incubator-druid/issues/7724#issuecomment-494723333\r\n- https://github.com/apache/incubator-druid/pull/7082#discussion_r264888771", "commitUser": "fjy", "commitDateTime": "2019-05-24 12:30:49", "commitParents": ["db3792727e6cba33a48c9c305753629f96d670f8"], "commitGHEventType": "referenced", "nameRev": "7ec7257e1dbc095019e20958a48f3aea89632609 tags/druid-0.16.0-incubating-rc1~367", "commitHash": "7ec7257e1dbc095019e20958a48f3aea89632609"}], "body": "### Affected Version\r\n\r\nmaster\r\n\r\n### Description\r\n\r\nWhen trying to resolve an attribute via a lookup at ingestion time I get the following error: \r\n\r\n\r\n```\r\n2019-05-21T21:06:54,247 ERROR [KafkaSupervisor-example] org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor - SeekableStreamSupervisor[example] failed to handle notice: {class=org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor, exceptionType=class org.apache.druid.java.util.common.RE, exceptionMessage=function 'lookup' is not defined., noticeClass=RunNotice}\r\norg.apache.druid.java.util.common.RE: function 'lookup' is not defined.\r\n        at org.apache.druid.math.expr.ExprListenerImpl.exitFunctionExpr(ExprListenerImpl.java:303) ~[druid-core-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n        at org.apache.druid.math.expr.antlr.ExprParser$FunctionExprContext.exitRule(ExprParser.java:212) ~[druid-core-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n        at org.antlr.v4.runtime.tree.ParseTreeWalker.exitRule(ParseTreeWalker.java:71) ~[antlr4-runtime-4.5.1.jar:4.5.1]\r\n        at org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:54) ~[antlr4-runtime-4.5.1.jar:4.5.1]\r\n        at org.apache.druid.math.expr.Parser.parse(Parser.java:85) ~[druid-core-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n        at org.apache.druid.math.expr.Parser.parse(Parser.java:72) ~[druid-core-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n        at org.apache.druid.segment.transform.ExpressionTransform.getRowFunction(ExpressionTransform.java:68) ~[druid-processing-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n        at org.apache.druid.segment.transform.Transformer.<init>(Transformer.java:50) ~[druid-processing-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n        at org.apache.druid.segment.transform.TransformSpec.toTransformer(TransformSpec.java:122) ~[druid-processing-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n        at org.apache.druid.segment.transform.TransformingStringInputRowParser.<init>(TransformingStringInputRowParser.java:44) ~[druid-processing-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n        at org.apache.druid.segment.transform.TransformSpec.decorate(TransformSpec.java:108) ~[druid-processing-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n        at org.apache.druid.segment.indexing.DataSchema.getParser(DataSchema.java:125) ~[druid-server-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n        at org.apache.druid.indexing.seekablestream.SeekableStreamIndexTask.<init>(SeekableStreamIndexTask.java:102) ~[druid-indexing-service-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n        at org.apache.druid.indexing.kafka.KafkaIndexTask.<init>(KafkaIndexTask.java:70) ~[?:?]\r\n        at org.apache.druid.indexing.kafka.supervisor.KafkaSupervisor.createIndexTasks(KafkaSupervisor.java:246) ~[?:?]\r\n        at org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.createTasksForGroup(SeekableStreamSupervisor.java:2492) ~[druid-indexing-service-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n        at org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.createNewTasks(SeekableStreamSupervisor.java:2306) ~[druid-indexing-service-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n        at org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.runInternal(SeekableStreamSupervisor.java:1012) ~[druid-indexing-service-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n        at org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor$RunNotice.handle(SeekableStreamSupervisor.java:264) ~[druid-indexing-service-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n        at org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.lambda$tryInit$3(SeekableStreamSupervisor.java:723) ~[druid-indexing-service-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_181]\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_181]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_181]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_181]\r\n        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]\r\n```\r\n\r\n\r\nHere is the ingestion spec:\r\n\r\n```json\r\n{\r\n  \"type\": \"kafka\",\r\n  \"dataSchema\": {\r\n    \"dataSource\": \"example\",\r\n    \"parser\": {\r\n      \"type\": \"string\",\r\n      \"parseSpec\": {\r\n        \"format\": \"json\",\r\n        \"timestampSpec\": {\r\n          \"column\": \"timestamp\",\r\n          \"format\": \"posix\"\r\n        },\r\n        \"flattenSpec\": {\r\n          \"fields\": [\r\n            {\r\n              \"type\": \"path\",\r\n              \"name\": \"item_id\",\r\n              \"expr\": \"$.parameters.item_id\"\r\n            }\r\n          ]\r\n        },\r\n        \"dimensionsSpec\": {\r\n          \"dimensions\": [\r\n            \"item_id\",\r\n            \"item_lang\"\r\n          ],\r\n          \"dimensionExclusions\": []\r\n        }\r\n      }\r\n    },\r\n    \"metricsSpec\": [\r\n      {\r\n        \"type\": \"count\",\r\n        \"name\": \"count\"\r\n      }\r\n    ],\r\n    \"transformSpec\": {\r\n      \"transforms\": [ {\r\n        \"type\": \"expression\",\r\n        \"name\": \"item_lang\",\r\n        \"expression\": \"lookup(item_id, 'item-lang-lookup')\"\r\n      }]\r\n    },\r\n    \"granularitySpec\": {\r\n      \"type\": \"uniform\",\r\n      \"segmentGranularity\": \"DAY\",\r\n      \"rollup\": true\r\n    }\r\n  },\r\n  \"tuningConfig\": {\r\n    \"type\": \"kafka\",\r\n    \"maxRowsPerSegment\": 2000000\r\n  },\r\n  \"ioConfig\": {\r\n    \"topic\": \"topic\",\r\n    \"consumerProperties\": {\r\n      \"bootstrap.servers\": \"kafka:9092\"\r\n    },\r\n    \"taskCount\": 1,\r\n    \"replicas\": 1,\r\n    \"taskDuration\": \"PT1H\",\r\n    \"useEarliestOffset\": true\r\n  }\r\n}\r\n```"}, {"user": "gianm", "commits": {"bcea05e4e8d4cabd698aedbc8f60fdfa8666e2ca": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Area - SQL", "Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "SQL: Exception with OR of impossible filters", "numCommits": 0, "created": "2019-05-16 02:56:23", "closed": "2019-05-21 18:32:10", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/7671", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 5.000277777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "sql/src/test/java/org/apache/druid/sql/calcite/CalciteQueryTest.java", "deletions": 0, "insertions": 22, "lines": 22}, {"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/filtration/CombineAndSimplifyBounds.java", "deletions": 45, "insertions": 44, "lines": 89}], "commitSpoonAstDiffStats": [{"spoonFilePath": "CombineAndSimplifyBounds.java", "spoonMethods": [{"INS": 5, "UPD": 2, "DEL": 7, "spoonMethodName": "org.apache.druid.sql.calcite.filtration.CombineAndSimplifyBounds.doSimplify(java.util.List,boolean)", "MOV": 18, "TOT": 32}]}, {"spoonFilePath": "CalciteQueryTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.CalciteQueryTest.testGroupByNothingWithImpossibleTimeFilter()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2019-05-21 11:32:09", "commitMessage": "SQL: Fix exception with OR of impossible filters. (#7707)\n\nFixes #7671.", "commitUser": "fjy", "commitDateTime": "2019-05-21 11:32:09", "commitParents": ["b6941551aee143074049c8187c84b84aa1014a49"], "commitGHEventType": "referenced", "nameRev": "bcea05e4e8d4cabd698aedbc8f60fdfa8666e2ca tags/druid-0.16.0-incubating-rc1~390", "commitHash": "bcea05e4e8d4cabd698aedbc8f60fdfa8666e2ca"}], "body": "### Description\r\n\r\nA query like this, which has and OR of two impossible time filters (a floor to day can never be any hour other than \"00:00:00\"):\r\n\r\n```sql\r\nSELECT COUNT(*) FROM wikiticker WHERE FLOOR(__time TO DAY) = TIMESTAMP '2000-01-02 01:00:00' OR FLOOR(__time TO DAY) = TIMESTAMP '2000-01-02 02:00:00'\r\n```\r\n\r\nGives an error like the following:\r\n\r\n```\r\njava.lang.RuntimeException: Error while applying rule DruidQueryRule(WHERE_FILTER), args [rel#30:LogicalFilter.NONE.[](input=rel#19:Subset#0.NONE.[],condition=OR(=(TIME_FLOOR($0, 'P1M', null, 'America/Los_Angeles'), 2000-01-01 00:00:00), =(TIME_FLOOR($0, 'P1M', null, 'America/Los_Angeles'), 2000-02-01 00:00:00))), rel#43:DruidQueryRel.NONE.[](query={\"queryType\":\"scan\",\"dataSource\":{\"type\":\"table\",\"name\":\"foo\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"virtualColumns\":[],\"resultFormat\":\"compactedList\",\"batchSize\":20480,\"limit\":9223372036854775807,\"order\":\"none\",\"filter\":null,\"columns\":[\"__time\",\"cnt\",\"dim1\",\"dim2\",\"dim3\",\"m1\",\"m2\",\"unique_dim1\"],\"legacy\":false,\"context\":{\"defaultTimeout\":300000,\"maxScatterGatherBytes\":9223372036854775807,\"sqlCurrentTimestamp\":\"2000-01-01T00:00:00Z\",\"sqlQueryId\":\"dummy\"},\"descending\":false,\"granularity\":{\"type\":\"all\"}},signature={__time:LONG, cnt:LONG, dim1:STRING, dim2:STRING, dim3:STRING, m1:FLOAT, m2:DOUBLE, unique_dim1:COMPLEX})]\r\n\r\n\tat org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:236)\r\n\tat org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:646)\r\n\tat org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:339)\r\n\tat org.apache.calcite.tools.Programs$SequenceProgram.run(Programs.java:358)\r\n\tat org.apache.calcite.prepare.PlannerImpl.transform(PlannerImpl.java:337)\r\n\tat org.apache.druid.sql.calcite.planner.DruidPlanner.planWithDruidConvention(DruidPlanner.java:119)\r\n\tat org.apache.druid.sql.calcite.planner.DruidPlanner.plan(DruidPlanner.java:89)\r\n\tat org.apache.druid.sql.SqlLifecycle.plan(SqlLifecycle.java:143)\r\n\tat org.apache.druid.sql.SqlLifecycle.planAndAuthorize(SqlLifecycle.java:209)\r\n\tat org.apache.druid.sql.SqlLifecycle.runSimple(SqlLifecycle.java:246)\r\n\tat org.apache.druid.sql.calcite.BaseCalciteQueryTest.getResults(BaseCalciteQueryTest.java:571)\r\n\tat org.apache.druid.sql.calcite.BaseCalciteQueryTest.getResults(BaseCalciteQueryTest.java:518)\r\n\tat org.apache.druid.sql.calcite.BaseCalciteQueryTest.testQuery(BaseCalciteQueryTest.java:507)\r\n\tat org.apache.druid.sql.calcite.BaseCalciteQueryTest.testQuery(BaseCalciteQueryTest.java:458)\r\n\tat org.apache.druid.sql.calcite.CalciteQueryTest.testGroupAndFilterOnTimeFloorWithTimeZone(CalciteQueryTest.java:5289)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n\tat org.apache.druid.sql.calcite.util.QueryLogHook$1.evaluate(QueryLogHook.java:95)\r\n\tat org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)\r\n\tat org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)\r\n\tat org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:363)\r\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:137)\r\n\tat com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)\r\n\tat com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)\r\n\tat com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)\r\n\tat com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)\r\nCaused by: java.lang.IllegalStateException: newChildren.size > 0\r\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:176)\r\n\tat org.apache.druid.sql.calcite.filtration.CombineAndSimplifyBounds.doSimplify(CombineAndSimplifyBounds.java:203)\r\n\tat org.apache.druid.sql.calcite.filtration.CombineAndSimplifyBounds.doSimplifyOr(CombineAndSimplifyBounds.java:114)\r\n\tat org.apache.druid.sql.calcite.filtration.CombineAndSimplifyBounds.process(CombineAndSimplifyBounds.java:62)\r\n\tat org.apache.druid.sql.calcite.filtration.BottomUpTransform.checkedProcess(BottomUpTransform.java:38)\r\n\tat org.apache.druid.sql.calcite.filtration.BottomUpTransform.apply0(BottomUpTransform.java:81)\r\n\tat org.apache.druid.sql.calcite.filtration.BottomUpTransform.apply(BottomUpTransform.java:46)\r\n\tat org.apache.druid.sql.calcite.filtration.BottomUpTransform.apply(BottomUpTransform.java:32)\r\n\tat org.apache.druid.sql.calcite.filtration.Filtration.transform(Filtration.java:86)\r\n\tat org.apache.druid.sql.calcite.filtration.Filtration.optimize(Filtration.java:113)\r\n\tat org.apache.druid.sql.calcite.rel.DruidQuery.toScanQuery(DruidQuery.java:995)\r\n\tat org.apache.druid.sql.calcite.rel.DruidQuery.computeQuery(DruidQuery.java:760)\r\n\tat org.apache.druid.sql.calcite.rel.DruidQuery.<init>(DruidQuery.java:167)\r\n\tat org.apache.druid.sql.calcite.rel.PartialDruidQuery.build(PartialDruidQuery.java:332)\r\n\tat org.apache.druid.sql.calcite.rel.DruidQueryRel.toDruidQuery(DruidQueryRel.java:97)\r\n\tat org.apache.druid.sql.calcite.rel.DruidQueryRel.toDruidQueryForExplaining(DruidQueryRel.java:109)\r\n\tat org.apache.druid.sql.calcite.rel.DruidRel.isValidDruidQuery(DruidRel.java:62)\r\n\tat org.apache.druid.sql.calcite.rule.DruidRules$DruidQueryRule.onMatch(DruidRules.java:137)\r\n\tat org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:212)\r\n\t... 44 more\r\n```\r\n\r\nI am working on a fix for it. It should return as if it was running on an empty dataset (since the filter matches nothing)."}, {"user": "jon-wei", "commits": {}, "labels": ["Area - Querying", "Bug"], "created": "2019-05-14 22:40:38", "title": "Quantiles sketch agg fails on inner query numeric post-agg columns", "url": "https://github.com/apache/druid/issues/7660", "closed": "2019-05-23 18:13:42", "ttf": 8.000277777777777, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.12.0+\r\n\r\n### Description\r\n\r\nWhen using a quantiles sketch agg (http://druid.io/docs/latest/development/extensions-core/datasketches-quantiles.html) in the outer query of a nested GroupBy that references a numeric column generated by a post-agg in the inner query, the following exception occurs:\r\n\r\n```\r\njava.lang.ClassCastException: java.lang.Double cannot be cast to com.yahoo.sketches.quantiles.DoublesSketch\r\n\tat org.apache.druid.query.aggregation.datasketches.quantiles.DoublesSketchMergeBufferAggregator.aggregate(DoublesSketchMergeBufferAggregator.java:65) ~[?:?]\r\n\tat org.apache.druid.query.groupby.epinephelinae.AbstractBufferHashGrouper.aggregate(AbstractBufferHashGrouper.java:165) ~[druid-processing-0.14.2-incubating.jar:0.14.2-incubating]\r\n\tat org.apache.druid.query.groupby.epinephelinae.SpillingGrouper.aggregate(SpillingGrouper.java:167) ~[druid-processing-0.14.2-incubating.jar:0.14.2-incubating]\r\n\tat org.apache.druid.query.groupby.epinephelinae.Grouper.aggregate(Grouper.java:82) ~[druid-processing-0.14.2-incubating.jar:0.14.2-incubating]\r\n\tat org.apache.druid.query.groupby.epinephelinae.RowBasedGrouperHelper$1.accumulate(RowBasedGrouperHelper.java:270) ~[druid-processing-0.14.2-incubating.jar:0.14.2-incubating]\r\n\tat org.apache.druid.query.groupby.epinephelinae.RowBasedGrouperHelper$1.accumulate(RowBasedGrouperHelper.java:247) ~[druid-processing-0.14.2-incubating.jar:0.14.2-incubating]\r\n\tat org.apache.druid.java.util.common.guava.FilteringAccumulator.accumulate(FilteringAccumulator.java:41) ~[druid-core-0.14.2-incubating.jar:0.14.2-incubating]\r\n\tat org.apache.druid.java.util.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:40) ~[druid-core-0.14.2-incubating.jar:0.14.2-incubating]\r\n```\r\n\r\nThis occurs because the `factorizeBuffered` method in `DoublesSketchAggregatorFactory` relies on `metricFactory.getColumnCapabilities(fieldName)` to determine if an input column is numeric. If the column is not numeric, the aggregator assumes the input is a complex DoublesSketch object. For postaggs, the type information is not available, so the type mismatch occurs. \r\n\r\nThis issue may also be present in other aggregator types, I have not searched through the other implementations.\r\n\r\nThe following query structure will reproduce the issue:\r\n\r\n```\r\n{\r\n  \"queryType\": \"groupBy\",\r\n  \"intervals\": [\r\n    \"2015-09-12/2015-09-13\"\r\n  ],\r\n  \"dataSource\": {\r\n    \"type\": \"query\",\r\n    \"query\": {\r\n      \"queryType\": \"groupBy\",\r\n      \"dataSource\": \"wikipedia\",\r\n      \"intervals\": [\r\n        \"2015-09-12/2015-09-13\"\r\n      ],\r\n      \"dimensions\": [\r\n        \"page\"\r\n      ],\r\n      \"aggregations\": [\r\n        {\r\n          \"type\": \"quantilesDoublesSketch\",\r\n          \"name\": \"innerSketch\",\r\n          \"fieldName\": \"added\"\r\n        },\r\n        {\r\n          \"type\": \"count\",\r\n          \"name\": \"sampleCount\"\r\n        }\r\n      ],\r\n      \"postAggregations\": [\r\n        {\r\n          \"type\": \"quantilesDoublesSketchToQuantile\",\r\n          \"name\": \"innerMedian\",\r\n          \"field\": {\r\n            \"type\": \"fieldAccess\",\r\n            \"fieldName\": \"innerSketch\"\r\n          },\r\n          \"fraction\": 0.5\r\n        }\r\n      ],\r\n      \"granularity\": \"all\"\r\n    }\r\n  },\r\n  \"dimensions\": [\r\n    \"page\"\r\n  ],\r\n  \"aggregations\": [\r\n    {\r\n      \"type\": \"quantilesDoublesSketch\",\r\n      \"name\": \"outerSketch\",\r\n      \"fieldName\": \"innerMedian\"\r\n    },\r\n    {\r\n      \"type\": \"count\",\r\n      \"name\": \"clientCount\"\r\n    }\r\n  ],\r\n  \"postAggregations\": [\r\n    {\r\n      \"type\": \"quantilesDoublesSketchToQuantile\",\r\n      \"name\": \"outerMedian\",\r\n      \"field\": {\r\n        \"type\": \"fieldAccess\",\r\n        \"fieldName\": \"outerSketch\"\r\n      },\r\n      \"fraction\": 0.5\r\n    }\r\n  ],\r\n  \"granularity\": \"all\",\r\n  \"context\": {\r\n    \"skipEmptyBuckets\": \"true\"\r\n  }\r\n}\r\n```"}, {"user": "clintropolis", "commits": {}, "labels": ["Area - Lookups", "Area - Web Console", "Bug"], "created": "2019-05-13 05:10:17", "title": "web-console lookup editor doesn't correctly allow creating new lookups", "url": "https://github.com/apache/druid/issues/7646", "closed": "2019-05-14 20:30:52", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "The web-console lookup editor view uses a dropdown that is populated with [historical tiers from `/druid/coordinator/v1/tiers`](https://github.com/apache/incubator-druid/blob/master/web-console/src/views/lookups-view.tsx#L92) instead of lookup tiers which are unfortunately for the editor a totally different thing. It would be less painful if the [default lookup tier](https://github.com/apache/incubator-druid/blob/master/server/src/main/java/org/apache/druid/query/lookup/LookupListeningAnnouncerConfig.java#L33) config was the same value as the [default historical tier](https://github.com/apache/incubator-druid/blob/master/server/src/main/java/org/apache/druid/client/DruidServer.java#L52) but it is not unfortunately. \r\n\r\nThis information can be fetched from a combination of the API calls `/druid/coordinator/v1/lookups/config` and `/druid/coordinator/v1/lookups/config?discover=true` which either returns a list of configured tiers _or_ discovered tiers, the latter of which is [incorrectly listed in the documentation as `/druid/coordinator/v1/lookups?discover=true`](http://druid.io/docs/latest/querying/lookups.html#list-tier-names). \r\n\r\nThis dropdowns value should be sourced from the combination of these 2 calls to both have the tiers that have configs, and also tiers which have historical servers associated with them but no defined lookups (maybe we add an `all` query parameter that combines the output of the 2?). \r\n\r\nThe input on the editor should also maybe be free-form input to allow new tiers be defined before any historicals are provisioned in that tier, but that seems less important than using the correct set of tiers for the dropdown."}, {"user": "jihoonson", "commits": {}, "labels": ["Bug"], "created": "2019-05-09 06:34:45", "title": "NPE when both populateResultLevelCache and grandTotal are set", "url": "https://github.com/apache/druid/issues/7621", "closed": "2019-05-10 01:11:05", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\nAll versions since 0.13.0\r\n\r\n### Description\r\n\r\nThe grandTotal doesn't have timestamp and it is filled with null in the result set. If `populateResultLevelCache` is set, the below code is executed and `input.getTimestamp().getMillis()` throws NPE.\r\n\r\n```java\r\n      @Override\r\n      public Function<Result<TimeseriesResultValue>, Object> prepareForCache(boolean isResultLevelCache)\r\n      {\r\n        return input -> {\r\n          TimeseriesResultValue results = input.getValue();\r\n          final List<Object> retVal = Lists.newArrayListWithCapacity(1 + aggs.size());\r\n\r\n          retVal.add(input.getTimestamp().getMillis());\r\n          for (AggregatorFactory agg : aggs) {\r\n            retVal.add(results.getMetric(agg.getName()));\r\n          }\r\n          if (isResultLevelCache) {\r\n            for (PostAggregator postAgg : query.getPostAggregatorSpecs()) {\r\n              retVal.add(results.getMetric(postAgg.getName()));\r\n            }\r\n          }\r\n          return retVal;\r\n        };\r\n      }\r\n```"}, {"user": "pzhdfy", "commits": {"4116917573c6d1e5505c16ffcd88c9eb54333fd8": {"commitGHEventType": "referenced", "commitUser": "clintropolis"}, "cf9db11bd9b68d75fc5b52508f6bbd9d03d2eb3c": {"commitGHEventType": "referenced", "commitUser": "clintropolis"}, "59f9ff38c7a15a5fa92b264863cb0a34a4786cd9": {"commitGHEventType": "referenced", "commitUser": "clintropolis"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {}, "title": "thetaSketch(with sketches-core-0.13.1) in groupBy always return value no more than 16384", "numCommits": 0, "created": "2019-05-07 06:50:33", "closed": "2019-05-10 00:48:39", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "filteredCommits": [], "url": "https://github.com/apache/druid/issues/7607", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 1, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 2.000277777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "extensions-core/datasketches/pom.xml", "deletions": 1, "insertions": 7, "lines": 8}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "", "authoredDateTime": "2019-05-10 19:16:29", "commitMessage": "fix issue #7607 (#7619) (#7636)\n\n* fix issue #7607\r\n\r\n* exclude com.google.code.findbugs:annotations", "commitUser": "clintropolis", "commitDateTime": "2019-05-10 19:16:29", "commitParents": ["10725ca96d2d3d872f9e0f0a060f036f3b8c786c"], "commitGHEventType": "referenced", "nameRev": "4116917573c6d1e5505c16ffcd88c9eb54333fd8 tags/druid-0.15.0-incubating-rc1~38", "commitHash": "4116917573c6d1e5505c16ffcd88c9eb54333fd8"}, {"commitGitStats": [{"filePath": "extensions-core/datasketches/pom.xml", "deletions": 1, "insertions": 7, "lines": 8}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "", "authoredDateTime": "2019-05-09 17:33:29", "commitMessage": "fix issue #7607 (#7619)\n\n* fix issue #7607\r\n\r\n* exclude com.google.code.findbugs:annotations\r\n", "commitUser": "clintropolis", "commitDateTime": "2019-05-09 17:33:29", "commitParents": ["b542bb9f341182c74733172b7d55d0c4e2a5a092"], "commitGHEventType": "referenced", "nameRev": "59f9ff38c7a15a5fa92b264863cb0a34a4786cd9 tags/druid-0.16.0-incubating-rc1~433", "commitHash": "59f9ff38c7a15a5fa92b264863cb0a34a4786cd9"}, {"commitGitStats": [{"filePath": "extensions-core/datasketches/pom.xml", "deletions": 1, "insertions": 7, "lines": 8}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "", "authoredDateTime": "2019-05-09 17:33:29", "commitMessage": "fix issue #7607 (#7619)\n\n* fix issue #7607\r\n\r\n* exclude com.google.code.findbugs:annotations\r\n", "commitUser": "clintropolis", "commitDateTime": "2019-05-09 18:50:32", "commitParents": ["7e825b7c2e47783e208392b7922f8ec20c05cbe2"], "commitGHEventType": "referenced", "nameRev": "cf9db11bd9b68d75fc5b52508f6bbd9d03d2eb3c tags/druid-0.14.2-incubating-rc1~6", "commitHash": "cf9db11bd9b68d75fc5b52508f6bbd9d03d2eb3c"}], "body": "### Affected Version\r\n\r\nThe Druid version with sketches-core-0.13.1\r\n\r\n### Description\r\nwe updated to sketches-core-0.13.1 , because it Bug fix for Quantiles Sketches in direct mode.\r\nthen we found   using thetaSketch in groupBy always return value no more than 16384(the size).\r\nif set size to another value, such as 32768,  the thetaSketch return <= 32768.\r\nBut thetaSketch in topN and timeseries return expected data\r\n\r\nThen we roll back to sketches-core-0.12.0, thetaSketch works well.\r\nBut the Quantiles Sketches will has bug\r\n\r\n@AlexanderSaydakov "}, {"user": "jon-wei", "commits": {}, "labels": ["Area - SQL", "Bug", "Security"], "created": "2019-04-26 23:25:43", "title": "System tables access requires extraneous permissions", "url": "https://github.com/apache/druid/issues/7563", "closed": "2019-05-02 21:38:42", "ttf": 5.000277777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.13.0, 0.14.0\r\n\r\n### Description\r\n\r\nSQL queries on `sys.*` tables currently require the following permission:\r\n\r\n```\r\n  {\r\n    \"resource\": {\r\n      \"name\": \"sys\",\r\n      \"type\": \"DATASOURCE\"\r\n    },\r\n    \"action\": \"READ\"\r\n  }\r\n```\r\n\r\nIn one sense, this is incorrect since \"sys\" is a schema and not a datasource-like table, so the permission needed should really be on something like \"sys.segments\". This occurs because the following block in DruidPlanner only checks the first part of the table's qualified name:\r\n\r\n```\r\n            if (node instanceof Bindables.BindableTableScan) {\r\n              Bindables.BindableTableScan bts = (Bindables.BindableTableScan) node;\r\n              RelOptTable table = bts.getTable();\r\n              String tableName = table.getQualifiedName().get(0);\r\n              datasourceNames.add(tableName);\r\n            }\r\n```\r\n\r\nIn another sense, treating these system tables as a \"datasource\" is not consistent with the equivalent non-SQL APIs. \r\n\r\nRetrieving information about a datasource's segments via MetadataResource APIs only requires datasource-read permissions, and not a higher level \"view segment information\" permission, which is what a permission on \"sys.segments' would represent. \r\n\r\nThe non-SQL APIs only require additional STATE-read permissions for APIs involving server information (http://druid.io/docs/latest/development/extensions-core/druid-basic-security.html#defining-permissions has more info).\r\n\r\nThe permissions needed for system table access could be adjusted to not require additional permissions beyond datasource-specific permissions for non-server related info (segments, tasks) and to require STATE-read permissions for server-related tables (servers, server_segments) to be consistent with the non-SQL APIs.\r\n\r\nIt's worth mentioning here that there is some inconsistency in the non-SQL task APIs themselves. The task APIs in OverlordResource only require datasource permissions, but the running tasks do contain information about where they exist (server-related info). On the other hand, retrieving task info from the middle managers via WorkerResource requires STATE but not datasource permissions."}, {"user": "DanoOM", "commits": {}, "labels": ["Area - Web Console", "Bug"], "created": "2019-04-26 14:42:18", "title": "Druid Console 0.14.0 scroll bar not shown in sql pane", "url": "https://github.com/apache/druid/issues/7559", "closed": "2019-05-29 00:12:51", "ttf": 32.000277777777775, "commitsDetails": [], "body": "Please provide a detailed title (e.g. \"Broker crashes when using TopN query with Bound filter\" instead of just \"Broker crashes\").\r\n\r\n### Affected Version\r\n\r\n0.0.14\r\n\r\n### Description\r\nThe SQL editor/pane allows you to select number of rows to show, however, when incresing the rows count, ..it appears to 'show' them, but they are not visible, as you cannot scroll down to the rows."}, {"user": "hueiyuan", "commits": {}, "labels": ["Area - Web Console", "Bug"], "created": "2019-04-25 04:14:10", "title": "Druid Web console button sometime not available", "url": "https://github.com/apache/druid/issues/7548", "closed": "2019-06-17 19:52:35", "ttf": 53.000277777777775, "commitsDetails": [], "body": "Hello everyone.\r\n\r\nI have built druid 0.14.\r\nThen, When I use druid console(port 8888), some button can not be use.\r\nFor example, I want to disable datasource. I should  click \"drop data\" in datasource page, right?\r\nBut.. it always not be disable and running there.\r\n\r\nHave someone can help or tell me solution.\r\n\r\nThanks,\r\nMax"}, {"user": "kaka11chen", "commits": {}, "labels": ["Area - Operations", "Bug"], "created": "2019-04-19 02:45:00", "title": "Druid historical node running caused os kernel creating too many kernel's dentry slab objects.", "url": "https://github.com/apache/druid/issues/7508", "closed": "2019-04-20 03:39:50", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "Druid historical node running caused os kernel creating too many kernel's dentry slab objects.\r\n\r\n### Affected Version\r\nall versions\r\n\r\n### Description\r\nDruid historical node running caused os kernel created too many kernel's dentry slab objects, run out of memory, and caused performance issue(Use 'sar -B' and found majfit/s high).\r\n\r\n#### OS Kernel version: \r\n2.6.32-754.9.1.el6.x86_64\r\n\r\n#### meminfo: Slab objects occupy 65G, most is dentry.\r\n\r\n```\r\n$ cat /proc/meminfo \r\nMemTotal:       198340116 kB\r\nMemFree:         3411764 kB\r\nBuffers:          329324 kB\r\nCached:         50916944 kB\r\nSwapCached:            0 kB\r\nActive:         81630316 kB\r\nInactive:       44296244 kB\r\nActive(anon):   74680324 kB\r\nInactive(anon):      148 kB\r\nActive(file):    6949992 kB\r\nInactive(file): 44296096 kB\r\nUnevictable:           0 kB\r\nMlocked:               0 kB\r\nSwapTotal:             0 kB\r\nSwapFree:              0 kB\r\nDirty:               136 kB\r\nWriteback:             0 kB\r\nAnonPages:      74679564 kB\r\nMapped:          4465992 kB\r\nShmem:               176 kB\r\nSlab:           65894636 kB\r\nSReclaimable:   40123808 kB\r\nSUnreclaim:     25770828 kB\r\nKernelStack:       23616 kB\r\nPageTables:       216640 kB\r\nNFS_Unstable:          0 kB\r\nBounce:                0 kB\r\nWritebackTmp:          0 kB\r\nCommitLimit:    99170056 kB\r\nCommitted_AS:   75530612 kB\r\nVmallocTotal:   34359738367 kB\r\nVmallocUsed:      715408 kB\r\nVmallocChunk:   34359015920 kB\r\nHardwareCorrupted:     0 kB\r\nAnonHugePages:         0 kB\r\nHugePages_Total:       0\r\nHugePages_Free:        0\r\nHugePages_Rsvd:        0\r\nHugePages_Surp:        0\r\nHugepagesize:       2048 kB\r\nDirectMap4k:       10508 kB\r\nDirectMap2M:     1939456 kB\r\nDirectMap1G:    199229440 kB\r\n```\r\n\r\n#### slabtop\r\n\r\n```\r\n OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME                   \r\n193075920 193075920 100%    0.19K 9653796\t20  38615184K dentry\r\n193031130 193031130  99%    0.12K 6434371\t30  25737484K size-128\r\n12781095 12779631  99%    0.10K 345435       37   1381740K buffer_head\r\n214746 214542  99%    0.55K  30678        7    122712K radix_tree_node\r\n 46860  46802  99%    0.98K  11715        4     46860K ext4_inode_cache\r\n```\r\n\r\n#### It will create dentry every second.\r\n\r\n```\r\n$ sudo strace -fp 29211 -e trace=open,stat,close,unlink\r\nProcess 29211 attached with 370 threads\r\n[pid 33862] stat(\"/data/druid/var/tmp/druid-groupBy-2c2e19ef-f883-496f-b88d-10dab759eab0_e0677686-1751-43a5-b244-817accd836f8\", 0x7efaa8549a10) = -1 ENOENT (No such file or directory)\r\n[pid 33856] stat(\"/data/druid/var/tmp/druid-groupBy-4033e0b6-2878-4fbc-b941-ab11f075e468_8889a78d-7afb-4290-af6c-5047fd499330\", 0x7efaa8b4fb10) = -1 ENOENT (No such file or directory)\r\n[pid 33881] stat(\"/data/druid/var/tmp/druid-groupBy-e944fe64-64b6-42db-96b3-27857fef7dc1_0490c768-3b39-4b37-b413-d84a03de7025\", 0x7efa6b1efb80) = -1 ENOENT (No such file or directory)\r\n[pid 33936] stat(\"/data/druid/var/tmp/druid-groupBy-b7452444-8665-482b-bec6-42de3abd756e_800e1931-f294-44b1-b844-e4bbec77aff5\", 0x7ef9679f7af0) = -1 ENOENT (No such file or directory)\r\n[pid 33890] stat(\"/data/druid/var/tmp/druid-groupBy-a72b752a-d047-4438-8d3a-8d4f30b98699_7d6382f6-62eb-4e47-85e7-7060014742c2\", 0x7efa6a8e6800) = -1 ENOENT (No such file or directory)\r\n[pid 33885] stat(\"/data/druid/var/tmp/druid-groupBy-96f1429b-53e5-4893-a084-19890e2c1ce7_c0a62ded-af6f-4ed2-a486-2489f639795f\", 0x7efa6adeb970) = -1 ENOENT (No such file or directory)\r\n[pid 33869] stat(\"/data/druid/var/tmp/druid-groupBy-ad823d55-a654-424c-9f96-6c7cd3fb8cef_03746689-7ea4-4d0a-aaf8-a99d3e17288d\", 0x7efa6bdfb980) = -1 ENOENT (No such file or directory)\r\n[pid 33878] stat(\"/data/druid/var/tmp/druid-groupBy-867609dd-82a8-43c3-863d-2fe8672f7937_97d3ebb4-cb0a-4e16-ab51-4a830094334e\", 0x7efa6b4f29f0) = -1 ENOENT (No such file or directory)\r\n[pid 33880] stat(\"/data/druid/var/tmp/druid-groupBy-ef4d32c8-a5b9-4557-b712-bfc7f1589cf7_9e82521f-32d5-496a-9c2d-2a9e187e67dd\", 0x7efa6b2f0b10) = -1 ENOENT (No such file or directory)\r\n[pid 33875] stat(\"/data/druid/var/tmp/druid-groupBy-f6398511-ed75-4547-8d02-c7004f38c1f1_6078cf5e-ef53-4c60-8f57-6413937a0fed\", 0x7efa6b7f5880) = -1 ENOENT (No such file or directory)\r\n[pid 33927] stat(\"/data/druid/var/tmp/druid-groupBy-f3647873-65bc-4d5f-8e3b-ae74781b4601_9154c59f-456a-4d69-a5d9-9ef90d60d1b5\", 0x7efa683c1a80) = -1 ENOENT (No such file or directory)\r\n[pid 33942] stat(\"/data/druid/var/tmp/druid-groupBy-c46029f8-1a26-4782-9413-6af301907289_14b7d398-b600-45c3-9861-46306f20e7ee\", 0x7ef9673f1a10) = -1 ENOENT (No such file or directory)\r\n[pid 33861] stat(\"/data/druid/var/tmp/druid-groupBy-ce632ef2-25a6-4ddb-b09e-67acd1708497_7d188a95-1fb6-4a50-99a3-73ee49d08f53\", 0x7efaa864a990) = -1 ENOENT (No such file or directory)\r\n[pid 33897] stat(\"/data/druid/var/tmp/druid-groupBy-963f0eb8-cdae-4fd4-a119-1152db5effd3_2f33c650-7f91-4e7f-a7b2-ba992a328461\", 0x7efa6a1dfb90) = -1 ENOENT (No such file or directory)\r\n[pid 33867] stat(\"/data/druid/var/tmp/druid-groupBy-d410a69b-a283-475d-a977-f4a5e04ad47a_431f471e-e02a-4fc1-a7ed-ab658215575a\", 0x7efa6bffd880) = -1 ENOENT (No such file or directory)\r\n[pid 33903] stat(\"/data/druid/var/tmp/druid-groupBy-821bd461-493a-4b8d-a948-96733175c553_fdadd570-0dd5-4e97-b7e9-cefc8e06cc15\", 0x7efa69bd9a90) = -1 ENOENT (No such file or directory)\r\n[pid 33899] stat(\"/data/druid/var/tmp/druid-groupBy-e4133c0c-e2ef-46ad-9533-ead801d83491_9e1ababa-4baf-4858-a332-a080bd70a97d\", 0x7efa69fdd880) = -1 ENOENT (No such file or directory)\r\n```\r\n\r\n### Root Cause\r\nIn the LimitedTemporaryStorage::close() method, if will check directory existence whatever the directory if created by storageDirectory.exists(). (We use maxOnDiskStorage's default value 0)\r\n\r\n```java\r\n  @Override\r\n  public void close()\r\n  {\r\n    synchronized (files) {\r\n      if (closed) {\r\n        return;\r\n      }\r\n      closed = true;\r\n      for (File file : ImmutableSet.copyOf(files)) {\r\n        delete(file);\r\n      }\r\n      files.clear();\r\n      if (storageDirectory.exists() && !storageDirectory.delete()) {\r\n        log.warn(\"Cannot delete storageDirectory: %s\", storageDirectory);\r\n      }\r\n    }\r\n  }\r\n```\r\n\r\n### Conclusion\r\n1. Increase vm.vfs_cache_pressure, vm.extra_free_kbytes should can let kernel reclaim dentry and inode memory more quickly. \r\n2. When maxOnDiskStorage = 0 or there is no file created, it is unnecessary to check the storageDirectory.exists().\r\n"}, {"user": "adocortes", "commits": {}, "labels": ["Area - Web Console", "Bug"], "created": "2019-04-17 09:35:02", "title": "0.14 incubating New router unified-console UI shows 0 historical nodes when a cluster has been installed and historicals has no data yet", "url": "https://github.com/apache/druid/issues/7498", "closed": "2019-04-23 23:15:03", "ttf": 6.000277777777778, "commitsDetails": [], "body": "0.14 incubating New router unified-console UI shows 0 historical nodes when a cluster has been installed and historicals has no data yet. Historicals are running and the list of historicals can be seen in the legacy console but the new console shows 0 historicals in Data servers, and empty list of data servers.  It shows the number of middlemanagers correctly.\r\n\r\nOnce some data is loaded, the new unified-console shows the number of nodes in the cluster. \r\n\r\nIt would show the number of running historical nodes.\r\n\r\n/unified-console.html#servers"}, {"user": "pdeva", "commits": {"1fb5ec39890cde4b882c1f8359c6ef8f4d52b98f": {"commitGHEventType": "referenced", "commitUser": "fjy"}, "020a3d214734398887ba908db692581d8a7729f2": {"commitGHEventType": "referenced", "commitUser": "clintropolis"}}, "changesInPackagesGIT": [], "labels": ["Area - Streaming Ingestion", "Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "KIS error when upgrading from 0.13 to 0.14", "numCommits": 0, "created": "2019-04-13 23:50:57", "closed": "2019-04-19 20:19:46", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/7470", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 1, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 5.000277777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "indexing-service/src/main/java/org/apache/druid/indexing/seekablestream/SeekableStreamEndSequenceNumbers.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "indexing-service/src/main/java/org/apache/druid/indexing/seekablestream/SeekableStreamStartSequenceNumbers.java", "deletions": 5, "insertions": 39, "lines": 44}, {"filePath": "indexing-service/src/test/java/org/apache/druid/indexing/seekablestream/SeekableStreamStartSequenceNumbersTest.java", "deletions": 0, "insertions": 77, "lines": 77}], "commitSpoonAstDiffStats": [{"spoonFilePath": "SeekableStreamStartSequenceNumbers.java", "spoonMethods": [{"INS": 7, "UPD": 0, "DEL": 2, "spoonMethodName": "org.apache.druid.indexing.seekablestream.SeekableStreamStartSequenceNumbers", "MOV": 6, "TOT": 15}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.seekablestream.SeekableStreamStartSequenceNumbers.getTopic()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.seekablestream.SeekableStreamStartSequenceNumbers.getPartitionOffsetMap()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "SeekableStreamStartSequenceNumbersTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.seekablestream.SeekableStreamStartSequenceNumbersTest", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "SeekableStreamEndSequenceNumbers.java", "spoonMethods": []}], "spoonStatsSkippedReason": "", "authoredDateTime": "2019-04-19 13:19:45", "commitMessage": "Adds backwards-compatible serde for SeekableStreamStartSequenceNumbers. (#7512)\n\nThis allows them to be deserialized by older Druid versions as\r\nKafkaPartitions objects.\r\n\r\nFixes #7470.", "commitUser": "fjy", "commitDateTime": "2019-04-19 13:19:45", "commitParents": ["5463ecb9795c9324228f87a0796741769bb5ea82"], "commitGHEventType": "referenced", "nameRev": "1fb5ec39890cde4b882c1f8359c6ef8f4d52b98f tags/druid-0.15.0-incubating-rc1~86", "commitHash": "1fb5ec39890cde4b882c1f8359c6ef8f4d52b98f"}, {"commitGitStats": [{"filePath": "indexing-service/src/main/java/org/apache/druid/indexing/seekablestream/SeekableStreamEndSequenceNumbers.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "indexing-service/src/main/java/org/apache/druid/indexing/seekablestream/SeekableStreamStartSequenceNumbers.java", "deletions": 5, "insertions": 39, "lines": 44}, {"filePath": "indexing-service/src/test/java/org/apache/druid/indexing/seekablestream/SeekableStreamStartSequenceNumbersTest.java", "deletions": 0, "insertions": 77, "lines": 77}], "commitSpoonAstDiffStats": [{"spoonFilePath": "SeekableStreamStartSequenceNumbers.java", "spoonMethods": [{"INS": 7, "UPD": 0, "DEL": 2, "spoonMethodName": "org.apache.druid.indexing.seekablestream.SeekableStreamStartSequenceNumbers", "MOV": 6, "TOT": 15}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.seekablestream.SeekableStreamStartSequenceNumbers.getTopic()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.seekablestream.SeekableStreamStartSequenceNumbers.getPartitionOffsetMap()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "SeekableStreamStartSequenceNumbersTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.seekablestream.SeekableStreamStartSequenceNumbersTest", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "SeekableStreamEndSequenceNumbers.java", "spoonMethods": []}], "spoonStatsSkippedReason": "", "authoredDateTime": "2019-04-19 13:19:45", "commitMessage": "Adds backwards-compatible serde for SeekableStreamStartSequenceNumbers. (#7512)\n\nThis allows them to be deserialized by older Druid versions as\r\nKafkaPartitions objects.\r\n\r\nFixes #7470.", "commitUser": "clintropolis", "commitDateTime": "2019-04-23 18:58:36", "commitParents": ["edcf73b9fda68f802f1898f61defd42e1abc13ef"], "commitGHEventType": "referenced", "nameRev": "020a3d214734398887ba908db692581d8a7729f2 tags/druid-0.14.1-incubating-rc1~22", "commitHash": "020a3d214734398887ba908db692581d8a7729f2"}], "body": "Please provide a detailed title (e.g. \"Broker crashes when using TopN query with Bound filter\" instead of just \"Broker crashes\").\r\n\r\n### Affected Version\r\n\r\n0.14\r\n\r\n### Description\r\n\r\n```\r\n2019-04-13 23:49:30,496 WARN o.a.d.i.c.a.RemoteTaskActionClient [task-runner-0-priority-0] Exception submitting action for task[index_kafka_infraserver-minute_b570a1d94fb7c21_dlmpiogd]\r\norg.apache.druid.java.util.common.IOE: Error with status[400 Bad Request] and message[{\"error\":\"Instantiation of [simple type, class org.apache.druid.indexing.kafka.KafkaPartitions] value failed: null (through reference chain: org.apache.druid.indexing.common.actions.TaskActionHolder[\\\"action\\\"]->org.apache.druid.indexing.common.actions.CheckPointDataSourceMetadataAction[\\\"previousCheckPoint\\\"]->org.apache.druid.indexing.kafka.KafkaDataSourceMetadata[\\\"partitions\\\"])\"}]. Check overlord logs for details.\r\n\tat org.apache.druid.indexing.common.actions.RemoteTaskActionClient.submit(RemoteTaskActionClient.java:95) [druid-indexing-service-0.14.0-incubating.jar:0.14.0-incubating]\r\n\tat org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner.runInternal(SeekableStreamIndexTaskRunner.java:695) [druid-indexing-service-0.14.0-incubating.jar:0.14.0-incubating]\r\n\tat org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner.run(SeekableStreamIndexTaskRunner.java:246) [druid-indexing-service-0.14.0-incubating.jar:0.14.0-incubating]\r\n\tat org.apache.druid.indexing.seekablestream.SeekableStreamIndexTask.run(SeekableStreamIndexTask.java:166) [druid-indexing-service-0.14.0-incubating.jar:0.14.0-incubating]\r\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:419) [druid-indexing-service-0.14.0-incubating.jar:0.14.0-incubating]\r\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:391) [druid-indexing-service-0.14.0-incubating.jar:0.14.0-incubating]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_191]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_191]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_191]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_191]\r\n```\r\n"}, {"user": "gianm", "commits": {}, "labels": ["Area - Querying", "Bug"], "created": "2019-04-12 20:10:05", "title": "HLLSketchBuild: Intermittent errors at query time", "url": "https://github.com/apache/druid/issues/7461", "closed": "2019-04-13 02:04:08", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\nObserved in Druid 0.13.0-incubating-iap9, an Imply build, but one that has no differences from 0.13.0-incubating in the `extensions-core/datasketches` tree. Btw, both use sketches-core 0.10.3.\r\n\r\n### Description\r\n\r\nI ran this query 200 times and saw two errors on historicals:\r\n\r\n```json\r\n{\r\n  \"queryType\": \"topN\",\r\n  \"dataSource\": \"my-ds-name\",\r\n  \"intervals\": [\r\n    \"2019-03-01T22:45:26.000Z/2019-03-08T22:45:26.000Z\"\r\n  ],\r\n  \"granularity\": {\r\n    \"type\": \"period\",\r\n    \"period\": \"P1W\",\r\n    \"origin\": \"2019-03-01T22:45:26.000Z\"\r\n  },\r\n  \"dimension\": \"geo.city\",\r\n  \"threshold\": 200,\r\n  \"metric\": \"distinctUserId\",\r\n  \"aggregations\": [\r\n    {\r\n      \"name\": \"distinctUserId\",\r\n      \"type\": \"HLLSketchBuild\",\r\n      \"fieldName\": \"userId\",\r\n      \"lgK\": 12,\r\n      \"tgtHllType\": \"HLL_4\"\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nThe first was:\r\n\r\n```\r\njava.lang.NullPointerException\r\n        at com.yahoo.sketches.hll.DirectHll4Array$DirectHll4Iterator.value(DirectHll4Array.java:173) ~[?:?]\r\n        at com.yahoo.sketches.hll.HllPairIterator.nextValid(HllPairIterator.java:75) ~[?:?]\r\n        at com.yahoo.sketches.hll.Union.unionImpl(Union.java:374) ~[?:?]\r\n        at com.yahoo.sketches.hll.Union.update(Union.java:264) ~[?:?]\r\n        at org.apache.druid.query.aggregation.datasketches.hll.HllSketchAggregatorFactory.combine(HllSketchAggregatorFactory.java:123) ~[?:?]\r\n        at org.apache.druid.query.aggregation.datasketches.hll.HllSketchBuildAggregatorFactory.combine(HllSketchBuildAggregatorFactory.java:40) ~[?:?]\r\n        at org.apache.druid.query.aggregation.datasketches.hll.HllSketchAggregatorFactory.combine(HllSketchAggregatorFactory.java:44) ~[?:?]\r\n        at org.apache.druid.query.topn.TopNBinaryFn.apply(TopNBinaryFn.java:103) ~[druid-processing-0.13.0-incubating-iap9.jar:0.13.0-incubating-iap9]\r\n```\r\n\r\nAnd the second was:\r\n\r\n```\r\njava.lang.NegativeArraySizeException\r\n        at com.yahoo.sketches.hll.Hll4Array.<init>(Hll4Array.java:32) ~[?:?]\r\n        at com.yahoo.sketches.hll.Conversions.convertToHll4(Conversions.java:20) ~[?:?]\r\n        at com.yahoo.sketches.hll.AbstractHllArray.copyAs(AbstractHllArray.java:34) ~[?:?]\r\n        at com.yahoo.sketches.hll.AbstractHllArray.copyAs(AbstractHllArray.java:17) ~[?:?]\r\n        at com.yahoo.sketches.hll.HllSketch.copyAs(HllSketch.java:241) ~[?:?]\r\n        at com.yahoo.sketches.hll.Union.getResult(Union.java:179) ~[?:?]\r\n        at org.apache.druid.query.aggregation.datasketches.hll.HllSketchAggregatorFactory.combine(HllSketchAggregatorFactory.java:124) ~[?:?]\r\n        at org.apache.druid.query.aggregation.datasketches.hll.HllSketchBuildAggregatorFactory.combine(HllSketchBuildAggregatorFactory.java:40) ~[?:?]\r\n        at org.apache.druid.query.aggregation.datasketches.hll.HllSketchAggregatorFactory.combine(HllSketchAggregatorFactory.java:44) ~[?:?]\r\n        at org.apache.druid.query.topn.TopNBinaryFn.apply(TopNBinaryFn.java:103) ~[druid-processing-0.13.0-incubating-iap9.jar:0.13.0-incubating-iap9]\r\n```"}, {"user": "leventov", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug"], "created": "2019-04-06 09:23:02", "title": "Unused parameter in ParallelIndexTuningConfig", "url": "https://github.com/apache/druid/issues/7423", "closed": "2019-04-16 00:41:54", "ttf": 9.000277777777777, "commitsDetails": [], "body": "`@JsonProperty(\"chatHandlerTimeout\") @Nullable Duration chatHandlerTimeout` is unused.\r\n\r\nFYI @jihoonson "}, {"user": "jihoonson", "commits": {}, "labels": ["Bug"], "created": "2019-03-27 19:08:22", "title": "Broken master build", "url": "https://github.com/apache/druid/issues/7357", "closed": "2019-03-28 02:42:59", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\nAfter #6953 \r\n\r\n### Description\r\n\r\nBuild with `-Pdist` flag fails with the below error.\r\n\r\n```\r\nERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.\r\nMar 27, 2019 12:03:32 PM org.hibernate.validator.internal.util.Version <clinit>\r\nINFO: HV000001: Hibernate Validator 5.1.3.Final\r\n12:03:34.459 [main] ERROR org.apache.druid.cli.PullDependencies - Unable to resolve artifacts for [org.apache.druid.extensions:druid-google-extensions:jar:0.15.0-incubating-SNAPSHOT (runtime) -> [] < [ (https://repo1.maven.org/maven2/, releases+snapshots)]].\r\norg.eclipse.aether.resolution.DependencyResolutionException: Could not find artifact org.apache.druid.extensions:druid-google-extensions:jar:0.15.0-incubating-SNAPSHOT in  (https://repo1.maven.org/maven2/)\r\n\tat org.eclipse.aether.internal.impl.DefaultRepositorySystem.resolveDependencies(DefaultRepositorySystem.java:384) ~[aether-impl-0.9.0.M2.jar:?]\r\n\tat io.tesla.aether.internal.DefaultTeslaAether.resolveArtifacts(DefaultTeslaAether.java:289) ~[tesla-aether-0.0.5.jar:0.0.5]\r\n\tat org.apache.druid.cli.PullDependencies.downloadExtension(PullDependencies.java:400) [druid-services-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\n\tat org.apache.druid.cli.PullDependencies.run(PullDependencies.java:299) [druid-services-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\nException in thread \"main\" java.lang.RuntimeException: java.lang.RuntimeException: org.eclipse.aether.resolution.DependencyResol\tat org.apache.druid.cli.Main.main(Main.java:117) [druid-services-0.15.0-incubating-SNAPSHOT.jar:0.15.0-incubating-SNAPSHOT]\r\nutionException: Could not find artifact org.apache.druid.extensions:druid-google-extensions:jar:0.15.0-incubating-SNAPSHOT in  (https://repo1.maven.org/maven2/)\r\nCaused by: org.eclipse.aether.resolution.ArtifactResolutionException: Could not find artifact org.apache.druid.extensions:druid-\tat org.apache.druid.cli.PullDependencies.run(PullDependencies.java:323)\r\n\tat org.apache.druid.cli.Main.main(Main.java:117)\r\ngoogle-extensions:jar:0.15.0-incubating-SNAPSHOT in  (https://repo1.maven.org/maven2/)\r\nCaused by: java.lang.RuntimeException: org.eclipse.aether.resolution.DependencyResolutionException: Could not find artifact org.\tat org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolve(DefaultArtifactResolver.java:459) ~[aether-impl-0.9.0.M2.jar:?]\r\napache.druid.extensions:druid-google-extensions:jar:0.15.0-incubating-SNAPSHOT in  (https://repo1.maven.org/maven2/)\r\n\tat org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolveArtifacts(DefaultArtifactResolver.java:262) ~[aether-impl-0.\tat org.apache.druid.cli.PullDependencies.downloadExtension(PullDependencies.java:413)\r\n9.0.M2.jar:?]\r\n\tat org.apache.druid.cli.PullDependencies.run(PullDependencies.java:299)\r\n\t... 1 more\r\n\tat org.eclipse.aether.internal.impl.DefaultRepositorySystem.resolveDependencies(DefaultRepositorySystem.java:367) ~[aether-impl-0.9.0.M2.jar:?]\r\nCaused by: org.eclipse.aether.resolution.DependencyResolutionException: Could not find artifact org.apache.druid.extensions:drui\t... 4 more\r\nd-google-extensions:jar:0.15.0-incubating-SNAPSHOT in  (https://repo1.maven.org/maven2/)\r\nCaused by: org.eclipse.aether.transfer.ArtifactNotFoundException: Could not find artifact org.apache.druid.extensions:druid-google-extensions:jar:0.15.0-incubating-SNAPSHOT in  (https://repo1.maven.org/maven2/)\r\n\tat io.tesla.aether.connector.AetherRepositoryConnector$2.wrap(AetherRepositoryConnector.java:828) ~[aether-connector\tat org.eclipse.aether.internal.impl.DefaultRepositorySystem.resolveDependencies(DefaultRepositorySystem.java:384)\r\n-okhttp-0.0.9.jar:0.0.9]\r\n\tat io.tesla.aether.internal.DefaultTeslaAether.resolveArtifacts(DefaultTeslaAether.java:289)\r\n\tat org.apache.druid.cli.PullDependencies.downloadExtension(PullDependencies.java:400)\r\n\t... 2 more\r\n\tat io.tesla.aether.connector.AetherRepositoryConnector$2.wrap(AetherRepositoryConnector.java:824) ~[aether-connector-okhttp-0.0.9.jar:0.0.9]\r\nCaused by: org.eclipse.aether.resolution.ArtifactResolutionException: Could not find artifact org.apache.druid.extensions:druid-google-extensions:jar:0.15.0-incubating-SNAPSHOT in  (https://repo1.maven.org/maven2/)\r\n\tat io.tesla.aether.connector.AetherRepositoryConnector$GetTask.flush(AetherRepositoryConnector.java:619) ~[aether-connector-okh\tat org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolve(DefaultArtifactResolver.java:459)\r\nttp-0.0.9.jar:0.0.9]\r\n\tat io.tesla.aether.connector.AetherRepositoryConnector.get(AetherRepositoryConnector.java:238) ~[aether-connector-okhttp-0.0.9.jar:0.0.9]\r\n\tat org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolveArtifacts(DefaultArtifactResolver.java:262)\r\n\tat org.eclipse.aether.internal.impl.DefaultArtifactResolver.performDownloads(DefaultArtifactResolver.java:535) ~[aether-impl-0.9.0.M2.jar:?]\r\n\tat org.eclipse.aether.internal.impl.DefaultRepositorySystem.resolveDependencies(DefaultRepositorySystem.java:367)\r\n\t... 4 more\r\n\tat org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolve(DefaultArtifactResolver.java:436) ~[aether-impl-0.9.0.M2.jar:?]\r\nCaused by: org.eclipse.aether.transfer.ArtifactNotFoundException: Could not find artifact org.apache.druid.extensions:druid-google-extensions:jar:0.15.0-incubating-SNAPSHOT in  (https://repo1.maven.org/maven2/)\r\n\tat io.tesla.aether.connector.AetherRepositoryConnector$2.wrap(AetherRepositoryConnector.java:828)\r\n\tat io.tesla.aether.connector.AetherRepositoryConnector$2.wrap(AetherRepositoryConnector.java:824)\r\n\tat io.tesla.aether.connector.AetherRepositoryConnector$GetTask.flush(AetherRepositoryConnector.java:619)\r\n\tat io.tesla.aether.connector.AetherRepositoryConnector.get(AetherRepositoryConnector.java:238)\r\n\tat org.eclipse.aether.internal.impl.DefaultArtifactResolver.performDownloads(DefaultArtifactResolver.java:535)\r\n\tat org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolve(DefaultArtifactResolver.java:436)\r\n\t... 6 more\r\n\tat org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolveArtifacts(DefaultArtifactResolver.java:262) ~[aether-impl-0.9.0.M2.jar:?]\r\n\tat org.eclipse.aether.internal.impl.DefaultRepositorySystem.resolveDependencies(DefaultRepositorySystem.java:367) ~[aether-impl-0.9.0.M2.jar:?]\r\n\t... 4 more\r\n```"}, {"user": "RestfulBlue", "commits": {}, "labels": ["Bug"], "created": "2019-03-27 18:28:59", "title": "Exception with auto compaction", "url": "https://github.com/apache/druid/issues/7356", "closed": "2019-03-29 03:25:10", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "Hi , i am trying to use druid 0.14.0(rc2), but it looks like there is a bug with auto compaction.\r\n\r\nI submit compaction config to coordinator: \r\n```json\r\n{\r\n  \"dataSource\" : \"druid-metrics\",\r\n}\r\n```\r\nbut there are exceptions in coordinator logs : \r\norg.apache.druid.java.util.common.ISE : Failed to post task at HttpIndexingServiceClient:135\r\nLooks like there is problem with that : \r\n1) In DataSourceCompactionConfig, if targetCompactionSizeBytes is null then it get replaced with default value\r\n2) In ClientCompactQueryTuningConfig maxRowsPerSegment is replaced with default value\r\nAccording to docs : \r\n```\r\ntargetCompactionSizeBytes | Target segment size after comapction. Cannot be used with\u00a0targetPartitionSize,\u00a0maxTotalRows, and\u00a0numShards\u00a0in tuningConfig.\r\n```\r\nAnd as a result every submit of compaction is failing because this condition always failing. Am i doing something wrong? Changing tuningConfig will not help, because null vals will be replaced anyway \r\n\r\nAlso, throwing exception in HttpIndexingServiceClient:135 does not show error message and only print .toString() method(which is not overriden in CompactionTask) , not logging overlord error at all. understand what is going on is hard (that was a debugger in my case) "}, {"user": "jihoonson", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2019-03-23 01:11:26", "title": "Rolling update failure in Kafka indexing service", "url": "https://github.com/apache/druid/issues/7330", "closed": "2019-03-23 04:12:15", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.14.0-rc2\r\n\r\n### Description\r\n\r\n[When publishing a segment, the start offsets of the new sequence is matched to the end offsets of the previous sequence](https://github.com/apache/incubator-druid/blob/master/server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java#L889). Here, the start offsets must contain `SeekableStreamStartSequenceNumbers`. However, the Kafka tasks of an old version would send `SeekableStreamEndSequenceNumbers` because it's the default implementation of `SeekableStreamSequenceNumbers`.\r\n\r\nHere is the stack trace.\r\n\r\n```\r\nCaused by: org.skife.jdbi.v2.exceptions.CallbackFailedException: org.apache.druid.java.util.common.IAE: Expected instance of org.apache.druid.indexing.seekables\r\ntream.SeekableStreamEndSequenceNumbers, got org.apache.druid.indexing.seekablestream.SeekableStreamStartSequenceNumbers\r\n        at org.skife.jdbi.v2.DBI.withHandle(DBI.java:284) ~[jdbi-2.63.1.jar:2.63.1]\r\n        at org.skife.jdbi.v2.DBI.inTransaction(DBI.java:353) ~[jdbi-2.63.1.jar:2.63.1]\r\n        at org.apache.druid.metadata.SQLMetadataConnector.lambda$retryTransaction$1(SQLMetadataConnector.java:148) ~[druid-server-0.14.0-iap-pre5.jar:0.14.0-iap\r\n-pre5]\r\n        at org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:86) ~[druid-core-0.14.0-iap-pre5.jar:0.14.0-iap-pre5]\r\n        at org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:114) ~[druid-core-0.14.0-iap-pre5.jar:0.14.0-iap-pre5]\r\n        at org.apache.druid.metadata.SQLMetadataConnector.retryTransaction(SQLMetadataConnector.java:148) ~[druid-server-0.14.0-iap-pre5.jar:0.14.0-iap-pre5]\r\n        at org.apache.druid.metadata.IndexerSQLMetadataStorageCoordinator.announceHistoricalSegments(IndexerSQLMetadataStorageCoordinator.java:285) ~[druid-serv\r\ner-0.14.0-iap-pre5.jar:0.14.0-iap-pre5]\r\n        at org.apache.druid.indexing.common.actions.SegmentTransactionalInsertAction.lambda$perform$0(SegmentTransactionalInsertAction.java:115) ~[druid-indexin\r\ng-service-0.14.0-iap-pre5.jar:0.14.0-iap-pre5]\r\n        at org.apache.druid.indexing.overlord.CriticalAction.perform(CriticalAction.java:53) ~[druid-indexing-service-0.14.0-iap-pre5.jar:0.14.0-iap-pre5]\r\n        at org.apache.druid.indexing.overlord.TaskLockbox.doInCriticalSection(TaskLockbox.java:627) ~[druid-indexing-service-0.14.0-iap-pre5.jar:0.14.0-iap-pre5\r\n]\r\n        at org.apache.druid.indexing.common.actions.SegmentTransactionalInsertAction.perform(SegmentTransactionalInsertAction.java:110) ~[druid-indexing-service\r\n-0.14.0-iap-pre5.jar:0.14.0-iap-pre5]\r\n        ... 75 more\r\nCaused by: org.apache.druid.java.util.common.IAE: Expected instance of org.apache.druid.indexing.seekablestream.SeekableStreamEndSequenceNumbers, got org.apache\r\n.druid.indexing.seekablestream.SeekableStreamStartSequenceNumbers\r\n        at org.apache.druid.indexing.seekablestream.SeekableStreamEndSequenceNumbers.plus(SeekableStreamEndSequenceNumbers.java:132) ~[druid-indexing-service-0.\r\n14.0-iap-pre5.jar:0.14.0-iap-pre5]\r\n        at org.apache.druid.indexing.seekablestream.SeekableStreamDataSourceMetadata.plus(SeekableStreamDataSourceMetadata.java:77) ~[druid-indexing-service-0.1\r\n4.0-iap-pre5.jar:0.14.0-iap-pre5]\r\n        at org.apache.druid.indexing.seekablestream.SeekableStreamDataSourceMetadata.matches(SeekableStreamDataSourceMetadata.java:59) ~[druid-indexing-service-\r\n0.14.0-iap-pre5.jar:0.14.0-iap-pre5]\r\n        at org.apache.druid.metadata.IndexerSQLMetadataStorageCoordinator.updateDataSourceMetadataWithHandle(IndexerSQLMetadataStorageCoordinator.java:890) ~[dr\r\nuid-server-0.14.0-iap-pre5.jar:0.14.0-iap-pre5]\r\n        at org.apache.druid.metadata.IndexerSQLMetadataStorageCoordinator$1.inTransaction(IndexerSQLMetadataStorageCoordinator.java:300) ~[druid-server-0.14.0-i\r\nap-pre5.jar:0.14.0-iap-pre5]\r\n        at org.apache.druid.metadata.IndexerSQLMetadataStorageCoordinator$1.inTransaction(IndexerSQLMetadataStorageCoordinator.java:287) ~[druid-server-0.14.0-i\r\nap-pre5.jar:0.14.0-iap-pre5]\r\n        at org.skife.jdbi.v2.tweak.transactions.LocalTransactionHandler.inTransaction(LocalTransactionHandler.java:184) ~[jdbi-2.63.1.jar:2.63.1]\r\n        at org.skife.jdbi.v2.tweak.transactions.LocalTransactionHandler.inTransaction(LocalTransactionHandler.java:216) ~[jdbi-2.63.1.jar:2.63.1]\r\n        at org.skife.jdbi.v2.BasicHandle.inTransaction(BasicHandle.java:350) ~[jdbi-2.63.1.jar:2.63.1]\r\n        at org.skife.jdbi.v2.DBI$7.withHandle(DBI.java:357) ~[jdbi-2.63.1.jar:2.63.1]\r\n        at org.skife.jdbi.v2.DBI.withHandle(DBI.java:281) ~[jdbi-2.63.1.jar:2.63.1]\r\n        at org.skife.jdbi.v2.DBI.inTransaction(DBI.java:353) ~[jdbi-2.63.1.jar:2.63.1]\r\n        at org.apache.druid.metadata.SQLMetadataConnector.lambda$retryTransaction$1(SQLMetadataConnector.java:148) ~[druid-server-0.14.0-iap-pre5.jar:0.14.0-iap\r\n-pre5]\r\n        at org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:86) ~[druid-core-0.14.0-iap-pre5.jar:0.14.0-iap-pre5]\r\n        at org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:114) ~[druid-core-0.14.0-iap-pre5.jar:0.14.0-iap-pre5]\r\n        at org.apache.druid.metadata.SQLMetadataConnector.retryTransaction(SQLMetadataConnector.java:148) ~[druid-server-0.14.0-iap-pre5.jar:0.14.0-iap-pre5]\r\n        at org.apache.druid.metadata.IndexerSQLMetadataStorageCoordinator.announceHistoricalSegments(IndexerSQLMetadataStorageCoordinator.java:285) ~[druid-serv\r\ner-0.14.0-iap-pre5.jar:0.14.0-iap-pre5]\r\n        at org.apache.druid.indexing.common.actions.SegmentTransactionalInsertAction.lambda$perform$0(SegmentTransactionalInsertAction.java:115) ~[druid-indexin\r\ng-service-0.14.0-iap-pre5.jar:0.14.0-iap-pre5]\r\n        at org.apache.druid.indexing.overlord.CriticalAction.perform(CriticalAction.java:53) ~[druid-indexing-service-0.14.0-iap-pre5.jar:0.14.0-iap-pre5]\r\n        at org.apache.druid.indexing.overlord.TaskLockbox.doInCriticalSection(TaskLockbox.java:627) ~[druid-indexing-service-0.14.0-iap-pre5.jar:0.14.0-iap-pre5\r\n]\r\n        at org.apache.druid.indexing.common.actions.SegmentTransactionalInsertAction.perform(SegmentTransactionalInsertAction.java:110) ~[druid-indexing-service\r\n-0.14.0-iap-pre5.jar:0.14.0-iap-pre5]\r\n        ... 75 more\r\n```"}, {"user": "lxqfy", "commits": {}, "labels": ["Bug"], "created": "2019-03-22 06:28:38", "title": "Automatic segment compaction segment size not matching targetCompactionSizeBytes", "url": "https://github.com/apache/druid/issues/7324", "closed": "2019-05-07 16:21:30", "ttf": 46.000277777777775, "commitsDetails": [], "body": "Automatic segment compaction segment size not matching targetCompactionSizeBytes\r\n### Affected Version\r\n0.13.0-incubationg\r\nThe Druid version where the problem was encountered.\r\n\r\n### Description\r\nI am trying to use the \"Automatic segment compaction\". My auto-compaction config is as follows:\r\n{\r\n  \"dataSource\": ds,\r\n  \"inputSegmentSizeBytes\":524288000,\r\n  \"targetCompactionSizeBytes\": 524288000,\r\n  \"skipOffsetFromLatest\": \"PT3H\",\r\n  \"keepSegmentGranularity\": false\r\n}\r\n\r\nHowever, after the compact task finished, I can see that the segment(shard) size is around 130+ MB. Those segments will also be involved in the next round compaction tasks and result in the same-size segments as before. Infinite loop.\r\n\r\nFor example, the compaction task tries to compact 2 segment shards with targetCompactionSizeBytes=500mb:\r\n2019-03-06T04:00:00.000Z/2019-03-08T01:00:00.000Z_1 (130mb)\r\n2019-03-06T04:00:00.000Z/2019-03-08T01:00:00.000Z_2 (130mb)\r\nAfter the compaction task, those 2 segments shards size not compacted to 1 shard, they remain pretty much the same size, just with a new version. And the coordinator will try to compact those segment shard again and again without actually compact to single shard of 260mb.\r\n\r\nAfter some investigation, I found that:\r\n\r\nThe compaction task will generate an internal index task. The targetPartionSize is calculated by targetCompactionSizeBytes and avgRowsPerByte. \r\n\r\nEstimated targetPartitionSize[%d] = avgRowsPerByte[%f] * targetCompactionSizeBytes[%d]\r\n\r\nThere is another configuration, maxTotalRows for index task, the default value is 20000000. If targetPartionSize is larger than maxTotalRows, it won't work as expected.\r\n\r\nThe workaround is set the maxTotalRows to a larger value. By default, users won't have any idea about this, maxTotalRows should be overwritten.\r\n"}, {"user": "lxqfy", "commits": {}, "labels": ["Area - Cache", "Bug"], "created": "2019-03-20 03:37:24", "title": "Druid Broker Result Level Cache Not Working Properly When Different Query Intervals Cover the Same Set of Segments", "url": "https://github.com/apache/druid/issues/7302", "closed": "2019-05-03 20:28:56", "ttf": 44.000277777777775, "commitsDetails": [], "body": "Druid Broker Result Level Cache Not Working Properly When Different Query Intervals Cover the Same Set of Segments.\r\n\r\n### Affected Version\r\n\r\n0.13.0-incubating\r\n\r\n### Description\r\nWe have a segment with Segment Granularity of DAY and Query Granularity of HOUR.\r\nWhen we send the same queries with only a different query interval, the result level cache will return the wrong result.\r\n\r\nExample: QueryA and QueryB has same query body.\r\nQueryA Interval: 2019-03-15T14:00:00/ 2019-03-16T14:00:00\r\nQueryB Interval: 2019-03-15T18:00:00/ 2019-03-16T18:00:00\r\n\r\nInvolved Segments:\r\nQueryA: segment2019-03-15_2019-03-16\r\nQueryB: segment2019-03-15_2019-03-16\r\n\r\nResponse:\r\nQueryA gets the correct result.\r\nQueryB gets the result of QueryA. \r\n\r\nSome Investigations:\r\nResult level cache will use the query without interval as key. When the key match, the E-Tag is used to check if the cached result should be returned. However, E-Tag is only calculated with the Identifiers of involved segments. "}, {"user": "jihoonson", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2019-03-18 20:09:03", "title": "Duplicate data ingestion in Kinesis indexing service", "url": "https://github.com/apache/druid/issues/7286", "closed": "2019-03-21 20:12:23", "ttf": 3.000277777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.14.0-rc1\r\n\r\n### Description\r\n\r\nI created two dataSources for the same Kinesis stream, but it returned different results for the same query. Duplicate raw events was found in one dataSource (it was about 110 events). I'm suspecting [this line](https://github.com/apache/incubator-druid/blob/master/indexing-service/src/main/java/org/apache/druid/indexing/seekablestream/supervisor/SeekableStreamSupervisor.java#L2347). Maybe `isExclusive` should be set to true. But, I'm not sure why it happened for only one dataSource."}, {"user": "clintropolis", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2019-03-13 05:48:19", "title": "Kafka Tasks unable restore persisted segments properly", "url": "https://github.com/apache/druid/issues/7252", "closed": "2019-03-13 21:29:40", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\nAll versions after #6431\r\n\r\n### Description\r\n\r\nKakfa indexing tasks (likely kinesis as well) are unable to restore properly if segments have been incrementally persisted because they cannot deserialize `SequenceMetadata`, resulting in an error in the form of:\r\n\r\n```\r\n2019-03-12T02:18:50,238 ERROR [task-runner-0-priority-0] org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner - Encountered exception while running task.\r\ncom.fasterxml.jackson.databind.JsonMappingException: Unrecognized Type: [null]\r\n    at com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCache2(DeserializerCache.java:269) ~[jackson-databind-2.6.7.jar:2.6.7]\r\n    at com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCacheValueDeserializer(DeserializerCache.java:244) ~[jackson-databind-2.6.7.jar:2.6.7]\r\n    at com.fasterxml.jackson.databind.deser.DeserializerCache.findValueDeserializer(DeserializerCache.java:142) ~[jackson-databind-2.6.7.jar:2.6.7]\r\n    at com.fasterxml.jackson.databind.DeserializationContext.findContextualValueDeserializer(DeserializationContext.java:428) ~[jackson-databind-2.6.7.jar:2.6.7]\r\n    at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.createContextual(CollectionDeserializer.java:164) ~[jackson-databind-2.6.7.jar:2.6.7]\r\n    at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.createContextual(CollectionDeserializer.java:25) ~[jackson-databind-2.6.7.jar:2.6.7]\r\n    at com.fasterxml.jackson.databind.DeserializationContext.handleSecondaryContextualization(DeserializationContext.java:669) ~[jackson-databind-2.6.7.jar:2.6.7]\r\n    at com.fasterxml.jackson.databind.DeserializationContext.findRootValueDeserializer(DeserializationContext.java:466) ~[jackson-databind-2.6.7.jar:2.6.7]\r\n    at com.fasterxml.jackson.databind.ObjectMapper._findRootDeserializer(ObjectMapper.java:3838) ~[jackson-databind-2.6.7.jar:2.6.7]\r\n    at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3732) ~[jackson-databind-2.6.7.jar:2.6.7]\r\n    at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2639) ~[jackson-databind-2.6.7.jar:2.6.7]\r\n    at org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner.restoreSequences(SeekableStreamIndexTaskRunner.java:942) ~[druid-indexing-service-0.14.0-iap-pre3.jar:0.14.0-iap-pre3]\r\n...\r\n```\r\n\r\nThe issue is that `SequenceMetadata` is a private class using generic types defined in the outer class, `SeekableStreamIndexTaskRunner<PartitionIdType, SequenceOffsetType>`, which determine the map types which store offsets in the persisted sequence metadata json file. The file is written correctly, but the `TypeReference` given to the object mapper is not able to properly read it during deserialization, resulting in the above error. \r\n\r\nFix underway, but untangling this is sort of messy unfortunately."}, {"user": "jihoonson", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2019-03-12 05:37:22", "title": "Kafka tasks fail after resuming for incremental handoff", "url": "https://github.com/apache/druid/issues/7239", "closed": "2019-03-13 04:12:22", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "### Affected Version\r\n\r\n0.14.0 (This bug was introduced in #6431).\r\n\r\n### Description\r\n\r\nHere is the stack trace.\r\n\r\n```\r\n2019-03-12T02:59:51,878 INFO [appenderator_persist_0] org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner - Persist completed with metadata [AppenderatorDriverMetadata{segments={index_kafka_clarity-cloud0_d756771c2863b20_0=[SegmentWithState{segmentIdentifier=clarity-cloud0_2019-03-12T02:00:00.000Z_2019-03-12T03:00:00.000Z_2019-03-12T01:56:11.119Z_10, state=APPENDING}]}, lastSegmentIds={index_kafka_clarity-cloud0_d756771c2863b20_0=clarity-cloud0_2019-03-12T02:00:00.000Z_2019-03-12T03:00:00.000Z_2019-03-12T01:56:11.119Z_10}, callerMetadata={nextPartitions=SeekableStreamPartitions{stream/topic='clarity-cloud0', partitionSequenceNumberMap/partitionOffsetMap={1=103431503737, 4=6100519619, 7=6100600785, 10=6098083460, 13=6101049368, 16=6101000251, 19=6097385400, 22=6100601015, 25=6101101786, 28=6101055354}}}}]\r\n2019-03-12T02:59:52,005 INFO [task-runner-0-priority-0] org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner - Pausing ingestion until resumed\r\n2019-03-12T02:59:52,070 INFO [qtp606061176-123] org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner - Persisting Sequences Metadata [[SequenceMetadata{sequenceName='index_kafka_clarity-cloud0_d756771c2863b20_0', sequenceId=0, startOffsets={16=6100335393, 1=103430839175, 19=6096726535, 4=6099861583, 22=6099935437, 7=6099935104, 25=6100439713, 10=6097421649, 28=6100393134, 13=6100392241}, endOffsets={16=6101282427, 1=103431786274, 19=6097668960, 4=6100807623, 22=6100884284, 7=6100884039, 25=6101382663, 10=6098354147, 28=6101340178, 13=6101339869}, assignments=[16, 1, 19, 4, 22, 7, 25, 10, 28, 13], sentinel=false, checkpointed=true}, SequenceMetadata{sequenceName='index_kafka_clarity-cloud0_d756771c2863b20_1', sequenceId=1, startOffsets={16=6101282427, 1=103431786274, 19=6097668960, 4=6100807623, 22=6100884284, 7=6100884039, 25=6101382663, 10=6098354147, 28=6101340178, 13=6101339869}, endOffsets={16=9223372036854775807, 1=9223372036854775807, 19=9223372036854775807, 4=9223372036854775807, 22=9223372036854775807, 7=9223372036854775807, 25=9223372036854775807, 10=9223372036854775807, 28=9223372036854775807, 13=9223372036854775807}, assignments=[16, 1, 19, 4, 22, 7, 25, 10, 28, 13], sentinel=false, checkpointed=false}]]\r\n2019-03-12T02:59:52,086 INFO [task-runner-0-priority-0] org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner - Ingestion loop resumed\r\n2019-03-12T02:59:52,087 INFO [task-runner-0-priority-0] org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner - Seeking partition[25] to sequence[6101110684].\r\n2019-03-12T02:59:52,087 INFO [task-runner-0-priority-0] org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner - Seeking partition[10] to sequence[6098092421].\r\n2019-03-12T02:59:52,087 INFO [task-runner-0-priority-0] org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner - Seeking partition[4] to sequence[6100527087].\r\n2019-03-12T02:59:52,087 INFO [task-runner-0-priority-0] org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner - Seeking partition[7] to sequence[6100609294].\r\n2019-03-12T02:59:52,087 INFO [task-runner-0-priority-0] org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner - Seeking partition[22] to sequence[6100609476].\r\n2019-03-12T02:59:52,087 INFO [task-runner-0-priority-0] org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner - Seeking partition[1] to sequence[103431512339].\r\n2019-03-12T02:59:52,087 INFO [task-runner-0-priority-0] org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner - Seeking partition[16] to sequence[6101007019].\r\n2019-03-12T02:59:52,087 INFO [task-runner-0-priority-0] org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner - Seeking partition[19] to sequence[6097392961].\r\n2019-03-12T02:59:52,087 INFO [task-runner-0-priority-0] org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner - Seeking partition[13] to sequence[6101057094].\r\n2019-03-12T02:59:52,087 INFO [task-runner-0-priority-0] org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner - Seeking partition[28] to sequence[6101057589].\r\n2019-03-12T02:59:52,091 ERROR [task-runner-0-priority-0] org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner - Encountered exception in run() before persisting.\r\norg.apache.druid.java.util.common.ISE: Starting sequenceNumber [6101007019] does not match expected [6101282427] for partition [16]\r\n\tat org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner.verifyInitialRecordAndSkipExclusivePartition(SeekableStreamIndexTaskRunner.java:1895) ~[druid-indexing-service-0.14.0-iap-pre3.jar:0.14.0-iap-pre3]\r\n\tat org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner.runInternal(SeekableStreamIndexTaskRunner.java:517) [druid-indexing-service-0.14.0-iap-pre3.jar:0.14.0-iap-pre3]\r\n\tat org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner.run(SeekableStreamIndexTaskRunner.java:246) [druid-indexing-service-0.14.0-iap-pre3.jar:0.14.0-iap-pre3]\r\n\tat org.apache.druid.indexing.seekablestream.SeekableStreamIndexTask.run(SeekableStreamIndexTask.java:166) [druid-indexing-service-0.14.0-iap-pre3.jar:0.14.0-iap-pre3]\r\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:419) [druid-indexing-service-0.14.0-iap-pre3.jar:0.14.0-iap-pre3]\r\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:391) [druid-indexing-service-0.14.0-iap-pre3.jar:0.14.0-iap-pre3]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_163]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_163]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_163]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_163]\r\n```\r\n\r\nThis happens only when `setEndOffsets()` is called with `finish = false` which is missing in our unit tests. If `finish` is false, `initialOffsetsSnapshot` is updated to the start offsets of the new sequence in `setEndOffsets()`. However, if there are remaining offsets for a task to consume before starting a new sequence, `verifyInitialRecordAndSkipExclusivePartition()` can throw an error because the offset of the read record can be smaller than that in `intialSequenceSnapshot`:\r\n\r\n```\r\n  private boolean verifyInitialRecordAndSkipExclusivePartition(\r\n      final OrderedPartitionableRecord<PartitionIdType, SequenceOffsetType> record,\r\n      final Map<PartitionIdType, SequenceOffsetType> intialSequenceSnapshot\r\n  )\r\n  {\r\n    if (intialSequenceSnapshot.containsKey(record.getPartitionId())) {\r\n      if (record.getSequenceNumber().compareTo(intialSequenceSnapshot.get(record.getPartitionId())) < 0) {\r\n        throw new ISE(\r\n            \"Starting sequenceNumber [%s] does not match expected [%s] for partition [%s]\",\r\n            record.getSequenceNumber(),\r\n            intialSequenceSnapshot.get(record.getPartitionId()),\r\n            record.getPartitionId()\r\n        );\r\n      }\r\n```\r\n\r\nThis error is reproducible in `KafkaIndexTaskTest.testIncrementalHandOffReadsThroughEndOffsets()` if you set `finish` to false [here](https://github.com/apache/incubator-druid/blob/master/extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java#L935).\r\n\r\nI'll raise a PR to fix this bug soon."}, {"user": "justinborromeo", "commits": {}, "labels": ["Bug", "Flaky test"], "created": "2019-03-05 01:49:55", "title": "KafkaSupervisorTest#testCheckpointForUnknownTaskGroup() is flaky", "url": "https://github.com/apache/druid/issues/7193", "closed": "2019-03-12 01:20:24", "ttf": 6.000277777777778, "commitsDetails": [], "body": "# Affected Version\r\n\r\n`master` and `0.12.3-iap` branches.\r\n\r\n# Description\r\n\r\nTransient failures observed by both me and @clintropolis on Travis and when run locally in IntelliJ.\r\n\r\n# Steps to Replicate\r\n\r\n1. Run the test\r\n2. If it passes (which seems to happen ~60-70% of the time), return to step 1\r\n\r\n# Test Output\r\n\r\n```\r\n2019-03-05T01:49:07,109 WARN [Time-limited test] org.apache.kafka.clients.consumer.ConsumerConfig - The configuration 'myCustomKey' was supplied but isn't a known config.\r\n2019-03-05T01:49:07,109 INFO [Time-limited test] org.apache.kafka.common.utils.AppInfoParser - Kafka version : 2.1.0\r\n2019-03-05T01:49:07,109 INFO [Time-limited test] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : eec43959745f444f\r\n2019-03-05T01:49:07,110 INFO [Time-limited test] org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor - Started SeekableStreamSupervisor[testDS], first run in [PT86400S], with spec: [KafkaSupervisorSpec{dataSchema=DataSchema{dataSource='testDS', parser={type=string, parseSpec={format=json, timestampSpec={column=timestamp, format=iso, missingValue=null}, dimensionsSpec={dimensions=[{type=string, name=dim1, multiValueHandling=SORTED_ARRAY, createBitmapIndex=true}, {type=string, name=dim2, multiValueHandling=SORTED_ARRAY, createBitmapIndex=true}], dimensionExclusions=[]}, flattenSpec={useFieldDiscovery=true, fields=[]}, featureSpec={}}, encoding=UTF-8}, aggregators=[CountAggregatorFactory{name='rows'}], granularitySpec=UniformGranularitySpec{segmentGranularity={type=period, period=PT1H, timeZone=UTC, origin=null}, queryGranularity=NoneGranularity, rollup=true, inputIntervals=[], wrappedSpec=ArbitraryGranularitySpec{intervals=[], queryGranularity=NoneGranularity, rollup=true}}, transformSpec=TransformSpec{filter=null, transforms=[]}}, tuningConfig=KafkaSupervisorTuningConfig{maxRowsInMemory=1000, maxRowsPerSegment=50000, maxTotalRows=null, maxBytesInMemory=233046016, intermediatePersistPeriod=P1Y, basePersistDirectory=/test, maxPendingPersists=0, indexSpec=IndexSpec{bitmapSerdeFactory=ConciseBitmapSerdeFactory{}, dimensionCompression=lz4, metricCompression=lz4, longEncoding=longs}, reportParseExceptions=false, handoffConditionTimeout=0, resetOffsetAutomatically=false, segmentWriteOutMediumFactory=null, workerThreads=8, chatThreads=3, chatRetries=9, httpTimeout=PT10S, shutdownTimeout=PT80S, offsetFetchPeriod=PT30S, intermediateHandoffPeriod=P2147483647D, logParseExceptions=false, maxParseExceptions=2147483647, maxSavedParseExceptions=0}, ioConfig=KafkaSupervisorIOConfig{topic='testTopic68', replicas=2, taskCount=1, taskDuration=PT1S, consumerProperties={myCustomKey=myCustomValue, isolation.level=read_committed, bootstrap.servers=localhost:13818}, pollTimeout=100, startDelay=PT86400S, period=PT30S, useEarliestOffset=true, completionTimeout=PT1800S, earlyMessageRejectionPeriod=Optional.absent(), lateMessageRejectionPeriod=Optional.absent()}, context=null, suspend=false}]\r\n2019-03-05T01:49:07,110 INFO [Time-limited test] org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor - Checkpointing [SeekableStreamDataSourceMetadata{SeekableStreamPartitions=SeekableStreamPartitions{stream/topic='testTopic68', partitionSequenceNumberMap/partitionOffsetMap={}}}] for taskGroup [0]\r\n2019-03-05T01:49:07,110 ERROR [KafkaSupervisor-testDS] org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor - SeekableStreamSupervisor[testDS] failed to handle notice: {class=org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor, exceptionType=class org.apache.druid.java.util.common.ISE, exceptionMessage=WTH?! cannot find taskGroup [0] among all activelyReadingTaskGroups [{}], noticeClass=CheckpointNotice}\r\norg.apache.druid.java.util.common.ISE: WTH?! cannot find taskGroup [0] among all activelyReadingTaskGroups [{}]\r\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor$CheckpointNotice.isValidTaskGroup(SeekableStreamSupervisor.java:417) ~[classes/:?]\r\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor$CheckpointNotice.handle(SeekableStreamSupervisor.java:371) ~[classes/:?]\r\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.lambda$tryInit$3(SeekableStreamSupervisor.java:724) ~[classes/:?]\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_191]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_191]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_191]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_191]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_191]\r\n2019-03-05T01:49:07,184 WARN [KafkaSupervisor-testDS] org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor - Could not fetch partitions for topic/stream [testTopic57]\r\n2019-03-05T01:49:07,184 DEBUG [KafkaSupervisor-testDS] org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor - full stack trace\r\norg.apache.druid.java.util.common.ISE: Topic [testTopic57] is not found in KafkaConsumer's list of topics\r\n\tat org.apache.druid.indexing.kafka.KafkaRecordSupplier.getPartitionIds(KafkaRecordSupplier.java:156) ~[classes/:?]\r\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.updatePartitionDataFromStream(SeekableStreamSupervisor.java:1703) ~[classes/:?]\r\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.runInternal(SeekableStreamSupervisor.java:1003) ~[classes/:?]\r\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor$RunNotice.handle(SeekableStreamSupervisor.java:265) ~[classes/:?]\r\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.lambda$tryInit$3(SeekableStreamSupervisor.java:724) ~[classes/:?]\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_191]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_191]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_191]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_191]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_191]\r\n2019-03-05T01:49:07,185 DEBUG [KafkaSupervisor-testDS] org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor - Found [3] seekablestream indexing tasks for dataSource [testDS]\r\n2019-03-05T01:49:07,185 ERROR [KafkaSupervisor-testDS] org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor - SeekableStreamSupervisor[testDS] failed to handle notice: {class=org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor, exceptionType=class java.lang.NullPointerException, exceptionMessage=null, noticeClass=RunNotice}\r\njava.lang.NullPointerException\r\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.checkPendingCompletionTasks(SeekableStreamSupervisor.java:2132) ~[classes/:?]\r\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.runInternal(SeekableStreamSupervisor.java:1007) ~[classes/:?]\r\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor$RunNotice.handle(SeekableStreamSupervisor.java:265) ~[classes/:?]\r\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.lambda$tryInit$3(SeekableStreamSupervisor.java:724) ~[classes/:?]\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_191]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_191]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_191]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_191]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_191]\r\n\r\njava.lang.AssertionError\r\n\tat org.junit.Assert.fail(Assert.java:86)\r\n\tat org.junit.Assert.assertTrue(Assert.java:41)\r\n\tat org.junit.Assert.assertTrue(Assert.java:52)\r\n\tat org.apache.druid.indexing.kafka.supervisor.KafkaSupervisorTest.testCheckpointForUnknownTaskGroup(KafkaSupervisorTest.java:2334)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n```"}, {"user": "zhiyuany", "commits": {}, "labels": ["Bug"], "created": "2019-02-28 22:37:18", "title": "Parquet Hadoop parser fails to parse columns specified in transformSpec only.", "url": "https://github.com/apache/druid/issues/7169", "closed": "2019-04-01 14:54:19", "ttf": 31.00027777777778, "commitsDetails": [], "body": "# Affected Version\r\n\r\ndruid-0.13.0-incubating\r\n\r\n# Description\r\n\r\n**The scene** \r\nWe have data stored in hdfs with parquet format, named `p-event`.\r\nEach row in the data has a column `execute-timestamp`, Long or Null indicating execution time.\r\nNow we want a metric `execution-count` to count how many of the p-event has been executed.\r\n\r\nWe use below indexing-spec:\r\n```\r\n{\r\n  \"type\": \"index_hadoop\",\r\n  \"spec\": {\r\n    ...\r\n    \"dataSchema\": {\r\n      ...,\r\n      \"parser\": {\r\n        \"type\": \"parquet\",\r\n        \"parseSpec\": {\r\n          \"format\": \"timeAndDims\",\r\n          \"timestampSpec\": ...,\r\n          \"dimensionsSpec\": {\r\n            \"dimensions\": [\r\n              ...\r\n            ],\r\n            \"dimensionExclusions\": [],\r\n            \"spatialDimensions\": []\r\n          }\r\n        }\r\n      },\r\n      \"metricsSpec\": [\r\n        ...\r\n        {\r\n          \"type\" : \"longSum\",\r\n          \"name\" : \"execution-count\",\r\n          \"fieldName\" : \"has-execution\"\r\n        }\r\n      ],\r\n      \"transformSpec\": {\r\n        \"transforms\": [\r\n          {\r\n            \"type\": \"expression\",\r\n            \"name\": \"has-execution\",\r\n            \"expression\": \"if (execute-timestamp > 0, 1, 0)\"\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n```\r\n_Note that `execute-timestamp` is not part of the `dimensions`, only declared in `transform`._\r\n\r\n**The bug**\r\n_**`execution-count` is constantly 0**_\r\nThe data has tons of event with `execute-timestamp` satisfying the predicate  _\"if (execute-timestamp > 0, 1, 0)\"_, but metric `execution-count` is constantly 0, as if the column `execute-timestamp` is always null/absent.\r\nThe hypothesis is that column `execute-timestamp` is not parsed. \r\n\r\n**The reproduce**\r\nAbove scene walk through provide the way to reproduce the problem. \r\n\r\n**How to unbroke/fix it**\r\n***A***\r\nOne fix to the problem is _adding `execute-timestamp` to the indexing-spec `dimensions` section_, and this leads to \"execution-count\" produce correct metric.\r\nBut apparently this is not the right use case, and output would explode in size when we put high cardinality fields like timestamp to dimension.\r\n```\r\n{\r\n  ...\r\n          \"dimensionsSpec\": {\r\n            \"dimensions\": [\r\n              \"execute-timestamp\"\r\n            ],\r\n            ...\r\n          }\r\n        }\r\n      },\r\n  ...\r\n}\r\n```\r\n***B***\r\nSame as solution A, _adding `execute-timestamp` to the indexing-spec `metrics` section_ has same effect.\r\n```\r\n{\r\n  ...\r\n          \"metricsSpec\": [\r\n             {\r\n               \"type\" : \"longSum\",\r\n               \"name\" : \"dummy-metric\",\r\n               \"fieldName\" : \"execute-timestamp\"\r\n             }\r\n            ]\r\n            ...\r\n          }\r\n        }\r\n      },\r\n  ...\r\n}\r\n```\r\n\r\n***C***\r\nAnother hack fix to the problem is _transform the field in place_ or _shadow_ .\r\nBut this remove the original value of `execute-timestamp` and disallow other transformation.\r\n```\r\n{\r\n  ...\r\n      \"metricsSpec\": [\r\n        ...\r\n        {\r\n          \"type\" : \"longSum\",\r\n          \"name\" : \"execution-count\",\r\n          \"fieldName\" : \"execute-timestamp\"\r\n        }\r\n      ],\r\n      \"transformSpec\": {\r\n        \"transforms\": [\r\n          {\r\n            \"type\": \"expression\",\r\n            \"name\": \"execute-timestamp\",\r\n            \"expression\": \"if (execute-timestamp > 0, 1, 0)\"\r\n          }\r\n        ]\r\n      }\r\n  ...\r\n}\r\n\r\n```\r\n\r\n**Debugging**\r\nThere is no error found in the indexing job, besides the result is inaccurate. \r\nNo further debugging performed. \r\n"}, {"user": "clintropolis", "commits": {}, "labels": ["Area - Web Console", "Bug"], "created": "2019-02-28 01:11:12", "title": "web console 'tasks' view group tasks by 'status' broken", "url": "https://github.com/apache/druid/issues/7156", "closed": "2019-04-11 23:10:38", "ttf": 42.000277777777775, "commitsDetails": [], "body": "# Affected Version\r\n\r\n`0.14.0-incubating` and newer\r\n\r\n# Description\r\nThe 'tasks' view of the new web console doesn't appear to correctly group tasks by 'status':\r\n\r\n<img width=\"1665\" alt=\"screen shot 2019-02-27 at 5 09 56 pm\" src=\"https://user-images.githubusercontent.com/1577461/53534155-8f0b8f80-3ab2-11e9-8e86-79227050285d.png\">\r\n"}, {"user": "clintropolis", "commits": {}, "labels": ["Area - Web Console", "Bug"], "created": "2019-02-28 01:03:16", "title": "web console segments view unable to search by exact datasource", "url": "https://github.com/apache/druid/issues/7155", "closed": "2019-04-18 06:05:45", "ttf": 49.000277777777775, "commitsDetails": [], "body": "# Affected Version\r\n\r\n`0.14.0-incubating` and newer.\r\n\r\n# Description\r\n\r\nThe 'segments' view of the new web console is unable to filter to a specific datasource, which is a problem for clusters which have datasource names that are substrings of other datasource names. For example:\r\n\r\n<img width=\"1671\" alt=\"screen shot 2019-02-27 at 4 50 51 pm\" src=\"https://user-images.githubusercontent.com/1577461/53533768-f3c5ea80-3ab0-11e9-91d8-8e14f3bd8fe7.png\">\r\n\r\nIf we click the 'segments' link we are taken to the 'segments' view, which has segments from other datasources:\r\n\r\n<img width=\"1659\" alt=\"screen shot 2019-02-27 at 4 50 24 pm\" src=\"https://user-images.githubusercontent.com/1577461/53533830-35569580-3ab1-11e9-86c4-a88874efae82.png\">\r\n\r\nThis is due to the system tables query it is using issuing a substring search instead of an exact match:\r\n```\r\n\"SELECT \"segment_id\", \"datasource\", \"start\", \"end\", \"size\", \"version\", \"partition_num\", \"num_replicas\", \"num_rows\", \"is_published\", \"is_available\", \"is_realtime\", \"payload\"\u21b5FROM sys.segments\u21b5WHERE \"datasource\" LIKE '%twitter%'\u21b5ORDER BY \"start\" DESC\u21b5LIMIT 50\r\n```\r\n\r\nI'm not certain the best way to fix from a UI sense, as the substring search probably makes sense as a search box input, but it seems necessary to be able to exactly filter by datasource as well.\r\n\r\n"}, {"user": "justinborromeo", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2019-02-27 22:32:14", "title": "Inconsistent KafkaConsumer ConcurrentModificationException upon Supervisor Shutdown", "url": "https://github.com/apache/druid/issues/7153", "closed": "2019-05-07 16:39:51", "ttf": 68.00027777777778, "commitsDetails": [], "body": "# Affected Version\r\n\r\n`master` branch.\r\n\r\n# Description\r\n\r\nI encountered the following error message twice when performing the following sequence of actions:\r\n1. Create a supervisor using the example spec provided by the [docs](http://druid.io/docs/latest/development/extensions-core/kafka-ingestion.html) and the `/druid/indexer/v1/supervisor` endpoint\r\n2. Get a list of all supervisors (GET `/druid/indexer/v1/supervisor?full`)\r\n3. Terminate all supervisors (POST `/druid/indexer/v1/supervisor/terminateAll`)\r\n\r\nIt doesn't consistently occur as it only appeared twice in ~10 runs.  @dclim has mentioned that he's seen that exception before but not when stopping a supervisor.\r\n\r\nWithin the context of stopping a supervisor, it seems as if the bug's impact is minimal since the supervisor is stopped and its existence deleted from the metadata store.\r\n\r\nCluster Info:\r\n- Running all nodes and Zookeeper on a Macbook Pro\r\n- Overlord was running in Intellij\r\n- MySQL used as a metadata store\r\n\r\nConfigurations:\r\n- The default config was used except `druid-kafka-indexing-service` was included in the extensions loadList\r\n\r\n# Log\r\n```\r\n2019-02-27T21:15:58,878 INFO [qtp1788582153-83] org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor - Beginning shutdown of [KafkaSupervisor-metrics-kafka]\r\n2019-02-27T21:15:58,879 ERROR [qtp1788582153-83] org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor - Exception stopping [KafkaSupervisor-metrics-kafka]: {class=org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor, exceptionType=class java.util.ConcurrentModificationException, exceptionMessage=KafkaConsumer is not safe for multi-threaded access}\r\njava.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access\r\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:2201) ~[?:?]\r\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:2090) ~[?:?]\r\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:2045) ~[?:?]\r\n\tat org.apache.druid.indexing.kafka.KafkaRecordSupplier.close(KafkaRecordSupplier.java:168) ~[?:?]\r\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.stop(SeekableStreamSupervisor.java:638) ~[classes/:?]\r\n\tat org.apache.druid.indexing.overlord.supervisor.SupervisorManager.possiblyStopAndRemoveSupervisorInternal(SupervisorManager.java:249) ~[classes/:?]\r\n\tat org.apache.druid.indexing.overlord.supervisor.SupervisorManager.lambda$stopAndRemoveAllSupervisors$0(SupervisorManager.java:111) ~[classes/:?]\r\n\tat java.util.concurrent.ConcurrentHashMap$KeySetView.forEach(ConcurrentHashMap.java:4649) ~[?:1.8.0_191]\r\n\tat org.apache.druid.indexing.overlord.supervisor.SupervisorManager.stopAndRemoveAllSupervisors(SupervisorManager.java:111) ~[classes/:?]\r\n\tat org.apache.druid.indexing.overlord.supervisor.SupervisorResource.lambda$terminateAll$8(SupervisorResource.java:286) ~[classes/:?]\r\n\tat org.apache.druid.indexing.overlord.supervisor.SupervisorResource.asLeaderWithSupervisorManager(SupervisorResource.java:371) [classes/:?]\r\n\tat org.apache.druid.indexing.overlord.supervisor.SupervisorResource.terminateAll(SupervisorResource.java:284) [classes/:?]\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_191]\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_191]\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_191]\r\n\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_191]\r\n\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409) [jersey-servlet-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558) [jersey-servlet-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733) [jersey-servlet-1.19.3.jar:1.19.3]\r\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790) [javax.servlet-api-3.1.0.jar:3.1.0]\r\n\tat com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:286) [guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:276) [guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:181) [guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91) [guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85) [guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:120) [guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:135) [guice-servlet-4.1.0.jar:?]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.apache.druid.server.http.RedirectFilter.doFilter(RedirectFilter.java:71) [classes/:?]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.apache.druid.server.security.PreResponseAuthorizationCheckFilter.doFilter(PreResponseAuthorizationCheckFilter.java:82) [classes/:?]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.apache.druid.server.security.AllowOptionsResourceFilter.doFilter(AllowOptionsResourceFilter.java:75) [classes/:?]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.apache.druid.server.security.AllowAllAuthenticator$1.doFilter(AllowAllAuthenticator.java:84) [classes/:?]\r\n\tat org.apache.druid.server.security.AuthenticationWrappingFilter.doFilter(AuthenticationWrappingFilter.java:59) [classes/:?]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.apache.druid.server.security.SecuritySanityCheckFilter.doFilter(SecuritySanityCheckFilter.java:86) [classes/:?]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1253) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1155) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:724) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:61) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.Server.handle(Server.java:531) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:352) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:281) [jetty-io-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102) [jetty-io-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118) [jetty-io-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:760) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:678) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_191]\r\n2019-02-27T21:15:58,885 INFO [qtp1788582153-83] org.apache.druid.java.util.emitter.core.LoggingEmitter - Event [{\"feed\":\"alerts\",\"timestamp\":\"2019-02-27T21:15:58.883Z\",\"service\":\"overlord\",\"host\":\"localhost:8090\",\"version\":\"\",\"severity\":\"component-failure\",\"description\":\"Exception stopping [KafkaSupervisor-metrics-kafka]\",\"data\":{\"class\":\"org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor\",\"exceptionType\":\"java.util.ConcurrentModificationException\",\"exceptionMessage\":\"KafkaConsumer is not safe for multi-threaded access\",\"exceptionStackTrace\":\"java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access\\n\\tat org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:2201)\\n\\tat org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:2090)\\n\\tat org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:2045)\\n\\tat org.apache.druid.indexing.kafka.KafkaRecordSupplier.close(KafkaRecordSupplier.java:168)\\n\\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.stop(SeekableStreamSupervisor.java:638)\\n\\tat org.apache.druid.indexing.overlord.supervisor.SupervisorManager.possiblyStopAndRemoveSupervisorInternal(SupervisorManager.java:249)\\n\\tat org.apache.druid.indexing.overlord.supervisor.SupervisorManager.lambda$stopAndRemoveAllSupervisors$0(SupervisorManager.java:111)\\n\\tat java.util.concurrent.ConcurrentHashMap$KeySetView.forEach(ConcurrentHashMap.java:4649)\\n\\tat org.apache.druid.indexing.overlord.supervisor.SupervisorManager.stopAndRemoveAllSupervisors(SupervisorManager.java:111)\\n\\tat org.apache.druid.indexing.overlord.supervisor.SupervisorResource.lambda$terminateAll$8(SupervisorResource.java:286)\\n\\tat org.apache.druid.indexing.overlord.supervisor.SupervisorResource.asLeaderWithSupervisorManager(SupervisorResource.java:371)\\n\\tat org.apache.druid.indexing.overlord.supervisor.SupervisorResource.terminateAll(SupervisorResource.java:284)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\\n\\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\\n\\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\\n\\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)\\n\\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\\n\\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\\n\\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\\n\\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)\\n\\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)\\n\\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)\\n\\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:286)\\n\\tat com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:276)\\n\\tat com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:181)\\n\\tat com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)\\n\\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)\\n\\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:120)\\n\\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:135)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.apache.druid.server.http.RedirectFilter.doFilter(RedirectFilter.java:71)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.apache.druid.server.security.PreResponseAuthorizationCheckFilter.doFilter(PreResponseAuthorizationCheckFilter.java:82)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.apache.druid.server.security.AllowOptionsResourceFilter.doFilter(AllowOptionsResourceFilter.java:75)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.apache.druid.server.security.AllowAllAuthenticator$1.doFilter(AllowAllAuthenticator.java:84)\\n\\tat org.apache.druid.server.security.AuthenticationWrappingFilter.doFilter(AuthenticationWrappingFilter.java:59)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.apache.druid.server.security.SecuritySanityCheckFilter.doFilter(SecuritySanityCheckFilter.java:86)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1253)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1155)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:724)\\n\\tat org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:61)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:531)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:352)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:281)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:760)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:678)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n\"}}]\r\n```\r\n\r\n\r\n"}, {"user": "chenhaiyan", "commits": {}, "labels": ["Bug"], "created": "2019-02-27 07:36:27", "title": "hadoop overwriteFiles parameter doesn't work", "url": "https://github.com/apache/druid/issues/7150", "closed": "2019-05-07 16:42:58", "ttf": 69.00027777777778, "commitsDetails": [], "body": "I load hdfs json file to Druid,the load json is below\r\n`{\r\n  \"type\" : \"index_hadoop\",\r\n  \"spec\" : {\r\n    \"dataSchema\" : {\r\n      \"dataSource\" : \"up_stu_tag\",\r\n      \"parser\" : {\r\n        \"type\" : \"hadoopyString\",\r\n        \"parseSpec\" : {\r\n          \"format\" : \"json\",\r\n          \"dimensionsSpec\" : {\r\n            \"dimensions\" : [\r\n              \"row_key\",\"les_study_state\",\r\n              \"pay_latest_create_date\",\r\n              \"pay_latest_deal_date\",\r\n              \"pay_latest_hours\",\r\n              \"pay_latest_bu\",\r\n              \"pay_way_prefer\",\"pay_way_prefer_id\",\r\n              \"is_renew\",\"is_paid\",\"pay_freq\",\r\n              \"create_td\",\"pay_cum_order_count\",\"pay_first_deal_time\",\r\n              \"subject_edition_id\",\"stu_prov_code\",\"stu_city_code\",\"stu_area_code\",\r\n              \"stu_city_level\",\"stu_gender_code\",\"stu_fill_student_files\",\"is_bound_1v1\",\r\n              \"is_bound_kid\",\"is_bound_pl\",\"is_subscribe_1v1\",\r\n              \"is_subscribe_kid\",\"is_subscribe_pl\",\"first_les_time\",\r\n              \"pay_first_money\",\"pay_first_hours\",\"pay_cum_money\",\r\n              \"pay_cum_hours\",\"les_regu_sub_num\",\"les_deduct_hours\"\r\n             ]\r\n          },\r\n          \"timestampSpec\": {\r\n           \"column\": \"timestamp\",\r\n           \"format\": \"auto\",\r\n           \"missingValue\" : \"2019-02-24\"\r\n          }\r\n        }\r\n      },\r\n      \"metricsSpec\" : [],\r\n      \"granularitySpec\" : {\r\n        \"type\" : \"uniform\",\r\n        \"segmentGranularity\" : \"day\",\r\n        \"queryGranularity\" : \"none\",\r\n        \"intervals\" : [\"2019-02-24/2019-02-25\"],\r\n        \"rollup\" : false\r\n      }\r\n    },\r\n    \"ioConfig\" : {\r\n      \"type\" : \"hadoop\",\r\n      \"inputSpec\" : {\r\n        \"type\" : \"static\",\r\n        \"paths\" : \"hdfs://10.80.230.158:9000/druid/hbase_data/up_stu_tag_test/\"\r\n      }\r\n    },\r\n    \"tuningConfig\" : {\r\n      \"type\" : \"hadoop\",\r\n      \"partitionsSpec\" : {\r\n        \"type\" : \"hashed\",\r\n        \"targetPartitionSize\" : 4000000\r\n      },\r\n      \"maxParseExceptions\" : 1000,\r\n      \"targetPartitionSize\" : 4000000,\r\n      **\"overwriteFiles\" : false,**\r\n      \"forceExtendableShardSpecs\" : true,\r\n      \"logParseExceptions\":true,\r\n      \"jobProperties\" : {\r\n        \"mapreduce.job.maps\" : 10,\r\n        \"mapreduce.job.reduces\" : 2,\r\n        \"mapreduce.map.cpu.vcores\" : 4,\r\n        \"mapreduce.reduce.cpu.vcores\":2,\r\n        \"mapreduce.map.memory.mb\" : 5120,\r\n        \"mapreduce.reduce.memory.mb\" : 1024\r\n      }\r\n    }\r\n  }\r\n}\r\n`\r\nno matter overwriteFiles is false or true, when I load another hdfs file to the same interval,it will always overwrite the older files. How can I append data like the native batch parameter when I load hdfs file to druid ? Is this a bug or something else that I don't know?\r\n"}, {"user": "hellobabygogo", "commits": {}, "labels": ["Bug"], "created": "2019-02-26 05:13:55", "title": "not in subquery and except", "url": "https://github.com/apache/druid/issues/7142", "closed": "2019-02-26 05:15:28", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "except operator can cause npe\uff0chow to fit it?\r\n\r\n# Affected Version\r\n\r\n0.12.3\r\n\r\n# Description\r\n\r\n"}, {"user": "mihai-cazacu-adswizz", "commits": {}, "labels": ["Bug"], "created": "2019-02-21 09:55:54", "title": "[materialized view] The last day of month and the last 24h are not ingested by the Supervisor", "url": "https://github.com/apache/druid/issues/7119", "closed": "2019-03-04 06:31:34", "ttf": 10.000277777777777, "commitsDetails": [], "body": "Based on what @zhangxinyu1 [said](https://github.com/apache/incubator-druid/issues/6945#issuecomment-459635347):\r\n\r\n> The timeline of derivativeDataSource is expected to be the same with baseDataSource's. Your can get the timeline of a dataSource from coordinator UI.\r\n\r\nand inspecting the timeline, the derivative data source has one day missing (the last day of the month).\r\n\r\nThe base data source:\r\n![base-data-source](https://user-images.githubusercontent.com/45362709/52112263-a5870e00-260e-11e9-9722-a38192dca60c.png)\r\n\r\nand the derivative one:\r\n![derivative-data-source](https://user-images.githubusercontent.com/45362709/52112328-d36c5280-260e-11e9-8e58-cbeb3211f5bb.png)\r\n\r\nI have observed the same behavior on every view that I've created, no matter of the base data source and its type (`kafka` or `index_hadoop`).\r\n\r\nHere you can find a `kafka` ingestion spec:\r\n\r\n```\r\n{\r\n  \"type\": \"kafka\",\r\n  \"dataSchema\": {\r\n    \"dataSource\": \"some-data-source\",\r\n    \"parser\": {\r\n      \"type\": \"string\",\r\n      \"parseSpec\": {\r\n        \"format\": \"json\",\r\n        \"timestampSpec\": {\r\n          \"column\": \"timestamp\",\r\n          \"format\": \"auto\"\r\n        },\r\n        \"dimensionsSpec\": {\r\n          \"dimensions\": [],\r\n          \"dimensionExclusions\": [\r\n            ...\r\n          ],\r\n          \"spatialDimensions\": []\r\n        }\r\n      }\r\n    },\r\n    \"metricsSpec\": [\r\n      ...\r\n    ],\r\n    \"granularitySpec\": {\r\n      \"type\": \"uniform\",\r\n      \"segmentGranularity\": \"HOUR\",\r\n      \"queryGranularity\": \"HOUR\",\r\n      \"rollup\": true,\r\n      \"intervals\": null\r\n    },\r\n    \"transformSpec\": {\r\n      \"filter\": null,\r\n      \"transforms\": []\r\n    }\r\n  },\r\n  \"tuningConfig\": {\r\n    \"type\": \"kafka\",\r\n    \"maxRowsInMemory\": 40000,\r\n    \"maxRowsPerSegment\": 5000000,\r\n    \"intermediatePersistPeriod\": \"PT10M\",\r\n    \"basePersistDirectory\": \"/mnt/druid/tmp/1550607663100-0\",\r\n    \"maxPendingPersists\": 0,\r\n    \"indexSpec\": {\r\n      \"bitmap\": {\r\n        \"type\": \"concise\"\r\n      },\r\n      \"dimensionCompression\": \"lz4\",\r\n      \"metricCompression\": \"lz4\",\r\n      \"longEncoding\": \"longs\"\r\n    },\r\n    \"buildV9Directly\": true,\r\n    \"reportParseExceptions\": false,\r\n    \"handoffConditionTimeout\": 0,\r\n    \"resetOffsetAutomatically\": true,\r\n    \"segmentWriteOutMediumFactory\": null,\r\n    \"workerThreads\": null,\r\n    \"chatThreads\": null,\r\n    \"chatRetries\": 20,\r\n    \"httpTimeout\": \"PT60S\",\r\n    \"shutdownTimeout\": \"PT180S\",\r\n    \"offsetFetchPeriod\": \"PT30S\"\r\n  },\r\n  \"ioConfig\": {\r\n    \"topic\": \"some-topic\",\r\n    \"replicas\": 1,\r\n    \"taskCount\": 12,\r\n    \"taskDuration\": \"PT3600S\",\r\n    \"consumerProperties\": {\r\n      \"bootstrap.servers\": \"kafka-broker.site.com:9092\",\r\n      \"group.id\": \"someGroupId\",\r\n      \"auto.offset.reset\": \"latest\",\r\n      \"max.partition.fetch.bytes\": \"4000000\"\r\n    },\r\n    \"startDelay\": \"PT0S\",\r\n    \"period\": \"PT30S\",\r\n    \"useEarliestOffset\": false,\r\n    \"completionTimeout\": \"PT1800S\",\r\n    \"lateMessageRejectionPeriod\": null,\r\n    \"earlyMessageRejectionPeriod\": null,\r\n    \"skipOffsetGaps\": false\r\n  },\r\n  \"context\": null\r\n}\r\n```\r\n\r\nand a `index_hadoop` one:\r\n\r\n```\r\n{\r\n  \"type\": \"index_hadoop\",\r\n  \"spec\": {\r\n    \"dataSchema\": {\r\n      \"dataSource\": \"some-data-source\",\r\n      \"metricsSpec\": [\r\n        ...\r\n      ],\r\n      \"granularitySpec\": {\r\n        \"type\": \"uniform\",\r\n        \"segmentGranularity\": \"day\",\r\n        \"queryGranularity\": \"hour\",\r\n        \"intervals\": [\r\n          \"$INTERVALS\"\r\n        ]\r\n      },\r\n      \"parser\": {\r\n        \"parseSpec\": {\r\n          \"format\": \"json\",\r\n          \"timestampSpec\": {\r\n            \"column\": \"timestamp\",\r\n            \"format\": \"auto\"\r\n          },\r\n          \"dimensionsSpec\": {\r\n            \"dimensions\": [],\r\n            \"dimensionExclusions\": [\r\n              ...\r\n            ],\r\n            \"spatialDimensions\": []\r\n          }\r\n        }\r\n      }\r\n    },\r\n    \"ioConfig\": {\r\n      \"type\": \"hadoop\",\r\n      \"inputSpec\": {\r\n        \"type\": \"static\",\r\n        \"paths\": \"s3://some/path/\"\r\n      }\r\n    },\r\n    \"tuningConfig\": {\r\n      \"type\": \"hadoop\",\r\n      \"partitionsSpec\": {\r\n        \"type\": \"hashed\",\r\n        \"targetPartitionSize\": 10000000\r\n      },\r\n      \"maxRowsInMemory\": 75000,\r\n      \"forceExtendableShardSpecs\": true,\r\n      \"jobProperties\": {\r\n        \"mapreduce.job.classloader\": \"true\",\r\n        \"fs.s3.impl\": \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\",\r\n        \"fs.s3n.impl\": \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\",\r\n        \"io.compression.codecs\": \"org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec\",\r\n        \"mapreduce.reduce.shuffle.memory.limit.percent\": \"0.15\",\r\n        \"mapreduce.reduce.shuffle.input.buffer.percent\": \"0.5\"\r\n      }\r\n    }\r\n  },\r\n  \"hadoopDependencyCoordinates\": [\r\n    \"org.apache.hadoop:hadoop-client:2.7.3\"\r\n  ]\r\n}\r\n```"}, {"user": "Caroline1000", "commits": {}, "labels": ["Bug"], "created": "2019-02-20 23:03:35", "title": "Segment metadata queries fail on string columns with disabled bitmaps", "url": "https://github.com/apache/druid/issues/7114", "closed": "2019-02-26 19:27:42", "ttf": 5.000277777777778, "commitsDetails": [], "body": "# Bug Report\r\n\r\nSegment metadata queries return an error on a string column with bitmaps disabled:\r\n`\"errorMessage\": \"error:string_no_bitmap\"`\r\n\r\nRecent support for disabling bitmaps (https://github.com/apache/incubator-druid/pull/5402) didn't include modification to incubator-druid/processing/src/main/java/org/apache/druid/query/metadata/SegmentAnalyzer.java.\r\n\r\nanalyzeStringColumn in SegmentAnalyzer.java should be modified to allow disabled bitmaps:\r\n\r\nhttps://github.com/apache/incubator-druid/blob/676f5e6d7f184101b8763e4249b18b237bbe0ec7/processing/src/main/java/org/apache/druid/query/metadata/SegmentAnalyzer.java#L202\r\n\r\n\r\n\r\n"}, {"user": "pdeva", "commits": {}, "labels": ["Bug"], "created": "2019-02-20 14:38:53", "title": "quantiles-sketch plugin in 0.13.0 does not implement latest interface, results in runtime exception", "url": "https://github.com/apache/druid/issues/7111", "closed": "2019-02-20 15:28:30", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "druid 0.13.0 \r\nseeing this when the KIS task wants to persist:\r\n\r\n\r\n```\r\n2019-02-20 08:34:30,620 ERROR o.a.d.i.k.IncrementalPublishingKafkaIndexTaskRunner [task-runner-0-priority-0] Encountered exception while running task.\r\njava.util.concurrent.ExecutionException: org.apache.druid.java.util.common.UOE: [org.apache.druid.query.aggregation.datasketches.quantiles.DoublesSketchAggregatorFactory] does not implement makeAggregateCombiner()\r\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299) ~[guava-16.0.1.jar:?]\r\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286) ~[guava-16.0.1.jar:?]\r\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) ~[guava-16.0.1.jar:?]\r\n\tat org.apache.druid.indexing.kafka.IncrementalPublishingKafkaIndexTaskRunner.runInternal(IncrementalPublishingKafkaIndexTaskRunner.java:661) ~[druid-kafka-indexing-service-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.indexing.kafka.IncrementalPublishingKafkaIndexTaskRunner.run(IncrementalPublishingKafkaIndexTaskRunner.java:232) [druid-kafka-indexing-service-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.indexing.kafka.KafkaIndexTask.run(KafkaIndexTask.java:210) [druid-kafka-indexing-service-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:421) [druid-indexing-service-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:393) [druid-indexing-service-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_181]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_181]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_181]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]\r\nCaused by: org.apache.druid.java.util.common.UOE: [org.apache.druid.query.aggregation.datasketches.quantiles.DoublesSketchAggregatorFactory] does not implement makeAggregateCombiner()\r\n\tat org.apache.druid.query.aggregation.AggregatorFactory.makeAggregateCombiner(AggregatorFactory.java:79) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.query.aggregation.AggregatorFactory.makeNullableAggregateCombiner(AggregatorFactory.java:93) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.segment.RowCombiningTimeAndDimsIterator.lambda$new$1(RowCombiningTimeAndDimsIterator.java:120) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat java.util.Arrays.setAll(Arrays.java:4698) ~[?:1.8.0_181]\r\n\tat org.apache.druid.segment.RowCombiningTimeAndDimsIterator.<init>(RowCombiningTimeAndDimsIterator.java:120) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.segment.IndexMergerV9.lambda$merge$2(IndexMergerV9.java:909) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.segment.IndexMergerV9.makeMergedTimeAndDimsIterator(IndexMergerV9.java:1031) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.segment.IndexMergerV9.makeIndexFiles(IndexMergerV9.java:179) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.segment.IndexMergerV9.merge(IndexMergerV9.java:914) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.segment.IndexMergerV9.mergeQueryableIndex(IndexMergerV9.java:832) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.segment.IndexMergerV9.mergeQueryableIndex(IndexMergerV9.java:810) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.mergeAndPush(AppenderatorImpl.java:719) ~[druid-server-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.lambda$push$1(AppenderatorImpl.java:623) ~[druid-server-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat com.google.common.util.concurrent.Futures$1.apply(Futures.java:713) ~[guava-16.0.1.jar:?]\r\n\tat com.google.common.util.concurrent.Futures$ChainingListenableFuture.run(Futures.java:861) ~[guava-16.0.1.jar:?]\r\n\t... 3 more\r\n```"}, {"user": "hnakamor", "commits": {}, "labels": ["Bug"], "created": "2019-02-20 06:12:38", "title": "Data ingestion fails on Quantiles DoublesSketch containing NULL When usingDefaultValueForNull = false", "url": "https://github.com/apache/druid/issues/7108", "closed": "2019-04-09 01:01:31", "ttf": 47.000277777777775, "commitsDetails": [], "body": "Hi All,\r\n\r\nIn 0.13.0-incubating, setting usingDefaultValueForNull = false and using a quantilesDoubleSketch aggregator at ingestion containing NULL will result in the following exception during index task:\r\n\r\n```\r\n2019-02-18T10:11:33,248 ERROR [task-runner-0-priority-0] org.apache.druid.indexing.common.task.IndexTask - Encountered exception in BUILD_SEGMENTS.\r\njava.lang.StringIndexOutOfBoundsException: String index out of range: 0\r\n    at java.lang.String.charAt(String.java:658) ~[?:1.8.0_191]\r\n    at org.apache.druid.query.aggregation.datasketches.quantiles.DoublesSketchComplexMetricSerde$1.extractValue(DoublesSketchComplexMetricSerde.java:78) ~[?:?]\r\n    at org.apache.druid.segment.incremental.IncrementalIndex$1IncrementalIndexInputRowColumnSelectorFactory$1.getObject(IncrementalIndex.java:197) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n    at org.apache.druid.query.aggregation.datasketches.quantiles.DoublesSketchMergeAggregator.aggregate(DoublesSketchMergeAggregator.java:42) ~[?:?]\r\n    at org.apache.druid.segment.incremental.OnheapIncrementalIndex.doAggregate(OnheapIncrementalIndex.java:253) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n    at org.apache.druid.segment.incremental.OnheapIncrementalIndex.addToFacts(OnheapIncrementalIndex.java:167) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n    at org.apache.druid.segment.incremental.IncrementalIndex.add(IncrementalIndex.java:610) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n    at org.apache.druid.segment.realtime.plumber.Sink.add(Sink.java:181) ~[druid-server-0.13.0-incubating.jar:0.13.0-incubating]\r\n    at org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.add(AppenderatorImpl.java:246) ~[druid-server-0.13.0-incubating.jar:0.13.0-incubating]\r\n    at org.apache.druid.segment.realtime.appenderator.BaseAppenderatorDriver.append(BaseAppenderatorDriver.java:403) ~[druid-server-0.13.0-incubating.jar:0.13.0-incubating]\r\n    at org.apache.druid.segment.realtime.appenderator.BatchAppenderatorDriver.add(BatchAppenderatorDriver.java:106) ~[druid-server-0.13.0-incubating.jar:0.13.0-incubating]\r\n    at org.apache.druid.indexing.common.task.IndexTask.generateAndPublishSegments(IndexTask.java:1032) ~[druid-indexing-service-0.13.0-incubating.jar:0.13.0-incubating]\r\n    at org.apache.druid.indexing.common.task.IndexTask.run(IndexTask.java:466) [druid-indexing-service-0.13.0-incubating.jar:0.13.0-incubating]\r\n    at org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:421) [druid-indexing-service-0.13.0-incubating.jar:0.13.0-incubating]\r\n    at org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:393) [druid-indexing-service-0.13.0-incubating.jar:0.13.0-incubating]\r\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_191]\r\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_191]\r\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_191]\r\n    at java.lang.Thread.run(Thread.java:748) [?:1.8.0_191]\r\n```\r\n\r\nIngest data:\r\n\r\n```\r\n1549929600000\t1\t\t1\r\n1549929600000\t2\t\t1\r\n1549929600000\t3\t0.332583716\t1\r\n1549929600000\t4\t0.47188857\t1\r\n1549929700000\t1\t\t1\r\n1549929700000\t2\t0.39832398\t1\r\n1549929700000\t3\t\t1\r\n1549929700000\t4\t0.366082674\t1\r\n```\r\n\r\nIngest Setting:\r\n\r\n```\r\n      \"parser\" : {\r\n        \"type\" : \"string\",\r\n        \"parseSpec\" : {\r\n          \"format\" : \"tsv\",\r\n          \"columns\": [\"timestamp\", \"index\", \"value\", \"extra\"],\r\n          \"timestampSpec\": {\r\n            \"column\": \"timestamp\",\r\n            \"format\": \"millis\"\r\n          },\r\n          \"dimensionsSpec\" : {\r\n            \"dimensions\" : [\"index\"]\r\n          }\r\n        }\r\n      },\r\n      \"metricsSpec\" : [\r\n        {\"type\": \"quantilesDoublesSketch\", \"name\": \"sketch\", \"fieldName\": \"value\", \"k\": 128},\r\n        {\"type\": \"count\", \"name\": \"count\"}\r\n      ],\r\n```"}, {"user": "ferristseng", "commits": {}, "labels": ["Area - Batch Ingestion", "Area - Null Handling", "Bug"], "created": "2019-02-11 19:15:33", "title": "[ERROR] Issue with Hadoop Indexer and null numeric dimensions", "url": "https://github.com/apache/druid/issues/7050", "closed": "2019-05-07 21:06:06", "ttf": 85.00027777777778, "commitsDetails": [], "body": "I'm running into an issue where the Hadoop Indexer is still treating null numeric dimensions as 0 in `0.13.0-incubating`, even when the system property `druid.generic.useDefaultValueForNull` is set to `false`. \r\n\r\nI dug a little bit into the code, and noticed that the Hadoop Indexer was not updated to treat null numeric dimensions in a SQL-compatible way:\r\n\r\nhttps://github.com/apache/incubator-druid/blob/7e48593b5780c3bc4852d183dc473be283c84d25/indexing-hadoop/src/main/java/org/apache/druid/indexer/InputRowSerde.java#L179\r\n\r\nI made a fix for the issue here:\r\n\r\nhttps://github.com/apache/incubator-druid/pull/7020"}, {"user": "justinborromeo", "commits": {}, "labels": ["Area - Testing", "Bug", "Flaky test"], "created": "2019-02-06 22:01:00", "title": "Flaky ITBasicAuthConfigurationTest", "url": "https://github.com/apache/druid/issues/7021", "closed": "2019-12-20 01:38:56", "ttf": 316.0002777777778, "commitsDetails": [], "body": "I noticed that the `testAuthConfiguration` test in `ITBasicAuthConfigurationTest` experiences transient failures.  Based on the stack trace, it seems that the attempted GET request to the router status endpoint returned a 403 (which might be indicative of an actual issue since it's an auth test?).\r\n\r\nStack trace:\r\n\r\n`testAuthConfiguration(org.apache.druid.tests.security.ITBasicAuthConfigurationTest)  Time elapsed: 22.143 s  <<< FAILURE!\r\norg.apache.druid.java.util.common.ISE: Error while making request to url[http://127.0.0.1:8888/status] status[403 Forbidden] content[{\"Access-Check-Result\":\"Allowed:false, Message:\"}]\r\n\tat org.apache.druid.tests.security.ITBasicAuthConfigurationTest.makeRequest(ITBasicAuthConfigurationTest.java:362)\r\n\tat org.apache.druid.tests.security.ITBasicAuthConfigurationTest.checkNodeAccess(ITBasicAuthConfigurationTest.java:297)\r\n\tat org.apache.druid.tests.security.ITBasicAuthConfigurationTest.testAuthConfiguration(ITBasicAuthConfigurationTest.java:165)`\r\n\r\nThe full log can be found at https://api.travis-ci.org/v3/job/489746498/log.txt."}, {"user": "jihoonson", "commits": {}, "labels": ["Area - Testing", "Bug", "Flaky test"], "created": "2019-02-06 01:50:31", "title": "Flaky ITRealtimeIndexTaskTest", "url": "https://github.com/apache/druid/issues/7017", "closed": "2019-03-12 03:25:25", "ttf": 34.000277777777775, "commitsDetails": [], "body": "```[ERROR] Tests run: 8, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1,419.876 s <<< FAILURE! - in TestSuite\r\n[ERROR] testRealtimeIndexTask(org.apache.druid.tests.indexer.ITRealtimeIndexTaskTest)  Time elapsed: 182.638 s  <<< FAILURE!\r\njava.lang.RuntimeException: java.util.concurrent.ExecutionException: org.jboss.netty.channel.ChannelException: Faulty channel in resource pool\r\n\tat org.apache.druid.tests.indexer.ITRealtimeIndexTaskTest.postEvents(ITRealtimeIndexTaskTest.java:119)\r\n\tat org.apache.druid.tests.indexer.ITRealtimeIndexTaskTest.testRealtimeIndexTask(ITRealtimeIndexTaskTest.java:57)\r\nCaused by: java.util.concurrent.ExecutionException: org.jboss.netty.channel.ChannelException: Faulty channel in resource pool\r\n\tat org.apache.druid.tests.indexer.ITRealtimeIndexTaskTest.postEvents(ITRealtimeIndexTaskTest.java:119)\r\n\tat org.apache.druid.tests.indexer.ITRealtimeIndexTaskTest.testRealtimeIndexTask(ITRealtimeIndexTaskTest.java:57)\r\nCaused by: org.jboss.netty.channel.ChannelException: Faulty channel in resource pool\r\n\tat org.apache.druid.tests.indexer.ITRealtimeIndexTaskTest.postEvents(ITRealtimeIndexTaskTest.java:119)\r\n\tat org.apache.druid.tests.indexer.ITRealtimeIndexTaskTest.testRealtimeIndexTask(ITRealtimeIndexTaskTest.java:57)\r\nCaused by: org.jboss.netty.channel.ChannelException: Failed to handshake with host[https://127.0.0.1:8101]\r\nCaused by: java.nio.channels.ClosedChannelException\r\n```"}, {"user": "gianm", "commits": {}, "labels": ["Bug"], "created": "2019-02-05 16:26:18", "title": "Integer overflow in CompactionTask", "url": "https://github.com/apache/druid/issues/7010", "closed": "2019-02-28 03:07:38", "ttf": 22.00027777777778, "commitsDetails": [], "body": "Got this when running a CompactionTask, it looks like it was trying to compute `maxRowsPerSegment`. Probably it should have some reasonable maximum.\r\n\r\n```\r\n2019-02-02T02:56:28,778 ERROR [task-runner-0-priority-0] org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner - Exception while running task[AbstractTask{id='compact_ds-1_2019-02-02T02:56:01.131Z', groupId='compact_ds-1_2019-02-02T02:56:01.131Z', taskResource=TaskResource{availabilityGroup='compact_ds-1_2019-02-02T02:56:01.131Z', requiredCapacity=1}, dataSource='ds-1', context={priority=25}}]\r\njava.lang.ArithmeticException: integer overflow\r\n\tat java.lang.Math.toIntExact(Math.java:1011) ~[?:1.8.0_131]\r\n\tat org.apache.druid.indexing.common.task.CompactionTask$PartitionConfigurationManager.computeTuningConfig(CompactionTask.java:712) ~[druid-indexing-service-0.13.0-incubating-iap5.jar:0.13.0-incubating-iap5]\r\n\tat org.apache.druid.indexing.common.task.CompactionTask.createIngestionSchema(CompactionTask.java:325) ~[druid-indexing-service-0.13.0-incubating-iap5.jar:0.13.0-incubating-iap5]\r\n\tat org.apache.druid.indexing.common.task.CompactionTask.run(CompactionTask.java:240) ~[druid-indexing-service-0.13.0-incubating-iap5.jar:0.13.0-incubating-iap5]\r\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:422) [druid-indexing-service-0.13.0-incubating-iap5.jar:0.13.0-incubating-iap5]\r\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:394) [druid-indexing-service-0.13.0-incubating-iap5.jar:0.13.0-incubating-iap5]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_131]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]\r\n```"}, {"user": "yurmix", "commits": {}, "labels": ["Area - Documentation", "Bug"], "created": "2019-02-04 19:53:46", "title": "A removed documentation page appears as part of the latest documentation", "url": "https://github.com/apache/druid/issues/6998", "closed": "2019-03-09 23:16:24", "ttf": 33.000277777777775, "commitsDetails": [], "body": "The following documentation page: [Production Cluster Configuration](http://druid.io/docs/latest/configuration/production-cluster.html) was removed in version 0.9.2 (The PR: https://github.com/apache/incubator-druid/pull/3188).\r\n\r\nHowever, this page, which will appear in search results (i.e. [\"druid\"  + \"Production Cluster Configuration\"](https://www.google.com/search?q=%22druid%22+%22Production+Cluster+Configuration%22)), is identified as part of the latest documentation.\r\n\r\nThe bug might lay in not removing deleted pages. If you look at the \"Docs\" link in the top banner, it points to version 0.9.2, so this page was probably not have been rendered since 2016."}, {"user": "glasser", "commits": {"a81b1b8c9c9ccf60ea28a2c2ffeedbc8dc19de5e": {"commitGHEventType": "referenced", "commitUser": "jihoonson"}, "a8ae7156b1751fdc2ce2a978f9728cbf79b0ed2c": {"commitGHEventType": "referenced", "commitUser": "jihoonson"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "Behavior of index_parallel with appendToExisting=false and no bucketIntervals in GranularitySpec is surprising", "numCommits": 0, "created": "2019-02-02 03:57:19", "closed": "2019-02-20 18:54:27", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/6989", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 18.00027777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractITBatchIndexTest.java", "deletions": 5, "insertions": 20, "lines": 25}, {"filePath": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "deletions": 16, "insertions": 54, "lines": 70}, {"filePath": "integration-tests/src/test/resources/indexer/wikipedia_parallel_reindex_queries.json", "deletions": 0, "insertions": 18, "lines": 18}, {"filePath": "indexing-service/src/test/java/org/apache/druid/indexing/common/task/IngestionTestBase.java", "deletions": 0, "insertions": 5, "lines": 5}, {"filePath": "indexing-service/src/main/java/org/apache/druid/indexing/common/Counters.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "docs/content/ingestion/ingestion-spec.md", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "docs/content/ingestion/native_tasks.md", "deletions": 0, "insertions": 27, "lines": 27}, {"filePath": "indexing-service/src/test/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTaskTest.java", "deletions": 2, "insertions": 26, "lines": 28}, {"filePath": "integration-tests/src/test/java/org/apache/druid/tests/indexer/ITIndexerTest.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "integration-tests/src/test/java/org/apache/druid/tests/indexer/ITParallelIndexTest.java", "deletions": 1, "insertions": 13, "lines": 14}, {"filePath": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSubTask.java", "deletions": 2, "insertions": 1, "lines": 3}, {"filePath": "integration-tests/src/main/java/org/apache/druid/testing/clients/CoordinatorResourceTestClient.java", "deletions": 0, "insertions": 27, "lines": 27}, {"filePath": "integration-tests/src/test/resources/indexer/wikipedia_parallel_reindex_task.json", "deletions": 0, "insertions": 65, "lines": 65}], "commitSpoonAstDiffStats": [{"spoonFilePath": "ParallelIndexSupervisorTaskTest.java", "spoonMethods": [{"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.common.task.batch.parallel.ParallelIndexSupervisorTaskTest.testWithoutInterval()", "MOV": 1, "TOT": 3}]}, {"spoonFilePath": "ITParallelIndexTest.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.tests.indexer.ITParallelIndexTest.testIndexData()", "MOV": 0, "TOT": 2}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.tests.indexer.ITParallelIndexTest", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "AbstractITBatchIndexTest.java", "spoonMethods": [{"INS": 3, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.tests.indexer.AbstractITBatchIndexTest.submitTaskAndWait(java.lang.String,java.lang.String,boolean)", "MOV": 0, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.tests.indexer.AbstractITBatchIndexTest.doReindexTest(java.lang.String,java.lang.String,java.lang.String,java.lang.String)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.tests.indexer.AbstractITBatchIndexTest.doIndexTestSqlTest(java.lang.String,java.lang.String,java.lang.String)", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.tests.indexer.AbstractITBatchIndexTest.doIndexTestTest(java.lang.String,java.lang.String,java.lang.String,boolean)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "IngestionTestBase.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.common.task.IngestionTestBase.getStorageCoordinator()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "ParallelIndexSubTask.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 2, "spoonMethodName": "org.apache.druid.indexing.common.task.batch.parallel.ParallelIndexSubTask.createSegmentAllocator(org.apache.druid.indexing.common.TaskToolbox,org.apache.druid.indexing.common.task.batch.parallel.ParallelIndexTaskClient,org.apache.druid.indexing.common.task.batch.parallel.ParallelIndexIngestionSpec)", "MOV": 1, "TOT": 3}]}, {"spoonFilePath": "ITIndexerTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.tests.indexer.ITIndexerTest.testIndexData()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "Counters.java", "spoonMethods": [{"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.common.Counters.incrementAndGetInt(java.util.concurrent.ConcurrentHashMap,java.lang.Object)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "ParallelIndexSupervisorTask.java", "spoonMethods": [{"INS": 10, "UPD": 6, "DEL": 1, "spoonMethodName": "org.apache.druid.indexing.common.task.batch.parallel.ParallelIndexSupervisorTask.allocateNewSegment(org.joda.time.DateTime)", "MOV": 12, "TOT": 29}, {"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.common.task.batch.parallel.ParallelIndexSupervisorTask.findVersion(java.util.Map,org.joda.time.Interval)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "CoordinatorResourceTestClient.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.testing.clients.CoordinatorResourceTestClient.getSegmentVersions(java.lang.String)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.testing.clients.CoordinatorResourceTestClient.getLoadStatus()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.testing.clients.CoordinatorResourceTestClient.getFullSegmentsURL(java.lang.String)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.testing.clients.CoordinatorResourceTestClient.getLoadStatus().3", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2019-02-20 10:54:26", "commitMessage": "index_parallel: support !appendToExisting with no explicit intervals (#7046)\n\n* index_parallel: support !appendToExisting with no explicit intervals\r\n\r\nThis enables ParallelIndexSupervisorTask to dynamically request locks at runtime\r\nif it is run without explicit intervals in the granularity spec and with\r\nappendToExisting set to false.  Previously, it behaved as if appendToExisting\r\nwas set to true, which was undocumented and inconsistent with IndexTask and\r\nHadoop indexing.\r\n\r\nAlso, when ParallelIndexSupervisorTask allocates segments in the explicit\r\ninterval case, fail if its locks on the interval have been revoked.\r\n\r\nAlso make a few other additions/clarifications to native ingestion docs.\r\n\r\nFixes #6989.\r\n\r\n* Review feedback.\r\n\r\nPR description on GitHub updated to match.\r\n\r\n* Make native batch ingestion partitions start at 0\r\n\r\n* Fix to previous commit\r\n\r\n* Unit test. Verified to fail without the other commits on this branch.\r\n\r\n* Another round of review\r\n\r\n* Slightly scarier warning\r\n", "commitUser": "jihoonson", "commitDateTime": "2019-02-20 10:54:26", "commitParents": ["9a521526c7e15808ded5978abba40e1228eed784"], "commitGHEventType": "referenced", "nameRev": "a81b1b8c9c9ccf60ea28a2c2ffeedbc8dc19de5e tags/druid-0.15.0-incubating-rc1~271", "commitHash": "a81b1b8c9c9ccf60ea28a2c2ffeedbc8dc19de5e"}, {"commitGitStats": [{"filePath": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractITBatchIndexTest.java", "deletions": 5, "insertions": 20, "lines": 25}, {"filePath": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "deletions": 16, "insertions": 54, "lines": 70}, {"filePath": "integration-tests/src/test/resources/indexer/wikipedia_parallel_reindex_queries.json", "deletions": 0, "insertions": 18, "lines": 18}, {"filePath": "indexing-service/src/test/java/org/apache/druid/indexing/common/task/IngestionTestBase.java", "deletions": 0, "insertions": 5, "lines": 5}, {"filePath": "indexing-service/src/main/java/org/apache/druid/indexing/common/Counters.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "docs/content/ingestion/ingestion-spec.md", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "docs/content/ingestion/native_tasks.md", "deletions": 0, "insertions": 27, "lines": 27}, {"filePath": "indexing-service/src/test/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTaskTest.java", "deletions": 2, "insertions": 26, "lines": 28}, {"filePath": "integration-tests/src/test/java/org/apache/druid/tests/indexer/ITIndexerTest.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "integration-tests/src/test/java/org/apache/druid/tests/indexer/ITParallelIndexTest.java", "deletions": 1, "insertions": 13, "lines": 14}, {"filePath": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSubTask.java", "deletions": 2, "insertions": 1, "lines": 3}, {"filePath": "integration-tests/src/main/java/org/apache/druid/testing/clients/CoordinatorResourceTestClient.java", "deletions": 0, "insertions": 27, "lines": 27}, {"filePath": "integration-tests/src/test/resources/indexer/wikipedia_parallel_reindex_task.json", "deletions": 0, "insertions": 65, "lines": 65}], "commitSpoonAstDiffStats": [{"spoonFilePath": "ParallelIndexSupervisorTaskTest.java", "spoonMethods": [{"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.common.task.batch.parallel.ParallelIndexSupervisorTaskTest.testWithoutInterval()", "MOV": 1, "TOT": 3}]}, {"spoonFilePath": "ITParallelIndexTest.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.tests.indexer.ITParallelIndexTest.testIndexData()", "MOV": 0, "TOT": 2}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.tests.indexer.ITParallelIndexTest", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "AbstractITBatchIndexTest.java", "spoonMethods": [{"INS": 3, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.tests.indexer.AbstractITBatchIndexTest.submitTaskAndWait(java.lang.String,java.lang.String,boolean)", "MOV": 0, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.tests.indexer.AbstractITBatchIndexTest.doReindexTest(java.lang.String,java.lang.String,java.lang.String,java.lang.String)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.tests.indexer.AbstractITBatchIndexTest.doIndexTestSqlTest(java.lang.String,java.lang.String,java.lang.String)", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.tests.indexer.AbstractITBatchIndexTest.doIndexTestTest(java.lang.String,java.lang.String,java.lang.String,boolean)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "IngestionTestBase.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.common.task.IngestionTestBase.getStorageCoordinator()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "ParallelIndexSubTask.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 2, "spoonMethodName": "org.apache.druid.indexing.common.task.batch.parallel.ParallelIndexSubTask.createSegmentAllocator(org.apache.druid.indexing.common.TaskToolbox,org.apache.druid.indexing.common.task.batch.parallel.ParallelIndexTaskClient,org.apache.druid.indexing.common.task.batch.parallel.ParallelIndexIngestionSpec)", "MOV": 1, "TOT": 3}]}, {"spoonFilePath": "ITIndexerTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.tests.indexer.ITIndexerTest.testIndexData()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "Counters.java", "spoonMethods": [{"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.common.Counters.incrementAndGetInt(java.util.concurrent.ConcurrentHashMap,java.lang.Object)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "ParallelIndexSupervisorTask.java", "spoonMethods": [{"INS": 10, "UPD": 6, "DEL": 1, "spoonMethodName": "org.apache.druid.indexing.common.task.batch.parallel.ParallelIndexSupervisorTask.allocateNewSegment(org.joda.time.DateTime)", "MOV": 12, "TOT": 29}, {"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.common.task.batch.parallel.ParallelIndexSupervisorTask.findVersion(java.util.Map,org.joda.time.Interval)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "CoordinatorResourceTestClient.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.testing.clients.CoordinatorResourceTestClient.getSegmentVersions(java.lang.String)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.testing.clients.CoordinatorResourceTestClient.getLoadStatus()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.testing.clients.CoordinatorResourceTestClient.getFullSegmentsURL(java.lang.String)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.testing.clients.CoordinatorResourceTestClient.getLoadStatus().3", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2019-02-22 16:39:09", "commitMessage": "index_parallel: support !appendToExisting with no explicit intervals (#7046) (#7113)\n\n* index_parallel: support !appendToExisting with no explicit intervals\r\n\r\nThis enables ParallelIndexSupervisorTask to dynamically request locks at runtime\r\nif it is run without explicit intervals in the granularity spec and with\r\nappendToExisting set to false.  Previously, it behaved as if appendToExisting\r\nwas set to true, which was undocumented and inconsistent with IndexTask and\r\nHadoop indexing.\r\n\r\nAlso, when ParallelIndexSupervisorTask allocates segments in the explicit\r\ninterval case, fail if its locks on the interval have been revoked.\r\n\r\nAlso make a few other additions/clarifications to native ingestion docs.\r\n\r\nFixes #6989.\r\n\r\n* Review feedback.\r\n\r\nPR description on GitHub updated to match.\r\n\r\n* Make native batch ingestion partitions start at 0\r\n\r\n* Fix to previous commit\r\n\r\n* Unit test. Verified to fail without the other commits on this branch.\r\n\r\n* Another round of review\r\n\r\n* Slightly scarier warning", "commitUser": "jihoonson", "commitDateTime": "2019-02-22 16:39:09", "commitParents": ["9990e043896eb8023c1a98c7d3cdc957d93ec0ef"], "commitGHEventType": "referenced", "nameRev": "a8ae7156b1751fdc2ce2a978f9728cbf79b0ed2c tags/druid-0.14.0-incubating~45", "commitHash": "a8ae7156b1751fdc2ce2a978f9728cbf79b0ed2c"}], "body": "We're experimenting with native batch ingestion on our 0.13-incubating cluster for the first time (with a custom firehose reading from files saved to GCS by Secor, with a custom InputRowParser).\r\n\r\nThere was a period of a week where the data source had no data.  We ran batch ingestion (index_parallel) over one particular hour (5am-6am on December 16th) and it successfully ingested that hour \u2014 a segment showed up in the coordinator, it could be queried, etc.  (Our segment granularity is HOUR.)\r\n\r\nThen we ran it again on the entire 24 hours of December 16th.  It ran 24 subtasks (our firehose divides up by hour) and ingested the full day, yay!\r\n\r\nExcept when we look in the coordinator, it now lists 2 segments with identical sizes for the 5am hour that we first tested with. Also both of them are listed with the same version which was from the first batch ingestion, not the version that the other 23 segments have from the second batch ingestion.\r\n\r\nWe did not explicitly specify appendToExisting in our ioConfig but I believe the default is false and looking at the task payload it is expanded to false.\r\n\r\nAre we doing something wrong if our goal is to replace existing segments?  Isn't that what `appendToExisting: false` should do?\r\n\r\nThe bad hour in the coordinator:\r\n![image](https://user-images.githubusercontent.com/16724/52159609-89f62480-265b-11e9-897d-7255bf522f00.png)\r\n\r\nThe good hour:\r\n![image](https://user-images.githubusercontent.com/16724/52159612-8f536f00-265b-11e9-9bab-561f01179e3c.png)\r\n"}, {"user": "jon-wei", "commits": {}, "labels": ["Bug"], "created": "2019-02-01 21:25:43", "title": "PreResponseAuthorizationCheckFilter error when loading coordinator console", "url": "https://github.com/apache/druid/issues/6982", "closed": "2019-02-27 02:10:40", "ttf": 25.00027777777778, "commitsDetails": [], "body": "in 0.13.0, when loading the coordinator console, the following error occurs (although the subsequent redirect works and authorization checks are performed correctly in the actual console).\r\n\r\n```\r\nERROR [qtp2058526846-335] io.druid.server.security.PreResponseAuthorizationCheckFilter - Request did not have an authorization check performed.: {class=io.druid.server.security.PreResponseAuthorizationCheckFilter, uri=/, method=GET, remoteAddr=1.1.1.1 remoteHost=1.1.1.1}\r\n```"}, {"user": "jon-wei", "commits": {"16a4a50e9147a04dd47bf2eb897138197519b0bb": {"commitGHEventType": "referenced", "commitUser": "jon-wei"}, "b2f90c186223150d70db5963268c6c5cf160c6f3": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Area - Deep Storage", "Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "NoClassDefFoundError when using druid-hdfs-storage", "numCommits": 0, "created": "2019-01-31 03:16:53", "closed": "2019-02-11 22:25:35", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/6967", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 11.000277777777777, "commitsDetails": [{"commitGitStats": [{"filePath": "indexing-hadoop/pom.xml", "deletions": 16, "insertions": 16, "lines": 32}, {"filePath": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/HadoopIndexTask.java", "deletions": 27, "insertions": 37, "lines": 64}, {"filePath": "extensions-core/druid-kerberos/pom.xml", "deletions": 0, "insertions": 1, "lines": 1}, {"filePath": "extensions-core/hdfs-storage/pom.xml", "deletions": 7, "insertions": 131, "lines": 138}], "commitSpoonAstDiffStats": [{"spoonFilePath": "HadoopIndexTask.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.indexing.common.task.HadoopIndexTask.HadoopKillMRJobIdProcessingRunner", "MOV": 2, "TOT": 4}, {"INS": 2, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.indexing.common.task.HadoopIndexTask.HadoopKillMRJobIdProcessingRunner.runTask(java.lang.String[])", "MOV": 6, "TOT": 9}, {"INS": 5, "UPD": 3, "DEL": 4, "spoonMethodName": "org.apache.druid.indexing.common.task.HadoopIndexTask.stopGracefully(org.apache.druid.indexing.common.config.TaskConfig)", "MOV": 14, "TOT": 26}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2019-02-08 18:26:37", "commitMessage": "[Issue #6967] NoClassDefFoundError when using druid-hdfs-storage (#7015)\n\n* Fix:\r\n  1. hadoop-common dependency for druid-hdfs and druid-kerberos extensions\r\n Refactoring:\r\n  2. Hadoop config call in the inner static class to avoid class path conflicts for stopGracefully kill\r\n\r\n* Fix:\r\n  1. hadoop-common test dependency\r\n\r\n* Fix:\r\n  1. Avoid issue of kill command once the job is actually completed\r\n", "commitUser": "jon-wei", "commitDateTime": "2019-02-08 18:26:37", "commitParents": ["d42de574d66c6188ec10a3e0acd6a3d8694b9463"], "commitGHEventType": "referenced", "nameRev": "16a4a50e9147a04dd47bf2eb897138197519b0bb tags/druid-0.15.0-incubating-rc1~293", "commitHash": "16a4a50e9147a04dd47bf2eb897138197519b0bb"}, {"commitGitStats": [{"filePath": "indexing-hadoop/pom.xml", "deletions": 16, "insertions": 16, "lines": 32}, {"filePath": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/HadoopIndexTask.java", "deletions": 27, "insertions": 37, "lines": 64}, {"filePath": "extensions-core/druid-kerberos/pom.xml", "deletions": 0, "insertions": 1, "lines": 1}, {"filePath": "extensions-core/hdfs-storage/pom.xml", "deletions": 7, "insertions": 131, "lines": 138}], "commitSpoonAstDiffStats": [{"spoonFilePath": "HadoopIndexTask.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.indexing.common.task.HadoopIndexTask.HadoopKillMRJobIdProcessingRunner", "MOV": 2, "TOT": 4}, {"INS": 2, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.indexing.common.task.HadoopIndexTask.HadoopKillMRJobIdProcessingRunner.runTask(java.lang.String[])", "MOV": 6, "TOT": 9}, {"INS": 5, "UPD": 3, "DEL": 4, "spoonMethodName": "org.apache.druid.indexing.common.task.HadoopIndexTask.stopGracefully(org.apache.druid.indexing.common.config.TaskConfig)", "MOV": 14, "TOT": 26}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2019-02-08 22:26:34", "commitMessage": "[Issue #6967] NoClassDefFoundError when using druid-hdfs-storage (#7015) (#7045)\n\n* Fix:\r\n  1. hadoop-common dependency for druid-hdfs and druid-kerberos extensions\r\n Refactoring:\r\n  2. Hadoop config call in the inner static class to avoid class path conflicts for stopGracefully kill\r\n\r\n* Fix:\r\n  1. hadoop-common test dependency\r\n\r\n* Fix:\r\n  1. Avoid issue of kill command once the job is actually completed", "commitUser": "fjy", "commitDateTime": "2019-02-08 22:26:34", "commitParents": ["91ecb84608e1665e4293415a7b2f8856eb1fa2b4"], "commitGHEventType": "referenced", "nameRev": "b2f90c186223150d70db5963268c6c5cf160c6f3 tags/druid-0.14.0-incubating~55", "commitHash": "b2f90c186223150d70db5963268c6c5cf160c6f3"}], "body": "Running the current master with druid-hdfs-storage enabled results in the following exception on startup:\r\n\r\n```\r\n2019-01-31T02:53:27,216 INFO [main] org.apache.druid.initialization.Initialization - Adding implementation [org.apache.druid.storage.hdfs.HdfsStorageDruidModule] for class [interface org.apache.druid.initialization.DruidModule] from local file system extension\r\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/hadoop/conf/Configuration\r\n\tat org.apache.druid.storage.hdfs.HdfsStorageDruidModule.configure(HdfsStorageDruidModule.java:97)\r\n\tat com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:340)\r\n\tat com.google.inject.spi.Elements.getElements(Elements.java:110)\r\n\tat com.google.inject.util.Modules$OverrideModule.configure(Modules.java:198)\r\n\tat com.google.inject.AbstractModule.configure(AbstractModule.java:62)\r\n\tat com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:340)\r\n\tat com.google.inject.spi.Elements.getElements(Elements.java:110)\r\n\tat com.google.inject.internal.InjectorShell$Builder.build(InjectorShell.java:138)\r\n\tat com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:104)\r\n\tat com.google.inject.Guice.createInjector(Guice.java:99)\r\n\tat com.google.inject.Guice.createInjector(Guice.java:73)\r\n\tat com.google.inject.Guice.createInjector(Guice.java:62)\r\n\tat org.apache.druid.initialization.Initialization.makeInjectorWithModules(Initialization.java:419)\r\n\tat org.apache.druid.cli.GuiceRunnable.makeInjector(GuiceRunnable.java:69)\r\n\tat org.apache.druid.cli.ServerRunnable.run(ServerRunnable.java:57)\r\n\tat org.apache.druid.cli.Main.main(Main.java:118)\r\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.conf.Configuration\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\t... 16 more\r\n\r\n```\r\n\r\nLikely related to changes with `hadoop-common` in https://github.com/apache/incubator-druid/pull/6828"}, {"user": "jon-wei", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug"], "created": "2019-01-16 23:52:30", "title": "DoublesSketchAggregatorFactory doesn't implement makeAggregateCombiner ", "url": "https://github.com/apache/druid/issues/6877", "closed": "2019-01-17 22:09:55", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "In 0.13.0-incubating, using a `quantilesDoubleSketch` aggregator at ingestion will result in the following exception during index merging:\r\n\r\n```\r\n2019-01-16T02:51:54,693 ERROR [publish-0] org.apache.druid.indexing.kafka.IncrementalPublishingKafkaIndexTaskRunner - Error while publishing segments for sequence[SequenceMetadata{sequenceName='index_kafka_BRS_PAGE_MIN_0361b88dfbdfe35_0', sequenceId=0, startOffsets={0=10213925, 1=10824117}, endOffsets={0=10480548, 1=11090740}, assignments=[], sentinel=false, checkpointed=true}]\r\norg.apache.druid.java.util.common.UOE: [org.apache.druid.query.aggregation.datasketches.quantiles.DoublesSketchAggregatorFactory] does not implement makeAggregateCombiner()\r\n\tat org.apache.druid.query.aggregation.AggregatorFactory.makeAggregateCombiner(AggregatorFactory.java:79) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.query.aggregation.AggregatorFactory.makeNullableAggregateCombiner(AggregatorFactory.java:93) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.segment.RowCombiningTimeAndDimsIterator.lambda$new$1(RowCombiningTimeAndDimsIterator.java:120) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat java.util.Arrays.setAll(Arrays.java:4698) ~[?:1.8.0_60]\r\n\tat org.apache.druid.segment.RowCombiningTimeAndDimsIterator.<init>(RowCombiningTimeAndDimsIterator.java:120) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.segment.IndexMergerV9.lambda$merge$2(IndexMergerV9.java:909) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.segment.IndexMergerV9.makeMergedTimeAndDimsIterator(IndexMergerV9.java:1031) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.segment.IndexMergerV9.makeIndexFiles(IndexMergerV9.java:179) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.segment.IndexMergerV9.merge(IndexMergerV9.java:914) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.segment.IndexMergerV9.mergeQueryableIndex(IndexMergerV9.java:832) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.segment.IndexMergerV9.mergeQueryableIndex(IndexMergerV9.java:810) ~[druid-processing-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.mergeAndPush(AppenderatorImpl.java:719) ~[druid-server-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.lambda$push$1(AppenderatorImpl.java:623) ~[druid-server-0.13.0-incubating.jar:0.13.0-incubating]\r\n\tat com.google.common.util.concurrent.Futures$1.apply(Futures.java:713) ~[guava-16.0.1.jar:?]\r\n\tat com.google.common.util.concurrent.Futures$ChainingListenableFuture.run(Futures.java:861) [guava-16.0.1.jar:?]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_60]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_60]\r\n\tat java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]\r\n```\r\n\r\nAs reported here: https://github.com/apache/incubator-druid/issues/6853#issuecomment-454657583\r\n\r\nIt looks like the sketch agg was added before https://github.com/apache/incubator-druid/pull/5335 was merged and doesn't have `makeAggregateCombiner()`."}, {"user": "zhaojiandong", "commits": {}, "labels": ["Area - SQL", "Bug"], "created": "2019-01-16 02:37:00", "title": "deadlock found in DruidStatement", "url": "https://github.com/apache/druid/issues/6867", "closed": "2019-01-21 08:04:50", "ttf": 5.000277777777778, "commitsDetails": [], "body": "We found deadlock in DruidStatement and DruidConnection, this will cause exhaust connections.\r\n\r\n```\r\nFound one Java-level deadlock:\r\n=============================\r\n\"DruidMeta@4e50ae56-ScheduledExecutor\":\r\n  waiting to lock monitor 0x00007fa5084802b8 (object 0x00000000d6389830, a java.lang.Object),\r\n  which is held by \"qtp1871838170-210\"\r\n\"qtp1871838170-210\":\r\n  waiting to lock monitor 0x00007fa5084916b8 (object 0x00000000d603e4b8, a java.util.HashMap),\r\n  which is held by \"DruidMeta@4e50ae56-ScheduledExecutor\"\r\n\r\nJava stack information for the threads listed above:\r\n===================================================\r\n\"DruidMeta@4e50ae56-ScheduledExecutor\":\r\n\tat io.druid.sql.avatica.DruidStatement.close(DruidStatement.java:310)\r\n\t- waiting to lock <0x00000000d6389830> (a java.lang.Object)\r\n\tat io.druid.sql.avatica.DruidConnection.close(DruidConnection.java:150)\r\n\t- locked <0x00000000d603e4b8> (a java.util.HashMap)\r\n\tat io.druid.sql.avatica.DruidMeta.closeConnection(DruidMeta.java:120)\r\n\tat io.druid.sql.avatica.DruidMeta.lambda$getDruidConnection$0(DruidMeta.java:588)\r\n\tat io.druid.sql.avatica.DruidMeta$$Lambda$90/10531789.run(Unknown Source)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\"qtp1871838170-210\":\r\n\tat io.druid.sql.avatica.DruidConnection.lambda$createStatement$0(DruidConnection.java:107)\r\n\t- waiting to lock <0x00000000d603e4b8> (a java.util.HashMap)\r\n\tat io.druid.sql.avatica.DruidConnection$$Lambda$91/1439788572.run(Unknown Source)\r\n\tat io.druid.sql.avatica.DruidStatement.close(DruidStatement.java:348)\r\n\t- locked <0x00000000d6389830> (a java.lang.Object)\r\n\tat io.druid.sql.avatica.DruidStatement.execute(DruidStatement.java:213)\r\n\t- locked <0x00000000d6389830> (a java.lang.Object)\r\n\tat io.druid.sql.avatica.DruidMeta.prepareAndExecute(DruidMeta.java:187)\r\n\tat org.apache.calcite.avatica.remote.LocalService.apply(LocalService.java:206)\r\n\tat org.apache.calcite.avatica.remote.Service$PrepareAndExecuteRequest.accept(Service.java:928)\r\n\tat org.apache.calcite.avatica.remote.Service$PrepareAndExecuteRequest.accept(Service.java:880)\r\n\tat org.apache.calcite.avatica.remote.AbstractHandler.apply(AbstractHandler.java:94)\r\n\tat org.apache.calcite.avatica.remote.JsonHandler.apply(JsonHandler.java:52)\r\n\tat org.apache.calcite.avatica.server.AvaticaJsonHandler.handle(AvaticaJsonHandler.java:130)\r\n\tat io.druid.sql.avatica.DruidAvaticaHandler.handle(DruidAvaticaHandler.java:60)\r\n\tat org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\r\n\tat org.eclipse.jetty.server.Server.handle(Server.java:534)\r\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\r\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\r\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\r\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\r\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\nFound one Java-level deadlock:\r\n=============================\r\n\"qtp1871838170-283\":\r\n  waiting to lock monitor 0x00007fa50b53c918 (object 0x00000007712471d8, a java.lang.Object),\r\n  which is held by \"qtp1871838170-199\"\r\n\"qtp1871838170-199\":\r\n  waiting to lock monitor 0x00007fa508d0ae68 (object 0x00000007766ce418, a java.util.HashMap),\r\n  which is held by \"qtp1871838170-283\"\r\n\r\nJava stack information for the threads listed above:\r\n===================================================\r\n\"qtp1871838170-283\": \r\n\tat io.druid.sql.avatica.DruidStatement.close(DruidStatement.java:310)\r\n\t- waiting to lock <0x00000007712471d8> (a java.lang.Object)\r\n\tat io.druid.sql.avatica.DruidConnection.close(DruidConnection.java:150)\r\n\t- locked <0x00000007766ce418> (a java.util.HashMap)\r\n\tat io.druid.sql.avatica.DruidMeta.closeConnection(DruidMeta.java:120)\r\n\tat org.apache.calcite.avatica.remote.LocalService.apply(LocalService.java:292)\r\n\tat org.apache.calcite.avatica.remote.Service$CloseConnectionRequest.accept(Service.java:1907)\r\n\tat org.apache.calcite.avatica.remote.Service$CloseConnectionRequest.accept(Service.java:1890)\r\n\tat org.apache.calcite.avatica.remote.AbstractHandler.apply(AbstractHandler.java:94)\r\n\tat org.apache.calcite.avatica.remote.JsonHandler.apply(JsonHandler.java:52)\r\n\tat org.apache.calcite.avatica.server.AvaticaJsonHandler.handle(AvaticaJsonHandler.java:130)\r\n\tat io.druid.sql.avatica.DruidAvaticaHandler.handle(DruidAvaticaHandler.java:60)\r\n\tat org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\r\n\tat org.eclipse.jetty.server.Server.handle(Server.java:534)\r\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\r\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\r\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\r\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\r\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\"qtp1871838170-199\":  \r\n\tat io.druid.sql.avatica.DruidConnection.lambda$createStatement$0(DruidConnection.java:107)\r\n\t- waiting to lock <0x00000007766ce418> (a java.util.HashMap)\r\n\tat io.druid.sql.avatica.DruidConnection$$Lambda$91/1439788572.run(Unknown Source)\r\n\tat io.druid.sql.avatica.DruidStatement.close(DruidStatement.java:348)\r\n\t- locked <0x00000007712471d8> (a java.lang.Object)\r\n\tat io.druid.sql.avatica.DruidStatement.nextFrame(DruidStatement.java:290)\r\n\t- locked <0x00000007712471d8> (a java.lang.Object)\r\n\tat io.druid.sql.avatica.DruidMeta.prepareAndExecute(DruidMeta.java:188)\r\n\tat org.apache.calcite.avatica.remote.LocalService.apply(LocalService.java:206)\r\n\tat org.apache.calcite.avatica.remote.Service$PrepareAndExecuteRequest.accept(Service.java:928)\r\n\tat org.apache.calcite.avatica.remote.Service$PrepareAndExecuteRequest.accept(Service.java:880)\r\n\tat org.apache.calcite.avatica.remote.AbstractHandler.apply(AbstractHandler.java:94)\r\n\tat org.apache.calcite.avatica.remote.JsonHandler.apply(JsonHandler.java:52)\r\n\tat org.apache.calcite.avatica.server.AvaticaJsonHandler.handle(AvaticaJsonHandler.java:130)\r\n\tat io.druid.sql.avatica.DruidAvaticaHandler.handle(DruidAvaticaHandler.java:60)\r\n\tat org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\r\n\tat org.eclipse.jetty.server.Server.handle(Server.java:534)\r\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\r\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\r\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\r\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\r\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n```"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2019-01-10 03:42:31", "title": "0.13.0-incubating tag does not compile", "url": "https://github.com/apache/druid/issues/6831", "closed": "2019-05-07 19:08:39", "ttf": 117.00027777777778, "commitsDetails": [], "body": "```sh\r\n$ git checkout druid-0.13.0-incubating\r\n$ mvn clean test-compile -pl extensions-core/avro-extensions\r\n[ERROR] /Users/charles.allen/src/druid/extensions-core/avro-extensions/src/test/java/org/apache/druid/data/input/AvroStreamInputRowParserTest.java:[127,81] cannot find symbol\r\n  symbol:   method asMap(java.util.HashSet<java.lang.CharSequence>,<anonymous com.google.common.base.Function<java.lang.CharSequence,java.lang.Integer>>)\r\n  location: class com.google.common.collect.Maps\r\n[ERROR] /Users/charles.allen/src/druid/extensions-core/avro-extensions/src/test/java/org/apache/druid/data/input/AvroStreamInputRowParserTest.java:[138,89] cannot find symbol\r\n  symbol:   method asMap(java.util.HashSet<java.lang.CharSequence>,<anonymous com.google.common.base.Function<java.lang.CharSequence,java.lang.CharSequence>>)\r\n  location: class com.google.common.collect.Maps\r\n\r\n```\r\n\r\nFor some reason the pig h2 dependency is rolling in guava but not shading it."}, {"user": "leventov", "commits": {}, "labels": ["Area - SQL", "Bug"], "created": "2019-01-09 12:02:04", "title": "Concurrency bug in DruidSchema.refreshSegmentsForDataSource()", "url": "https://github.com/apache/druid/issues/6826", "closed": "2019-04-12 15:51:36", "ttf": 93.00027777777778, "commitsDetails": [], "body": "segmentMap is created outside of lock, so by the time of `SegmentMetadataHolder holder = dataSourceSegments.get(segment);` segment could be already removed. It could cause NPE below.\r\n\r\nFYI @surekhasaharan @gianm "}, {"user": "QiuMM", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug"], "created": "2018-12-10 17:46:36", "title": "tasks failed because of no sink for identifier", "url": "https://github.com/apache/druid/issues/6722", "closed": "2019-01-05 01:09:12", "ttf": 25.00027777777778, "commitsDetails": [], "body": "Recently, I observed some kafka index tasks failed, below is the log:\r\n```\r\n2018-12-09 20:33:15,603 INFO [task-runner-0-priority-0] io.druid.segment.realtime.appenderator.BaseAppenderatorDriver - Pushing segments in background: [aweme_low_frequency_all_event_2018-12-08T01:00:00.000Z_2018-12-08T02:00:00.000Z_2018-12-09T16:40:28.781Z_58, aweme_low_frequency_all_event_2018-12-09T20:00:00.000Z_2018-12-09T21:00:00.000Z_2018-12-09T20:00:01.734Z_355]\r\n2018-12-09 20:33:15,604 INFO [appenderator_persist_0] io.druid.segment.realtime.appenderator.AppenderatorImpl - Removing sink for segment[aweme_low_frequency_all_event_2018-12-08T01:00:00.000Z_2018-12-08T02:00:00.000Z_2018-12-09T16:40:28.781Z_54].\r\n2018-12-09 20:33:15,620 INFO [appenderator_persist_0] io.druid.segment.realtime.appenderator.AppenderatorImpl - Deleting Index File[var/druid/task/index_kafka_aweme_low_frequency_all_event_da2a8411a415b8b_dfjkpjih/work/persist/aweme_low_frequency_all_event_2018-12-08T01:00:00.000Z_2018-12-08T02:00:00.000Z_2018-12-09T16:40:28.781Z_54]\r\n2018-12-09 20:33:15,604 ERROR [task-runner-0-priority-0] io.druid.indexing.kafka.KafkaIndexTask - Encountered exception in run() before persisting.\r\nio.druid.java.util.common.ISE: No sink for identifier: aweme_low_frequency_all_event_2018-12-08T01:00:00.000Z_2018-12-08T02:00:00.000Z_2018-12-09T16:40:28.781Z_54\r\n\tat io.druid.segment.realtime.appenderator.AppenderatorImpl.persistAll(AppenderatorImpl.java:434) ~[druid-server-0.12.2.jar:0.12.2]\r\n\tat io.druid.segment.realtime.appenderator.AppenderatorImpl.push(AppenderatorImpl.java:553) ~[druid-server-0.12.2.jar:0.12.2]\r\n\tat io.druid.segment.realtime.appenderator.BaseAppenderatorDriver.pushInBackground(BaseAppenderatorDriver.java:470) ~[druid-server-0.12.2.jar:0.12.2]\r\n\tat io.druid.segment.realtime.appenderator.StreamAppenderatorDriver.publish(StreamAppenderatorDriver.java:265) ~[druid-server-0.12.2.jar:0.12.2]\r\n\tat io.druid.indexing.kafka.KafkaIndexTask.publishAndRegisterHandoff(KafkaIndexTask.java:1243) ~[druid-kafka-indexing-service-0.12.2.jar:0.12.2]\r\n\tat io.druid.indexing.kafka.KafkaIndexTask.maybePersistAndPublishSequences(KafkaIndexTask.java:1373) ~[druid-kafka-indexing-service-0.12.2.jar:0.12.2]\r\n\tat io.druid.indexing.kafka.KafkaIndexTask.run(KafkaIndexTask.java:544) [druid-kafka-indexing-service-0.12.2.jar:0.12.2]\r\n\tat io.druid.indexing.overlord.ThreadPoolTaskRunner$ThreadPoolTaskRunnerCallable.call(ThreadPoolTaskRunner.java:444) [druid-indexing-service-0.12.2.jar:0.12.2]\r\n\tat io.druid.indexing.overlord.ThreadPoolTaskRunner$ThreadPoolTaskRunnerCallable.call(ThreadPoolTaskRunner.java:416) [druid-indexing-service-0.12.2.jar:0.12.2]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_91]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_91]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_91]\r\n\tat java.lang.Thread.run(Thread.java:745) [?:1.8.0_91]\r\n2018-12-09 20:33:15,622 INFO [task-runner-0-priority-0] io.druid.indexing.kafka.KafkaIndexTask - Persisting all pending data\r\n2018-12-09 20:33:15,623 INFO [task-runner-0-priority-0] io.druid.segment.realtime.appenderator.StreamAppenderatorDriver - Persisting data.\r\n2018-12-09 20:33:15,623 INFO [task-runner-0-priority-0] io.druid.segment.realtime.appenderator.AppenderatorImpl - Submitting persist runnable for dataSource[aweme_low_frequency_all_event]\r\n```\r\n\r\nAccording the log and checked the code, I found why this happened.  See below, code 1 is `AppenderatorImpl#persistAll`, code 2 is `AppenderatorImpl#abandonSegment`. Although `sinks` is a ConcurrentHashMap, if `sinks.keySet()` is called at code 1 and after which `sinks.remove(identifier)` is called at code 2, then the for loop go on at code 1, `sinks.get(identifier)` would try to get a object which has been removed. Finally, an exception throwed and task failed.\r\n\r\nI think we should add an extra lock to solve this bug.\r\n\r\n- code 1\r\n```java\r\npublic ListenableFuture<Object> persistAll(@Nullable final Committer committer)\r\n{\r\n    ....\r\n    for (SegmentIdentifier identifier : sinks.keySet()) {\r\n      final Sink sink = sinks.get(identifier);\r\n      if (sink == null) {\r\n        throw new ISE(\"No sink for identifier: %s\", identifier);\r\n      }\r\n      .....\r\n    }\r\n    .....\r\n}\r\n```\r\n- code 2\r\n```java\r\nprivate ListenableFuture<?> abandonSegment(\r\n      final SegmentIdentifier identifier,\r\n      final Sink sink,\r\n      final boolean removeOnDiskData\r\n  )\r\n{\r\n    .....\r\n    log.info(\"Removing sink for segment[%s].\", identifier);\r\n    sinks.remove(identifier);\r\n    .....\r\n}\r\n```"}, {"user": "leventov", "commits": {}, "labels": ["Area - Automation/Static Analysis", "Bug"], "created": "2018-12-03 12:48:51", "title": "Throwables.propagate() without `throw`", "url": "https://github.com/apache/druid/issues/6701", "closed": "2019-03-15 20:27:16", "ttf": 102.00027777777778, "commitsDetails": [], "body": "`(?<!throw )Throwables.propagate\\(` regex gives 39 places in the Druid source code. See https://github.com/google/guava/wiki/Why-we-deprecated-Throwables.propagate#propagate-is-magic. Solution: refactor the code to not use Throwables.propagate()."}, {"user": "jihoonson", "commits": {}, "labels": ["Bug"], "created": "2018-11-29 23:32:20", "title": "Two bugs in Overlord API and console", "url": "https://github.com/apache/druid/issues/6684", "closed": "2018-11-30 07:45:29", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "Some overlord APIs emit the error message of `org.apache.druid.java.util.common.ISE: Request did not have an authorization check performed`.\r\n\r\nAlso, the `kill` button to kill a task in Overlord UI prints an error of `Kill request failed with status: 404 please check overlord logs`.\r\n\r\nI've fixed all bugs and am adding some unit tests. Will raise a PR soon."}, {"user": "gianm", "commits": {"e3260866046015183ca91257376ad36f95269a27": {"commitGHEventType": "referenced", "commitUser": "gianm"}, "ee6139adbb1489f7ee54d17ed4f280387d7a68a6": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Area - Streaming Ingestion", "Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "Kafka: Publishing improperly starts immediately after setEndOffsets", "numCommits": 0, "created": "2018-11-11 09:35:14", "closed": "2018-11-12 22:27:32", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/6602", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 1.0002777777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java", "deletions": 0, "insertions": 67, "lines": 67}, {"filePath": "extensions-core/kafka-indexing-service/src/main/java/org/apache/druid/indexing/kafka/IncrementalPublishingKafkaIndexTaskRunner.java", "deletions": 0, "insertions": 3, "lines": 3}], "commitSpoonAstDiffStats": [{"spoonFilePath": "IncrementalPublishingKafkaIndexTaskRunner.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.indexing.kafka.IncrementalPublishingKafkaIndexTaskRunner.runInternal(org.apache.druid.indexing.common.TaskToolbox)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "KafkaIndexTaskTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.kafka.KafkaIndexTaskTest.testIncrementalHandOffReadsThroughEndOffsets()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-11-12 14:27:32", "commitMessage": "fix kafka indexing task not processing through end offsets on publish, fixes #6602 (#6603)\n\n", "commitUser": "gianm", "commitDateTime": "2018-11-12 14:27:32", "commitParents": ["c2f020eacc318a0dc5501209922b5b93a17c91fa"], "commitGHEventType": "referenced", "nameRev": "e3260866046015183ca91257376ad36f95269a27 tags/druid-0.14.0-incubating~231", "commitHash": "e3260866046015183ca91257376ad36f95269a27"}, {"commitGitStats": [{"filePath": "extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java", "deletions": 0, "insertions": 67, "lines": 67}, {"filePath": "extensions-core/kafka-indexing-service/src/main/java/org/apache/druid/indexing/kafka/IncrementalPublishingKafkaIndexTaskRunner.java", "deletions": 0, "insertions": 3, "lines": 3}], "commitSpoonAstDiffStats": [{"spoonFilePath": "IncrementalPublishingKafkaIndexTaskRunner.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.indexing.kafka.IncrementalPublishingKafkaIndexTaskRunner.runInternal(org.apache.druid.indexing.common.TaskToolbox)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "KafkaIndexTaskTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.indexing.kafka.KafkaIndexTaskTest.testIncrementalHandOffReadsThroughEndOffsets()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-11-13 14:31:10", "commitMessage": "fix kafka indexing task not processing through end offsets on publish, fixes #6602 (#6603) (#6608)\n\n", "commitUser": "fjy", "commitDateTime": "2018-11-14 06:31:10", "commitParents": ["362feedc167288ba1d58f9064cb78880cc7fccf3"], "commitGHEventType": "referenced", "nameRev": "ee6139adbb1489f7ee54d17ed4f280387d7a68a6 tags/druid-0.13.0-incubating-rc2~6", "commitHash": "ee6139adbb1489f7ee54d17ed4f280387d7a68a6"}], "body": "#6129 contained a change that causes publishing to happen immediately when setEndOffsets is called: https://github.com/apache/incubator-druid/pull/6129/files#diff-d4624efd6954ee4ac442c4dc05b1f77aL434. This is not right: instead, the task should read up to its endOffsets.\r\n\r\nThis looks to be a regression since 0.12.3, so we should fix it before the release of 0.13.0."}, {"user": "jon-wei", "commits": {}, "labels": ["Bug"], "created": "2018-11-06 20:40:09", "title": "FileRequestLogger fails when used with FilteredRequestLogger", "url": "https://github.com/apache/druid/issues/6580", "closed": "2019-01-30 19:09:33", "ttf": 84.00027777777778, "commitsDetails": [], "body": "Using a filtered request logger with a file logger delegate causes NPEs, it seems like FileRequestLogger's `@LifecycleStart` method is not getting called when used like this.\r\n\r\n```\r\ndruid.request.logging.type=filtered\r\ndruid.request.logging.queryTimeThresholdMs=1\r\ndruid.request.logging.delegate={\"type\":\"file\",\"dir\":\"var/druid/request-logs\"}\r\n```\r\n\r\n```\r\n2018-11-06T20:37:27,108 ERROR [qtp159823385-116] org.apache.druid.server.QueryLifecycle - Unable to log query [ScanQuery{dataSource='fbh', querySegmentSpec=MultipleIntervalSegmentSpec{intervals=[-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z]}, virtualColumns=[], resultFormat='compactedList', batchSize=20480, limit=9223372036854775807, dimFilter=null, columns=[__time, added, count, fhisto, hostname, microservice, port, timestamp], legacy=false}]!\r\njava.lang.NullPointerException\r\n\tat org.apache.druid.server.log.FileRequestLogger.log(FileRequestLogger.java:133) ~[druid-server-0.13.0-SNAPSHOT.jar:0.13.0-SNAPSHOT]\r\n\tat org.apache.druid.server.log.FilteredRequestLoggerProvider$FilteredRequestLogger.log(FilteredRequestLoggerProvider.java:69) ~[druid-server-0.13.0-SNAPSHOT.jar:0.13.0-SNAPSHOT]\r\n\tat org.apache.druid.server.QueryLifecycle.emitLogsAndMetrics(QueryLifecycle.java:328) [druid-server-0.13.0-SNAPSHOT.jar:0.13.0-SNAPSHOT]\r\n\tat org.apache.druid.server.QueryLifecycle$1.after(QueryLifecycle.java:151) [druid-server-0.13.0-SNAPSHOT.jar:0.13.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.WrappingYielder.close(WrappingYielder.java:96) [java-util-0.13.0-SNAPSHOT.jar:0.13.0-SNAPSHOT]\r\n\tat org.apache.druid.java.util.common.guava.Yielders$2.close(Yielders.java:73) [java-util-0.13.0-SNAPSHOT.jar:0.13.0-SNAPSHOT]\r\n\tat org.apache.druid.sql.http.SqlResource$1.write(SqlResource.java:152) [druid-sql-0.13.0-SNAPSHOT.jar:0.13.0-SNAPSHOT]\r\n\tat com.sun.jersey.core.impl.provider.entity.StreamingOutputProvider.writeTo(StreamingOutputProvider.java:71) [jersey-core-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.core.impl.provider.entity.StreamingOutputProvider.writeTo(StreamingOutputProvider.java:57) [jersey-core-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.spi.container.ContainerResponse.write(ContainerResponse.java:302) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1510) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409) [jersey-servlet-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558) [jersey-servlet-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733) [jersey-servlet-1.19.3.jar:1.19.3]\r\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790) [javax.servlet-api-3.1.0.jar:3.1.0]\r\n\tat com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:286) [guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:276) [guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:181) [guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91) [guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85) [guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:120) [guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:135) [guice-servlet-4.1.0.jar:?]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.apache.druid.server.security.PreResponseAuthorizationCheckFilter.doFilter(PreResponseAuthorizationCheckFilter.java:84) [druid-server-0.13.0-SNAPSHOT.jar:0.13.0-SNAPSHOT]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.apache.druid.server.security.AllowOptionsResourceFilter.doFilter(AllowOptionsResourceFilter.java:76) [druid-server-0.13.0-SNAPSHOT.jar:0.13.0-SNAPSHOT]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.apache.druid.security.basic.authentication.BasicHTTPAuthenticator$BasicHTTPAuthenticationFilter.doFilter(BasicHTTPAuthenticator.java:190) [druid-basic-security-0.13.0-SNAPSHOT.jar:0.13.0-SNAPSHOT]\r\n\tat org.apache.druid.server.security.AuthenticationWrappingFilter.doFilter(AuthenticationWrappingFilter.java:60) [druid-server-0.13.0-SNAPSHOT.jar:0.13.0-SNAPSHOT]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.apache.druid.server.security.SecuritySanityCheckFilter.doFilter(SecuritySanityCheckFilter.java:88) [druid-server-0.13.0-SNAPSHOT.jar:0.13.0-SNAPSHOT]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1253) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1155) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:724) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:61) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:169) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.Server.handle(Server.java:531) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:352) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:281) [jetty-io-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102) [jetty-io-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118) [jetty-io-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:760) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:678) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_162]\r\n```\r\n\r\n"}, {"user": "vogievetsky", "commits": {}, "labels": ["Bug"], "created": "2018-11-02 04:56:17", "title": "AVG aggregator does not work with sys.segments", "url": "https://github.com/apache/druid/issues/6572", "closed": "2018-11-13 22:31:33", "ttf": 11.000277777777777, "commitsDetails": [], "body": "The SQL query\r\n\r\n```sql\r\nSELECT COUNT(*), SUM(\"size\"), AVG(\"size\") FROM sys.segments\r\n```\r\n\r\nproduces an error of:\r\n\r\n```\r\nCannot build plan for query: SELECT COUNT(*), SUM(\"size\"), AVG(\"size\") FROM sys.segments\r\n```\r\n\r\nThis only happens when the AVG agg is used\r\n"}, {"user": "vogievetsky", "commits": {"fe69da0d95a361708923fbe5b82ded0d461aa495": {"commitGHEventType": "referenced", "commitUser": "gianm"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "Unexpected results from concat on a real and fake (null) column", "numCommits": 0, "created": "2018-10-31 01:56:06", "closed": "2018-11-16 06:13:32", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/6556", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 16.00027777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "processing/src/test/java/org/apache/druid/segment/virtual/ExpressionVirtualColumnTest.java", "deletions": 16, "insertions": 55, "lines": 71}, {"filePath": "processing/src/main/java/org/apache/druid/segment/virtual/ExpressionSelectors.java", "deletions": 3, "insertions": 5, "lines": 8}], "commitSpoonAstDiffStats": [{"spoonFilePath": "ExpressionSelectors.java", "spoonMethods": [{"INS": 3, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.segment.virtual.ExpressionSelectors.createBindings(org.apache.druid.math.expr.Expr,org.apache.druid.segment.ColumnSelectorFactory)", "MOV": 2, "TOT": 5}]}, {"spoonFilePath": "ExpressionVirtualColumnTest.java", "spoonMethods": [{"INS": 0, "UPD": 3, "DEL": 0, "spoonMethodName": "org.apache.druid.segment.virtual.ExpressionVirtualColumnTest.testRequiredColumns()", "MOV": 0, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.segment.virtual.ExpressionVirtualColumnTest.testDimensionSelectorUsingNonexistentColumn()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.segment.virtual.ExpressionVirtualColumnTest.testLongSelectorUsingStringFunction()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.segment.virtual.ExpressionVirtualColumnTest.testLongSelector()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.segment.virtual.ExpressionVirtualColumnTest.testDimensionSelectorWithExtraction()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.segment.virtual.ExpressionVirtualColumnTest.testLongSelectorWithZLikeExprMacro()", "MOV": 1, "TOT": 2}, {"INS": 1, "UPD": 4, "DEL": 0, "spoonMethodName": "org.apache.druid.segment.virtual.ExpressionVirtualColumnTest", "MOV": 0, "TOT": 5}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.segment.virtual.ExpressionVirtualColumnTest.testLongSelectorOfTimeColumn()", "MOV": 1, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.segment.virtual.ExpressionVirtualColumnTest.testDimensionSelectorUsingStringFunction()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.segment.virtual.ExpressionVirtualColumnTest.testFloatSelector()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.segment.virtual.ExpressionVirtualColumnTest.testDimensionSelector()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.segment.virtual.ExpressionVirtualColumnTest.testObjectSelector()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-11-15 22:13:32", "commitMessage": "Expressions: Fix improper supplier reuse with missing columns. (#6600)\n\n* Expressions: Fix improper supplier reuse with missing columns.\r\n\r\nExpressionSelectors has an optimization that skips building a Map\r\nwhen there is only one input supplier. However, this optimization\r\nshould not be used in the case where the is one input supplier but\r\nmore than one input identifier (which can happen when only one\r\ninput identifier corresponds to an actual column).\r\n\r\nFixes #6556.\r\n\r\n* Add underscores to statics.\r\n", "commitUser": "gianm", "commitDateTime": "2018-11-15 22:13:32", "commitParents": ["93b0d585710668b4e2920db9e6f40ba92b6c50b1"], "commitGHEventType": "referenced", "nameRev": "fe69da0d95a361708923fbe5b82ded0d461aa495 tags/druid-0.14.0-incubating~215", "commitHash": "fe69da0d95a361708923fbe5b82ded0d461aa495"}], "body": "So check out this query, note that `channel` exists but `foo` and `bar` do not\r\n\r\n```json\r\n{\r\n  \"queryType\": \"topN\",\r\n  \"dataSource\": \"wikipedia\",\r\n  \"intervals\": \"2015-09-12T00Z/2015-09-13T00Z\",\r\n  \"granularity\": \"all\",\r\n  \"virtualColumns\": [\r\n    {\r\n      \"type\": \"expression\",\r\n      \"name\": \"v:channel\",\r\n      \"expression\": \"concat(\\\"foo\\\",\\\"channel\\\",\\\"bar\\\")\",\r\n      \"outputType\": \"STRING\"\r\n    }\r\n  ],\r\n  \"dimension\": {\r\n    \"type\": \"default\",\r\n    \"dimension\": \"v:channel\",\r\n    \"outputName\": \"channel\",\r\n    \"outputType\": \"STRING\"\r\n  },\r\n  \"aggregations\": [\r\n    {\r\n      \"name\": \"count\",\r\n      \"type\": \"count\"\r\n    }\r\n  ],\r\n  \"metric\": \"count\",\r\n  \"threshold\": 5\r\n}\r\n```\r\n\r\nThis returns:\r\n\r\n```json\r\n[ {\r\n  \"timestamp\" : \"2015-09-12T00:46:00.000Z\",\r\n  \"result\" : [ {\r\n    \"count\" : 114006,\r\n    \"channel\" : \"enenen\"\r\n  }, {\r\n    \"count\" : 99004,\r\n    \"channel\" : \"vivivi\"\r\n  }, {\r\n    \"count\" : 25022,\r\n    \"channel\" : \"dedede\"\r\n  }, {\r\n    \"count\" : 21191,\r\n    \"channel\" : \"frfrfr\"\r\n  }, {\r\n    \"count\" : 13945,\r\n    \"channel\" : \"rururu\"\r\n  } ]\r\n} ]\r\n```\r\n\r\nThe expected return is:\r\n\r\n```json\r\n[ {\r\n  \"timestamp\" : \"2015-09-12T00:46:00.000Z\",\r\n  \"result\" : [ {\r\n    \"count\" : 114006,\r\n    \"channel\" : \"en\"\r\n  }, {\r\n    \"count\" : 99004,\r\n    \"channel\" : \"vi\"\r\n  }, {\r\n    \"count\" : 25022,\r\n    \"channel\" : \"de\"\r\n  }, {\r\n    \"count\" : 21191,\r\n    \"channel\" : \"fr\"\r\n  }, {\r\n    \"count\" : 13945,\r\n    \"channel\" : \"ru\"\r\n  } ]\r\n} ]\r\n```\r\n\r\nThe issue is obviously with `\"expression\": \"concat(\\\"foo\\\",\\\"channel\\\",\\\"bar\\\")\"` maybe there is some buggy optimization? If you have a concat with two actual columns this does not happen.\r\nThis appears to be an issue in Druid 0.12.x and 0.13.x.\r\n\r\nThis is not an issue with SQL as SQL does not let you put in a column that does not exist.\r\nWait.... that does not sound right.... what if you have a column that exists in one segment and not in another and you query over the segment where the column does not exist?\r\n\r\n![image](https://user-images.githubusercontent.com/177816/47761362-4dd6b080-dc75-11e8-916a-7e88728b362f.png)\r\n\r\nHot DANG! Seems to be the same issue in SQL... not surprising.\r\n\r\n\r\n\r\n"}, {"user": "clintropolis", "commits": {}, "labels": ["Bug"], "created": "2018-10-30 06:45:36", "title": "BloomKFilter used by BloomDimFilter not thread-safe leading to unstable results", "url": "https://github.com/apache/druid/issues/6546", "closed": "2018-11-09 18:55:17", "ttf": 10.000277777777777, "commitsDetails": [], "body": "Experienced unstable results from using `BloomDimFilter` - after investigating it would appear that the [`test` method of the `BloomKFilter` is stateful and not thread-safe](https://github.com/apache/hive/blob/master/storage-api/src/java/org/apache/hive/common/util/BloomKFilter.java#L210) leading to understandably very bizarre behavior when querying a large number of segments.\r\n\r\nThe hive `BloomFilter` implementation does not seem to have that problem with it's `test` methods, which [only read from the `BitSet`](https://github.com/apache/hive/blob/master/storage-api/src/java/org/apache/hive/common/util/BloomFilter.java#L167), and produces stable results in my testing."}, {"user": "jihoonson", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2018-10-26 17:06:44", "title": "TransformSpec doesn't work with KafkaIndexTask", "url": "https://github.com/apache/druid/issues/6534", "closed": "2018-10-26 18:10:35", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "The original discussion is [here](https://groups.google.com/forum/#!topic/druid-user/3lWm80NLrmQ).\r\n\r\nIt looks that transformSpec works with indexTask, but not with KafkaIndexTask."}, {"user": "swapnilpandit", "commits": {}, "labels": ["Bug"], "created": "2018-10-17 13:01:05", "title": "Exception during sketch aggregations while using Result level cache", "url": "https://github.com/apache/druid/issues/6483", "closed": "2019-05-09 20:49:12", "ttf": 204.00027777777777, "commitsDetails": [], "body": "As part of #4843 proposal Result level cache was introduced in durid with #5028. When a theta sketch is used in a metric, druid broker attempts to deserialize a precomputed sketch result stored in a result level cache and fails with an exception from SketchAggregatorFactory. This error is thrown in TopN, GroupBy and Time series queries.\r\n\r\nException stack trace:\r\n`2018-10-16 18:33:31.866 [qtp1084339924-262[timeseries_[audience_insights_theta]_a1e1363b-e0d5-48f3-8732-9341e2d7fdf1]] ERROR org.apache.druid.java.util.common.logger.Logger.error(Logger.java:123)o - Exception handling request: {class=org.apache.druid.server.QueryResource, exceptionType=class org.apache.druid.java.util.common.ISE, exceptionMessage=Object is not of a type[class java.lang.Double] that can be deserialized to sketch., exception=org.apache.druid.java.util.common.ISE: Object is not of a type[class java.lang.Double] that can be deserialized to sketch.\r\n        at org.apache.druid.query.aggregation.datasketches.theta.SketchHolder.deserialize(SketchHolder.java:219) ~[?:?]\r\n        at org.apache.druid.query.aggregation.datasketches.theta.SketchAggregatorFactory.deserialize(SketchAggregatorFactory.java:83) ~[?:?]\r\n        at org.apache.druid.query.timeseries.TimeseriesQueryQueryToolChest$4$1.apply(TimeseriesQueryQueryToolChest.java:322) ~[druid-processing-0.13.0-incubating-rc1.jar:0.13.0-incubating-rc1]\r\n        at org.apache.druid.query.timeseries.TimeseriesQueryQueryToolChest$4$1.apply(TimeseriesQueryQueryToolChest.java:306) ~[druid-processing-0.13.0-incubating-rc1.jar:0.13.0-incubating-rc1]\r\n        at org.apache.druid.java.util.common.guava.MappingYieldingAccumulator.accumulate(MappingYieldingAccumulator.java:61) ~[java-util-0.13.0-incubating-rc1.jar:0.13.0-incubating-rc1]\r\n        at org.apache.druid.java.util.common.guava.BaseSequence.makeYielder(BaseSequence.java:88) ~[java-util-0.13.0-incubating-rc1.jar:0.13.0-incubating-rc1]\r\n        at org.apache.druid.java.util.common.guava.BaseSequence.toYielder(BaseSequence.java:67) ~[java-util-0.13.0-incubating-rc1.jar:0.13.0-incubating-rc1]\r\n        at org.apache.druid.java.util.common.guava.MappedSequence.toYielder(MappedSequence.java:49) ~[java-util-0.13.0-incubating-rc1.jar:0.13.0-incubating-rc1]\r\n        at org.apache.druid.java.util.common.guava.Yielders.each(Yielders.java:32) ~[java-util-0.13.0-incubating-rc1.jar:0.13.0-incubating-rc1]\r\n        at org.apache.druid.server.QueryResource.doPost(QueryResource.java:201) [druid-server-0.13.0-incubating-rc1.jar:0.13.0-incubating-rc1]\r\n        at sun.reflect.GeneratedMethodAccessor161.invoke(Unknown Source) ~[?:?]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_60]\r\n        at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_60]\r\n        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409) [jersey-servlet-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558) [jersey-servlet-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733) [jersey-servlet-1.19.3.jar:1.19.3]\r\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) [javax.servlet-api-3.1.0.jar:3.1.0]\r\n        at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:286) [guice-servlet-4.1.0.jar:?]\r\n        at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:276) [guice-servlet-4.1.0.jar:?]\r\n        at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:181) [guice-servlet-4.1.0.jar:?]\r\n        at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91) [guice-servlet-4.1.0.jar:?]\r\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85) [guice-servlet-4.1.0.jar:?]\r\n        at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:120) [guice-servlet-4.1.0.jar:?]\r\n        at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:135) [guice-servlet-4.1.0.jar:?]\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.apache.druid.server.security.PreResponseAuthorizationCheckFilter.doFilter(PreResponseAuthorizationCheckFilter.java:84) [druid-server-0.13.0-incubating-rc1.jar:0.13.0-incubating-rc1]\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.apache.druid.server.security.AllowOptionsResourceFilter.doFilter(AllowOptionsResourceFilter.java:76) [druid-server-0.13.0-incubating-rc1.jar:0.13.0-incubating-rc1]\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.apache.druid.server.security.AllowAllAuthenticator$1.doFilter(AllowAllAuthenticator.java:85) [druid-server-0.13.0-incubating-rc1.jar:0.13.0-incubating-rc1]\r\n        at org.apache.druid.server.security.AuthenticationWrappingFilter.doFilter(AuthenticationWrappingFilter.java:60) [druid-server-0.13.0-incubating-rc1.jar:0.13.0-incubating-rc1]\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.apache.druid.server.security.SecuritySanityCheckFilter.doFilter(SecuritySanityCheckFilter.java:88) [druid-server-0.13.0-incubating-rc1.jar:0.13.0-incubating-rc1]\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1253) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473) [jetty-servlet-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1155) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:724) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:61) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:169) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.server.Server.handle(Server.java:531) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:352) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260) [jetty-server-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:281) [jetty-io-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102) [jetty-io-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118) [jetty-io-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:760) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:678) [jetty-util-9.4.10.v20180503.jar:9.4.10.v20180503]\r\n        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]\r\n`"}, {"user": "gianm", "commits": {"244046fda5efb72d18044577c8ce7d741ac58df3": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Area - SQL", "Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "\"Response header too large\" on SQL responses with lots of columns", "numCommits": 0, "created": "2018-10-01 18:47:34", "closed": "2018-10-02 01:13:09", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/6409", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 0.0002777777777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "sql/src/main/java/org/apache/druid/sql/http/ArrayWriter.java", "deletions": 0, "insertions": 13, "lines": 13}, {"filePath": "sql/src/main/java/org/apache/druid/sql/http/SqlQuery.java", "deletions": 2, "insertions": 13, "lines": 15}, {"filePath": "sql/src/main/java/org/apache/druid/sql/http/ObjectWriter.java", "deletions": 0, "insertions": 13, "lines": 13}, {"filePath": "sql/src/test/java/org/apache/druid/sql/calcite/http/SqlResourceTest.java", "deletions": 35, "insertions": 97, "lines": 132}, {"filePath": "sql/src/main/java/org/apache/druid/sql/http/ArrayLinesWriter.java", "deletions": 0, "insertions": 13, "lines": 13}, {"filePath": "sql/src/main/java/org/apache/druid/sql/http/ObjectLinesWriter.java", "deletions": 1, "insertions": 14, "lines": 15}, {"filePath": "sql/src/main/java/org/apache/druid/sql/http/ResultFormat.java", "deletions": 0, "insertions": 3, "lines": 3}, {"filePath": "sql/src/test/java/org/apache/druid/sql/calcite/http/SqlQueryTest.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "docs/content/querying/sql.md", "deletions": 6, "insertions": 15, "lines": 21}, {"filePath": "sql/src/main/java/org/apache/druid/sql/http/CsvWriter.java", "deletions": 0, "insertions": 6, "lines": 6}, {"filePath": "sql/src/main/java/org/apache/druid/sql/http/SqlResource.java", "deletions": 7, "insertions": 8, "lines": 15}], "commitSpoonAstDiffStats": [{"spoonFilePath": "SqlResource.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.http.SqlResource.doPost(org.apache.druid.sql.http.SqlQuery,javax.servlet.http.HttpServletRequest).1.write(java.io.OutputStream)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 3, "spoonMethodName": "org.apache.druid.sql.http.SqlResource.doPost(org.apache.druid.sql.http.SqlQuery,javax.servlet.http.HttpServletRequest)", "MOV": 1, "TOT": 5}]}, {"spoonFilePath": "CsvWriter.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.http.CsvWriter.writeHeader(java.util.List)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "ResultFormat.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.http.ResultFormat.writeHeader(java.util.List)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "ObjectLinesWriter.java", "spoonMethods": [{"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.http.ObjectLinesWriter.writeResponseStart()", "MOV": 1, "TOT": 3}]}, {"spoonFilePath": "SqlQueryTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlQueryTest.testSerde()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "ArrayWriter.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.http.ArrayWriter.writeHeader(java.util.List)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "ArrayLinesWriter.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.http.ArrayLinesWriter.writeHeader(java.util.List)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "SqlResourceTest.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testCsvResultFormatWithHeaders()", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testCannotValidate()", "MOV": 1, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testObjectResultFormat().2", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testCsvResultFormat()", "MOV": 2, "TOT": 4}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testArrayResultFormatWithHeader()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testResourceLimitExceeded()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 5, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testFieldAliasingSelect()", "MOV": 5, "TOT": 11}, {"INS": 0, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.doPost(org.apache.druid.sql.http.SqlQuery)", "MOV": 1, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testTimestampsInResponseLosAngelesTimeZone()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testFieldAliasingGroupBy()", "MOV": 1, "TOT": 3}, {"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testObjectResultFormat()", "MOV": 2, "TOT": 4}, {"INS": 0, "UPD": 6, "DEL": 1, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testXDruidColumnHeaders()", "MOV": 8, "TOT": 15}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testExplainCountStar()", "MOV": 1, "TOT": 2}, {"INS": 1, "UPD": 5, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testTimestampsInResponse()", "MOV": 5, "TOT": 11}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testArrayResultFormat()", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testCannotConvert()", "MOV": 2, "TOT": 4}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testArrayLinesResultFormat()", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testArrayLinesResultFormatWithHeader()", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testObjectLinesResultFormat()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 4, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testCountStar()", "MOV": 6, "TOT": 11}]}, {"spoonFilePath": "SqlQuery.java", "spoonMethods": [{"INS": 3, "UPD": 2, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.http.SqlQuery.toString()", "MOV": 3, "TOT": 8}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.http.SqlQuery.includeHeader()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.http.SqlQuery.hashCode()", "MOV": 0, "TOT": 1}, {"INS": 3, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.http.SqlQuery", "MOV": 0, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.http.SqlQuery.equals(java.lang.Object)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "ObjectWriter.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.http.ObjectWriter.writeHeader(java.util.List)", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-10-01 18:13:08", "commitMessage": "SQL: Fix too-long headers in http responses. (#6411)\n\nFixes #6409 by moving column name info from HTTP headers into the\r\nresult body.", "commitUser": "fjy", "commitDateTime": "2018-10-01 18:13:08", "commitParents": ["42e5385e566a7bfb6e00c835b77eb7020d19d8ce"], "commitGHEventType": "referenced", "nameRev": "244046fda5efb72d18044577c8ce7d741ac58df3 tags/druid-0.13.0-incubating-rc1~37", "commitHash": "244046fda5efb72d18044577c8ce7d741ac58df3"}], "body": "I did a SQL query with a few hundred columns and got \"org.eclipse.jetty.http.BadMessageException: 500: Response header too large\". I think it's because of the header that lists out what all the columns are, which was added in #6191. We need to rethink this before it's released in 0.13.0, since it is a regression since 0.12.x."}, {"user": "jon-wei", "commits": {}, "labels": ["Bug"], "created": "2018-09-27 02:06:20", "title": "forbidden-api check prevents building individual modules", "url": "https://github.com/apache/druid/issues/6392", "closed": "2018-09-27 18:25:11", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "After #6371 was merged I am no longer able to build individual druid modules like druid-server:\r\n\r\n```\r\ncd server\r\nmvn clean && mvn install -DskipTests=true\r\n```\r\n```\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD FAILURE\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Total time: 15.322 s\r\n[INFO] Finished at: 2018-09-26T16:42:00-07:00\r\n[INFO] ------------------------------------------------------------------------\r\n[ERROR] Failed to execute goal de.thetaphi:forbiddenapis:2.3:check (compile) on project druid-server: IO problem while reading files with API signatures.: /Users/jw/jw_repo/druid/server/codestyle/joda-time-forbidden-apis.txt (No such file or directory) -> [Help 1]\r\n[ERROR] \r\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\r\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\r\n[ERROR] \r\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\r\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\r\n```"}, {"user": "himanshug", "commits": {}, "labels": ["Area - Testing", "Bug", "Flaky test"], "created": "2018-08-22 21:53:30", "title": "BlockingPoolTest.testConcurrentTakeBatch transient failure", "url": "https://github.com/apache/druid/issues/6218", "closed": "2020-05-03 19:54:27", "ttf": 619.0002777777778, "commitsDetails": [], "body": "seems like a bug in BlockingPool .\r\n\r\nfrom build log...\r\n```\r\ntestConcurrentTakeBatch(io.druid.collections.BlockingPoolTest)  Time elapsed: 0.019 sec  <<< FAILURE!\r\njava.lang.AssertionError\r\n\tat io.druid.collections.BlockingPoolTest.testConcurrentTakeBatch(BlockingPoolTest.java:246)\r\n\r\n```"}, {"user": "jihoonson", "commits": {}, "labels": ["Area - SQL", "Bug"], "created": "2018-08-22 17:44:49", "title": "SQL planning error for nested queries", "url": "https://github.com/apache/druid/issues/6211", "closed": "2018-08-25 21:05:18", "ttf": 3.000277777777778, "commitsDetails": [], "body": "The below query works in 0.10, but not in 0.12.2.\r\n\r\nSQL\r\n\r\n```sql\r\nselect\r\n  l_linenumber,\r\n  l_linestatus,\r\n  sum(l_quantity) as quantity\r\nfrom\r\n  lineitem\r\nwhere\r\n  true and l_linenumber IN (\r\n    select\r\n      l_linenumber\r\n    from\r\n      lineitem\r\n    group by\r\n      l_linenumber\r\n    order by sum(l_quantity) desc limit 100\r\n  )\r\ngroup by\r\n  l_linenumber,\r\n  l_linestatus\r\nhaving sum(l_quantity) > 0\r\n```\r\n\r\nError\r\n\r\n```\r\n2018-08-22T17:38:30,597 WARN [qtp1403539444-112] io.druid.sql.http.SqlResource - Failed to handle query: SqlQuery{query='select l_linenumber, l_linestatus,  sum(l_quantity) as quantity from lineitem where true and l_linenumber IN (   select  l_linenumber  from    lineitem group by  l_linenumber  order by sum(l_quantity) desc limit 100 ) group by l_linenumber, l_linestatus having sum(l_quantity) > 0 ', resultFormat=OBJECT, context={}}\r\norg.apache.calcite.plan.RelOptPlanner$CannotPlanException: Node [rel#76:Subset#9.DRUID.[]] could not be implemented; planner state:\r\n\r\nRoot: rel#76:Subset#9.DRUID.[]\r\nOriginal rel:\r\nLogicalFilter(subset=[rel#76:Subset#9.DRUID.[]], condition=[>($2, 0)]): rowcount = 7.5, cumulative cost = {7.5 rows, 15.0 cpu, 0.0 io}, id = 74\r\n  LogicalAggregate(subset=[rel#73:Subset#8.NONE.[]], group=[{0, 1}], quantity=[SUM($2)]): rowcount = 15.0, cumulative cost = {17.062500715255737 rows, 0.0 cpu, 0.0 io}, id = 72\r\n    LogicalProject(subset=[rel#71:Subset#7.NONE.[]], l_linenumber=[$0], l_linestatus=[$1], l_quantity=[$2]): rowcount = 150.0, cumulative cost = {150.0 rows, 450.0 cpu, 0.0 io}, id = 70\r\n      LogicalJoin(subset=[rel#69:Subset#6.NONE.[]], condition=[=($0, $3)], joinType=[inner]): rowcount = 150.0, cumulative cost = {150.0 rows, 0.0 cpu, 0.0 io}, id = 68\r\n        LogicalProject(subset=[rel#59:Subset#1.NONE.[]], l_linenumber=[$6], l_linestatus=[$7], l_quantity=[$10]): rowcount = 100.0, cumulative cost = {100.0 rows, 300.0 cpu, 0.0 io}, id = 58\r\n          LogicalTableScan(subset=[rel#57:Subset#0.NONE.[]], table=[[druid, lineitem]]): rowcount = 100.0, cumulative cost = {100.0 rows, 101.0 cpu, 0.0 io}, id = 11\r\n        LogicalProject(subset=[rel#67:Subset#5.NONE.[]], l_linenumber=[$0]): rowcount = 10.0, cumulative cost = {10.0 rows, 10.0 cpu, 0.0 io}, id = 66\r\n          LogicalSort(subset=[rel#65:Subset#4.NONE.[1 DESC]], sort0=[$1], dir0=[DESC], fetch=[100]): rowcount = 10.0, cumulative cost = {10.0 rows, 184.2068074395237 cpu, 0.0 io}, id = 64\r\n            LogicalAggregate(subset=[rel#63:Subset#3.NONE.[]], group=[{0}], agg#0=[SUM($1)]): rowcount = 10.0, cumulative cost = {11.375000476837158 rows, 0.0 cpu, 0.0 io}, id = 62\r\n              LogicalProject(subset=[rel#61:Subset#2.NONE.[]], l_linenumber=[$6], l_quantity=[$10]): rowcount = 100.0, cumulative cost = {100.0 rows, 200.0 cpu, 0.0 io}, id = 60\r\n                LogicalTableScan(subset=[rel#57:Subset#0.NONE.[]], table=[[druid, lineitem]]): rowcount = 100.0, cumulative cost = {100.0 rows, 101.0 cpu, 0.0 io}, id = 11\r\n\r\nSets:\r\nSet#0, type: RecordType(TIMESTAMP(0) __time, BIGINT count, VARCHAR l_comment, VARCHAR l_commitdate, DOUBLE l_discount, DOUBLE l_extendedprice, VARCHAR l_linenumber, VARCHAR l_linestatus, VARCHAR l_orderkey, VARCHAR l_partkey, BIGINT l_quantity, VARCHAR l_receiptdate, VARCHAR l_returnflag, VARCHAR l_shipdate, VARCHAR l_shipinstruct, VARCHAR l_shipmode, VARCHAR l_suppkey, DOUBLE l_tax)\r\n        rel#57:Subset#0.NONE.[], best=null, importance=0.531441\r\n                rel#11:LogicalTableScan.NONE.[](table=[druid, lineitem]), rowcount=100.0, cumulative cost={inf}\r\n                rel#101:DruidQueryRel.NONE.[](query={\"queryType\":\"scan\",\"dataSource\":{\"type\":\"table\",\"name\":\"lineitem\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"virtualColumns\":[],\"resultFormat\":\"compactedList\",\"batchSize\":20480,\"limit\":9223372036854775807,\"filter\":null,\"columns\":[\"__time\",\"count\",\"l_comment\",\"l_commitdate\",\"l_discount\",\"l_extendedprice\",\"l_linenumber\",\"l_linestatus\",\"l_orderkey\",\"l_partkey\",\"l_quantity\",\"l_receiptdate\",\"l_returnflag\",\"l_shipdate\",\"l_shipinstruct\",\"l_shipmode\",\"l_suppkey\",\"l_tax\"],\"legacy\":false,\"context\":{},\"descending\":false,\"granularity\":{\"type\":\"all\"}},signature={__time:LONG, count:LONG, l_comment:STRING, l_commitdate:STRING, l_discount:DOUBLE, l_extendedprice:DOUBLE, l_linenumber:STRING, l_linestatus:STRING, l_orderkey:STRING, l_partkey:STRING, l_quantity:LONG, l_receiptdate:STRING, l_returnflag:STRING, l_shipdate:STRING, l_shipinstruct:STRING, l_shipmode:STRING, l_suppkey:STRING, l_tax:DOUBLE}), rowcount=1.0, cumulative cost={inf}\r\n        rel#116:Subset#0.DRUID.[], best=rel#115, importance=0.2657205\r\n                rel#115:DruidQueryRel.DRUID.[](query={\"queryType\":\"scan\",\"dataSource\":{\"type\":\"table\",\"name\":\"lineitem\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"virtualColumns\":[],\"resultFormat\":\"compactedList\",\"batchSize\":20480,\"limit\":9223372036854775807,\"filter\":null,\"columns\":[\"__time\",\"count\",\"l_comment\",\"l_commitdate\",\"l_discount\",\"l_extendedprice\",\"l_linenumber\",\"l_linestatus\",\"l_orderkey\",\"l_partkey\",\"l_quantity\",\"l_receiptdate\",\"l_returnflag\",\"l_shipdate\",\"l_shipinstruct\",\"l_shipmode\",\"l_suppkey\",\"l_tax\"],\"legacy\":false,\"context\":{},\"descending\":false,\"granularity\":{\"type\":\"all\"}},signature={__time:LONG, count:LONG, l_comment:STRING, l_commitdate:STRING, l_discount:DOUBLE, l_extendedprice:DOUBLE, l_linenumber:STRING, l_linestatus:STRING, l_orderkey:STRING, l_partkey:STRING, l_quantity:LONG, l_receiptdate:STRING, l_returnflag:STRING, l_shipdate:STRING, l_shipinstruct:STRING, l_shipmode:STRING, l_suppkey:STRING, l_tax:DOUBLE}), rowcount=1.0, cumulative cost={1.0 rows, 0.0 cpu, 0.0 io}\r\nSet#1, type: RecordType(VARCHAR l_linenumber, VARCHAR l_linestatus, BIGINT l_quantity)\r\n        rel#59:Subset#1.NONE.[], best=null, importance=0.5904900000000001\r\n                rel#58:LogicalProject.NONE.[](input=rel#57:Subset#0.NONE.[],l_linenumber=$6,l_linestatus=$7,l_quantity=$10), rowcount=100.0, cumulative cost={inf}\r\n                rel#105:DruidQueryRel.NONE.[](query={\"queryType\":\"scan\",\"dataSource\":{\"type\":\"table\",\"name\":\"lineitem\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"virtualColumns\":[],\"resultFormat\":\"compactedList\",\"batchSize\":20480,\"limit\":9223372036854775807,\"filter\":null,\"columns\":[\"l_linenumber\",\"l_linestatus\",\"l_quantity\"],\"legacy\":false,\"context\":{},\"descending\":false,\"granularity\":{\"type\":\"all\"}},signature={l_linenumber:STRING, l_linestatus:STRING, l_quantity:LONG}), rowcount=1.0, cumulative cost={inf}\r\n        rel#114:Subset#1.DRUID.[], best=rel#113, importance=0.29524500000000004\r\n                rel#113:DruidQueryRel.DRUID.[](query={\"queryType\":\"scan\",\"dataSource\":{\"type\":\"table\",\"name\":\"lineitem\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"virtualColumns\":[],\"resultFormat\":\"compactedList\",\"batchSize\":20480,\"limit\":9223372036854775807,\"filter\":null,\"columns\":[\"l_linenumber\",\"l_linestatus\",\"l_quantity\"],\"legacy\":false,\"context\":{},\"descending\":false,\"granularity\":{\"type\":\"all\"}},signature={l_linenumber:STRING, l_linestatus:STRING, l_quantity:LONG}), rowcount=1.0, cumulative cost={1.003 rows, 0.0 cpu, 0.0 io}\r\nSet#2, type: RecordType(VARCHAR l_linenumber, BIGINT l_quantity)\r\n        rel#61:Subset#2.NONE.[], best=null, importance=0.4304672100000001\r\n                rel#60:LogicalProject.NONE.[](input=rel#57:Subset#0.NONE.[],l_linenumber=$6,l_quantity=$10), rowcount=100.0, cumulative cost={inf}\r\n                rel#104:DruidQueryRel.NONE.[](query={\"queryType\":\"scan\",\"dataSource\":{\"type\":\"table\",\"name\":\"lineitem\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"virtualColumns\":[],\"resultFormat\":\"compactedList\",\"batchSize\":20480,\"limit\":9223372036854775807,\"filter\":null,\"columns\":[\"l_linenumber\",\"l_quantity\"],\"legacy\":false,\"context\":{},\"descending\":false,\"granularity\":{\"type\":\"all\"}},signature={l_linenumber:STRING, l_quantity:LONG}), rowcount=1.0, cumulative cost={inf}\r\n        rel#108:Subset#2.DRUID.[], best=rel#107, importance=0.21523360500000005\r\n                rel#107:DruidQueryRel.DRUID.[](query={\"queryType\":\"scan\",\"dataSource\":{\"type\":\"table\",\"name\":\"lineitem\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"virtualColumns\":[],\"resultFormat\":\"compactedList\",\"batchSize\":20480,\"limit\":9223372036854775807,\"filter\":null,\"columns\":[\"l_linenumber\",\"l_quantity\"],\"legacy\":false,\"context\":{},\"descending\":false,\"granularity\":{\"type\":\"all\"}},signature={l_linenumber:STRING, l_quantity:LONG}), rowcount=1.0, cumulative cost={1.002 rows, 0.0 cpu, 0.0 io}\r\nSet#3, type: RecordType(VARCHAR l_linenumber, BIGINT $f1)\r\n        rel#63:Subset#3.NONE.[], best=null, importance=0.4782969000000001\r\n                rel#62:LogicalAggregate.NONE.[](input=rel#61:Subset#2.NONE.[],group={0},agg#0=SUM($1)), rowcount=10.0, cumulative cost={inf}\r\n                rel#98:LogicalAggregate.NONE.[](input=rel#57:Subset#0.NONE.[],group={6},agg#0=SUM($10)), rowcount=10.0, cumulative cost={inf}\r\n                rel#102:DruidQueryRel.NONE.[](query={\"queryType\":\"groupBy\",\"dataSource\":{\"type\":\"table\",\"name\":\"lineitem\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"virtualColumns\":[],\"filter\":null,\"granularity\":{\"type\":\"all\"},\"dimensions\":[{\"type\":\"default\",\"dimension\":\"l_linenumber\",\"outputName\":\"d0\",\"outputType\":\"STRING\"}],\"aggregations\":[{\"type\":\"longSum\",\"name\":\"a0\",\"fieldName\":\"l_quantity\",\"expression\":null}],\"postAggregations\":[],\"having\":null,\"limitSpec\":{\"type\":\"NoopLimitSpec\"},\"context\":{},\"descending\":false},signature={d0:STRING, a0:LONG}), rowcount=1.0, cumulative cost={inf}\r\n        rel#112:Subset#3.DRUID.[], best=rel#111, importance=0.23914845000000004\r\n                rel#111:DruidQueryRel.DRUID.[](query={\"queryType\":\"groupBy\",\"dataSource\":{\"type\":\"table\",\"name\":\"lineitem\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"virtualColumns\":[],\"filter\":null,\"granularity\":{\"type\":\"all\"},\"dimensions\":[{\"type\":\"default\",\"dimension\":\"l_linenumber\",\"outputName\":\"d0\",\"outputType\":\"STRING\"}],\"aggregations\":[{\"type\":\"longSum\",\"name\":\"a0\",\"fieldName\":\"l_quantity\",\"expression\":null}],\"postAggregations\":[],\"having\":null,\"limitSpec\":{\"type\":\"NoopLimitSpec\"},\"context\":{},\"descending\":false},signature={d0:STRING, a0:LONG}), rowcount=1.0, cumulative cost={0.5650000000000001 rows, 0.0 cpu, 0.0 io}\r\nSet#4, type: RecordType(VARCHAR l_linenumber, BIGINT $f1)\r\n        rel#65:Subset#4.NONE.[1 DESC], best=null, importance=0.531441\r\n                rel#64:LogicalSort.NONE.[1 DESC](input=rel#63:Subset#3.NONE.[],sort0=$1,dir0=DESC,fetch=100), rowcount=10.0, cumulative cost={inf}\r\n                rel#103:DruidQueryRel.NONE.[1 DESC](query={\"queryType\":\"topN\",\"dataSource\":{\"type\":\"table\",\"name\":\"lineitem\"},\"virtualColumns\":[],\"dimension\":{\"type\":\"default\",\"dimension\":\"l_linenumber\",\"outputName\":\"d0\",\"outputType\":\"STRING\"},\"metric\":{\"type\":\"numeric\",\"metric\":\"a0\"},\"threshold\":100,\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"filter\":null,\"granularity\":{\"type\":\"all\"},\"aggregations\":[{\"type\":\"longSum\",\"name\":\"a0\",\"fieldName\":\"l_quantity\",\"expression\":null}],\"postAggregations\":[],\"context\":{},\"descending\":false},signature={d0:STRING, a0:LONG}), rowcount=1.0, cumulative cost={inf}\r\n        rel#110:Subset#4.DRUID.[1 DESC], best=rel#109, importance=0.2657205\r\n                rel#109:DruidQueryRel.DRUID.[1 DESC](query={\"queryType\":\"topN\",\"dataSource\":{\"type\":\"table\",\"name\":\"lineitem\"},\"virtualColumns\":[],\"dimension\":{\"type\":\"default\",\"dimension\":\"l_linenumber\",\"outputName\":\"d0\",\"outputType\":\"STRING\"},\"metric\":{\"type\":\"numeric\",\"metric\":\"a0\"},\"threshold\":100,\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"filter\":null,\"granularity\":{\"type\":\"all\"},\"aggregations\":[{\"type\":\"longSum\",\"name\":\"a0\",\"fieldName\":\"l_quantity\",\"expression\":null}],\"postAggregations\":[],\"context\":{},\"descending\":false},signature={d0:STRING, a0:LONG}), rowcount=1.0, cumulative cost={0.28250000000000003 rows, 0.0 cpu, 0.0 io}\r\nSet#5, type: RecordType(VARCHAR l_linenumber)\r\n        rel#67:Subset#5.NONE.[], best=null, importance=0.5904900000000001\r\n                rel#66:LogicalProject.NONE.[](input=rel#65:Subset#4.NONE.[1 DESC],l_linenumber=$0), rowcount=10.0, cumulative cost={inf}\r\nSet#6, type: RecordType(VARCHAR l_linenumber, VARCHAR l_linestatus, BIGINT l_quantity, VARCHAR l_linenumber0)\r\n        rel#69:Subset#6.NONE.[], best=null, importance=0.6561\r\n                rel#68:LogicalJoin.NONE.[](left=rel#59:Subset#1.NONE.[],right=rel#67:Subset#5.NONE.[],condition==($0, $3),joinType=inner), rowcount=150.0, cumulative cost={inf}\r\n                rel#83:LogicalProject.NONE.[](input=rel#82:Subset#10.NONE.[],l_linenumber=$1,l_linestatus=$2,l_quantity=$3,l_linenumber0=$0), rowcount=150.0, cumulative cost={inf}\r\nSet#7, type: RecordType(VARCHAR l_linenumber, VARCHAR l_linestatus, BIGINT l_quantity)\r\n        rel#71:Subset#7.NONE.[], best=null, importance=0.7290000000000001\r\n                rel#70:LogicalProject.NONE.[](input=rel#69:Subset#6.NONE.[],l_linenumber=$0,l_linestatus=$1,l_quantity=$2), rowcount=150.0, cumulative cost={inf}\r\n                rel#88:LogicalProject.NONE.[](input=rel#82:Subset#10.NONE.[],l_linenumber=$1,l_linestatus=$2,l_quantity=$3), rowcount=150.0, cumulative cost={inf}\r\nSet#8, type: RecordType(VARCHAR l_linenumber, VARCHAR l_linestatus, BIGINT quantity)\r\n        rel#73:Subset#8.NONE.[], best=null, importance=0.81\r\n                rel#72:LogicalAggregate.NONE.[](input=rel#71:Subset#7.NONE.[],group={0, 1},quantity=SUM($2)), rowcount=15.0, cumulative cost={inf}\r\n                rel#78:LogicalAggregate.NONE.[](input=rel#69:Subset#6.NONE.[],group={0, 1},quantity=SUM($2)), rowcount=15.0, cumulative cost={inf}\r\n                rel#87:LogicalAggregate.NONE.[](input=rel#82:Subset#10.NONE.[],group={1, 2},quantity=SUM($3)), rowcount=15.0, cumulative cost={inf}\r\nSet#9, type: RecordType(VARCHAR l_linenumber, VARCHAR l_linestatus, BIGINT quantity)\r\n        rel#75:Subset#9.NONE.[], best=null, importance=0.9\r\n                rel#74:LogicalFilter.NONE.[](input=rel#73:Subset#8.NONE.[],condition=>($2, 0)), rowcount=7.5, cumulative cost={inf}\r\n        rel#76:Subset#9.DRUID.[], best=null, importance=1.0\r\n                rel#77:AbstractConverter.DRUID.[](input=rel#75:Subset#9.NONE.[],convention=DRUID,sort=[]), rowcount=7.5, cumulative cost={inf}\r\nSet#10, type: RecordType(VARCHAR l_linenumber, VARCHAR l_linenumber0, VARCHAR l_linestatus, BIGINT l_quantity)\r\n        rel#82:Subset#10.NONE.[], best=null, importance=0.5904900000000001\r\n                rel#80:LogicalJoin.NONE.[](left=rel#67:Subset#5.NONE.[],right=rel#59:Subset#1.NONE.[],condition==($1, $0),joinType=inner), rowcount=150.0, cumulative cost={inf}\r\n                rel#86:LogicalProject.NONE.[](input=rel#69:Subset#6.NONE.[],l_linenumber=$3,l_linenumber0=$0,l_linestatus=$1,l_quantity=$2), rowcount=150.0, cumulative cost={inf}\r\n\r\n\r\n        at org.apache.calcite.plan.volcano.RelSubset$CheapestPlanReplacer.visit(RelSubset.java:441) ~[calcite-core-1.15.0.jar:1.15.0]\r\n        at org.apache.calcite.plan.volcano.RelSubset.buildCheapestPlan(RelSubset.java:291) ~[calcite-core-1.15.0.jar:1.15.0]\r\n        at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:666) ~[calcite-core-1.15.0.jar:1.15.0]\r\n        at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:368) ~[calcite-core-1.15.0.jar:1.15.0]\r\n        at org.apache.calcite.tools.Programs$SequenceProgram.run(Programs.java:387) ~[calcite-core-1.15.0.jar:1.15.0]\r\n        at org.apache.calcite.prepare.PlannerImpl.transform(PlannerImpl.java:336) ~[calcite-core-1.15.0.jar:1.15.0]\r\n        at io.druid.sql.calcite.planner.DruidPlanner.planWithDruidConvention(DruidPlanner.java:144) ~[druid-sql-0.12.2.jar:0.12.2]\r\n        at io.druid.sql.calcite.planner.DruidPlanner.plan(DruidPlanner.java:112) ~[druid-sql-0.12.2.jar:0.12.2]\r\n        at io.druid.sql.http.SqlResource.doPost(SqlResource.java:88) [druid-sql-0.12.2.jar:0.12.2]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_161]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_161]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_161]\r\n        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]\r\n        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409) [jersey-servlet-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558) [jersey-servlet-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733) [jersey-servlet-1.19.3.jar:1.19.3]\r\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) [javax.servlet-api-3.1.0.jar:3.1.0]\r\n        at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:286) [guice-servlet-4.1.0.jar:?]\r\n        at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:276) [guice-servlet-4.1.0.jar:?]\r\n        at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:181) [guice-servlet-4.1.0.jar:?]\r\n        at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91) [guice-servlet-4.1.0.jar:?]\r\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85) [guice-servlet-4.1.0.jar:?]\r\n        at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:120) [guice-servlet-4.1.0.jar:?]\r\n        at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:135) [guice-servlet-4.1.0.jar:?]\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at io.druid.server.security.PreResponseAuthorizationCheckFilter.doFilter(PreResponseAuthorizationCheckFilter.java:84) [druid-server-0.12.2.jar:0.12.2]\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at io.druid.server.security.AllowOptionsResourceFilter.doFilter(AllowOptionsResourceFilter.java:76) [druid-server-0.12.2.jar:0.12.2]\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at io.druid.server.security.AllowAllAuthenticator$1.doFilter(AllowAllAuthenticator.java:85) [druid-server-0.12.2.jar:0.12.2]\r\n        at io.druid.server.security.AuthenticationWrappingFilter.doFilter(AuthenticationWrappingFilter.java:60) [druid-server-0.12.2.jar:0.12.2]\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at io.druid.server.security.SecuritySanityCheckFilter.doFilter(SecuritySanityCheckFilter.java:86) [druid-server-0.12.2.jar:0.12.2]\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:224) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.Server.handle(Server.java:534) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_161]\r\n        Suppressed: org.apache.calcite.plan.RelOptPlanner$CannotPlanException: Node [rel#171:Subset#20.BINDABLE.[]] could not be implemented; planner state:\r\nRoot: rel#171:Subset#20.BINDABLE.[]\r\nOriginal rel:\r\nLogicalFilter(subset=[rel#171:Subset#20.BINDABLE.[]], condition=[>($2, 0)]): rowcount = 7.5, cumulative cost = {7.5 rows, 15.0 cpu, 0.0 io}, id = 169\r\n  LogicalAggregate(subset=[rel#168:Subset#19.NONE.[]], group=[{0, 1}], quantity=[SUM($2)]): rowcount = 15.0, cumulative cost = {17.062500715255737 rows, 0.0 cpu, 0.0 io}, id = 167\r\n    LogicalProject(subset=[rel#166:Subset#18.NONE.[]], l_linenumber=[$0], l_linestatus=[$1], l_quantity=[$2]): rowcount = 150.0, cumulative cost = {150.0 rows, 450.0 cpu, 0.0 io}, id = 165\r\n      LogicalJoin(subset=[rel#164:Subset#17.NONE.[]], condition=[=($0, $3)], joinType=[inner]): rowcount = 150.0, cumulative cost = {150.0 rows, 0.0 cpu, 0.0 io}, id = 163\r\n        LogicalProject(subset=[rel#154:Subset#12.NONE.[]], l_linenumber=[$6], l_linestatus=[$7], l_quantity=[$10]): rowcount = 100.0, cumulative cost = {100.0 rows, 300.0 cpu, 0.0 io}, id = 153\r\n          LogicalTableScan(subset=[rel#152:Subset#11.NONE.[]], table=[[druid, lineitem]]): rowcount = 100.0, cumulative cost = {100.0 rows, 101.0 cpu, 0.0 io}, id = 11\r\n        LogicalProject(subset=[rel#162:Subset#16.NONE.[]], l_linenumber=[$0]): rowcount = 10.0, cumulative cost = {10.0 rows, 10.0 cpu, 0.0 io}, id = 161\r\n          LogicalSort(subset=[rel#160:Subset#15.NONE.[1 DESC]], sort0=[$1], dir0=[DESC], fetch=[100]): rowcount = 10.0, cumulative cost = {10.0 rows, 184.2068074395237 cpu, 0.0 io}, id = 159\r\n            LogicalAggregate(subset=[rel#158:Subset#14.NONE.[]], group=[{0}], agg#0=[SUM($1)]): rowcount = 10.0, cumulative cost = {11.375000476837158 rows, 0.0 cpu, 0.0 io}, id = 157\r\n              LogicalProject(subset=[rel#156:Subset#13.NONE.[]], l_linenumber=[$6], l_quantity=[$10]): rowcount = 100.0, cumulative cost = {100.0 rows, 200.0 cpu, 0.0 io}, id = 155\r\n                LogicalTableScan(subset=[rel#152:Subset#11.NONE.[]], table=[[druid, lineitem]]): rowcount = 100.0, cumulative cost = {100.0 rows, 101.0 cpu, 0.0 io}, id = 11\r\n\r\nSets:\r\nSet#11, type: RecordType(TIMESTAMP(0) __time, BIGINT count, VARCHAR l_comment, VARCHAR l_commitdate, DOUBLE l_discount, DOUBLE l_extendedprice, VARCHAR l_linenumber, VARCHAR l_linestatus, VARCHAR l_orderkey, VARCHAR l_partkey, BIGINT l_quantity, VARCHAR l_receiptdate, VARCHAR l_returnflag, VARCHAR l_shipdate, VARCHAR l_shipinstruct, VARCHAR l_shipmode, VARCHAR l_suppkey, DOUBLE l_tax)\r\n        rel#152:Subset#11.NONE.[], best=null, importance=0.531441\r\n                rel#11:LogicalTableScan.NONE.[](table=[druid, lineitem]), rowcount=100.0, cumulative cost={inf}\r\n                rel#267:DruidQueryRel.NONE.[](query={\"queryType\":\"scan\",\"dataSource\":{\"type\":\"table\",\"name\":\"lineitem\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"virtualColumns\":[],\"resultFormat\":\"compactedList\",\"batchSize\":20480,\"limit\":9223372036854775807,\"filter\":null,\"columns\":[\"__time\",\"count\",\"l_comment\",\"l_commitdate\",\"l_discount\",\"l_extendedprice\",\"l_linenumber\",\"l_linestatus\",\"l_orderkey\",\"l_partkey\",\"l_quantity\",\"l_receiptdate\",\"l_returnflag\",\"l_shipdate\",\"l_shipinstruct\",\"l_shipmode\",\"l_suppkey\",\"l_tax\"],\"legacy\":false,\"context\":{},\"descending\":false,\"granularity\":{\"type\":\"all\"}},signature={__time:LONG, count:LONG, l_comment:STRING, l_commitdate:STRING, l_discount:DOUBLE, l_extendedprice:DOUBLE, l_linenumber:STRING, l_linestatus:STRING, l_orderkey:STRING, l_partkey:STRING, l_quantity:LONG, l_receiptdate:STRING, l_returnflag:STRING, l_shipdate:STRING, l_shipinstruct:STRING, l_shipmode:STRING, l_suppkey:STRING, l_tax:DOUBLE}), rowcount=1.0, cumulative cost={inf}\r\n        rel#241:Subset#11.BINDABLE.[], best=null, importance=0.4782969000000001\r\n                rel#240:InterpretableConverter.BINDABLE.[](input=rel#152:Subset#11.NONE.[]), rowcount=100.0, cumulative cost={inf}\r\nSet#12, type: RecordType(VARCHAR l_linenumber, VARCHAR l_linestatus, BIGINT l_quantity)\r\n        rel#154:Subset#12.NONE.[], best=null, importance=0.5904900000000001\r\n                rel#153:LogicalProject.NONE.[](input=rel#152:Subset#11.NONE.[],l_linenumber=$6,l_linestatus=$7,l_quantity=$10), rowcount=100.0, cumulative cost={inf}\r\n                rel#273:DruidQueryRel.NONE.[](query={\"queryType\":\"scan\",\"dataSource\":{\"type\":\"table\",\"name\":\"lineitem\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"virtualColumns\":[],\"resultFormat\":\"compactedList\",\"batchSize\":20480,\"limit\":9223372036854775807,\"filter\":null,\"columns\":[\"l_linenumber\",\"l_linestatus\",\"l_quantity\"],\"legacy\":false,\"context\":{},\"descending\":false,\"granularity\":{\"type\":\"all\"}},signature={l_linenumber:STRING, l_linestatus:STRING, l_quantity:LONG}), rowcount=1.0, cumulative cost={inf}\r\n        rel#233:Subset#12.BINDABLE.[], best=null, importance=0.531441\r\n                rel#232:InterpretableConverter.BINDABLE.[](input=rel#154:Subset#12.NONE.[]), rowcount=100.0, cumulative cost={inf}\r\n                rel#258:BindableProject.BINDABLE.[](input=rel#241:Subset#11.BINDABLE.[],l_linenumber=$6,l_linestatus=$7,l_quantity=$10), rowcount=100.0, cumulative cost={inf}\r\nSet#13, type: RecordType(VARCHAR l_linenumber, BIGINT l_quantity)\r\n        rel#156:Subset#13.NONE.[], best=null, importance=0.4304672100000001\r\n                rel#155:LogicalProject.NONE.[](input=rel#152:Subset#11.NONE.[],l_linenumber=$6,l_quantity=$10), rowcount=100.0, cumulative cost={inf}\r\n                rel#272:DruidQueryRel.NONE.[](query={\"queryType\":\"scan\",\"dataSource\":{\"type\":\"table\",\"name\":\"lineitem\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"virtualColumns\":[],\"resultFormat\":\"compactedList\",\"batchSize\":20480,\"limit\":9223372036854775807,\"filter\":null,\"columns\":[\"l_linenumber\",\"l_quantity\"],\"legacy\":false,\"context\":{},\"descending\":false,\"granularity\":{\"type\":\"all\"}},signature={l_linenumber:STRING, l_quantity:LONG}), rowcount=1.0, cumulative cost={inf}\r\n        rel#209:Subset#13.BINDABLE.[], best=null, importance=0.3874204890000001\r\n                rel#208:InterpretableConverter.BINDABLE.[](input=rel#156:Subset#13.NONE.[]), rowcount=100.0, cumulative cost={inf}\r\n                rel#244:BindableProject.BINDABLE.[](input=rel#241:Subset#11.BINDABLE.[],l_linenumber=$6,l_quantity=$10), rowcount=100.0, cumulative cost={inf}\r\nSet#14, type: RecordType(VARCHAR l_linenumber, BIGINT $f1)\r\n        rel#158:Subset#14.NONE.[], best=null, importance=0.4782969000000001\r\n                rel#157:LogicalAggregate.NONE.[](input=rel#156:Subset#13.NONE.[],group={0},agg#0=SUM($1)), rowcount=10.0, cumulative cost={inf}\r\n                rel#198:LogicalAggregate.NONE.[](input=rel#152:Subset#11.NONE.[],group={6},agg#0=SUM($10)), rowcount=10.0, cumulative cost={inf}\r\n                rel#270:DruidQueryRel.NONE.[](query={\"queryType\":\"groupBy\",\"dataSource\":{\"type\":\"table\",\"name\":\"lineitem\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"virtualColumns\":[],\"filter\":null,\"granularity\":{\"type\":\"all\"},\"dimensions\":[{\"type\":\"default\",\"dimension\":\"l_linenumber\",\"outputName\":\"d0\",\"outputType\":\"STRING\"}],\"aggregations\":[{\"type\":\"longSum\",\"name\":\"a0\",\"fieldName\":\"l_quantity\",\"expression\":null}],\"postAggregations\":[],\"having\":null,\"limitSpec\":{\"type\":\"NoopLimitSpec\"},\"context\":{},\"descending\":false},signature={d0:STRING, a0:LONG}), rowcount=1.0, cumulative cost={inf}\r\n        rel#205:Subset#14.BINDABLE.[], best=null, importance=0.4304672100000001\r\n                rel#204:InterpretableConverter.BINDABLE.[](input=rel#158:Subset#14.NONE.[]), rowcount=10.0, cumulative cost={inf}\r\n                rel#247:BindableAggregate.BINDABLE.[](input=rel#241:Subset#11.BINDABLE.[],group={6},agg#0=SUM($10)), rowcount=10.0, cumulative cost={inf}\r\n                rel#263:BindableAggregate.BINDABLE.[](input=rel#209:Subset#13.BINDABLE.[],group={0},agg#0=SUM($1)), rowcount=10.0, cumulative cost={inf}\r\nSet#15, type: RecordType(VARCHAR l_linenumber, BIGINT $f1)\r\n        rel#160:Subset#15.NONE.[1 DESC], best=null, importance=0.531441\r\n                rel#159:LogicalSort.NONE.[1 DESC](input=rel#158:Subset#14.NONE.[],sort0=$1,dir0=DESC,fetch=100), rowcount=10.0, cumulative cost={inf}\r\n                rel#271:DruidQueryRel.NONE.[1 DESC](query={\"queryType\":\"topN\",\"dataSource\":{\"type\":\"table\",\"name\":\"lineitem\"},\"virtualColumns\":[],\"dimension\":{\"type\":\"default\",\"dimension\":\"l_linenumber\",\"outputName\":\"d0\",\"outputType\":\"STRING\"},\"metric\":{\"type\":\"numeric\",\"metric\":\"a0\"},\"threshold\":100,\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"filter\":null,\"granularity\":{\"type\":\"all\"},\"aggregations\":[{\"type\":\"longSum\",\"name\":\"a0\",\"fieldName\":\"l_quantity\",\"expression\":null}],\"postAggregations\":[],\"context\":{},\"descending\":false},signature={d0:STRING, a0:LONG}), rowcount=1.0, cumulative cost={inf}\r\n        rel#207:Subset#15.BINDABLE.[1 DESC], best=null, importance=0.4782969000000001\r\n                rel#206:InterpretableConverter.BINDABLE.[1 DESC](input=rel#160:Subset#15.NONE.[1 DESC]), rowcount=10.0, cumulative cost={inf}\r\n                rel#242:BindableSort.BINDABLE.[1 DESC](input=rel#205:Subset#14.BINDABLE.[],sort0=$1,dir0=DESC,fetch=100), rowcount=10.0, cumulative cost={inf}\r\nSet#16, type: RecordType(VARCHAR l_linenumber)\r\n        rel#162:Subset#16.NONE.[], best=null, importance=0.5904900000000001\r\n                rel#161:LogicalProject.NONE.[](input=rel#160:Subset#15.NONE.[1 DESC],l_linenumber=$0), rowcount=10.0, cumulative cost={inf}\r\n        rel#202:Subset#16.BINDABLE.[], best=null, importance=0.531441\r\n                rel#201:InterpretableConverter.BINDABLE.[](input=rel#162:Subset#16.NONE.[]), rowcount=10.0, cumulative cost={inf}\r\n                rel#243:BindableProject.BINDABLE.[](input=rel#207:Subset#15.BINDABLE.[1 DESC],l_linenumber=$0), rowcount=10.0, cumulative cost={inf}\r\nSet#17, type: RecordType(VARCHAR l_linenumber, VARCHAR l_linestatus, BIGINT l_quantity, VARCHAR l_linenumber0)\r\n        rel#164:Subset#17.NONE.[], best=null, importance=0.6561\r\n                rel#163:LogicalJoin.NONE.[](left=rel#154:Subset#12.NONE.[],right=rel#162:Subset#16.NONE.[],condition==($0, $3),joinType=inner), rowcount=150.0, cumulative cost={inf}\r\n                rel#183:LogicalProject.NONE.[](input=rel#182:Subset#21.NONE.[],l_linenumber=$1,l_linestatus=$2,l_quantity=$3,l_linenumber0=$0), rowcount=150.0, cumulative cost={inf}\r\n        rel#239:Subset#17.BINDABLE.[], best=null, importance=0.5904900000000001\r\n                rel#238:InterpretableConverter.BINDABLE.[](input=rel#164:Subset#17.NONE.[]), rowcount=150.0, cumulative cost={inf}\r\n                rel#260:BindableProject.BINDABLE.[](input=rel#236:Subset#21.BINDABLE.[],l_linenumber=$1,l_linestatus=$2,l_quantity=$3,l_linenumber0=$0), rowcount=150.0, cumulative cost={inf}\r\n                rel#262:BindableJoin.BINDABLE.[](left=rel#233:Subset#12.BINDABLE.[],right=rel#202:Subset#16.BINDABLE.[],condition==($0, $3),joinType=inner), rowcount=150.0, cumulative cost={inf}\r\nSet#18, type: RecordType(VARCHAR l_linenumber, VARCHAR l_linestatus, BIGINT l_quantity)\r\n        rel#166:Subset#18.NONE.[], best=null, importance=0.7290000000000001\r\n                rel#165:LogicalProject.NONE.[](input=rel#164:Subset#17.NONE.[],l_linenumber=$0,l_linestatus=$1,l_quantity=$2), rowcount=150.0, cumulative cost={inf}\r\n                rel#188:LogicalProject.NONE.[](input=rel#182:Subset#21.NONE.[],l_linenumber=$1,l_linestatus=$2,l_quantity=$3), rowcount=150.0, cumulative cost={inf}\r\n                rel#287:LogicalProject.NONE.[](input=rel#236:Subset#21.BINDABLE.[],l_linenumber=$1,l_linestatus=$2,l_quantity=$3), rowcount=150.0, cumulative cost={inf}\r\n                rel#292:LogicalProject.NONE.[](input=rel#239:Subset#17.BINDABLE.[],l_linenumber=$0,l_linestatus=$1,l_quantity=$2), rowcount=150.0, cumulative cost={inf}\r\n        rel#212:Subset#18.BINDABLE.[], best=null, importance=0.81\r\n                rel#211:InterpretableConverter.BINDABLE.[](input=rel#166:Subset#18.NONE.[]), rowcount=150.0, cumulative cost={inf}\r\n                rel#245:BindableProject.BINDABLE.[](input=rel#236:Subset#21.BINDABLE.[],l_linenumber=$1,l_linestatus=$2,l_quantity=$3), rowcount=150.0, cumulative cost={inf}\r\n                rel#246:BindableProject.BINDABLE.[](input=rel#239:Subset#17.BINDABLE.[],l_linenumber=$0,l_linestatus=$1,l_quantity=$2), rowcount=150.0, cumulative cost={inf}\r\nSet#19, type: RecordType(VARCHAR l_linenumber, VARCHAR l_linestatus, BIGINT quantity)\r\n        rel#168:Subset#19.NONE.[], best=null, importance=0.81\r\n                rel#167:LogicalAggregate.NONE.[](input=rel#166:Subset#18.NONE.[],group={0, 1},quantity=SUM($2)), rowcount=15.0, cumulative cost={inf}\r\n                rel#178:LogicalAggregate.NONE.[](input=rel#164:Subset#17.NONE.[],group={0, 1},quantity=SUM($2)), rowcount=15.0, cumulative cost={inf}\r\n                rel#187:LogicalAggregate.NONE.[](input=rel#182:Subset#21.NONE.[],group={1, 2},quantity=SUM($3)), rowcount=15.0, cumulative cost={inf}\r\n                rel#288:LogicalAggregate.NONE.[](input=rel#236:Subset#21.BINDABLE.[],group={1, 2},quantity=SUM($3)), rowcount=15.0, cumulative cost={inf}\r\n                rel#294:LogicalAggregate.NONE.[](input=rel#239:Subset#17.BINDABLE.[],group={0, 1},quantity=SUM($2)), rowcount=15.0, cumulative cost={inf}\r\n        rel#176:Subset#19.BINDABLE.[], best=null, importance=0.9\r\n                rel#214:InterpretableConverter.BINDABLE.[](input=rel#168:Subset#19.NONE.[]), rowcount=15.0, cumulative cost={inf}\r\n                rel#264:BindableAggregate.BINDABLE.[](input=rel#236:Subset#21.BINDABLE.[],group={1, 2},quantity=SUM($3)), rowcount=15.0, cumulative cost={inf}\r\n                rel#265:BindableAggregate.BINDABLE.[](input=rel#239:Subset#17.BINDABLE.[],group={0, 1},quantity=SUM($2)), rowcount=15.0, cumulative cost={inf}\r\n                rel#266:BindableAggregate.BINDABLE.[](input=rel#212:Subset#18.BINDABLE.[],group={0, 1},quantity=SUM($2)), rowcount=15.0, cumulative cost={inf}\r\nSet#20, type: RecordType(VARCHAR l_linenumber, VARCHAR l_linestatus, BIGINT quantity)\r\n        rel#170:Subset#20.NONE.[], best=null, importance=0.9\r\n                rel#169:LogicalFilter.NONE.[](input=rel#168:Subset#19.NONE.[],condition=>($2, 0)), rowcount=7.5, cumulative cost={inf}\r\n        rel#171:Subset#20.BINDABLE.[], best=null, importance=1.0\r\n                rel#172:AbstractConverter.BINDABLE.[](input=rel#170:Subset#20.NONE.[],convention=BINDABLE,sort=[]), rowcount=7.5, cumulative cost={inf}\r\n                rel#173:InterpretableConverter.BINDABLE.[](input=rel#170:Subset#20.NONE.[]), rowcount=7.5, cumulative cost={inf}\r\n                rel#177:BindableFilter.BINDABLE.[[]](input=rel#176:Subset#19.BINDABLE.[],condition=>($2, 0)), rowcount=7.5, cumulative cost={inf}\r\nSet#21, type: RecordType(VARCHAR l_linenumber, VARCHAR l_linenumber0, VARCHAR l_linestatus, BIGINT l_quantity)\r\n        rel#182:Subset#21.NONE.[], best=null, importance=0.5904900000000001\r\n                rel#180:LogicalJoin.NONE.[](left=rel#162:Subset#16.NONE.[],right=rel#154:Subset#12.NONE.[],condition==($1, $0),joinType=inner), rowcount=150.0, cumulative cost={inf}\r\n                rel#186:LogicalProject.NONE.[](input=rel#164:Subset#17.NONE.[],l_linenumber=$3,l_linenumber0=$0,l_linestatus=$1,l_quantity=$2), rowcount=150.0, cumulative cost={inf}\r\n        rel#236:Subset#21.BINDABLE.[], best=null, importance=0.6561\r\n                rel#235:InterpretableConverter.BINDABLE.[](input=rel#182:Subset#21.NONE.[]), rowcount=150.0, cumulative cost={inf}\r\n                rel#259:BindableProject.BINDABLE.[](input=rel#239:Subset#17.BINDABLE.[],l_linenumber=$3,l_linenumber0=$0,l_linestatus=$1,l_quantity=$2), rowcount=150.0, cumulative cost={inf}\r\n                rel#261:BindableJoin.BINDABLE.[](left=rel#202:Subset#16.BINDABLE.[],right=rel#233:Subset#12.BINDABLE.[],condition==($1, $0),joinType=inner), rowcount=150.0, cumulative cost={inf}\r\n\r\n\r\n                at org.apache.calcite.plan.volcano.RelSubset$CheapestPlanReplacer.visit(RelSubset.java:441) ~[calcite-core-1.15.0.jar:1.15.0]\r\n                at org.apache.calcite.plan.volcano.RelSubset.buildCheapestPlan(RelSubset.java:291) ~[calcite-core-1.15.0.jar:1.15.0]\r\n                at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:666) ~[calcite-core-1.15.0.jar:1.15.0]\r\n                at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:368) ~[calcite-core-1.15.0.jar:1.15.0]\r\n                at org.apache.calcite.tools.Programs$SequenceProgram.run(Programs.java:387) ~[calcite-core-1.15.0.jar:1.15.0]\r\n                at org.apache.calcite.prepare.PlannerImpl.transform(PlannerImpl.java:336) ~[calcite-core-1.15.0.jar:1.15.0]\r\n                at io.druid.sql.calcite.planner.DruidPlanner.planWithBindableConvention(DruidPlanner.java:265) ~[druid-sql-0.12.2.jar:0.12.2]\r\n                at io.druid.sql.calcite.planner.DruidPlanner.plan(DruidPlanner.java:117) ~[druid-sql-0.12.2.jar:0.12.2]\r\n                at io.druid.sql.http.SqlResource.doPost(SqlResource.java:88) [druid-sql-0.12.2.jar:0.12.2]\r\n                at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_161]\r\n                at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_161]\r\n                at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_161]\r\n                at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]\r\n                at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60) [jersey-server-1.19.3.jar:1.19.3]\r\n                at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205) [jersey-server-1.19.3.jar:1.19.3]\r\n                at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75) [jersey-server-1.19.3.jar:1.19.3]\r\n                at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302) [jersey-server-1.19.3.jar:1.19.3]\r\n                at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108) [jersey-server-1.19.3.jar:1.19.3]\r\n                at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) [jersey-server-1.19.3.jar:1.19.3]\r\n                at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84) [jersey-server-1.19.3.jar:1.19.3]\r\n                at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542) [jersey-server-1.19.3.jar:1.19.3]\r\n                at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473) [jersey-server-1.19.3.jar:1.19.3]\r\n                at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419) [jersey-server-1.19.3.jar:1.19.3]\r\n                at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409) [jersey-server-1.19.3.jar:1.19.3]\r\n                at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409) [jersey-servlet-1.19.3.jar:1.19.3]\r\n                at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558) [jersey-servlet-1.19.3.jar:1.19.3]\r\n                at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733) [jersey-servlet-1.19.3.jar:1.19.3]\r\n                at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) [javax.servlet-api-3.1.0.jar:3.1.0]\r\n                at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:286) [guice-servlet-4.1.0.jar:?]\r\n                at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:276) [guice-servlet-4.1.0.jar:?]\r\n                at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:181) [guice-servlet-4.1.0.jar:?]\r\n                at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91) [guice-servlet-4.1.0.jar:?]\r\n                at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85) [guice-servlet-4.1.0.jar:?]\r\n                at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:120) [guice-servlet-4.1.0.jar:?]\r\n                at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:135) [guice-servlet-4.1.0.jar:?]\r\n                at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at io.druid.server.security.PreResponseAuthorizationCheckFilter.doFilter(PreResponseAuthorizationCheckFilter.java:84) [druid-server-0.12.2.jar:0.12.2]\r\n                at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at io.druid.server.security.AllowOptionsResourceFilter.doFilter(AllowOptionsResourceFilter.java:76) [druid-server-0.12.2.jar:0.12.2]\r\n                at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at io.druid.server.security.AllowAllAuthenticator$1.doFilter(AllowAllAuthenticator.java:85) [druid-server-0.12.2.jar:0.12.2]\r\n                at io.druid.server.security.AuthenticationWrappingFilter.doFilter(AuthenticationWrappingFilter.java:60) [druid-server-0.12.2.jar:0.12.2]\r\n                at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at io.druid.server.security.SecuritySanityCheckFilter.doFilter(SecuritySanityCheckFilter.java:86) [druid-server-0.12.2.jar:0.12.2]\r\n                at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:224) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.server.Server.handle(Server.java:534) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n                at java.lang.Thread.run(Thread.java:748) [?:1.8.0_161]\r\n```"}, {"user": "jihoonson", "commits": {}, "labels": ["Area - Testing", "Bug"], "created": "2018-08-21 23:09:53", "title": "AssertionError in testCheckpointForUnknownTaskGroup of KafkaSupervisorTest", "url": "https://github.com/apache/druid/issues/6205", "closed": "2018-09-04 22:42:57", "ttf": 13.000277777777777, "commitsDetails": [], "body": "```\r\ntestCheckpointForUnknownTaskGroup[numThreads = 8](io.druid.indexing.kafka.supervisor.KafkaSupervisorTest)  Time elapsed: 0.147 sec  <<< FAILURE!\r\njava.lang.AssertionError\r\n\tat io.druid.indexing.kafka.supervisor.KafkaSupervisorTest.testCheckpointForUnknownTaskGroup(KafkaSupervisorTest.java:2197)\r\n```"}, {"user": "jon-wei", "commits": {"c3aaf8122d658fce578e75ccbba18f7c74b8114f": {"commitGHEventType": "referenced", "commitUser": "gianm"}}, "changesInPackagesGIT": ["indexing-service/src/main/java/io/druid/indexing/overlord/hrtr"], "labels": ["Bug", "HTTP"], "spoonStatsSummary": {"spoonFilesChanged": 1, "spoonMethodsChanged": 3, "INS": 8, "UPD": 0, "DEL": 1, "MOV": 4, "TOT": 13}, "title": "Deadlock on overlord with HttpRemoteTaskRunner", "numCommits": 1, "created": "2018-08-21 17:37:43", "closed": "2018-08-25 21:15:57", "gitStatsSummary": {"deletions": 10, "insertions": 25, "lines": 35, "gitFilesChange": 1}, "statsSkippedReason": "", "changesInPackagesSPOON": ["io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.taskAddedOrUpdated(io.druid.indexing.worker.TaskAnnouncement,io.druid.indexing.overlord.hrtr.WorkerHolder)", "io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.taskComplete(io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner$HttpRemoteTaskRunnerWorkItem,io.druid.indexing.overlord.hrtr.WorkerHolder,io.druid.indexer.TaskStatus)", "io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.runTaskOnWorker(io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner$HttpRemoteTaskRunnerWorkItem,java.lang.String)"], "filteredCommits": ["c3aaf8122d658fce578e75ccbba18f7c74b8114f"], "url": "https://github.com/apache/druid/issues/6201", "filteredCommitsReason": {"multipleIssueFixes": 0, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 4.000277777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "indexing-service/src/main/java/io/druid/indexing/overlord/hrtr/HttpRemoteTaskRunner.java", "deletions": 10, "insertions": 25, "lines": 35}], "commitSpoonAstDiffStats": [{"spoonFilePath": "HttpRemoteTaskRunner.java", "spoonMethods": [{"INS": 4, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.runTaskOnWorker(io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner$HttpRemoteTaskRunnerWorkItem,java.lang.String)", "MOV": 3, "TOT": 8}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.taskComplete(io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner$HttpRemoteTaskRunnerWorkItem,io.druid.indexing.overlord.hrtr.WorkerHolder,io.druid.indexer.TaskStatus)", "MOV": 0, "TOT": 1}, {"INS": 3, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.taskAddedOrUpdated(io.druid.indexing.worker.TaskAnnouncement,io.druid.indexing.overlord.hrtr.WorkerHolder)", "MOV": 1, "TOT": 4}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-08-25 14:15:57", "commitMessage": "fix TaskQueue-HRTR deadlock (#6212)\n\n* fix TaskQueue-HRTR deadlock causing https://github.com/apache/incubator-druid/issues/6201\r\n\r\n* address review comments\r\n", "commitUser": "gianm", "commitDateTime": "2018-08-25 14:15:57", "commitParents": ["28e6ae3664b02b17ee691968469905f8df5a8e28"], "commitGHEventType": "referenced", "nameRev": "c3aaf8122d658fce578e75ccbba18f7c74b8114f tags/druid-0.13.0-incubating-rc1~130", "commitHash": "c3aaf8122d658fce578e75ccbba18f7c74b8114f"}], "body": "We noticed deadlocks on the overlord in our test cluster in HttpRemoteTaskRunner:\r\n\r\n```\r\n\"hrtr-pending-tasks-runner-0\" #193 daemon prio=5 os_prio=0 tid=0x00007fc5a8020000 nid=0x72ec waiting for monitor entry [0x00007fc5644b4000]\r\n   java.lang.Thread.State: BLOCKED (on object monitor)\r\n        at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.runTaskOnWorker(HttpRemoteTaskRunner.java:379)\r\n        - waiting to lock <0x00000000c1544c10> (a java.lang.Object)\r\n        at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.lambda$addPendingTaskToExecutor$7(HttpRemoteTaskRunner.java:1005)\r\n        at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner$$Lambda$133/748833484.run(Unknown Source)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n```\r\n\r\n```\r\njava.lang.Thread.State: BLOCKED (on object monitor)\r\n        at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.getKnownTasks(HttpRemoteTaskRunner.java:1111)\r\n        - waiting to lock <0x00000000c1544c10> (a java.lang.Object)\r\n        at io.druid.indexing.overlord.TaskQueue.manage(TaskQueue.java:232)\r\n        at io.druid.indexing.overlord.TaskQueue.access$000(TaskQueue.java:69)\r\n        at io.druid.indexing.overlord.TaskQueue$1.run(TaskQueue.java:136)\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n```\r\n\r\n```\r\n\"HttpRemoteTaskRunner-worker-sync-4\" #116 daemon prio=5 os_prio=0 tid=0x00007fc588018800 nid=0x724c waiting for monitor entry [0x00007fc569801000]\r\n   java.lang.Thread.State: BLOCKED (on object monitor)\r\n        at io.druid.server.coordination.ChangeRequestHttpSyncer.stop(ChangeRequestHttpSyncer.java:138)\r\n        - waiting to lock <0x00000000c1544980> (a io.druid.concurrent.LifecycleLock)\r\n        at io.druid.indexing.overlord.hrtr.WorkerHolder.stop(WorkerHolder.java:333)\r\n        at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.removeWorker(HttpRemoteTaskRunner.java:532)\r\n        - locked <0x00000000c1540300> (a java.util.concurrent.ConcurrentHashMap)\r\n        at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.lambda$scheduleSyncMonitoring$2(HttpRemoteTaskRunner.java:642)\r\n        - locked <0x00000000c1540300> (a java.util.concurrent.ConcurrentHashMap)\r\n        at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner$$Lambda$90/1851975649.run(Unknown Source)\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n```\r\n\r\n```\r\njava.lang.Thread.State: BLOCKED (on object monitor)\r\n        at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.taskAddedOrUpdated(HttpRemoteTaskRunner.java:1174)\r\n        - waiting to lock <0x00000000c1544c10> (a java.lang.Object)\r\n        at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner$$Lambda$87/935105358.taskAddedOrUpdated(Unknown Source)\r\n        at io.druid.indexing.overlord.hrtr.WorkerHolder$2.notifyListener(WorkerHolder.java:449)\r\n        at io.druid.indexing.overlord.hrtr.WorkerHolder$2.deltaSync(WorkerHolder.java:442)\r\n        at io.druid.server.coordination.ChangeRequestHttpSyncer$1.onSuccess(ChangeRequestHttpSyncer.java:269)\r\n        - locked <0x00000000c1543b50> (a io.druid.concurrent.LifecycleLock)\r\n        at io.druid.server.coordination.ChangeRequestHttpSyncer$1.onSuccess(ChangeRequestHttpSyncer.java:225)\r\n        at com.google.common.util.concurrent.Futures$4.run(Futures.java:1181)\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n```"}, {"user": "gianm", "commits": {}, "labels": ["Area - Querying", "Bug"], "created": "2018-08-15 02:04:27", "title": "TimeBoundary, DataSourceMetadata filterSegments flawed with overlapping segments", "url": "https://github.com/apache/druid/issues/6177", "closed": "2019-02-08 18:03:03", "ttf": 177.00027777777777, "commitsDetails": [], "body": "The \"interval\" on a LogicalSegment corresponds to the timeline object holder interval, meaning that with overlapping segments, it is only the queryable part. This means that the code in filterSegments of both timeBoundary and dataSourceMetadata is flawed. It looks like this in both of them:\r\n\r\n```java\r\n    final T min = query.isMaxTime() ? null : segments.get(0);\r\n    final T max = query.isMinTime() ? null : segments.get(segments.size() - 1);\r\n\r\n    return Lists.newArrayList(\r\n        Iterables.filter(\r\n            segments,\r\n            input -> (min != null && input.getInterval().overlaps(min.getInterval())) ||\r\n                   (max != null && input.getInterval().overlaps(max.getInterval()))\r\n        )\r\n    );\r\n```\r\n\r\nBut because of how \"interval\" works on the LogicalSegments, `input.getInterval()` for two different LogicalSegments will never overlap. This causes erroneous results in cases like this:\r\n\r\n- Segment A is 2017/2018 (YEAR granularity) and has data up through May 2017.\r\n- Segment B is 2017-08-01/2017-08-02 (DAY granularity) and has data for August 1 2017.\r\n\r\nIn this case, three LogicalSegments are returned:\r\n\r\n- 2017/2017-08-01\r\n- 2017-08-01/2017-08-02\r\n- 2017-08-02/2018\r\n\r\nAnd the filterSegments methods on these two query types will only inspect the last LogicalSegment, meaning they'll miss the _actual_ max time, which is in the 2017-08-01/2017-08-02 holder."}, {"user": "jihoonson", "commits": {}, "labels": ["Area - Testing", "Bug"], "created": "2018-08-14 21:55:39", "title": "AssertionError at testCheckpointForInactiveTaskGroup in KafkaSupervisorTest", "url": "https://github.com/apache/druid/issues/6174", "closed": "2018-08-21 18:33:45", "ttf": 6.000277777777778, "commitsDetails": [], "body": "```\r\nTests run: 64, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 26.081 sec <<< FAILURE! - in io.druid.indexing.kafka.supervisor.KafkaSupervisorTest\r\ntestCheckpointForInactiveTaskGroup[numThreads = 8](io.druid.indexing.kafka.supervisor.KafkaSupervisorTest)  Time elapsed: 0.124 sec  <<< FAILURE!\r\njava.lang.AssertionError: \r\nexpected null, but was:<java.lang.AssertionError: \r\n  Unexpected method call TaskRunner.getRunningTasks():\r\n\tat org.easymock.internal.MockInvocationHandler.invoke(MockInvocationHandler.java:44)\r\n\tat org.easymock.internal.ObjectMethodsFilter.invoke(ObjectMethodsFilter.java:94)\r\n\tat com.sun.proxy.$Proxy43.getRunningTasks(Unknown Source)\r\n\tat io.druid.indexing.kafka.supervisor.KafkaSupervisor$1.getTaskLocation(KafkaSupervisor.java:296)\r\n\tat io.druid.indexing.kafka.supervisor.KafkaSupervisor.checkpointTaskGroup(KafkaSupervisor.java:1503)\r\n\tat io.druid.indexing.kafka.supervisor.KafkaSupervisor.checkTaskDuration(KafkaSupervisor.java:1433)\r\n\tat io.druid.indexing.kafka.supervisor.KafkaSupervisor.runInternal(KafkaSupervisor.java:879)\r\n\tat io.druid.indexing.kafka.supervisor.KafkaSupervisor$RunNotice.handle(KafkaSupervisor.java:595)\r\n\tat io.druid.indexing.kafka.supervisor.KafkaSupervisor$2.run(KafkaSupervisor.java:369)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n>\r\n\tat io.druid.indexing.kafka.supervisor.KafkaSupervisorTest.testCheckpointForInactiveTaskGroup(KafkaSupervisorTest.java:2118)\r\n```"}, {"user": "wpcnjupt", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2018-08-13 11:56:03", "title": "java.lang.NullPointerException: taskGroupId   when middlemanager rolling update to 0.12.2", "url": "https://github.com/apache/druid/issues/6163", "closed": "2018-08-17 17:15:46", "ttf": 4.000277777777778, "commitsDetails": [], "body": "hi\r\nI am rolling update to 0.12.2. \r\nI have updated the Historical nodes and one middlemanager node. \r\nwhen the updated middlemanage run KIS workers, it failed sometimes, the peon log is:\r\n\r\n`[19:33:41:332] [INFO] - io.druid.java.util.common.logger.Logger.info(Logger.java:78) - Unannouncing self[DruidServerMetadata{name='172.16.226.7:8104', hostAndPort='172.16.226.7:8104', hostAndTlsPort='null', maxSize=0, tier='_default_tier', type=indexer-executor, priority=0}] at [/druid/announcements/172.16.226.7:8104]\r\n[19:33:41:332] [INFO] - io.druid.java.util.common.logger.Logger.info(Logger.java:78) - unannouncing [/druid/announcements/172.16.226.7:8104]\r\n[19:33:41:338] [ERROR] - io.druid.java.util.common.logger.Logger.error(Logger.java:130) - Exception while running task[AbstractTask{id='index_kafka_crawler-worker-event_027f5c0ae282e3f_mdifgfpg', groupId='index_kafka_crawler-worker-event', taskResource=TaskResource{availabilityGroup='index_kafka_crawler-worker-event_027f5c0ae282e3f', requiredCapacity=1}, dataSource='crawler-worker-event', context={checkpoints={\"1\":{\"0\":21547776416,\"1\":11299531938,\"2\":11299574531,\"3\":11299914740},\"2\":{\"0\":21549477707,\"1\":11301233731,\"2\":11301277162,\"3\":11301615534}}, IS_INCREMENTAL_HANDOFF_SUPPORTED=true}}]\r\njava.lang.NullPointerException: taskGroupId\r\n\tat com.google.common.base.Preconditions.checkNotNull(Preconditions.java:229) ~[guava-16.0.1.jar:?]\r\n\tat io.druid.indexing.common.actions.CheckPointDataSourceMetadataAction.<init>(CheckPointDataSourceMetadataAction.java:45) ~[druid-indexing-service-0.12.2.jar:0.12.2]\r\n\tat io.druid.indexing.kafka.KafkaIndexTask.run(KafkaIndexTask.java:708) ~[?:?]\r\n\tat io.druid.indexing.overlord.ThreadPoolTaskRunner$ThreadPoolTaskRunnerCallable.call(ThreadPoolTaskRunner.java:444) [druid-indexing-service-0.12.2.jar:0.12.2]\r\n\tat io.druid.indexing.overlord.ThreadPoolTaskRunner$ThreadPoolTaskRunnerCallable.call(ThreadPoolTaskRunner.java:416) [druid-indexing-service-0.12.2.jar:0.12.2]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_131]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]\r\n[19:33:41:339] [INFO] - io.druid.java.util.common.logger.Logger.info(Logger.java:78) - Task [index_kafka_crawler-worker-event_027f5c0ae282e3f_mdifgfpg] status changed to [FAILED].\r\n[19:33:41:340] [INFO] - io.druid.java.util.common.logger.Logger.info(Logger.java:78) - Task completed with status: {\r\n  \"id\" : \"index_kafka_crawler-worker-event_027f5c0ae282e3f_mdifgfpg\",\r\n  \"status\" : \"FAILED\",\r\n  \"duration\" : 1333794\r\n}\r\n[19:33:41:346] [INFO] - io.druid.java.util.common.logger.Logger.info(Logger.java:78) - Invoking stop method[public void io.druid.server.listener.announcer.ListenerResourceAnnouncer.stop()] on object[io.druid.query.lookup.LookupResourceListenerAnnouncer@266da047].`"}, {"user": "jihoonson", "commits": {}, "labels": ["Area - Testing", "Bug"], "created": "2018-08-10 00:14:41", "title": "Race in testCheckpointForUnknownTaskGroup() of KafkaSupervisorTest", "url": "https://github.com/apache/druid/issues/6139", "closed": "2018-08-11 15:26:47", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "The stacktrace is:\r\n\r\n```\r\n2018-08-10T00:03:11,557 ERROR [KafkaSupervisor-testDS] io.druid.indexing.kafka.supervisor.KafkaSupervisor - KafkaSupervisor[testDS] failed to handle notice: {class=io.druid.indexing.kafka.supervisor.KafkaSupervisor, exceptionType=class io.druid.java.util.common.ISE, exceptionMessage=WTH?! cannot find taskGroup [0] among all taskGroups [{}], noticeClass=CheckpointNotice}\r\nio.druid.java.util.common.ISE: WTH?! cannot find taskGroup [0] among all taskGroups [{}]\r\n\tat io.druid.indexing.kafka.supervisor.KafkaSupervisor$CheckpointNotice.isValidTaskGroup(KafkaSupervisor.java:686) ~[classes/:?]\r\n\tat io.druid.indexing.kafka.supervisor.KafkaSupervisor$CheckpointNotice.handle(KafkaSupervisor.java:638) ~[classes/:?]\r\n\tat io.druid.indexing.kafka.supervisor.KafkaSupervisor$2.run(KafkaSupervisor.java:363) [classes/:?]\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_161]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_161]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_161]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_161]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_161]\r\n```\r\n\r\nThe corresponding code is:\r\n\r\n```java\r\nAssert.assertNotNull(serviceEmitter.getStackTrace());\r\n```\r\n\r\nSo, the issue is, `serviceEmitter.getStackTrace()` can return null when this check is called, so the test should wait for the stacktrace to be set."}, {"user": "KenjiTakahashi", "commits": {}, "labels": ["Bug"], "created": "2018-08-09 13:56:39", "title": "Coordinator `asOverlord` broken on master.", "url": "https://github.com/apache/druid/issues/6133", "closed": "2018-08-22 22:50:06", "ttf": 13.000277777777777, "commitsDetails": [], "body": "Fails at startup with:\r\n```\r\nException in thread \"main\" com.google.inject.CreationException: Unable to create injector, see the following errors:\r\n\r\n1) A binding to io.druid.client.indexing.IndexingServiceClient was already configured at io.druid.cli.CliCoordinator$1.configure(CliCoordinator.java:171) (via modules: com.google.inject.util.Modules$OverrideModule -> com.google.inject.util.Modules$OverrideModule -> io.druid.cli.CliCoordinator$1).\r\n  at io.druid.cli.CliOverlord$1.configure(CliOverlord.java:191) (via modules: com.google.inject.util.Modules$OverrideModule -> com.google.inject.util.Modules$OverrideModule -> io.druid.cli.CliOverlord$1)\r\n\r\n1 error\r\n        at com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:470)\r\n        at com.google.inject.internal.InternalInjectorCreator.initializeStatically(InternalInjectorCreator.java:155)\r\n        at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:107)\r\n        at com.google.inject.Guice.createInjector(Guice.java:99)\r\n        at com.google.inject.Guice.createInjector(Guice.java:73)\r\n        at com.google.inject.Guice.createInjector(Guice.java:62)\r\n        at io.druid.initialization.Initialization.makeInjectorWithModules(Initialization.java:421)\r\n        at io.druid.cli.GuiceRunnable.makeInjector(GuiceRunnable.java:68)\r\n        at io.druid.cli.ServerRunnable.run(ServerRunnable.java:49)\r\n        at io.druid.cli.Main.main(Main.java:116)\r\n```\r\n\r\nLast known working at e4ef753a6041d90289c33d34eba819659315ce23."}, {"user": "jihoonson", "commits": {"5ce3185b9cd2704aa86af3348529324292b4a593": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Area - Streaming Ingestion", "Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "KafkaIndexTask can delete published segments on restart", "numCommits": 0, "created": "2018-08-08 00:12:45", "closed": "2018-08-28 17:13:36", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/6124", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 20.00027777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "indexing-service/src/main/java/io/druid/indexing/common/task/batch/parallel/SinglePhaseParallelIndexTaskRunner.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/LegacyKafkaIndexTaskRunner.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "server/src/main/java/io/druid/segment/realtime/appenderator/BaseAppenderatorDriver.java", "deletions": 15, "insertions": 10, "lines": 25}, {"filePath": "server/src/test/java/io/druid/segment/realtime/appenderator/BatchAppenderatorDriverTest.java", "deletions": 2, "insertions": 3, "lines": 5}, {"filePath": "server/src/test/java/io/druid/segment/realtime/appenderator/StreamAppenderatorDriverFailTest.java", "deletions": 27, "insertions": 29, "lines": 56}, {"filePath": "server/src/main/java/io/druid/segment/realtime/appenderator/AppenderatorImpl.java", "deletions": 2, "insertions": 9, "lines": 11}, {"filePath": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/IncrementalPublishingKafkaIndexTaskRunner.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "indexing-service/src/main/java/io/druid/indexing/common/task/IndexTask.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "server/src/main/java/io/druid/segment/realtime/appenderator/TransactionalSegmentPublisher.java", "deletions": 2, "insertions": 6, "lines": 8}, {"filePath": "server/src/main/java/io/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "deletions": 26, "insertions": 38, "lines": 64}, {"filePath": "server/src/test/java/io/druid/segment/realtime/appenderator/StreamAppenderatorDriverTest.java", "deletions": 2, "insertions": 4, "lines": 6}, {"filePath": "indexing-service/src/main/java/io/druid/indexing/common/task/AppenderatorDriverRealtimeIndexTask.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "server/src/main/java/io/druid/indexing/overlord/IndexerMetadataStorageCoordinator.java", "deletions": 4, "insertions": 8, "lines": 12}], "commitSpoonAstDiffStats": [{"spoonFilePath": "IncrementalPublishingKafkaIndexTaskRunner.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.indexing.kafka.IncrementalPublishingKafkaIndexTaskRunner.SequenceMetadata.createPublisher(io.druid.indexing.common.TaskToolbox,boolean)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "TransactionalSegmentPublisher.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.publishSegments(java.util.Set,java.lang.Object)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "AppenderatorImpl.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.realtime.appenderator.AppenderatorImpl.mergeAndPush(io.druid.segment.realtime.appenderator.SegmentIdentifier,io.druid.segment.realtime.plumber.Sink,boolean)", "MOV": 4, "TOT": 7}]}, {"spoonFilePath": "BaseAppenderatorDriver.java", "spoonMethods": [{"INS": 3, "UPD": 3, "DEL": 3, "spoonMethodName": "io.druid.segment.realtime.appenderator.BaseAppenderatorDriver.publishInBackground(io.druid.segment.realtime.appenderator.SegmentsAndMetadata,io.druid.segment.realtime.appenderator.TransactionalSegmentPublisher)", "MOV": 2, "TOT": 11}, {"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.BaseAppenderatorDriver.wrapCommitter(io.druid.data.input.Committer)", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.BaseAppenderatorDriver.wrapCommitterSupplier(com.google.common.base.Supplier)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "SinglePhaseParallelIndexTaskRunner.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.indexing.common.task.batch.parallel.SinglePhaseParallelIndexTaskRunner.publish(io.druid.indexing.common.TaskToolbox)", "MOV": 2, "TOT": 4}]}, {"spoonFilePath": "IndexTask.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.indexing.common.task.IndexTask.generateAndPublishSegments(io.druid.indexing.common.TaskToolbox,io.druid.segment.indexing.DataSchema,io.druid.indexing.common.task.IndexTask$ShardSpecs,java.util.Map,io.druid.data.input.FirehoseFactory,java.io.File)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "LegacyKafkaIndexTaskRunner.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.indexing.kafka.LegacyKafkaIndexTaskRunner.runInternal(io.druid.indexing.common.TaskToolbox)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "AppenderatorDriverRealtimeIndexTask.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.indexing.common.task.AppenderatorDriverRealtimeIndexTask.run(io.druid.indexing.common.TaskToolbox)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "StreamAppenderatorDriverFailTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.StreamAppenderatorDriverFailTest.testFailDuringPublishInternal(boolean)", "MOV": 4, "TOT": 5}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.StreamAppenderatorDriverFailTest.testFailDuringPublish()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "IndexerSQLMetadataStorageCoordinator.java", "spoonMethods": [{"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.metadata.IndexerSQLMetadataStorageCoordinator.announceHistoricalSegments(java.util.Set,io.druid.indexing.overlord.DataSourceMetadata,io.druid.indexing.overlord.DataSourceMetadata).2.inTransaction(org.skife.jdbi.v2.Handle,org.skife.jdbi.v2.TransactionStatus)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 3, "DEL": 1, "spoonMethodName": "io.druid.metadata.IndexerSQLMetadataStorageCoordinator.announceHistoricalSegments(java.util.Set,io.druid.indexing.overlord.DataSourceMetadata,io.druid.indexing.overlord.DataSourceMetadata)", "MOV": 1, "TOT": 6}]}, {"spoonFilePath": "BatchAppenderatorDriverTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.realtime.appenderator.BatchAppenderatorDriverTest.makeOkPublisher()", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "StreamAppenderatorDriverTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.realtime.appenderator.StreamAppenderatorDriverTest.makeFailingPublisher(boolean)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.realtime.appenderator.StreamAppenderatorDriverTest.makeOkPublisher()", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "IndexerMetadataStorageCoordinator.java", "spoonMethods": []}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-08-15 14:55:53", "commitMessage": "Fix three bugs with segment publishing. (#6155)\n\n* Fix three bugs with segment publishing.\r\n\r\n1. In AppenderatorImpl: always use a unique path if requested, even if the segment\r\n   was already pushed. This is important because if we don't do this, it causes\r\n   the issue mentioned in #6124.\r\n2. In IndexerSQLMetadataStorageCoordinator: Fix a bug that could cause it to return\r\n   a \"not published\" result instead of throwing an exception, when there was one\r\n   metadata update failure, followed by some random exception. This is done by\r\n   resetting the AtomicBoolean that tracks what case we're in, each time the\r\n   callback runs.\r\n3. In BaseAppenderatorDriver: Only kill segments if we get an affirmative false\r\n   publish result. Skip killing if we just got some exception. The reason for this\r\n   is that we want to avoid killing segments if they are in an unknown state.\r\n\r\nTwo other changes to clarify the contracts a bit and hopefully prevent future bugs:\r\n\r\n1. Return SegmentPublishResult from TransactionalSegmentPublisher, to make it\r\nmore similar to announceHistoricalSegments.\r\n2. Make it explicit, at multiple levels of javadocs, that a \"false\" publish result\r\nmust indicate that the publish _definitely_ did not happen. Unknown states must be\r\nexceptions. This helps BaseAppenderatorDriver do the right thing.\r\n\r\n* Remove javadoc-only import.\r\n\r\n* Updates.\r\n\r\n* Fix test.\r\n\r\n* Fix tests.\r\n", "commitUser": "fjy", "commitDateTime": "2018-08-15 13:55:53", "commitParents": ["f447b784de282377836a6b07623e9cc14455d648"], "commitGHEventType": "referenced", "nameRev": "5ce3185b9cd2704aa86af3348529324292b4a593 tags/druid-0.13.0-incubating-rc1~150", "commitHash": "5ce3185b9cd2704aa86af3348529324292b4a593"}], "body": "This can happen in the following scenario.\r\n\r\n1. A kafka index task starts publishing segments.\r\n2. The task succeeds to publish segments and is stopped immediately (by restarting the machine).\r\n3. When the task is restored, it restores all sequences it kept in memory before restarting.\r\n4. After reading some more events from Kafka, the task tries to publish segments. These segments include the ones which were published before restarting because the restored sequences contain them.\r\n5. Since the segments which are published twice are already stored in metastore, the publish fails.\r\n6. The set of published segments in metastore is different from the set of segments the task is trying because the task read more data.\r\n7. The task thinks that the publish actually failed and removes the published segments from deep storage."}, {"user": "jihoonson", "commits": {"23ba6f7ad7dc92903567a8458994e9d8731621cf": {"commitGHEventType": "referenced", "commitUser": "fjy"}, "0b79e76720bb41dfc316632d1709898c8019fce8": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Area - Querying", "Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "ClassCastException in groupBy when sorting on numeric columns containing nulls", "numCommits": 0, "created": "2018-08-07 21:42:32", "closed": "2018-08-25 21:31:46", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/6123", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 17.00027777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "processing/src/main/java/io/druid/query/topn/types/TopNColumnSelectorStrategy.java", "deletions": 9, "insertions": 2, "lines": 11}, {"filePath": "processing/src/main/java/io/druid/query/topn/types/StringTopNColumnSelectorStrategy.java", "deletions": 28, "insertions": 21, "lines": 49}, {"filePath": "processing/src/main/java/io/druid/query/topn/TopNQueryQueryToolChest.java", "deletions": 6, "insertions": 6, "lines": 12}, {"filePath": "processing/src/main/java/io/druid/query/topn/TopNQueryEngine.java", "deletions": 0, "insertions": 4, "lines": 4}, {"filePath": "processing/src/main/java/io/druid/query/topn/DimExtractionTopNAlgorithm.java", "deletions": 14, "insertions": 0, "lines": 14}, {"filePath": "processing/src/main/java/io/druid/query/groupby/epinephelinae/RowBasedGrouperHelper.java", "deletions": 21, "insertions": 2, "lines": 23}, {"filePath": "processing/src/main/java/io/druid/query/topn/types/NumericTopNColumnSelectorStrategy.java", "deletions": 19, "insertions": 38, "lines": 57}, {"filePath": "processing/src/main/java/io/druid/query/topn/TopNResultBuilder.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "processing/src/main/java/io/druid/query/topn/TopNNumericResultBuilder.java", "deletions": 45, "insertions": 26, "lines": 71}, {"filePath": "processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java", "deletions": 25, "insertions": 2, "lines": 27}, {"filePath": "processing/src/main/java/io/druid/query/topn/DimValHolder.java", "deletions": 10, "insertions": 10, "lines": 20}, {"filePath": "processing/src/main/java/io/druid/segment/DimensionHandlerUtils.java", "deletions": 3, "insertions": 65, "lines": 68}, {"filePath": "processing/src/main/java/io/druid/query/topn/TopNLexicographicResultBuilder.java", "deletions": 41, "insertions": 11, "lines": 52}, {"filePath": "processing/src/main/java/io/druid/query/topn/TopNMapFn.java", "deletions": 31, "insertions": 1, "lines": 32}, {"filePath": "processing/src/main/java/io/druid/query/groupby/GroupByQuery.java", "deletions": 34, "insertions": 16, "lines": 50}, {"filePath": "processing/src/main/java/io/druid/query/topn/types/TopNColumnSelectorStrategyFactory.java", "deletions": 7, "insertions": 24, "lines": 31}, {"filePath": "processing/src/main/java/io/druid/query/groupby/epinephelinae/GroupByQueryEngineV2.java", "deletions": 19, "insertions": 1, "lines": 20}, {"filePath": "processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java", "deletions": 0, "insertions": 132, "lines": 132}, {"filePath": "processing/src/main/java/io/druid/query/groupby/GroupByQueryQueryToolChest.java", "deletions": 2, "insertions": 8, "lines": 10}], "commitSpoonAstDiffStats": [{"spoonFilePath": "StringTopNColumnSelectorStrategy.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.StringTopNColumnSelectorStrategy", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.StringTopNColumnSelectorStrategy.dimExtractionScanAndAggregateWithCardinalityUnknown(io.druid.query.topn.TopNQuery,io.druid.segment.Cursor,io.druid.segment.DimensionSelector,java.util.Map)", "MOV": 1, "TOT": 3}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.types.StringTopNColumnSelectorStrategy.getValueType()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.types.StringTopNColumnSelectorStrategy.makeDimExtractionAggregateStore()", "MOV": 0, "TOT": 2}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.StringTopNColumnSelectorStrategy.updateDimExtractionResults(java.util.Map,io.druid.query.topn.TopNResultBuilder)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.StringTopNColumnSelectorStrategy.dimExtractionScanAndAggregateWithCardinalityKnown(io.druid.query.topn.TopNQuery,io.druid.segment.Cursor,io.druid.segment.DimensionSelector,io.druid.query.aggregation.Aggregator[][],java.util.Map)", "MOV": 1, "TOT": 3}, {"INS": 0, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.query.topn.types.StringTopNColumnSelectorStrategy.updateDimExtractionResults(java.util.Map,com.google.common.base.Function,io.druid.query.topn.TopNResultBuilder)", "MOV": 2, "TOT": 4}]}, {"spoonFilePath": "GroupByQueryEngineV2.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.query.groupby.epinephelinae.GroupByQueryEngineV2.convertRowTypesToOutputTypes(java.util.List,java.util.Map)", "MOV": 2, "TOT": 5}]}, {"spoonFilePath": "TopNResultBuilder.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.topn.addEntry(java.lang.Comparable,java.lang.Object,java.lang.Object[])", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "TopNQueryQueryToolChest.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 1, "spoonMethodName": "io.druid.query.topn.TopNQueryQueryToolChest.getCacheStrategy(io.druid.query.topn.TopNQuery).7.pullFromCache(boolean).2.apply(java.lang.Object)", "MOV": 1, "TOT": 3}]}, {"spoonFilePath": "TopNNumericResultBuilder.java", "spoonMethods": [{"INS": 2, "UPD": 1, "DEL": 2, "spoonMethodName": "io.druid.query.topn.TopNNumericResultBuilder.build()", "MOV": 0, "TOT": 5}, {"INS": 0, "UPD": 3, "DEL": 1, "spoonMethodName": "io.druid.query.topn.TopNNumericResultBuilder.2.compare(io.druid.query.topn.DimValHolder,io.druid.query.topn.DimValHolder)", "MOV": 5, "TOT": 9}, {"INS": 0, "UPD": 4, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNNumericResultBuilder.addEntry(java.lang.Comparable,java.lang.Object,java.lang.Object[])", "MOV": 0, "TOT": 4}, {"INS": 0, "UPD": 3, "DEL": 1, "spoonMethodName": "io.druid.query.topn.TopNNumericResultBuilder.build().3.compare(io.druid.query.topn.DimValHolder,io.druid.query.topn.DimValHolder)", "MOV": 5, "TOT": 9}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNNumericResultBuilder.addEntry(io.druid.query.topn.DimensionAndMetricValueExtractor)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 4, "DEL": 1, "spoonMethodName": "io.druid.query.topn.TopNNumericResultBuilder", "MOV": 0, "TOT": 6}]}, {"spoonFilePath": "TopNColumnSelectorStrategy.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.types.updateDimExtractionResults(java.util.Map,com.google.common.base.Function,io.druid.query.topn.TopNResultBuilder)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.types.getValueType()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "DimValHolder.java", "spoonMethods": [{"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.query.topn.DimValHolder.getDimName()", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.topn.DimValHolder.Builder.build()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 4, "DEL": 0, "spoonMethodName": "io.druid.query.topn.DimValHolder", "MOV": 0, "TOT": 4}, {"INS": 0, "UPD": 4, "DEL": 0, "spoonMethodName": "io.druid.query.topn.DimValHolder.Builder.withDimName(java.lang.Comparable)", "MOV": 0, "TOT": 4}, {"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.query.topn.DimValHolder.Builder", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "RowBasedGrouperHelper.java", "spoonMethods": [{"INS": 3, "UPD": 1, "DEL": 4, "spoonMethodName": "io.druid.query.groupby.epinephelinae.RowBasedGrouperHelper.makeValueConvertFunctions(java.util.List)", "MOV": 6, "TOT": 14}]}, {"spoonFilePath": "TopNQueryEngine.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNQueryEngine.getMapFn(io.druid.query.topn.TopNQuery,io.druid.segment.StorageAdapter,io.druid.query.topn.TopNQueryMetrics)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "DimensionHandlerUtils.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.DimensionHandlerUtils.converterFromTypeToType(io.druid.segment.column.ValueType,io.druid.segment.column.ValueType)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.DimensionHandlerUtils.convertObjectToType(java.lang.Object,io.druid.segment.column.ValueType,boolean)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.DimensionHandlerUtils.convertObjectToType(java.lang.Object,io.druid.segment.column.ValueType)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.DimensionHandlerUtils.compareObjectsAsType(java.lang.Object,java.lang.Object,io.druid.segment.column.ValueType)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "TopNLexicographicResultBuilder.java", "spoonMethods": [{"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNLexicographicResultBuilder.1.compare(io.druid.query.topn.DimValHolder,io.druid.query.topn.DimValHolder)", "MOV": 3, "TOT": 5}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNLexicographicResultBuilder.addEntry(io.druid.query.topn.DimensionAndMetricValueExtractor)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 7, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNLexicographicResultBuilder.addEntry(java.lang.Comparable,java.lang.Object,java.lang.Object[])", "MOV": 0, "TOT": 7}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.TopNLexicographicResultBuilder", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 2, "DEL": 1, "spoonMethodName": "io.druid.query.topn.TopNLexicographicResultBuilder.build().2.compare(io.druid.query.topn.DimValHolder,io.druid.query.topn.DimValHolder)", "MOV": 3, "TOT": 6}, {"INS": 2, "UPD": 1, "DEL": 2, "spoonMethodName": "io.druid.query.topn.TopNLexicographicResultBuilder.build()", "MOV": 0, "TOT": 5}]}, {"spoonFilePath": "TopNColumnSelectorStrategyFactory.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.TopNColumnSelectorStrategyFactory", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 4, "DEL": 3, "spoonMethodName": "io.druid.query.topn.types.TopNColumnSelectorStrategyFactory.makeColumnSelectorStrategy(io.druid.segment.column.ColumnCapabilities,io.druid.segment.ColumnValueSelector)", "MOV": 0, "TOT": 8}]}, {"spoonFilePath": "TopNQueryRunnerTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNQueryRunnerTest.testSortOnTimeAsLong()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNQueryRunnerTest.testSortOnDoubleAsDouble()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNQueryRunnerTest.testSortOnStringAsDouble()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNQueryRunnerTest.testSortOnDoubleAsLong()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "TopNMapFn.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 5, "spoonMethodName": "io.druid.query.topn.TopNMapFn", "MOV": 0, "TOT": 5}, {"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNMapFn.apply(io.druid.segment.Cursor,io.druid.query.topn.TopNQueryMetrics)", "MOV": 1, "TOT": 3}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.TopNMapFn.getValueTransformer(io.druid.segment.column.ValueType)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "NumericTopNColumnSelectorStrategy.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfFloat", "MOV": 2, "TOT": 4}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfDouble", "MOV": 0, "TOT": 2}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfLong", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfLong.getValueType()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfDouble.convertAggregatorStoreKeyToColumnValue(java.lang.Object)", "MOV": 1, "TOT": 2}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfDouble.getValueType()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfFloat.convertAggregatorStoreKeyToColumnValue(java.lang.Object)", "MOV": 2, "TOT": 3}, {"INS": 0, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfFloat.dimExtractionScanAndAggregate(io.druid.query.topn.TopNQuery,io.druid.segment.BaseFloatColumnValueSelector,io.druid.segment.Cursor,io.druid.query.aggregation.Aggregator[][],it.unimi.dsi.fastutil.ints.Int2ObjectMap)", "MOV": 1, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.ofType(io.druid.segment.column.ValueType,io.druid.segment.column.ValueType)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfLong.convertAggregatorStoreKeyToColumnValue(java.lang.Object)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.updateDimExtractionResults(java.util.Map,io.druid.query.topn.TopNResultBuilder)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfFloat.makeDimExtractionAggregateStore()", "MOV": 1, "TOT": 1}, {"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfFloat.getValueType()", "MOV": 1, "TOT": 3}, {"INS": 0, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.updateDimExtractionResults(java.util.Map,com.google.common.base.Function,io.druid.query.topn.TopNResultBuilder)", "MOV": 1, "TOT": 3}]}, {"spoonFilePath": "DimExtractionTopNAlgorithm.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 3, "spoonMethodName": "io.druid.query.topn.DimExtractionTopNAlgorithm.updateResults(io.druid.query.topn.TopNParams,io.druid.query.aggregation.Aggregator[][],java.util.Map,io.druid.query.topn.TopNResultBuilder)", "MOV": 0, "TOT": 3}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.DimExtractionTopNAlgorithm.needsResultTypeConversion(io.druid.query.topn.TopNParams)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "GroupByQueryQueryToolChest.java", "spoonMethods": [{"INS": 1, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.query.groupby.GroupByQueryQueryToolChest.getCacheStrategy(io.druid.query.groupby.GroupByQuery).8.pullFromCache(boolean).2.apply(java.lang.Object)", "MOV": 1, "TOT": 4}]}, {"spoonFilePath": "GroupByQuery.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.groupby.GroupByQuery.getRowOrderingForPushDown(boolean,io.druid.query.groupby.orderby.DefaultLimitSpec).1.compare(io.druid.data.input.Row,io.druid.data.input.Row)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.groupby.GroupByQuery.getRowOrderingForPushDown(boolean,io.druid.query.groupby.orderby.DefaultLimitSpec).3.compare(io.druid.data.input.Row,io.druid.data.input.Row)", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 4, "DEL": 1, "spoonMethodName": "io.druid.query.groupby.GroupByQuery.compareDimsForLimitPushDown(java.util.List,java.util.List,java.util.List,java.util.List,io.druid.data.input.Row,io.druid.data.input.Row)", "MOV": 5, "TOT": 12}, {"INS": 1, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.query.groupby.GroupByQuery.compareDims(java.util.List,io.druid.data.input.Row,io.druid.data.input.Row)", "MOV": 6, "TOT": 9}, {"INS": 0, "UPD": 3, "DEL": 2, "spoonMethodName": "io.druid.query.groupby.GroupByQuery.getRowOrderingForPushDown(boolean,io.druid.query.groupby.orderby.DefaultLimitSpec)", "MOV": 2, "TOT": 7}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.groupby.GroupByQuery.getRowOrderingForPushDown(boolean,io.druid.query.groupby.orderby.DefaultLimitSpec).2.compare(io.druid.data.input.Row,io.druid.data.input.Row)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "PooledTopNAlgorithm.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.PooledTopNAlgorithm.PooledTopNParams.Builder", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 6, "spoonMethodName": "io.druid.query.topn.PooledTopNAlgorithm.updateResults(io.druid.query.topn.PooledTopNAlgorithm$PooledTopNParams,int[],io.druid.query.aggregation.BufferAggregator[],io.druid.query.topn.TopNResultBuilder)", "MOV": 1, "TOT": 7}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-08-25 14:31:46", "commitMessage": "Fix four bugs with numeric dimension output types. (#6220)\n\n* Fix four bugs with numeric dimension output types.\r\n\r\nThis patch includes the following bug fixes:\r\n\r\n- TopNColumnSelectorStrategyFactory: Cast dimension values to the output type\r\n  during dimExtractionScanAndAggregate instead of updateDimExtractionResults.\r\n  This fixes a bug where, for example, grouping on doubles-cast-to-longs would\r\n  fail to merge two doubles that should have been combined into the same long value.\r\n- TopNQueryEngine: Use DimExtractionTopNAlgorithm when treating string columns\r\n  as numeric dimensions. This fixes a similar bug: grouping on string-cast-to-long\r\n  would fail to merge two strings that should have been combined.\r\n- GroupByQuery: Cast numeric types to the expected output type before comparing them\r\n  in compareDimsForLimitPushDown. This fixes #6123.\r\n- GroupByQueryQueryToolChest: Convert Jackson-deserialized dimension values into\r\n  the proper output type. This fixes an inconsistency between results that came\r\n  from cache vs. not-cache: for example, Jackson sometimes deserializes integers\r\n  as Integers and sometimes as Longs.\r\n\r\nAnd the following code-cleanup changes, related to the fixes above:\r\n\r\n- DimensionHandlerUtils: Introduce convertObjectToType, compareObjectsAsType,\r\n  and converterFromTypeToType to make it easier to handle casting operations.\r\n- TopN in general: Rename various \"dimName\" variables to \"dimValue\" where they\r\n  actually represent dimension values. The old names were confusing.\r\n\r\n* Remove unused imports.\r\n", "commitUser": "fjy", "commitDateTime": "2018-08-25 14:31:46", "commitParents": ["c3aaf8122d658fce578e75ccbba18f7c74b8114f"], "commitGHEventType": "referenced", "nameRev": "23ba6f7ad7dc92903567a8458994e9d8731621cf tags/druid-0.13.0-incubating-rc1~129", "commitHash": "23ba6f7ad7dc92903567a8458994e9d8731621cf"}, {"commitGitStats": [{"filePath": "processing/src/main/java/io/druid/query/topn/types/TopNColumnSelectorStrategy.java", "deletions": 9, "insertions": 2, "lines": 11}, {"filePath": "processing/src/main/java/io/druid/query/topn/types/StringTopNColumnSelectorStrategy.java", "deletions": 28, "insertions": 32, "lines": 60}, {"filePath": "processing/src/main/java/io/druid/query/topn/TopNQueryQueryToolChest.java", "deletions": 6, "insertions": 6, "lines": 12}, {"filePath": "processing/src/main/java/io/druid/query/topn/TopNQueryEngine.java", "deletions": 0, "insertions": 4, "lines": 4}, {"filePath": "processing/src/main/java/io/druid/query/topn/DimExtractionTopNAlgorithm.java", "deletions": 14, "insertions": 0, "lines": 14}, {"filePath": "processing/src/main/java/io/druid/query/groupby/epinephelinae/RowBasedGrouperHelper.java", "deletions": 30, "insertions": 2, "lines": 32}, {"filePath": "processing/src/main/java/io/druid/query/topn/types/NumericTopNColumnSelectorStrategy.java", "deletions": 19, "insertions": 38, "lines": 57}, {"filePath": "processing/src/main/java/io/druid/query/topn/TopNResultBuilder.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "processing/src/main/java/io/druid/query/topn/TopNNumericResultBuilder.java", "deletions": 45, "insertions": 26, "lines": 71}, {"filePath": "processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java", "deletions": 25, "insertions": 2, "lines": 27}, {"filePath": "processing/src/main/java/io/druid/query/topn/DimValHolder.java", "deletions": 10, "insertions": 10, "lines": 20}, {"filePath": "processing/src/main/java/io/druid/segment/DimensionHandlerUtils.java", "deletions": 3, "insertions": 124, "lines": 127}, {"filePath": "processing/src/main/java/io/druid/query/topn/TopNLexicographicResultBuilder.java", "deletions": 41, "insertions": 11, "lines": 52}, {"filePath": "processing/src/main/java/io/druid/query/topn/TopNMapFn.java", "deletions": 39, "insertions": 1, "lines": 40}, {"filePath": "processing/src/main/java/io/druid/query/groupby/GroupByQuery.java", "deletions": 34, "insertions": 17, "lines": 51}, {"filePath": "processing/src/main/java/io/druid/query/topn/types/TopNColumnSelectorStrategyFactory.java", "deletions": 7, "insertions": 24, "lines": 31}, {"filePath": "processing/src/main/java/io/druid/query/groupby/epinephelinae/GroupByQueryEngineV2.java", "deletions": 22, "insertions": 1, "lines": 23}, {"filePath": "processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java", "deletions": 0, "insertions": 132, "lines": 132}, {"filePath": "processing/src/main/java/io/druid/query/groupby/GroupByQueryQueryToolChest.java", "deletions": 2, "insertions": 8, "lines": 10}], "commitSpoonAstDiffStats": [{"spoonFilePath": "StringTopNColumnSelectorStrategy.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.StringTopNColumnSelectorStrategy", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.StringTopNColumnSelectorStrategy.dimExtractionScanAndAggregateWithCardinalityUnknown(io.druid.query.topn.TopNQuery,io.druid.segment.Cursor,io.druid.segment.DimensionSelector,java.util.Map)", "MOV": 1, "TOT": 3}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.types.StringTopNColumnSelectorStrategy.getValueType()", "MOV": 1, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.types.StringTopNColumnSelectorStrategy.makeDimExtractionAggregateStore()", "MOV": 0, "TOT": 2}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.StringTopNColumnSelectorStrategy.updateDimExtractionResults(java.util.Map,io.druid.query.topn.TopNResultBuilder)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.StringTopNColumnSelectorStrategy.dimExtractionScanAndAggregateWithCardinalityKnown(io.druid.query.topn.TopNQuery,io.druid.segment.Cursor,io.druid.segment.DimensionSelector,io.druid.query.aggregation.Aggregator[][],java.util.Map)", "MOV": 1, "TOT": 3}, {"INS": 0, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.query.topn.types.StringTopNColumnSelectorStrategy.updateDimExtractionResults(java.util.Map,com.google.common.base.Function,io.druid.query.topn.TopNResultBuilder)", "MOV": 2, "TOT": 4}]}, {"spoonFilePath": "GroupByQueryEngineV2.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.query.groupby.epinephelinae.GroupByQueryEngineV2.convertRowTypesToOutputTypes(java.util.List,java.util.Map)", "MOV": 2, "TOT": 5}]}, {"spoonFilePath": "TopNResultBuilder.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.topn.addEntry(java.lang.Comparable,java.lang.Object,java.lang.Object[])", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "TopNQueryQueryToolChest.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 1, "spoonMethodName": "io.druid.query.topn.TopNQueryQueryToolChest.getCacheStrategy(io.druid.query.topn.TopNQuery).7.pullFromCache().2.apply(java.lang.Object)", "MOV": 1, "TOT": 3}]}, {"spoonFilePath": "TopNNumericResultBuilder.java", "spoonMethods": [{"INS": 2, "UPD": 1, "DEL": 2, "spoonMethodName": "io.druid.query.topn.TopNNumericResultBuilder.build()", "MOV": 0, "TOT": 5}, {"INS": 0, "UPD": 3, "DEL": 1, "spoonMethodName": "io.druid.query.topn.TopNNumericResultBuilder.2.compare(io.druid.query.topn.DimValHolder,io.druid.query.topn.DimValHolder)", "MOV": 5, "TOT": 9}, {"INS": 0, "UPD": 4, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNNumericResultBuilder.addEntry(java.lang.Comparable,java.lang.Object,java.lang.Object[])", "MOV": 0, "TOT": 4}, {"INS": 0, "UPD": 3, "DEL": 1, "spoonMethodName": "io.druid.query.topn.TopNNumericResultBuilder.build().3.compare(io.druid.query.topn.DimValHolder,io.druid.query.topn.DimValHolder)", "MOV": 5, "TOT": 9}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNNumericResultBuilder.addEntry(io.druid.query.topn.DimensionAndMetricValueExtractor)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 4, "DEL": 1, "spoonMethodName": "io.druid.query.topn.TopNNumericResultBuilder", "MOV": 0, "TOT": 6}]}, {"spoonFilePath": "TopNColumnSelectorStrategy.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.types.updateDimExtractionResults(java.util.Map,com.google.common.base.Function,io.druid.query.topn.TopNResultBuilder)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.types.getValueType()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "DimValHolder.java", "spoonMethods": [{"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.query.topn.DimValHolder.getDimName()", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.topn.DimValHolder.Builder.build()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 4, "DEL": 0, "spoonMethodName": "io.druid.query.topn.DimValHolder", "MOV": 0, "TOT": 4}, {"INS": 0, "UPD": 4, "DEL": 0, "spoonMethodName": "io.druid.query.topn.DimValHolder.Builder.withDimName(java.lang.Comparable)", "MOV": 0, "TOT": 4}, {"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.query.topn.DimValHolder.Builder", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "RowBasedGrouperHelper.java", "spoonMethods": [{"INS": 3, "UPD": 1, "DEL": 4, "spoonMethodName": "io.druid.query.groupby.epinephelinae.RowBasedGrouperHelper.makeValueConvertFunctions(java.util.List)", "MOV": 9, "TOT": 17}]}, {"spoonFilePath": "TopNQueryEngine.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNQueryEngine.getMapFn(io.druid.query.topn.TopNQuery,io.druid.segment.StorageAdapter,io.druid.query.topn.TopNQueryMetrics)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "DimensionHandlerUtils.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.DimensionHandlerUtils.converterFromTypeToTypeNonNull(io.druid.segment.column.ValueType,io.druid.segment.column.ValueType)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.DimensionHandlerUtils.converterFromTypeToType(io.druid.segment.column.ValueType,io.druid.segment.column.ValueType)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.DimensionHandlerUtils.nonNullify(java.lang.Comparable,io.druid.segment.column.ValueType)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.DimensionHandlerUtils.compareObjectsAsType(java.lang.Object,java.lang.Object,io.druid.segment.column.ValueType)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.DimensionHandlerUtils.convertObjectToTypeNonNull(java.lang.Object,io.druid.segment.column.ValueType)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.DimensionHandlerUtils.convertObjectToType(java.lang.Object,io.druid.segment.column.ValueType,boolean)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.DimensionHandlerUtils.convertObjectToString(java.lang.Object)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.DimensionHandlerUtils.convertObjectToType(java.lang.Object,io.druid.segment.column.ValueType)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "TopNLexicographicResultBuilder.java", "spoonMethods": [{"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNLexicographicResultBuilder.1.compare(io.druid.query.topn.DimValHolder,io.druid.query.topn.DimValHolder)", "MOV": 3, "TOT": 5}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNLexicographicResultBuilder.addEntry(io.druid.query.topn.DimensionAndMetricValueExtractor)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 7, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNLexicographicResultBuilder.addEntry(java.lang.Comparable,java.lang.Object,java.lang.Object[])", "MOV": 0, "TOT": 7}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.TopNLexicographicResultBuilder", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 2, "DEL": 1, "spoonMethodName": "io.druid.query.topn.TopNLexicographicResultBuilder.build().2.compare(io.druid.query.topn.DimValHolder,io.druid.query.topn.DimValHolder)", "MOV": 3, "TOT": 6}, {"INS": 2, "UPD": 1, "DEL": 2, "spoonMethodName": "io.druid.query.topn.TopNLexicographicResultBuilder.build()", "MOV": 0, "TOT": 5}]}, {"spoonFilePath": "TopNColumnSelectorStrategyFactory.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.TopNColumnSelectorStrategyFactory", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 4, "DEL": 3, "spoonMethodName": "io.druid.query.topn.types.TopNColumnSelectorStrategyFactory.makeColumnSelectorStrategy(io.druid.segment.column.ColumnCapabilities,io.druid.segment.ColumnValueSelector)", "MOV": 0, "TOT": 8}]}, {"spoonFilePath": "TopNQueryRunnerTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNQueryRunnerTest.testSortOnTimeAsLong()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNQueryRunnerTest.testSortOnDoubleAsDouble()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNQueryRunnerTest.testSortOnStringAsDouble()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNQueryRunnerTest.testSortOnDoubleAsLong()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "TopNMapFn.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 5, "spoonMethodName": "io.druid.query.topn.TopNMapFn", "MOV": 0, "TOT": 5}, {"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNMapFn.apply(io.druid.segment.Cursor,io.druid.query.topn.TopNQueryMetrics)", "MOV": 1, "TOT": 3}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.TopNMapFn.getValueTransformer(io.druid.segment.column.ValueType)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "NumericTopNColumnSelectorStrategy.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfFloat", "MOV": 2, "TOT": 4}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfDouble", "MOV": 0, "TOT": 2}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfLong", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfLong.getValueType()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfDouble.convertAggregatorStoreKeyToColumnValue(java.lang.Object)", "MOV": 1, "TOT": 2}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfDouble.getValueType()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfFloat.convertAggregatorStoreKeyToColumnValue(java.lang.Object)", "MOV": 2, "TOT": 3}, {"INS": 0, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfFloat.dimExtractionScanAndAggregate(io.druid.query.topn.TopNQuery,io.druid.segment.BaseFloatColumnValueSelector,io.druid.segment.Cursor,io.druid.query.aggregation.Aggregator[][],it.unimi.dsi.fastutil.ints.Int2ObjectMap)", "MOV": 1, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.ofType(io.druid.segment.column.ValueType,io.druid.segment.column.ValueType)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfLong.convertAggregatorStoreKeyToColumnValue(java.lang.Object)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.updateDimExtractionResults(java.util.Map,io.druid.query.topn.TopNResultBuilder)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfFloat.makeDimExtractionAggregateStore()", "MOV": 1, "TOT": 1}, {"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.OfFloat.getValueType()", "MOV": 1, "TOT": 3}, {"INS": 0, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.query.topn.types.NumericTopNColumnSelectorStrategy.updateDimExtractionResults(java.util.Map,com.google.common.base.Function,io.druid.query.topn.TopNResultBuilder)", "MOV": 1, "TOT": 3}]}, {"spoonFilePath": "DimExtractionTopNAlgorithm.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 3, "spoonMethodName": "io.druid.query.topn.DimExtractionTopNAlgorithm.updateResults(io.druid.query.topn.TopNParams,io.druid.query.aggregation.Aggregator[][],java.util.Map,io.druid.query.topn.TopNResultBuilder)", "MOV": 0, "TOT": 3}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.DimExtractionTopNAlgorithm.needsResultTypeConversion(io.druid.query.topn.TopNParams)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "GroupByQueryQueryToolChest.java", "spoonMethods": [{"INS": 1, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.query.groupby.GroupByQueryQueryToolChest.getCacheStrategy(io.druid.query.groupby.GroupByQuery).8.pullFromCache().2.apply(java.lang.Object)", "MOV": 1, "TOT": 4}]}, {"spoonFilePath": "GroupByQuery.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.groupby.GroupByQuery.getRowOrderingForPushDown(boolean,io.druid.query.groupby.orderby.DefaultLimitSpec).1.compare(io.druid.data.input.Row,io.druid.data.input.Row)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.groupby.GroupByQuery.getRowOrderingForPushDown(boolean,io.druid.query.groupby.orderby.DefaultLimitSpec).3.compare(io.druid.data.input.Row,io.druid.data.input.Row)", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 4, "DEL": 1, "spoonMethodName": "io.druid.query.groupby.GroupByQuery.compareDimsForLimitPushDown(java.util.List,java.util.List,java.util.List,java.util.List,io.druid.data.input.Row,io.druid.data.input.Row)", "MOV": 5, "TOT": 12}, {"INS": 0, "UPD": 2, "DEL": 1, "spoonMethodName": "io.druid.query.groupby.GroupByQuery.compareDims(java.util.List,io.druid.data.input.Row,io.druid.data.input.Row)", "MOV": 1, "TOT": 4}, {"INS": 0, "UPD": 3, "DEL": 2, "spoonMethodName": "io.druid.query.groupby.GroupByQuery.getRowOrderingForPushDown(boolean,io.druid.query.groupby.orderby.DefaultLimitSpec)", "MOV": 2, "TOT": 7}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.groupby.GroupByQuery.getRowOrderingForPushDown(boolean,io.druid.query.groupby.orderby.DefaultLimitSpec).2.compare(io.druid.data.input.Row,io.druid.data.input.Row)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "PooledTopNAlgorithm.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.PooledTopNAlgorithm.PooledTopNParams.Builder", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 6, "spoonMethodName": "io.druid.query.topn.PooledTopNAlgorithm.updateResults(io.druid.query.topn.PooledTopNAlgorithm$PooledTopNParams,int[],io.druid.query.aggregation.BufferAggregator[],io.druid.query.topn.TopNResultBuilder)", "MOV": 1, "TOT": 7}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-08-26 16:14:59", "commitMessage": "Fix four bugs with numeric dimension output types. (#6220) (#6230)\n\n* Fix four bugs with numeric dimension output types.\r\n\r\nThis patch includes the following bug fixes:\r\n\r\n- TopNColumnSelectorStrategyFactory: Cast dimension values to the output type\r\n  during dimExtractionScanAndAggregate instead of updateDimExtractionResults.\r\n  This fixes a bug where, for example, grouping on doubles-cast-to-longs would\r\n  fail to merge two doubles that should have been combined into the same long value.\r\n- TopNQueryEngine: Use DimExtractionTopNAlgorithm when treating string columns\r\n  as numeric dimensions. This fixes a similar bug: grouping on string-cast-to-long\r\n  would fail to merge two strings that should have been combined.\r\n- GroupByQuery: Cast numeric types to the expected output type before comparing them\r\n  in compareDimsForLimitPushDown. This fixes #6123.\r\n- GroupByQueryQueryToolChest: Convert Jackson-deserialized dimension values into\r\n  the proper output type. This fixes an inconsistency between results that came\r\n  from cache vs. not-cache: for example, Jackson sometimes deserializes integers\r\n  as Integers and sometimes as Longs.\r\n\r\nAnd the following code-cleanup changes, related to the fixes above:\r\n\r\n- DimensionHandlerUtils: Introduce convertObjectToType, compareObjectsAsType,\r\n  and converterFromTypeToType to make it easier to handle casting operations.\r\n- TopN in general: Rename various \"dimName\" variables to \"dimValue\" where they\r\n  actually represent dimension values. The old names were confusing.\r\n\r\n* Remove unused imports.", "commitUser": "fjy", "commitDateTime": "2018-08-26 17:14:59", "commitParents": ["bf1d5d7b99d3147f42a9dc91ea351aa300803dc9"], "commitGHEventType": "referenced", "nameRev": "0b79e76720bb41dfc316632d1709898c8019fce8 tags/druid-0.12.3-rc1~13", "commitHash": "0b79e76720bb41dfc316632d1709898c8019fce8"}], "body": "Here is the sample query.\r\n\r\n```\r\n{\r\n  \"queryType\": \"groupBy\",\r\n  \"dataSource\": {\r\n    \"type\": \"table\",\r\n    \"name\": \"wikiticker\"\r\n  },\r\n  \"intervals\": {\r\n    \"type\": \"intervals\",\r\n    \"intervals\": [\r\n      \"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"\r\n    ]\r\n  },\r\n  \"virtualColumns\": [\r\n    {\r\n      \"type\": \"expression\",\r\n      \"name\": \"d0:v\",\r\n      \"expression\": \"timestamp_floor(\\\"__time\\\",'PT1H','','UTC')\",\r\n      \"outputType\": \"LONG\"\r\n    }\r\n  ],\r\n  \"filter\": null,\r\n  \"granularity\": {\r\n    \"type\": \"all\"\r\n  },\r\n  \"dimensions\": [\r\n    {\r\n      \"type\": \"default\",\r\n      \"dimension\": \"d0:v\",\r\n      \"outputName\": \"d0\",\r\n      \"outputType\": \"LONG\"\r\n    },\r\n    {\r\n      \"type\": \"default\",\r\n      \"dimension\": \"cityName\",\r\n      \"outputName\": \"d1\",\r\n      \"outputType\": \"STRING\"\r\n    }\r\n  ],\r\n  \"aggregations\": [],\r\n  \"postAggregations\": [],\r\n  \"having\": null,\r\n  \"limitSpec\": {\r\n    \"type\": \"default\",\r\n    \"columns\": [\r\n      {\r\n        \"dimension\": \"d0\",\r\n        \"direction\": \"descending\",\r\n        \"dimensionOrder\": {\r\n          \"type\": \"numeric\"\r\n        }\r\n      },\r\n      {\r\n        \"dimension\": \"d1\",\r\n        \"direction\": \"ascending\",\r\n        \"dimensionOrder\": {\r\n          \"type\": \"lexicographic\"\r\n        }\r\n      }\r\n    ],\r\n    \"limit\": 5000\r\n  },\r\n  \"context\": {\r\n    \"timeout\": 300000\r\n  },\r\n  \"descending\": false\r\n}\r\n```\r\n\r\nIf `__time` column contains nulls, this query throws an error like below.\r\n\r\n```\r\njava.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long\r\n        at java.lang.Long.compareTo(Long.java:54) ~[?:1.8.0_163]\r\n        at com.google.common.collect.NaturalOrdering.compare(NaturalOrdering.java:35) ~[guava-16.0.1.jar:?]\r\n        at com.google.common.collect.NaturalOrdering.compare(NaturalOrdering.java:26) ~[guava-16.0.1.jar:?]\r\n        at com.google.common.collect.NullsFirstOrdering.compare(NullsFirstOrdering.java:44) ~[guava-16.0.1.jar:?]\r\n        at io.druid.query.groupby.GroupByQuery.compareDimsForLimitPushDown(GroupByQuery.java:582) ~[druid-processing-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.query.groupby.GroupByQuery.access$000(GroupByQuery.java:81) ~[druid-processing-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.query.groupby.GroupByQuery$1.compare(GroupByQuery.java:416) ~[druid-processing-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.query.groupby.GroupByQuery$1.compare(GroupByQuery.java:412) ~[druid-processing-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at com.google.common.collect.ComparatorOrdering.compare(ComparatorOrdering.java:38) ~[guava-16.0.1.jar:?]\r\n        at io.druid.common.guava.CombiningSequence$CombiningAccumulator.accumulate(CombiningSequence.java:260) ~[druid-common-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.java.util.common.guava.YieldingAccumulators$1.accumulate(YieldingAccumulators.java:35) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.java.util.common.guava.MergeSequence.makeYielder(MergeSequence.java:106) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.java.util.common.guava.MergeSequence.toYielder(MergeSequence.java:94) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.java.util.common.guava.LazySequence.toYielder(LazySequence.java:46) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.query.RetryQueryRunner$1.toYielder(RetryQueryRunner.java:102) ~[druid-processing-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.java.util.common.guava.YieldingSequenceBase.accumulate(YieldingSequenceBase.java:32) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.common.guava.CombiningSequence.accumulate(CombiningSequence.java:64) ~[druid-common-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.java.util.common.guava.MappedSequence.accumulate(MappedSequence.java:43) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.query.groupby.orderby.TopNSequence$1.make(TopNSequence.java:55) ~[druid-processing-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.java.util.common.guava.BaseSequence.toYielder(BaseSequence.java:65) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.java.util.common.guava.WrappingSequence$2.get(WrappingSequence.java:87) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.java.util.common.guava.WrappingSequence$2.get(WrappingSequence.java:83) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.java.util.common.guava.SequenceWrapper.wrap(SequenceWrapper.java:55) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.java.util.common.guava.WrappingSequence.toYielder(WrappingSequence.java:82) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.java.util.common.guava.MappedSequence.toYielder(MappedSequence.java:49) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.java.util.common.guava.WrappingSequence$2.get(WrappingSequence.java:87) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.java.util.common.guava.WrappingSequence$2.get(WrappingSequence.java:83) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.query.CPUTimeMetricQueryRunner$1.wrap(CPUTimeMetricQueryRunner.java:74) ~[druid-processing-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.java.util.common.guava.WrappingSequence.toYielder(WrappingSequence.java:82) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.java.util.common.guava.Yielders.each(Yielders.java:32) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.server.QueryResource.doPost(QueryResource.java:193) [druid-server-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_163]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_163]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_163]\r\n        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_163]2018-08-07T20:49:53,620\r\n```\r\n\r\n[druid-0.12.1-iap8](https://github.com/implydata/druid/tree/druid-0.12.1-iap8) is almost same with 0.12.2."}, {"user": "gianm", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2018-08-06 14:11:57", "title": "Kafka supervisor with unresolvable bootstrap.servers blocks startup", "url": "https://github.com/apache/druid/issues/6114", "closed": "2018-10-04 02:10:45", "ttf": 58.000277777777775, "commitsDetails": [], "body": "Kafka supervisors with unresolvable bootstrap.servers will block overlord startup, because the KafkaConsumer can't be created. They should be more robust to the KafkaConsumer failing to start up; perhaps retrying periodically in the background to create the consumer would be a good idea.\r\n\r\nThe stack trace looks like this:\r\n\r\n```\r\n2018-08-06T14:10:00,850 ERROR [LeaderSelector[/druid/overlord/_OVERLORD]] io.druid.curator.discovery.CuratorDruidLeaderSelector - listener becomeLeader() failed. Unable to become leader: {class=io.druid\r\n.curator.discovery.CuratorDruidLeaderSelector, exceptionType=class java.lang.RuntimeException, exceptionMessage=java.lang.reflect.InvocationTargetException}\r\njava.lang.RuntimeException: java.lang.reflect.InvocationTargetException\r\n        at com.google.common.base.Throwables.propagate(Throwables.java:160) ~[guava-16.0.1.jar:?]\r\n        at io.druid.indexing.overlord.TaskMaster$1.becomeLeader(TaskMaster.java:144) ~[druid-indexing-service-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.curator.discovery.CuratorDruidLeaderSelector$1.isLeader(CuratorDruidLeaderSelector.java:98) [druid-server-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:665) [curator-recipes-4.0.0.jar:4.0.0]\r\n        at org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:661) [curator-recipes-4.0.0.jar:4.0.0]\r\n        at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:93) [curator-framework-4.0.0.jar:4.0.0]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_161]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_161]\r\n        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_161]\r\nCaused by: java.lang.reflect.InvocationTargetException\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_161]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_161]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_161]\r\n        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]\r\n        at io.druid.java.util.common.lifecycle.Lifecycle$AnnotationBasedHandler.start(Lifecycle.java:413) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.java.util.common.lifecycle.Lifecycle.start(Lifecycle.java:311) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.indexing.overlord.TaskMaster$1.becomeLeader(TaskMaster.java:141) ~[druid-indexing-service-0.12.1-iap8.jar:0.12.1-iap8]\r\n        ... 7 more\r\nCaused by: org.apache.kafka.common.KafkaException: Failed to construct kafka consumer\r\n        at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:717) ~[?:?]\r\n        at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:597) ~[?:?]\r\n        at io.druid.indexing.kafka.supervisor.KafkaSupervisor.getKafkaConsumer(KafkaSupervisor.java:904) ~[?:?]\r\n        at io.druid.indexing.kafka.supervisor.KafkaSupervisor.start(KafkaSupervisor.java:354) ~[?:?]\r\n        at io.druid.indexing.overlord.supervisor.SupervisorManager.createAndStartSupervisorInternal(SupervisorManager.java:231) ~[druid-indexing-service-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.indexing.overlord.supervisor.SupervisorManager.start(SupervisorManager.java:105) ~[druid-indexing-service-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_161]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_161]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_161]\r\n        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]\r\n        at io.druid.java.util.common.lifecycle.Lifecycle$AnnotationBasedHandler.start(Lifecycle.java:413) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.java.util.common.lifecycle.Lifecycle.start(Lifecycle.java:311) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.indexing.overlord.TaskMaster$1.becomeLeader(TaskMaster.java:141) ~[druid-indexing-service-0.12.1-iap8.jar:0.12.1-iap8]\r\n        ... 7 more\r\nCaused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers\r\n        at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:60) ~[?:?]\r\n        at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:654) ~[?:?]\r\n        at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:597) ~[?:?]\r\n        at io.druid.indexing.kafka.supervisor.KafkaSupervisor.getKafkaConsumer(KafkaSupervisor.java:904) ~[?:?]\r\n        at io.druid.indexing.kafka.supervisor.KafkaSupervisor.start(KafkaSupervisor.java:354) ~[?:?]\r\n        at io.druid.indexing.overlord.supervisor.SupervisorManager.createAndStartSupervisorInternal(SupervisorManager.java:231) ~[druid-indexing-service-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.indexing.overlord.supervisor.SupervisorManager.start(SupervisorManager.java:105) ~[druid-indexing-service-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_161]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_161]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_161]\r\n        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]\r\n        at io.druid.java.util.common.lifecycle.Lifecycle$AnnotationBasedHandler.start(Lifecycle.java:413) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.java.util.common.lifecycle.Lifecycle.start(Lifecycle.java:311) ~[java-util-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.indexing.overlord.TaskMaster$1.becomeLeader(TaskMaster.java:141) ~[druid-indexing-service-0.12.1-iap8.jar:0.12.1-iap8]\r\n        ... 7 more\r\n```"}, {"user": "jihoonson", "commits": {"c57f4a5db0f328cc551d835ed0fabc3cbda46dc8": {"commitGHEventType": "referenced", "commitUser": "gianm"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "FinalizingFieldAccessPostAggregator cannot be deserialized once it gets decorated", "numCommits": 0, "created": "2018-07-28 00:10:15", "closed": "2018-07-28 15:44:23", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/6063", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 0.0002777777777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "processing/src/test/java/io/druid/query/aggregation/post/FinalizingFieldAccessPostAggregatorTest.java", "deletions": 13, "insertions": 46, "lines": 59}, {"filePath": "processing/src/main/java/io/druid/query/aggregation/post/FinalizingFieldAccessPostAggregator.java", "deletions": 38, "insertions": 48, "lines": 86}], "commitSpoonAstDiffStats": [{"spoonFilePath": "FinalizingFieldAccessPostAggregator.java", "spoonMethods": [{"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.query.aggregation.post.FinalizingFieldAccessPostAggregator.decorate(java.util.Map).1", "MOV": 2, "TOT": 4}, {"INS": 0, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.query.aggregation.post.FinalizingFieldAccessPostAggregator.decorate(java.util.Map).1.getComparator()", "MOV": 3, "TOT": 5}, {"INS": 0, "UPD": 3, "DEL": 1, "spoonMethodName": "io.druid.query.aggregation.post.FinalizingFieldAccessPostAggregator.buildDecorated(java.lang.String,java.lang.String,java.util.Map)", "MOV": 1, "TOT": 5}, {"INS": 4, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.aggregation.post.FinalizingFieldAccessPostAggregator", "MOV": 2, "TOT": 6}, {"INS": 2, "UPD": 1, "DEL": 1, "spoonMethodName": "io.druid.query.aggregation.post.FinalizingFieldAccessPostAggregator.compute(java.util.Map)", "MOV": 1, "TOT": 5}, {"INS": 0, "UPD": 2, "DEL": 1, "spoonMethodName": "io.druid.query.aggregation.post.FinalizingFieldAccessPostAggregator.decorate(java.util.Map).1.compute(java.util.Map)", "MOV": 4, "TOT": 7}, {"INS": 3, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.aggregation.post.FinalizingFieldAccessPostAggregator.decorate(java.util.Map)", "MOV": 2, "TOT": 6}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.aggregation.post.FinalizingFieldAccessPostAggregator.getComparator()", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "FinalizingFieldAccessPostAggregatorTest.java", "spoonMethods": [{"INS": 2, "UPD": 1, "DEL": 2, "spoonMethodName": "io.druid.query.aggregation.post.FinalizingFieldAccessPostAggregatorTest.testComputedInArithmeticPostAggregator()", "MOV": 0, "TOT": 5}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.aggregation.post.FinalizingFieldAccessPostAggregatorTest.testComparatorsWithFinalizingAndComparatorNull()", "MOV": 0, "TOT": 1}, {"INS": 3, "UPD": 1, "DEL": 2, "spoonMethodName": "io.druid.query.aggregation.post.FinalizingFieldAccessPostAggregatorTest.testComputedWithFinalizing()", "MOV": 0, "TOT": 6}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.aggregation.post.FinalizingFieldAccessPostAggregatorTest.testComparatorsWithFinalizing()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.aggregation.post.FinalizingFieldAccessPostAggregatorTest.testSerde()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.aggregation.post.FinalizingFieldAccessPostAggregatorTest.buildDecorated(java.lang.String,java.lang.String,java.util.Map)", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-07-28 08:44:22", "commitMessage": "FinalizingFieldAccessPostAggregator: Fix serde. (#6067)\n\nFixes #6063.", "commitUser": "gianm", "commitDateTime": "2018-07-28 08:44:22", "commitParents": ["b0ecfee1ab97d5247c4a6ca08618f988a4dde6a2"], "commitGHEventType": "referenced", "nameRev": "c57f4a5db0f328cc551d835ed0fabc3cbda46dc8 tags/druid-0.13.0-incubating-rc1~192", "commitHash": "c57f4a5db0f328cc551d835ed0fabc3cbda46dc8"}], "body": "`FinalizingFieldAccessPostAggregator.decorate()` a different class from `FinalizingFieldAccessPostAggregator` which causes the below error in historicals.\r\n\r\n```\r\n{\r\n  \"error\" : \"Unknown exception\",\r\n  \"errorMessage\" : \"Could not resolve type id 'FinalizingFieldAccessPostAggregator$1' into a subtype of [simple type, class io.druid.query.aggregation.PostAggregator]: known type ids = [PostAggregator, arithmetic, arrayOfDoublesSketchSetOp, arrayOfDoublesSketchTTest, arrayOfDoublesSketchToEstimate, arrayOfDoublesSketchToEstimateAndBounds, arrayOfDoublesSketchToMeans, arrayOfDoublesSketchToNumEntries, arrayOfDoublesSketchToQuantilesSketch, arrayOfDoublesSketchToString, arrayOfDoublesSketchToVariances, buckets, constant, customBuckets, doubleGreatest, doubleLeast, equalBuckets, expression, fieldAccess, finalizingFieldAccess, hyperUniqueCardinality, javascript, longGreatest, longLeast, max, min, quantile, quantiles, quantilesDoublesSketchToHistogram, quantilesDoublesSketchToQuantile, quantilesDoublesSketchToQuantiles, quantilesDoublesSketchToString, sketchEstimate, sketchSetOper, thetaSketchConstant, thetaSketchEstimate, thetaSketchSetOp]\\n at [Source: HttpInputOverHTTP@3187587e[c=9269,q=0,[0]=null,s=STREAM]; line: -1, column: 8935] (through reference chain: io.druid.query.groupby.GroupByQuery[\\\"postAggregations\\\"]->java.util.ArrayList[0])\",\r\n  \"errorClass\" : \"com.fasterxml.jackson.databind.JsonMappingException\",\r\n  \"host\" : \"x.x.x.x\"\r\n* Connection #0 to host localhost left intact\r\n}\r\n```\r\n\r\nWe might fix this bug, but I wonder we can just remove `FinalizingFieldAccessPostAggregator` and use `ExpressionPostAggregator` instead."}, {"user": "clintropolis", "commits": {"20ae8aa6263e623564fc6bc0f3081554d12f44d5": {"commitGHEventType": "referenced", "commitUser": "jihoonson"}}, "changesInPackagesGIT": [], "labels": ["Area - Segment Format and Ser/De", "Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "'auto' long encoding + compression is broken for some 'bits per value' sizes", "numCommits": 0, "created": "2018-07-25 23:16:04", "closed": "2018-07-31 01:35:21", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/6044", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 5.000277777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "processing/src/main/java/io/druid/segment/data/BlockLayoutColumnarLongsSerializer.java", "deletions": 0, "insertions": 1, "lines": 1}, {"filePath": "processing/src/test/java/io/druid/segment/data/CompressedLongsAutoEncodingSerdeTest.java", "deletions": 0, "insertions": 138, "lines": 138}], "commitSpoonAstDiffStats": [{"spoonFilePath": "CompressedLongsAutoEncodingSerdeTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.data.CompressedLongsAutoEncodingSerdeTest", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "BlockLayoutColumnarLongsSerializer.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.data.BlockLayoutColumnarLongsSerializer.add(long)", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-07-30 18:35:20", "commitMessage": "Fix 'auto' encoded longs + compression serializer (#6045)\n\n* Fix 'auto' encoded longs + compression serializer\r\nFixes #6044\r\n\r\nchanges:\r\n* Fixes `VSizeLongSerde` serializers to treat 'close' as 'flush' when used with `BlockLayoutColumnarLongsSerializer`, allowing unwritten values to be flushed to the buffer when the block is compressed\r\n* Add exhaustive unit test that flexes a variety of value sizes, row counts, and compression strategies to catch issues such as these\r\n:\r\n\r\n* refactor LongSerializer close to be named flush instead\r\n\r\n* revert and just make new serializers per block\r\n", "commitUser": "jihoonson", "commitDateTime": "2018-07-30 18:35:20", "commitParents": ["3aa70179751d049669698f4becd8bb6cbee442fe"], "commitGHEventType": "referenced", "nameRev": "20ae8aa6263e623564fc6bc0f3081554d12f44d5 tags/druid-0.13.0-incubating-rc1~186", "commitHash": "20ae8aa6263e623564fc6bc0f3081554d12f44d5"}], "body": "Setting `\"longEncoding\":\"auto\"` and using any `CompressionStrategy` value for `metricCompression` other than `None` on `IndexSpec` can produce incorrect/broken segments due to a flaw in the interaction between `BlockLayoutColumnarLongsSerializer`, the `TableLongEncodingWriter` and `DeltaLongEncodingWriter` implementations, and the `Size1Ser`, `Size2Ser`, and `Mult4Ser` implementations of `VSizeLongSerde`.\r\n\r\nThe issue is that flushing a chunk of values to the output stream in `BlockLayoutColumnarLongs` calls `flush` on the `LongEncodingWriter`, which for `Table/DeltaLongEncodingWriter` calls `close` on the open `VSizeLongSerde` serializer. All serializer implementations in `VSizeLongSerde` have a `close` implementation that is as follows\r\n```\r\n    @Override\r\n    public void close() throws IOException\r\n    {\r\n      if (closed) {\r\n        return;\r\n      }\r\n ...\r\n      closed = true;\r\n    }\r\n```\r\nbut `Size1Ser` and `Mult4Ser` also rely on close to flush any remaining values that were being accumulated depending on the row count and bits per value of the block. However, the lifetime of the inner `VSizeLongSerde` serializer object is the same as `BlockLayoutColumnarLongsSerializer` and `LongEncodingWriter`, causing all blocks beyond the first one which is written to potentially be missing one or more final values, which would have been flushed by the `close` method if the serializer wasn't already closed. The `VSizeLongSerde` serializer in the 'closed' state will still continue to correctly encode most of the values to the buffer for all remaining chunks because `write` doesn't check closed, it just may leave off the end of them because the flushing logic will never be hit again.\r\n\r\nThis is not a problem with `EntireLayoutColumnarLongs` because it only flushes the writer once, and not a problem with `LongLongEncodingWriter` because it's `flush` method does nothing and always writes complete values. Similarly, `Mult8Ser` of `VSizeLongSerde` also always write complete values, so also will not run into this issue.\r\n\r\nPR with a fix incoming as soon as I write a test for this condition."}, {"user": "leventov", "commits": {"0c1b82ad73b6b6ec81d4411b4328ddf54e8dc016": {"commitGHEventType": "referenced", "commitUser": "jihoonson"}, "5ee7b0cada983912083a85f079086cb579488efd": {"commitGHEventType": "referenced", "commitUser": "leventov"}}, "changesInPackagesGIT": ["server/src/main/java/io/druid/metadata"], "labels": ["Area - Testing", "Bug"], "spoonStatsSummary": {"spoonFilesChanged": 2, "spoonMethodsChanged": 8, "INS": 12, "UPD": 17, "DEL": 9, "MOV": 21, "TOT": 59}, "title": "Error in SqlMetadataRuleManagerTest", "numCommits": 1, "created": "2018-07-20 20:48:31", "closed": "2018-07-24 19:00:49", "gitStatsSummary": {"deletions": 32, "insertions": 76, "lines": 108, "gitFilesChange": 2}, "statsSkippedReason": "", "changesInPackagesSPOON": ["io.druid.metadata.SQLMetadataRuleManager.poll()", "io.druid.metadata.SQLMetadataSegmentManager", "io.druid.metadata.SQLMetadataSegmentManager.poll()", "io.druid.metadata.SQLMetadataRuleManager", "io.druid.metadata.SQLMetadataSegmentManager.stop()", "io.druid.metadata.SQLMetadataSegmentManager.isStarted()", "io.druid.metadata.SQLMetadataSegmentManager.start()", "io.druid.metadata.SQLMetadataSegmentManager.start().1.run()"], "filteredCommits": ["5ee7b0cada983912083a85f079086cb579488efd"], "url": "https://github.com/apache/druid/issues/6028", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 3.000277777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "server/src/main/java/io/druid/metadata/SQLMetadataRuleManager.java", "deletions": 6, "insertions": 8, "lines": 14}, {"filePath": "server/src/main/java/io/druid/metadata/SQLMetadataSegmentManager.java", "deletions": 27, "insertions": 69, "lines": 96}, {"filePath": "server/src/test/java/io/druid/metadata/SQLMetadataSegmentManagerTest.java", "deletions": 0, "insertions": 5, "lines": 5}], "commitSpoonAstDiffStats": [{"spoonFilePath": "SQLMetadataSegmentManager.java", "spoonMethods": [{"INS": 2, "UPD": 10, "DEL": 3, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManager", "MOV": 0, "TOT": 15}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManager.poll()", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManager.isStarted()", "MOV": 1, "TOT": 4}, {"INS": 3, "UPD": 1, "DEL": 2, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManager.stop()", "MOV": 9, "TOT": 15}, {"INS": 2, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManager.start()", "MOV": 10, "TOT": 14}, {"INS": 3, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManager.start().1.run()", "MOV": 1, "TOT": 4}]}, {"spoonFilePath": "SQLMetadataRuleManager.java", "spoonMethods": [{"INS": 0, "UPD": 5, "DEL": 0, "spoonMethodName": "io.druid.metadata.SQLMetadataRuleManager.poll()", "MOV": 0, "TOT": 5}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.metadata.SQLMetadataRuleManager", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "SQLMetadataSegmentManagerTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManagerTest.testRemoveDataSegment()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManagerTest.testRemoveDataSource()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManagerTest.testPoll()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManagerTest.testPollWithCurroptedSegment()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManagerTest.testGetUnusedSegmentsForInterval()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-07-27 13:16:26", "commitMessage": "Synchronize scheduled poll() calls in SQLMetadataSegmentManager (#6041) (#6056)\n\nSimilar issue to https://github.com/apache/incubator-druid/issues/6028.", "commitUser": "jihoonson", "commitDateTime": "2018-07-27 13:16:26", "commitParents": ["f823e80d747c36d1957b82030f12c5f5bb552307"], "commitGHEventType": "referenced", "nameRev": "0c1b82ad73b6b6ec81d4411b4328ddf54e8dc016 tags/druid-0.12.2-rc1~7", "commitHash": "0c1b82ad73b6b6ec81d4411b4328ddf54e8dc016"}, {"commitGitStats": [{"filePath": "server/src/main/java/io/druid/metadata/SQLMetadataRuleManager.java", "deletions": 6, "insertions": 8, "lines": 14}, {"filePath": "server/src/main/java/io/druid/metadata/SQLMetadataSegmentManager.java", "deletions": 26, "insertions": 68, "lines": 94}, {"filePath": "server/src/test/java/io/druid/metadata/SQLMetadataSegmentManagerTest.java", "deletions": 0, "insertions": 5, "lines": 5}], "commitSpoonAstDiffStats": [{"spoonFilePath": "SQLMetadataSegmentManager.java", "spoonMethods": [{"INS": 2, "UPD": 10, "DEL": 3, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManager", "MOV": 0, "TOT": 15}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManager.poll()", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManager.isStarted()", "MOV": 1, "TOT": 4}, {"INS": 3, "UPD": 1, "DEL": 2, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManager.stop()", "MOV": 9, "TOT": 15}, {"INS": 2, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManager.start()", "MOV": 10, "TOT": 14}, {"INS": 3, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManager.start().1.run()", "MOV": 1, "TOT": 4}]}, {"spoonFilePath": "SQLMetadataRuleManager.java", "spoonMethods": [{"INS": 0, "UPD": 5, "DEL": 0, "spoonMethodName": "io.druid.metadata.SQLMetadataRuleManager.poll()", "MOV": 0, "TOT": 5}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.metadata.SQLMetadataRuleManager", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "SQLMetadataSegmentManagerTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManagerTest.testRemoveDataSegment()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManagerTest.testRemoveDataSource()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManagerTest.testPoll()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManagerTest.testPollWithCurroptedSegment()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.metadata.SQLMetadataSegmentManagerTest.testGetUnusedSegmentsForInterval()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-07-24 20:57:30", "commitMessage": "Synchronize scheduled poll() calls in SQLMetadataSegmentManager (#6041)\n\nSimilar issue to https://github.com/apache/incubator-druid/issues/6028.", "commitUser": "leventov", "commitDateTime": "2018-07-24 22:57:30", "commitParents": ["7d5eb0c21a62d8f40f3d7732f2a3e29f79bfa57f"], "commitGHEventType": "referenced", "nameRev": "5ee7b0cada983912083a85f079086cb579488efd tags/druid-0.13.0-incubating-rc1~200", "commitHash": "5ee7b0cada983912083a85f079086cb579488efd"}], "body": "```\r\nTests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 22.619 sec <<< FAILURE! - in io.druid.metadata.SQLMetadataRuleManagerTest\r\ntestMultipleStopAndStart(io.druid.metadata.SQLMetadataRuleManagerTest)  Time elapsed: 20.698 sec  <<< ERROR!\r\norg.skife.jdbi.v2.exceptions.CallbackFailedException: \r\norg.skife.jdbi.v2.exceptions.UnableToExecuteStatementException: java.sql.SQLTransactionRollbackException: A lock could not be obtained due to a deadlock, cycle of locks and waiters is:\r\nLock : ROW, SYSCOLUMNS, (5,12)\r\n  Waiting XID : {237, X} , APP, DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules\r\n  Granted XID : {217, S} \r\nLock : TABLE, DRUIDTESTC0DD9B47C4DF453DAEBA2ACA4613783E_RULES, Tablelock\r\n  Waiting XID : {217, IS} , APP, SELECT r.dataSource, r.payload FROM druidTestc0dd9b47c4df453daeba2aca4613783e_rules r INNER JOIN(SELECT dataSource, max(version) as version FROM druidTestc0dd9b47c4df453daeba2aca4613783e_rules GROUP BY dataSource) ds ON r.datasource = ds.datasource and r.version = ds.version\r\n  Granted XID : {237, X} \r\n. The selected victim is XID : 237. [statement:\"DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules\", located:\"DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules\", rewritten:\"DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules\", arguments:{ positional:{}, named:{}, finder:[]}]\r\n\tat io.druid.metadata.SQLMetadataRuleManagerTest.dropTable(SQLMetadataRuleManagerTest.java:209)\r\n\tat io.druid.metadata.SQLMetadataRuleManagerTest.cleanup(SQLMetadataRuleManagerTest.java:204)\r\nCaused by: org.skife.jdbi.v2.exceptions.UnableToExecuteStatementException: \r\njava.sql.SQLTransactionRollbackException: A lock could not be obtained due to a deadlock, cycle of locks and waiters is:\r\nLock : ROW, SYSCOLUMNS, (5,12)\r\n  Waiting XID : {237, X} , APP, DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules\r\n  Granted XID : {217, S} \r\nLock : TABLE, DRUIDTESTC0DD9B47C4DF453DAEBA2ACA4613783E_RULES, Tablelock\r\n  Waiting XID : {217, IS} , APP, SELECT r.dataSource, r.payload FROM druidTestc0dd9b47c4df453daeba2aca4613783e_rules r INNER JOIN(SELECT dataSource, max(version) as version FROM druidTestc0dd9b47c4df453daeba2aca4613783e_rules GROUP BY dataSource) ds ON r.datasource = ds.datasource and r.version = ds.version\r\n  Granted XID : {237, X} \r\n. The selected victim is XID : 237. [statement:\"DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules\", located:\"DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules\", rewritten:\"DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules\", arguments:{ positional:{}, named:{}, finder:[]}]\r\n\tat io.druid.metadata.SQLMetadataRuleManagerTest.dropTable(SQLMetadataRuleManagerTest.java:209)\r\n\tat io.druid.metadata.SQLMetadataRuleManagerTest.cleanup(SQLMetadataRuleManagerTest.java:204)\r\nCaused by: java.sql.SQLTransactionRollbackException: \r\nA lock could not be obtained due to a deadlock, cycle of locks and waiters is:\r\nLock : ROW, SYSCOLUMNS, (5,12)\r\n  Waiting XID : {237, X} , APP, DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules\r\n  Granted XID : {217, S} \r\nLock : TABLE, DRUIDTESTC0DD9B47C4DF453DAEBA2ACA4613783E_RULES, Tablelock\r\n  Waiting XID : {217, IS} , APP, SELECT r.dataSource, r.payload FROM druidTestc0dd9b47c4df453daeba2aca4613783e_rules r INNER JOIN(SELECT dataSource, max(version) as version FROM druidTestc0dd9b47c4df453daeba2aca4613783e_rules GROUP BY dataSource) ds ON r.datasource = ds.datasource and r.version = ds.version\r\n  Granted XID : {237, X} \r\n. The selected victim is XID : 237.\r\n\tat io.druid.metadata.SQLMetadataRuleManagerTest.dropTable(SQLMetadataRuleManagerTest.java:209)\r\n\tat io.druid.metadata.SQLMetadataRuleManagerTest.cleanup(SQLMetadataRuleManagerTest.java:204)\r\nCaused by: org.apache.derby.iapi.error.StandardException: \r\nA lock could not be obtained due to a deadlock, cycle of locks and waiters is:\r\nLock : ROW, SYSCOLUMNS, (5,12)\r\n  Waiting XID : {237, X} , APP, DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules\r\n  Granted XID : {217, S} \r\nLock : TABLE, DRUIDTESTC0DD9B47C4DF453DAEBA2ACA4613783E_RULES, Tablelock\r\n  Waiting XID : {217, IS} , APP, SELECT r.dataSource, r.payload FROM druidTestc0dd9b47c4df453daeba2aca4613783e_rules r INNER JOIN(SELECT dataSource, max(version) as version FROM druidTestc0dd9b47c4df453daeba2aca4613783e_rules GROUP BY dataSource) ds ON r.datasource = ds.datasource and r.version = ds.version\r\n  Granted XID : {237, X} \r\n. The selected victim is XID : 237.\r\n\tat io.druid.metadata.SQLMetadataRuleManagerTest.dropTable(SQLMetadataRuleManagerTest.java:209)\r\n\tat io.druid.metadata.SQLMetadataRuleManagerTest.cleanup(SQLMetadataRuleManagerTest.java:204)\r\n```"}, {"user": "jihoonson", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2018-07-18 23:48:46", "title": "NPE in KafkaSupervisor.checkpointTaskGroup", "url": "https://github.com/apache/druid/issues/6021", "closed": "2018-08-27 05:23:33", "ttf": 39.000277777777775, "commitsDetails": [], "body": "```\r\n2018-07-18T23:41:26,739 ERROR [KafkaSupervisor-xxxx] io.druid.indexing.kafka.supervisor.KafkaSupervisor - KafkaSupervisor[xxxx] failed to handle notice: {class=io.druid.indexing.kafka.supervisor.KafkaSupervisor, exceptionType=class java.lang.NullPointerException, exceptionMessage=null, noticeClass=GracefulShutdownNotice}\r\njava.lang.NullPointerException\r\n\tat io.druid.indexing.kafka.supervisor.KafkaSupervisor.checkpointTaskGroup(KafkaSupervisor.java:1434) ~[druid-kafka-indexing-service-0.12.1-iap8.jar:0.12.1-iap8]\r\n\tat io.druid.indexing.kafka.supervisor.KafkaSupervisor.checkTaskDuration(KafkaSupervisor.java:1382) ~[druid-kafka-indexing-service-0.12.1-iap8.jar:0.12.1-iap8]\r\n\tat io.druid.indexing.kafka.supervisor.KafkaSupervisor.gracefulShutdownInternal(KafkaSupervisor.java:813) ~[druid-kafka-indexing-service-0.12.1-iap8.jar:0.12.1-iap8]\r\n\tat io.druid.indexing.kafka.supervisor.KafkaSupervisor$GracefulShutdownNotice.handle(KafkaSupervisor.java:584) ~[druid-kafka-indexing-service-0.12.1-iap8.jar:0.12.1-iap8]\r\n\tat io.druid.indexing.kafka.supervisor.KafkaSupervisor$2.run(KafkaSupervisor.java:367) [druid-kafka-indexing-service-0.12.1-iap8.jar:0.12.1-iap8]\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_163]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_163]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_163]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_163]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_163]\r\n```\r\n\r\n`0.12.1-iap8` is available at https://github.com/implydata/druid/tree/druid-0.12.1-iap8, and very similar to https://github.com/apache/incubator-druid/tree/0.12.2."}, {"user": "jihoonson", "commits": {}, "labels": ["Bug"], "created": "2018-07-18 23:37:52", "title": "Race in taskMaster when the overlord becomes the leader", "url": "https://github.com/apache/druid/issues/6020", "closed": "2018-09-27 04:48:03", "ttf": 70.00027777777778, "commitsDetails": [], "body": "`TaskMaster` has the interfaces to return the variables (`taskRunner`, `taskQueue`, etc) which are initialized only when the overlord becomes the leader. The code of the interfaces is like this:\r\n\r\n```java\r\n  public Optional<TaskRunner> getTaskRunner()\r\n  {\r\n    if (overlordLeaderSelector.isLeader()) {\r\n      return Optional.of(taskRunner);\r\n    } else {\r\n      return Optional.absent();\r\n    }\r\n  }\r\n```\r\n\r\nHowever, `taskRunner` is initialized in `DruidLeaderSelector.Listener.becomeLeader()` which is called after the overlord becomes the leader, and thus `Optional.of()` throws an NPE. The full stack trace is as follows:\r\n\r\n```\r\njava.lang.NullPointerException\r\n        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:213) ~[guava-16.0.1.jar:?]\r\n        at com.google.common.base.Optional.of(Optional.java:85) ~[guava-16.0.1.jar:?]\r\n        at io.druid.indexing.overlord.TaskMaster.getTaskRunner(TaskMaster.java:214) ~[druid-indexing-service-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.indexing.overlord.http.OverlordResource.getWorkers(OverlordResource.java:810) ~[druid-indexing-service-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_163]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_163]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_163]\r\n        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_163]\r\n        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60) ~[jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205) ~[jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75) ~[jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302) ~[jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) ~[jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108) ~[jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) ~[jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84) ~[jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542) ~[jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409) [jersey-server-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409) [jersey-servlet-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558) [jersey-servlet-1.19.3.jar:1.19.3]\r\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733) [jersey-servlet-1.19.3.jar:1.19.3]\r\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) [javax.servlet-api-3.1.0.jar:3.1.0]\r\n        at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:286) [guice-servlet-4.1.0.jar:?]\r\n        at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:276) [guice-servlet-4.1.0.jar:?]\r\n        at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:181) [guice-servlet-4.1.0.jar:?]\r\n        at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91) [guice-servlet-4.1.0.jar:?]\r\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85) [guice-servlet-4.1.0.jar:?]\r\n        at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:120) [guice-servlet-4.1.0.jar:?]\r\n        at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:135) [guice-servlet-4.1.0.jar:?]\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at io.druid.server.http.RedirectFilter.doFilter(RedirectFilter.java:72) [druid-server-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at io.druid.server.security.PreResponseAuthorizationCheckFilter.doFilter(PreResponseAuthorizationCheckFilter.java:84) [druid-server-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at io.druid.server.security.AllowOptionsResourceFilter.doFilter(AllowOptionsResourceFilter.java:76) [druid-server-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at io.imply.security.ImplyAuthenticator$ImplyAuthenticationFilter.doFilter(ImplyAuthenticator.java:149) [imply-druid-security-0.12.0.3.jar:?]\r\n        at io.druid.server.security.AuthenticationWrappingFilter.doFilter(AuthenticationWrappingFilter.java:60) [druid-server-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at io.druid.security.basic.authentication.BasicHTTPAuthenticator$BasicHTTPAuthenticationFilter.doFilter(BasicHTTPAuthenticator.java:162) [druid-basic-security-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at io.druid.server.security.AuthenticationWrappingFilter.doFilter(AuthenticationWrappingFilter.java:60) [druid-server-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at io.druid.server.security.SecuritySanityCheckFilter.doFilter(SecuritySanityCheckFilter.java:86) [druid-server-0.12.1-iap8.jar:0.12.1-iap8]\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:224) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.Server.handle(Server.java:534) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:240) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_163]\r\n```"}, {"user": "TwojaWina", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2018-07-16 19:44:13", "title": "Kafka ingestion tasks are not killed and keep spawning after stopping Supervisor", "url": "https://github.com/apache/druid/issues/6006", "closed": "2019-01-31 20:08:52", "ttf": 199.00027777777777, "commitsDetails": [], "body": "Druid version `0.12.2 snapshot`\r\n\r\nAfter issuing stop command on Supervisor, there are still Kafka ingestion tasks running, and new tasks keep spawning afterwards. "}, {"user": "jihoonson", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2018-07-10 23:18:46", "title": "NPE while handling CheckpointNotice in KafkaSupervisor", "url": "https://github.com/apache/druid/issues/5992", "closed": "2018-07-14 00:14:58", "ttf": 3.000277777777778, "commitsDetails": [], "body": "The error occurs at the below check in the `CheckpointNotice` class.\r\n\r\n```java\r\n      Preconditions.checkNotNull(\r\n          sequenceTaskGroup.get(sequenceName),\r\n          \"WTH?! cannot find task group for this sequence [%s], sequencesTaskGroup map [%s], taskGroups [%s]\",\r\n          sequenceName,\r\n          sequenceTaskGroup,\r\n          taskGroups\r\n      );\r\n```\r\n\r\nThis happens in the below scenario.\r\n\r\n1) One of tasks in a taskGroup has succeeded, and the supervisor stopped all tasks in that group.\r\n2) One of tasks sent a checkpoint request to the supervisor before stopping.\r\n3) The supervisor received the checkpoint request after tidying up all information of the taskGroup."}, {"user": "jihoonson", "commits": {}, "labels": ["Bug"], "created": "2018-07-09 20:17:55", "title": "NPE in CuratorLoadQueuePeon", "url": "https://github.com/apache/druid/issues/5985", "closed": "2018-07-11 23:13:33", "ttf": 2.000277777777778, "commitsDetails": [], "body": "Introduced in https://github.com/apache/incubator-druid/pull/5929.\r\n\r\n```2018-07-09T20:04:48,973 ERROR [main-EventThread] org.apache.curator.framework.imps.CuratorFrameworkImpl - Watcher exception\r\njava.lang.NullPointerException\r\n        at io.druid.server.coordinator.CuratorLoadQueuePeon.entryRemoved(CuratorLoadQueuePeon.java:393) ~[druid-server-0.12.1-iap7.jar:0.12.1-iap7]\r\n        at io.druid.server.coordinator.CuratorLoadQueuePeon.lambda$processSegmentChangeRequest$1(CuratorLoadQueuePeon.java:270) ~[druid-server-0.12.1-iap7.jar:0.12.1-iap7]\r\n        at org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:83) [curator-framework-4.0.0.jar:4.0.0]\r\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:530) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]\r\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:505) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]```"}, {"user": "gianm", "commits": {}, "labels": ["Bug"], "created": "2018-07-09 16:25:26", "title": "Coordinators spinning in balancer", "url": "https://github.com/apache/druid/issues/5981", "closed": "2018-07-12 03:19:12", "ttf": 2.000277777777778, "commitsDetails": [], "body": "When rolling out the 0.12.2 branch to our test clusters, we noticed symptoms that suggest #5927 can hork up coordinators. They report a lot of time spent in these stack traces, and one of them has spent hours now without finishing a run:\r\n\r\n```\r\n\"Coordinator-Exec--0\" #120 daemon prio=5 os_prio=0 tid=0x00007f06802d4000 nid=0x20f7 runnable [0x00007f066a7d0000]\r\n   java.lang.Thread.State: RUNNABLE\r\n\tat io.druid.server.coordinator.ReservoirSegmentSampler.getRandomBalancerSegmentHolder(ReservoirSegmentSampler.java:46)\r\n\tat io.druid.server.coordinator.CostBalancerStrategy.pickSegmentToMove(CostBalancerStrategy.java:224)\r\n\tat io.druid.server.coordinator.helper.DruidCoordinatorBalancer.balanceTier(DruidCoordinatorBalancer.java:128)\r\n\tat io.druid.server.coordinator.helper.DruidCoordinatorBalancer.lambda$run$0(DruidCoordinatorBalancer.java:84)\r\n\tat io.druid.server.coordinator.helper.DruidCoordinatorBalancer$$Lambda$52/955068914.accept(Unknown Source)\r\n\tat java.util.HashMap.forEach(HashMap.java:1289)\r\n\tat io.druid.server.coordinator.helper.DruidCoordinatorBalancer.run(DruidCoordinatorBalancer.java:83)\r\n\tat io.druid.server.coordinator.DruidCoordinator$CoordinatorRunnable.run(DruidCoordinator.java:677)\r\n\tat io.druid.server.coordinator.DruidCoordinator$2.call(DruidCoordinator.java:571)\r\n\tat io.druid.server.coordinator.DruidCoordinator$2.call(DruidCoordinator.java:564)\r\n\tat io.druid.java.util.common.concurrent.ScheduledExecutors$2.run(ScheduledExecutors.java:102)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n```\r\n\r\nSo for 0.12.2 we should either revert this patch, or try to achieve the same thing in some other way.\r\n\r\n/cc @clintropolis"}, {"user": "leventov", "commits": {"b3021ec802c576edac5ed707be3f497769f0978d": {"commitGHEventType": "referenced", "commitUser": "jihoonson"}}, "changesInPackagesGIT": [], "labels": ["Area - Querying", "Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "Bug in SegmentAnalyzer.analyzeComplexColumn()", "numCommits": 0, "created": "2018-07-05 13:30:54", "closed": "2018-08-18 13:45:36", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/5939", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 44.000277777777775, "commitsDetails": [{"commitGitStats": [{"filePath": "processing/src/main/java/io/druid/segment/column/IndexedComplexColumn.java", "deletions": 0, "insertions": 6, "lines": 6}, {"filePath": "processing/src/main/java/io/druid/query/metadata/SegmentAnalyzer.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "processing/src/main/java/io/druid/segment/column/ComplexColumn.java", "deletions": 0, "insertions": 1, "lines": 1}], "commitSpoonAstDiffStats": [{"spoonFilePath": "IndexedComplexColumn.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.column.IndexedComplexColumn.getLength()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "SegmentAnalyzer.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.metadata.SegmentAnalyzer.analyzeComplexColumn(io.druid.segment.column.ColumnCapabilities,io.druid.segment.column.Column,java.lang.String)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "ComplexColumn.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.column.getLength()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-07-10 06:36:16", "commitMessage": "Fix bug in SegmentAnalyzer.analyzeComplexColumn() #5939 (#5954)\n\n", "commitUser": "jihoonson", "commitDateTime": "2018-07-09 15:36:16", "commitParents": ["441c9819d99677a5cb7b810ad126894ed3bf8eca"], "commitGHEventType": "referenced", "nameRev": "b3021ec802c576edac5ed707be3f497769f0978d tags/druid-0.13.0-incubating-rc1~218", "commitHash": "b3021ec802c576edac5ed707be3f497769f0978d"}], "body": "It calls to `Column.getLength()`, that's sole implementation in `SimpleColumn` tries to obtain a numeric column, that couldn't work in case of ComplexColumn."}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2018-06-26 22:15:42", "title": "JvmThreadsMonitor not injectable", "url": "https://github.com/apache/druid/issues/5904", "closed": "2018-09-25 03:41:17", "ttf": 90.00027777777778, "commitsDetails": [], "body": "This is against Master:\r\n\r\n```\r\nException in thread \"main\" com.google.inject.CreationException: Unable to create injector, see the following errors:\r\n\r\n1) Could not find a suitable constructor in io.druid.java.util.metrics.JvmThreadsMonitor. Classes must have either one (and only one) constructor annotated with @Inject or a zero-argument constructor that is not private.\r\n  at io.druid.java.util.metrics.JvmThreadsMonitor.class(JvmThreadsMonitor.java:35)\r\n  while locating io.druid.java.util.metrics.JvmThreadsMonitor\r\n  at io.druid.server.metrics.MetricsModule.getMonitorScheduler(MetricsModule.java:90) (via modules: com.google.inject.util.Modules$OverrideModule -> com.google.inject.util.Modules$OverrideModule -> io.druid.server.metrics.MetricsModule)\r\n  at io.druid.server.metrics.MetricsModule.getMonitorScheduler(MetricsModule.java:90) (via modules: com.google.inject.util.Modules$OverrideModule -> com.google.inject.util.Modules$OverrideModule -> io.druid.server.metrics.MetricsModule)\r\n  while locating io.druid.java.util.metrics.MonitorScheduler\r\n  at io.druid.server.metrics.MetricsModule.configure(MetricsModule.java:75) (via modules: com.google.inject.util.Modules$OverrideModule -> com.google.inject.util.Modules$OverrideModule -> io.druid.server.metrics.MetricsModule)\r\n  while locating io.druid.java.util.metrics.MonitorScheduler annotated with @com.google.inject.name.Named(value=ForTheEagerness)\r\n\r\n1 error\r\n\tat com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:470)\r\n\tat com.google.inject.internal.InternalInjectorCreator.injectDynamically(InternalInjectorCreator.java:184)\r\n\tat com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:110)\r\n\tat com.google.inject.Guice.createInjector(Guice.java:99)\r\n\tat com.google.inject.Guice.createInjector(Guice.java:73)\r\n\tat com.google.inject.Guice.createInjector(Guice.java:62)\r\n\tat io.druid.initialization.Initialization.makeInjectorWithModules(Initialization.java:421)\r\n\tat io.druid.cli.GuiceRunnable.makeInjector(GuiceRunnable.java:68)\r\n\tat io.druid.cli.ServerRunnable.run(ServerRunnable.java:49)\r\n\tat io.druid.cli.Main.main(Main.java:116)\r\n```"}, {"user": "gianm", "commits": {"a6eaaa5be95dc95bc6a1a391021e65afc94d97e1": {"commitGHEventType": "referenced", "commitUser": "gianm"}, "9bece8ce1e422257ab9a38ef8a56a9d91e728bfb": {"commitGHEventType": "referenced", "commitUser": "gianm"}}, "changesInPackagesGIT": [], "labels": ["Area - Streaming Ingestion", "Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "KafkaSupervisor NPE in checkPendingCompletionTasks when a group times out", "numCommits": 0, "created": "2018-06-23 01:01:16", "closed": "2018-07-05 06:46:15", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/5900", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 12.000277777777777, "commitsDetails": [{"commitGitStats": [{"filePath": "extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java", "deletions": 7, "insertions": 2, "lines": 9}, {"filePath": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java", "deletions": 30, "insertions": 33, "lines": 63}], "commitSpoonAstDiffStats": [{"spoonFilePath": "KafkaSupervisorTest.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisorTest.TestableKafkaSupervisor.generateSequenceName(int)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisorTest.TestableKafkaSupervisor.generateSequenceName(java.util.Map,com.google.common.base.Optional,com.google.common.base.Optional)", "MOV": 5, "TOT": 7}]}, {"spoonFilePath": "KafkaSupervisor.java", "spoonMethods": [{"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.createKafkaTasksForGroup(int,int)", "MOV": 0, "TOT": 2}, {"INS": 2, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.discoverTasks().4.apply(io.druid.indexing.kafka.KafkaIndexTask$Status)", "MOV": 1, "TOT": 4}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.TaskGroup", "MOV": 0, "TOT": 2}, {"INS": 2, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.createNewTasks()", "MOV": 1, "TOT": 4}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.verifyAndMergeCheckpoints(java.lang.Integer)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.resetInternal(io.druid.indexing.overlord.DataSourceMetadata)", "MOV": 1, "TOT": 3}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.checkPendingCompletionTasks()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.checkCurrentTaskState()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 2, "DEL": 3, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.generateSequenceName(int)", "MOV": 0, "TOT": 5}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.isTaskCurrent(int,java.lang.String)", "MOV": 1, "TOT": 2}, {"INS": 4, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.generateSequenceName(io.druid.indexing.kafka.supervisor.KafkaSupervisor$TaskGroup)", "MOV": 0, "TOT": 4}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.checkTaskDuration()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-07-09 11:22:03", "commitMessage": "Prevent KafkaSupervisor NPE in generateSequenceName (#5900) (#5902) (#5972)\n\n* Prevent KafkaSupervisor NPE in checkPendingCompletionTasks (#5900)\r\n\r\n* throw IAE in generateSequenceName if groupId not found in taskGroups\r\n* add null check in checkPendingCompletionTasks\r\n\r\n* Add warn log in checkPendingCompletionTasks\r\n\r\n* Address PR comments\r\n\r\nReplace warn with error log\r\n\r\n* Address PR comments\r\n\r\n* change signature of generateSequenceName to take a TaskGroup object instead of int\r\n\r\n* Address comments\r\n\r\n* Remove unnecessary method from KafkaSupervisorTest", "commitUser": "gianm", "commitDateTime": "2018-07-09 11:22:03", "commitParents": ["9c394727cb92673fdf981b5172120c993e17c76f"], "commitGHEventType": "referenced", "nameRev": "a6eaaa5be95dc95bc6a1a391021e65afc94d97e1 tags/druid-0.12.2-rc1~28", "commitHash": "a6eaaa5be95dc95bc6a1a391021e65afc94d97e1"}, {"commitGitStats": [{"filePath": "extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java", "deletions": 7, "insertions": 2, "lines": 9}, {"filePath": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java", "deletions": 30, "insertions": 33, "lines": 63}], "commitSpoonAstDiffStats": [{"spoonFilePath": "KafkaSupervisorTest.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisorTest.TestableKafkaSupervisor.generateSequenceName(int)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisorTest.TestableKafkaSupervisor.generateSequenceName(java.util.Map,com.google.common.base.Optional,com.google.common.base.Optional)", "MOV": 5, "TOT": 7}]}, {"spoonFilePath": "KafkaSupervisor.java", "spoonMethods": [{"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.createKafkaTasksForGroup(int,int)", "MOV": 0, "TOT": 2}, {"INS": 2, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.discoverTasks().4.apply(io.druid.indexing.kafka.KafkaIndexTask$Status)", "MOV": 1, "TOT": 4}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.TaskGroup", "MOV": 0, "TOT": 2}, {"INS": 2, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.createNewTasks()", "MOV": 1, "TOT": 4}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.verifyAndMergeCheckpoints(java.lang.Integer)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.resetInternal(io.druid.indexing.overlord.DataSourceMetadata)", "MOV": 1, "TOT": 3}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.checkPendingCompletionTasks()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.checkCurrentTaskState()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 2, "DEL": 3, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.generateSequenceName(int)", "MOV": 0, "TOT": 5}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.isTaskCurrent(int,java.lang.String)", "MOV": 1, "TOT": 2}, {"INS": 4, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.generateSequenceName(io.druid.indexing.kafka.supervisor.KafkaSupervisor$TaskGroup)", "MOV": 0, "TOT": 4}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.checkTaskDuration()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-07-04 23:45:42", "commitMessage": "Prevent KafkaSupervisor NPE in generateSequenceName (#5900) (#5902)\n\n* Prevent KafkaSupervisor NPE in checkPendingCompletionTasks (#5900)\r\n\r\n* throw IAE in generateSequenceName if groupId not found in taskGroups\r\n* add null check in checkPendingCompletionTasks\r\n\r\n* Add warn log in checkPendingCompletionTasks\r\n\r\n* Address PR comments\r\n\r\nReplace warn with error log\r\n\r\n* Address PR comments\r\n\r\n* change signature of generateSequenceName to take a TaskGroup object instead of int\r\n\r\n* Address comments\r\n\r\n* Remove unnecessary method from KafkaSupervisorTest\r\n", "commitUser": "gianm", "commitDateTime": "2018-07-04 23:45:42", "commitParents": ["4cd14e8158153ffc6432f7f09b148dca83517f5b"], "commitGHEventType": "referenced", "nameRev": "9bece8ce1e422257ab9a38ef8a56a9d91e728bfb tags/druid-0.13.0-incubating-rc1~226", "commitHash": "9bece8ce1e422257ab9a38ef8a56a9d91e728bfb"}], "body": "What happens is that task groups get moved from `taskGroups` to `pendingCompletionTaskGroups` in the `checkTaskDuration()` method. They are _moved_ in the sense that they are deleted from taskGroups and added to pendingCompletionTaskGroups. Then, in the `checkPendingCompletionTasks()` method, tasks group in `pendingCompletionTaskGroups` are examined to see if they have completed or not. The method then does some stuff to the ones that time out, including a `generateSequenceName` call that is guaranteed to NPE, since it is called for a groupId that has been removed from taskGroups (and which it tries to retrieve from taskGroups).\r\n\r\nThe relevant code in pendingCompletionTaskGroups is:\r\n\r\n```java\r\n        if ((!foundSuccess && group.completionTimeout.isBeforeNow()) || entireTaskGroupFailed) {\r\n          if (entireTaskGroupFailed) {\r\n            log.warn(\"All tasks in group [%d] failed to publish, killing all tasks for these partitions\", groupId);\r\n          } else {\r\n            log.makeAlert(\r\n                \"No task in [%s] succeeded before the completion timeout elapsed [%s]!\",\r\n                group.taskIds(),\r\n                ioConfig.getCompletionTimeout()\r\n            ).emit();\r\n          }\r\n\r\n          // reset partitions offsets for this task group so that they will be re-read from metadata storage\r\n          partitionGroups.get(groupId).replaceAll((partition, offset) -> NOT_SET);\r\n          sequenceTaskGroup.remove(generateSequenceName(groupId));\r\n\r\n          // kill all the tasks in this pending completion group\r\n          killTasksInGroup(group);\r\n          // set a flag so the other pending completion groups for this set of partitions will also stop\r\n          stopTasksInTaskGroup = true;\r\n\r\n          // kill all the tasks in the currently reading task group and remove the bad task group\r\n          killTasksInGroup(taskGroups.remove(groupId));\r\n          toRemove.add(group);\r\n        }\r\n```\r\n\r\nAnd generateSequenceName:\r\n\r\n```java\r\n  String generateSequenceName(int groupId)\r\n  {\r\n    return generateSequenceName(\r\n        taskGroups.get(groupId).partitionOffsets,\r\n        taskGroups.get(groupId).minimumMessageTime,\r\n        taskGroups.get(groupId).maximumMessageTime\r\n    );\r\n  }\r\n```\r\n\r\nRelated: #5656 and #5666, which experienced task timeouts due to a _different_ bug (#5899) and then NPEd due to _this_ bug.\r\n\r\nThis could be tested in KafkaSupervisorTest by adding a test that exercises the relevant code path in pendingCompletionTaskGroups. Something that simulates tasks never exiting after being told to finish.\r\n\r\n/cc @surekhasaharan "}, {"user": "jihoonson", "commits": {}, "labels": ["Bug"], "created": "2018-06-06 20:36:18", "title": "SegmentListerResource.getSegments() must return non-void type", "url": "https://github.com/apache/druid/issues/5853", "closed": "2019-07-18 22:02:28", "ttf": 407.0002777777778, "commitsDetails": [], "body": "When an error occurs in integration tests, the below message is shown instead of the real error.\r\n\r\n```\r\nWARNING: The following warnings have been detected with resource and/or provider classes:\r\n  WARNING: A HTTP GET method, public void io.druid.server.http.SegmentListerResource.getSegments(long,long,long,javax.servlet.http.HttpServletRequest) throws java.io.IOException, MUST return a non-void type.\r\n```"}, {"user": "jihoonson", "commits": {}, "labels": ["Bug"], "created": "2018-06-05 17:31:22", "title": "Map virtualColumn doesn't work properly in topN and groupBy", "url": "https://github.com/apache/druid/issues/5848", "closed": "2018-09-29 21:13:06", "ttf": 116.00027777777778, "commitsDetails": [], "body": "Map virtualCoumn works in select query, but doesn't in topN and groupBy.\r\nHere is the data and query to reproduce this bug.\r\n\r\n#### Data\r\nA tsv file containing a single row (\r\n```2011-01-12T00:00:00.000Z    a    key1,key2,key3    value1,value2,value3```)\r\n\r\n#### Ingestion spec\r\n```json\r\n{\r\n  \"type\" : \"index\",\r\n  \"spec\" : {\r\n    \"ioConfig\" : {\r\n      \"type\" : \"index\",\r\n      \"firehose\" : {\r\n        \"type\" : \"local\",\r\n        \"baseDir\" : \"/path/to/data\",\r\n        \"filter\" : \"test_map.tsv\"\r\n      }\r\n    },\r\n    \"dataSchema\" : {\r\n      \"dataSource\" : \"test_map\",\r\n      \"granularitySpec\" : {\r\n        \"type\" : \"uniform\",\r\n        \"segmentGranularity\" : \"DAY\",\r\n        \"intervals\" : [\"2011/2012\"]\r\n      },\r\n      \"parser\" : {\r\n        \"type\" : \"hadoopyString\",\r\n        \"parseSpec\" : {\r\n          \"format\" : \"tsv\",\r\n          \"delimiter\": \"\\t\",\r\n          \"listDelimiter\": \",\",\r\n          \"dimensionsSpec\" : {\r\n            \"dimensions\" : [\r\n              \"ts\",\r\n              \"dim\",\r\n              {\r\n                \"type\": \"string\",\r\n                \"name\": \"keys\",\r\n                \"multiValueHandling\": \"SORTED_ARRAY\"\r\n              },\r\n              {\r\n                \"type\": \"string\",\r\n                \"name\": \"values\",\r\n                \"multiValueHandling\": \"SORTED_ARRAY\"\r\n              }\r\n            ]\r\n          },\r\n          \"columns\": [ \"ts\", \"dim\", \"keys\", \"values\" ],\r\n          \"timestampSpec\" : {\r\n            \"format\" : \"auto\",\r\n            \"column\" : \"ts\"\r\n          }\r\n        }\r\n      },\r\n      \"metricsSpec\" : [\r\n        {\r\n          \"name\" : \"count\",\r\n          \"type\" : \"count\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n#### GroupBy query and result\r\n\r\n```json\r\n{\r\n  \"queryType\": \"groupBy\",\r\n  \"dataSource\": \"test_map\",\r\n  \"granularity\": \"day\",\r\n  \"dimensions\": [\"dim\", \"params.key1\"],\r\n  \"virtualColumns\": [\r\n    {\r\n      \"type\": \"map\",\r\n      \"keyDimension\": \"keys\",\r\n      \"valueDimension\": \"values\",\r\n      \"outputName\": \"params\"\r\n    }\r\n  ],\r\n  \"aggregations\": [\r\n    { \"type\": \"longSum\", \"name\": \"count\", \"fieldName\": \"count\" }\r\n  ],\r\n  \"intervals\": [ \"2011/2012\" ]\r\n}\r\n```\r\n\r\n```\r\n[ {\r\n  \"version\" : \"v1\",\r\n  \"timestamp\" : \"2011-01-12T00:00:00.000Z\",\r\n  \"event\" : {\r\n    \"count\" : 1,\r\n    \"dim\" : \"a\",\r\n    \"params.key1\" : null\r\n  }\r\n} ]\r\n```\r\n\r\n#### TopN query and result\r\n\r\n```json\r\n{\r\n  \"queryType\": \"topN\",\r\n  \"dataSource\": \"test_map\",\r\n  \"dimension\": \"params.key3\",\r\n  \"threshold\": 100,\r\n  \"metric\": \"count\",\r\n  \"granularity\": \"all\",\r\n  \"aggregations\": [\r\n    {\r\n      \"type\": \"longSum\",\r\n      \"name\": \"count\",\r\n      \"fieldName\": \"count\"\r\n    }\r\n  ],\r\n  \"virtualColumns\": [\r\n    {\r\n      \"type\": \"map\",\r\n      \"keyDimension\": \"keys\",\r\n      \"valueDimension\": \"values\",\r\n      \"outputName\": \"params\"\r\n    }\r\n  ],\r\n  \"intervals\": [\r\n    \"2011/2012\"\r\n  ]\r\n}\r\n```\r\n\r\n```\r\n[ {\r\n  \"timestamp\" : \"2011-01-12T00:00:00.000Z\",\r\n  \"result\" : [ {\r\n    \"count\" : 1,\r\n    \"params.key3\" : null\r\n  } ]\r\n} ]\r\n```"}, {"user": "clintropolis", "commits": {"47ad91a653d80f840b9496517b88dbd60c672efb": {"commitGHEventType": "referenced", "commitUser": "jihoonson"}, "2b45a6a42d09abcdcc259cf91cda9f98b3cf3c84": {"commitGHEventType": "referenced", "commitUser": "drcrallen"}}, "changesInPackagesGIT": [], "labels": ["Area - Querying", "Bug"], "spoonStatsSummary": {}, "title": "Lexicographic topN uses invalid optimization when query interval is smaller than segment granularity", "numCommits": 0, "created": "2018-05-30 16:46:30", "closed": "2018-05-31 16:53:30", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "filteredCommits": [], "url": "https://github.com/apache/druid/issues/5814", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 1.0002777777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "processing/src/main/java/io/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java", "deletions": 6, "insertions": 6, "lines": 12}, {"filePath": "processing/src/main/java/io/druid/query/topn/types/StringTopNColumnSelectorStrategy.java", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "processing/src/main/java/io/druid/segment/QueryableIndexStorageAdapter.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/main/java/io/druid/query/topn/TopNQueryEngine.java", "deletions": 10, "insertions": 10, "lines": 20}, {"filePath": "processing/src/main/java/io/druid/query/topn/DimExtractionTopNAlgorithm.java", "deletions": 5, "insertions": 6, "lines": 11}, {"filePath": "processing/src/main/java/io/druid/query/topn/types/NumericTopNColumnSelectorStrategy.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/test/java/io/druid/query/topn/PooledTopNAlgorithmTest.java", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "processing/src/main/java/io/druid/segment/QueryableIndexColumnSelectorFactory.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "processing/src/main/java/io/druid/query/topn/BaseTopNAlgorithm.java", "deletions": 14, "insertions": 26, "lines": 40}, {"filePath": "processing/src/main/java/io/druid/query/topn/types/TopNColumnSelectorStrategy.java", "deletions": 14, "insertions": 17, "lines": 31}, {"filePath": "processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java", "deletions": 5, "insertions": 8, "lines": 13}, {"filePath": "processing/src/main/java/io/druid/query/topn/TimeExtractionTopNAlgorithm.java", "deletions": 4, "insertions": 4, "lines": 8}, {"filePath": "processing/src/main/java/io/druid/segment/DimensionHandlerUtils.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "processing/src/main/java/io/druid/segment/incremental/IncrementalIndexStorageAdapter.java", "deletions": 0, "insertions": 1, "lines": 1}, {"filePath": "processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java", "deletions": 2, "insertions": 3, "lines": 5}, {"filePath": "processing/src/test/java/io/druid/query/topn/TopNMetricSpecOptimizationsTest.java", "deletions": 0, "insertions": 480, "lines": 480}, {"filePath": "processing/src/main/java/io/druid/segment/Capabilities.java", "deletions": 0, "insertions": 5, "lines": 5}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "tooManyChanges", "authoredDateTime": "2018-07-09 13:25:11", "commitMessage": "Fix topN lexicographic sort (#5815) (#5965)\n\n* fixes #5814\r\nchanges:\r\n* pass `StorageAdapter` to topn algorithms to get things like if column is 'sorted' or if query interval is smaller than segment granularity, instead of using `io.druid.segment.Capabilities`\r\n* remove `io.druid.segment.Capabilities` since it had one purpose, supplying `dimensionValuesSorted` which is now provided directly by `StorageAdapter`.\r\n* added test for topn optimization path checking\r\n\r\n* add Capabilities back since StorageAdapter is marked PublicApi\r\n\r\n* oops\r\n\r\n* add javadoc, fix build i think\r\n\r\n* correctly revert api changes\r\n\r\n* fix intellij fail\r\n\r\n* fix typo :(", "commitUser": "jihoonson", "commitDateTime": "2018-07-09 13:25:11", "commitParents": ["0b7bf6859eea9f0eb78fbcb6e42a019ace1586c5"], "commitGHEventType": "referenced", "nameRev": "47ad91a653d80f840b9496517b88dbd60c672efb tags/druid-0.12.2-rc1~14", "commitHash": "47ad91a653d80f840b9496517b88dbd60c672efb"}, {"commitGitStats": [{"filePath": "processing/src/main/java/io/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java", "deletions": 6, "insertions": 6, "lines": 12}, {"filePath": "processing/src/main/java/io/druid/query/topn/types/StringTopNColumnSelectorStrategy.java", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "processing/src/main/java/io/druid/segment/QueryableIndexStorageAdapter.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/main/java/io/druid/query/topn/TopNQueryEngine.java", "deletions": 10, "insertions": 10, "lines": 20}, {"filePath": "processing/src/main/java/io/druid/query/topn/DimExtractionTopNAlgorithm.java", "deletions": 5, "insertions": 6, "lines": 11}, {"filePath": "processing/src/main/java/io/druid/query/topn/types/NumericTopNColumnSelectorStrategy.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/test/java/io/druid/query/topn/PooledTopNAlgorithmTest.java", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "processing/src/main/java/io/druid/segment/QueryableIndexColumnSelectorFactory.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "processing/src/main/java/io/druid/query/topn/BaseTopNAlgorithm.java", "deletions": 14, "insertions": 26, "lines": 40}, {"filePath": "processing/src/main/java/io/druid/query/topn/types/TopNColumnSelectorStrategy.java", "deletions": 14, "insertions": 17, "lines": 31}, {"filePath": "processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java", "deletions": 5, "insertions": 8, "lines": 13}, {"filePath": "processing/src/main/java/io/druid/query/topn/TimeExtractionTopNAlgorithm.java", "deletions": 4, "insertions": 4, "lines": 8}, {"filePath": "processing/src/main/java/io/druid/segment/DimensionHandlerUtils.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "processing/src/main/java/io/druid/segment/incremental/IncrementalIndexStorageAdapter.java", "deletions": 0, "insertions": 1, "lines": 1}, {"filePath": "processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/test/java/io/druid/query/topn/TopNMetricSpecOptimizationsTest.java", "deletions": 0, "insertions": 480, "lines": 480}, {"filePath": "processing/src/main/java/io/druid/segment/Capabilities.java", "deletions": 0, "insertions": 5, "lines": 5}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "tooManyChanges", "authoredDateTime": "2018-05-31 09:53:29", "commitMessage": "Fix topN lexicographic sort (#5815)\n\n* fixes #5814\r\nchanges:\r\n* pass `StorageAdapter` to topn algorithms to get things like if column is 'sorted' or if query interval is smaller than segment granularity, instead of using `io.druid.segment.Capabilities`\r\n* remove `io.druid.segment.Capabilities` since it had one purpose, supplying `dimensionValuesSorted` which is now provided directly by `StorageAdapter`.\r\n* added test for topn optimization path checking\r\n\r\n* add Capabilities back since StorageAdapter is marked PublicApi\r\n\r\n* oops\r\n\r\n* add javadoc, fix build i think\r\n\r\n* correctly revert api changes\r\n\r\n* fix intellij fail\r\n\r\n* fix typo :(\r\n", "commitUser": "drcrallen", "commitDateTime": "2018-05-31 09:53:29", "commitParents": ["50ad7a45ffc50ea50d3d933d4264333734eb90ce"], "commitGHEventType": "referenced", "nameRev": "2b45a6a42d09abcdcc259cf91cda9f98b3cf3c84 tags/druid-0.13.0-incubating-rc1~277", "commitHash": "2b45a6a42d09abcdcc259cf91cda9f98b3cf3c84"}], "body": "Encountered an easily reproducible issue where using a `lexicographic` metrics spec on a `TopN` query produced an unexpectedly different number results than sorting by metric, `alphaNumeric`, or even `inverted` with `lexicographic`. \r\n\r\nDigging into the issue, it appears that specifically for `lexicographic`, [there is an optimization](https://github.com/druid-io/druid/blob/master/processing/src/main/java/io/druid/query/topn/BaseTopNAlgorithm.java#L309)   that can be taken if the query filters are not set. However, this strategy does not hold true if query interval is smaller than the segment granularity, so this must be checked or the query will produce incorrect results.\r\n"}, {"user": "nateMJ", "commits": {}, "labels": ["Bug"], "created": "2018-05-15 09:00:02", "title": "Shutdown supervisor without supervisorId", "url": "https://github.com/apache/druid/issues/5780", "closed": "2018-05-18 23:30:58", "ttf": 3.000277777777778, "commitsDetails": [], "body": "I am unable to shutdown a supervisor that was created without an ID or topic. If the only way to shutdown supervisors or tasks is using IDs, shouldn't there be an error when a supervisor is posted without an ID or kafka topic?"}, {"user": "vogievetsky", "commits": {"28e6ae3664b02b17ee691968469905f8df5a8e28": {"commitGHEventType": "referenced", "commitUser": "gianm"}, "98234e87741b577e451b9128dc9e2c674de41208": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Area - SQL", "Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "HLL in inner group by does not get finalized", "numCommits": 0, "created": "2018-05-15 01:28:05", "closed": "2018-08-25 20:56:24", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/5779", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 102.00027777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/builtin/CountSqlAggregator.java", "deletions": 2, "insertions": 4, "lines": 6}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/rel/DruidRel.java", "deletions": 1, "insertions": 5, "lines": 6}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/rel/PartialDruidQuery.java", "deletions": 2, "insertions": 3, "lines": 5}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/rule/DruidRules.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/rel/DruidQueryRel.java", "deletions": 4, "insertions": 9, "lines": 13}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/builtin/AvgSqlAggregator.java", "deletions": 4, "insertions": 5, "lines": 9}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/DimensionExpression.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "sql/src/test/java/io/druid/sql/calcite/planner/CalcitesTest.java", "deletions": 10, "insertions": 10, "lines": 20}, {"filePath": "sql/src/test/java/io/druid/sql/calcite/CalciteQueryTest.java", "deletions": 5, "insertions": 81, "lines": 86}, {"filePath": "extensions-core/histogram/src/main/java/io/druid/query/aggregation/histogram/sql/QuantileSqlAggregator.java", "deletions": 1, "insertions": 4, "lines": 5}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/SqlAggregator.java", "deletions": 1, "insertions": 5, "lines": 6}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/planner/Calcites.java", "deletions": 3, "insertions": 8, "lines": 11}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/rel/DruidSemiJoin.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/rule/GroupByRules.java", "deletions": 2, "insertions": 4, "lines": 6}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/builtin/SumSqlAggregator.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/builtin/MaxSqlAggregator.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/rel/DruidOuterQueryRel.java", "deletions": 5, "insertions": 13, "lines": 18}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/builtin/ApproxCountDistinctSqlAggregator.java", "deletions": 7, "insertions": 20, "lines": 27}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/rel/DruidQuery.java", "deletions": 13, "insertions": 21, "lines": 34}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/builtin/MinSqlAggregator.java", "deletions": 1, "insertions": 2, "lines": 3}], "commitSpoonAstDiffStats": [{"spoonFilePath": "DruidQuery.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidQuery.computeGrouping(io.druid.sql.calcite.rel.PartialDruidQuery,io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,boolean)", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidQuery.computeAggregations(io.druid.sql.calcite.rel.PartialDruidQuery,io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidQuery.computeSelectProjection(io.druid.sql.calcite.rel.PartialDruidQuery,io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature)", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidQuery", "MOV": 0, "TOT": 2}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidQuery.computeAggregations(io.druid.sql.calcite.rel.PartialDruidQuery,io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,boolean)", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidQuery.computeDimensions(io.druid.sql.calcite.rel.PartialDruidQuery,io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "DruidRel.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidRel.toDruidQuery(boolean)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "ApproxCountDistinctSqlAggregator.java", "spoonMethods": [{"INS": 4, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.aggregation.builtin.ApproxCountDistinctSqlAggregator.toDruidAggregation(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,java.lang.String,org.apache.calcite.rel.core.AggregateCall,org.apache.calcite.rel.core.Project,java.util.List,boolean)", "MOV": 0, "TOT": 4}, {"INS": 0, "UPD": 5, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.aggregation.builtin.ApproxCountDistinctSqlAggregator.toDruidAggregation(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,java.lang.String,org.apache.calcite.rel.core.AggregateCall,org.apache.calcite.rel.core.Project,java.util.List)", "MOV": 1, "TOT": 6}]}, {"spoonFilePath": "SumSqlAggregator.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.aggregation.builtin.SumSqlAggregator.toDruidAggregation(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,java.lang.String,org.apache.calcite.rel.core.AggregateCall,org.apache.calcite.rel.core.Project,java.util.List,boolean)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "QuantileSqlAggregator.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.aggregation.histogram.sql.QuantileSqlAggregator.toDruidAggregation(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,java.lang.String,org.apache.calcite.rel.core.AggregateCall,org.apache.calcite.rel.core.Project,java.util.List,boolean)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "AvgSqlAggregator.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.aggregation.builtin.AvgSqlAggregator.toDruidAggregation(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,java.lang.String,org.apache.calcite.rel.core.AggregateCall,org.apache.calcite.rel.core.Project,java.util.List,boolean)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 4, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.aggregation.builtin.AvgSqlAggregator.toDruidAggregation(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,java.lang.String,org.apache.calcite.rel.core.AggregateCall,org.apache.calcite.rel.core.Project,java.util.List)", "MOV": 0, "TOT": 4}]}, {"spoonFilePath": "SqlAggregator.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.aggregation.toDruidAggregation(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,java.lang.String,org.apache.calcite.rel.core.AggregateCall,org.apache.calcite.rel.core.Project,java.util.List,boolean)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "DruidOuterQueryRel.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidOuterQueryRel.toDruidQueryForExplaining()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidOuterQueryRel.runQuery()", "MOV": 0, "TOT": 1}, {"INS": 3, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidOuterQueryRel.toDruidQuery(boolean)", "MOV": 0, "TOT": 3}]}, {"spoonFilePath": "CalcitesTest.java", "spoonMethods": [{"INS": 0, "UPD": 10, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.planner.CalcitesTest.testFindOutputNamePrefix()", "MOV": 0, "TOT": 10}]}, {"spoonFilePath": "MaxSqlAggregator.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.aggregation.builtin.MaxSqlAggregator.toDruidAggregation(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,java.lang.String,org.apache.calcite.rel.core.AggregateCall,org.apache.calcite.rel.core.Project,java.util.List,boolean)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "PartialDruidQuery.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.PartialDruidQuery.build(io.druid.query.DataSource,io.druid.sql.calcite.table.RowSignature,io.druid.sql.calcite.planner.PlannerContext,org.apache.calcite.rex.RexBuilder,boolean)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "GroupByRules.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rule.GroupByRules.translateAggregateCall(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,org.apache.calcite.rel.core.Project,org.apache.calcite.rel.core.AggregateCall,java.util.List,java.lang.String,boolean)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "CountSqlAggregator.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.aggregation.builtin.CountSqlAggregator.toDruidAggregation(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,java.lang.String,org.apache.calcite.rel.core.AggregateCall,org.apache.calcite.rel.core.Project,java.util.List,boolean)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "DruidQueryRel.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidQueryRel.runQuery()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidQueryRel.toDruidQueryForExplaining()", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidQueryRel.toDruidQuery(boolean)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "Calcites.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.planner.Calcites.isUsablePrefix(java.util.NavigableSet,java.lang.String)", "MOV": 1, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.planner.Calcites.makePrefixedName(java.lang.String,java.lang.String)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.planner.Calcites.findOutputNamePrefix(java.lang.String,java.util.NavigableSet)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "MinSqlAggregator.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.aggregation.builtin.MinSqlAggregator.toDruidAggregation(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,java.lang.String,org.apache.calcite.rel.core.AggregateCall,org.apache.calcite.rel.core.Project,java.util.List,boolean)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "DimensionExpression.java", "spoonMethods": [{"INS": 0, "UPD": 3, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.aggregation.DimensionExpression.getVirtualColumnName()", "MOV": 1, "TOT": 4}]}, {"spoonFilePath": "DruidSemiJoin.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidSemiJoin.toDruidQuery(boolean)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "DruidRules.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rule.DruidRules.DruidOuterQueryRule", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rule.DruidRules.DruidQueryRule", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "CalciteQueryTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.CalciteQueryTest.testAvgDailyCountDistinct()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-08-25 13:56:23", "commitMessage": "SQL: Finalize aggregations for inner queries when necessary. (#6221)\n\n* SQL: Finalize aggregations for inner queries when necessary.\r\n\r\nFixes #5779.\r\n\r\n* Fixed test method name.\r\n", "commitUser": "gianm", "commitDateTime": "2018-08-25 13:56:23", "commitParents": ["9803ce954ac42e9330f6de4a5497a0adfb44dc5e"], "commitGHEventType": "referenced", "nameRev": "28e6ae3664b02b17ee691968469905f8df5a8e28 tags/druid-0.13.0-incubating-rc1~131", "commitHash": "28e6ae3664b02b17ee691968469905f8df5a8e28"}, {"commitGitStats": [{"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/builtin/CountSqlAggregator.java", "deletions": 2, "insertions": 4, "lines": 6}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/rel/DruidRel.java", "deletions": 1, "insertions": 5, "lines": 6}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/rel/PartialDruidQuery.java", "deletions": 2, "insertions": 3, "lines": 5}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/rule/DruidRules.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/rel/DruidQueryRel.java", "deletions": 4, "insertions": 9, "lines": 13}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/builtin/AvgSqlAggregator.java", "deletions": 4, "insertions": 5, "lines": 9}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/DimensionExpression.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "sql/src/test/java/io/druid/sql/calcite/planner/CalcitesTest.java", "deletions": 10, "insertions": 10, "lines": 20}, {"filePath": "sql/src/test/java/io/druid/sql/calcite/CalciteQueryTest.java", "deletions": 0, "insertions": 69, "lines": 69}, {"filePath": "extensions-core/histogram/src/main/java/io/druid/query/aggregation/histogram/sql/QuantileSqlAggregator.java", "deletions": 1, "insertions": 4, "lines": 5}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/SqlAggregator.java", "deletions": 1, "insertions": 5, "lines": 6}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/planner/Calcites.java", "deletions": 3, "insertions": 8, "lines": 11}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/rel/DruidSemiJoin.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/rule/GroupByRules.java", "deletions": 2, "insertions": 4, "lines": 6}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/builtin/SumSqlAggregator.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/builtin/MaxSqlAggregator.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/rel/DruidOuterQueryRel.java", "deletions": 5, "insertions": 13, "lines": 18}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/builtin/ApproxCountDistinctSqlAggregator.java", "deletions": 7, "insertions": 20, "lines": 27}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/rel/DruidQuery.java", "deletions": 13, "insertions": 21, "lines": 34}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/builtin/MinSqlAggregator.java", "deletions": 1, "insertions": 2, "lines": 3}], "commitSpoonAstDiffStats": [{"spoonFilePath": "DruidQuery.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidQuery.computeGrouping(io.druid.sql.calcite.rel.PartialDruidQuery,io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,boolean)", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidQuery.computeAggregations(io.druid.sql.calcite.rel.PartialDruidQuery,io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidQuery.computeSelectProjection(io.druid.sql.calcite.rel.PartialDruidQuery,io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature)", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidQuery", "MOV": 0, "TOT": 2}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidQuery.computeAggregations(io.druid.sql.calcite.rel.PartialDruidQuery,io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,boolean)", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidQuery.computeDimensions(io.druid.sql.calcite.rel.PartialDruidQuery,io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "DruidRel.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidRel.toDruidQuery(boolean)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "ApproxCountDistinctSqlAggregator.java", "spoonMethods": [{"INS": 4, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.aggregation.builtin.ApproxCountDistinctSqlAggregator.toDruidAggregation(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,java.lang.String,org.apache.calcite.rel.core.AggregateCall,org.apache.calcite.rel.core.Project,java.util.List,boolean)", "MOV": 0, "TOT": 4}, {"INS": 0, "UPD": 5, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.aggregation.builtin.ApproxCountDistinctSqlAggregator.toDruidAggregation(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,java.lang.String,org.apache.calcite.rel.core.AggregateCall,org.apache.calcite.rel.core.Project,java.util.List)", "MOV": 1, "TOT": 6}]}, {"spoonFilePath": "SumSqlAggregator.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.aggregation.builtin.SumSqlAggregator.toDruidAggregation(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,java.lang.String,org.apache.calcite.rel.core.AggregateCall,org.apache.calcite.rel.core.Project,java.util.List,boolean)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "QuantileSqlAggregator.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.aggregation.histogram.sql.QuantileSqlAggregator.toDruidAggregation(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,java.lang.String,org.apache.calcite.rel.core.AggregateCall,org.apache.calcite.rel.core.Project,java.util.List,boolean)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "AvgSqlAggregator.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.aggregation.builtin.AvgSqlAggregator.toDruidAggregation(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,java.lang.String,org.apache.calcite.rel.core.AggregateCall,org.apache.calcite.rel.core.Project,java.util.List,boolean)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 4, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.aggregation.builtin.AvgSqlAggregator.toDruidAggregation(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,java.lang.String,org.apache.calcite.rel.core.AggregateCall,org.apache.calcite.rel.core.Project,java.util.List)", "MOV": 0, "TOT": 4}]}, {"spoonFilePath": "SqlAggregator.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.aggregation.toDruidAggregation(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,java.lang.String,org.apache.calcite.rel.core.AggregateCall,org.apache.calcite.rel.core.Project,java.util.List,boolean)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "DruidOuterQueryRel.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidOuterQueryRel.toDruidQueryForExplaining()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidOuterQueryRel.runQuery()", "MOV": 0, "TOT": 1}, {"INS": 3, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidOuterQueryRel.toDruidQuery(boolean)", "MOV": 0, "TOT": 3}]}, {"spoonFilePath": "CalcitesTest.java", "spoonMethods": [{"INS": 0, "UPD": 10, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.planner.CalcitesTest.testFindOutputNamePrefix()", "MOV": 0, "TOT": 10}]}, {"spoonFilePath": "MaxSqlAggregator.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.aggregation.builtin.MaxSqlAggregator.toDruidAggregation(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,java.lang.String,org.apache.calcite.rel.core.AggregateCall,org.apache.calcite.rel.core.Project,java.util.List,boolean)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "PartialDruidQuery.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.PartialDruidQuery.build(io.druid.query.DataSource,io.druid.sql.calcite.table.RowSignature,io.druid.sql.calcite.planner.PlannerContext,org.apache.calcite.rex.RexBuilder,boolean)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "GroupByRules.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rule.GroupByRules.translateAggregateCall(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,org.apache.calcite.rel.core.Project,org.apache.calcite.rel.core.AggregateCall,java.util.List,java.lang.String,boolean)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "CountSqlAggregator.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.aggregation.builtin.CountSqlAggregator.toDruidAggregation(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,java.lang.String,org.apache.calcite.rel.core.AggregateCall,org.apache.calcite.rel.core.Project,java.util.List,boolean)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "DruidQueryRel.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidQueryRel.runQuery()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidQueryRel.toDruidQueryForExplaining()", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidQueryRel.toDruidQuery(boolean)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "Calcites.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.planner.Calcites.isUsablePrefix(java.util.NavigableSet,java.lang.String)", "MOV": 1, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.planner.Calcites.makePrefixedName(java.lang.String,java.lang.String)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.planner.Calcites.findOutputNamePrefix(java.lang.String,java.util.NavigableSet)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "MinSqlAggregator.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.aggregation.builtin.MinSqlAggregator.toDruidAggregation(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexBuilder,java.lang.String,org.apache.calcite.rel.core.AggregateCall,org.apache.calcite.rel.core.Project,java.util.List,boolean)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "DimensionExpression.java", "spoonMethods": [{"INS": 0, "UPD": 3, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.aggregation.DimensionExpression.getVirtualColumnName()", "MOV": 1, "TOT": 4}]}, {"spoonFilePath": "DruidSemiJoin.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rel.DruidSemiJoin.toDruidQuery(boolean)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "DruidRules.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rule.DruidRules.DruidOuterQueryRule", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rule.DruidRules.DruidQueryRule", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "CalciteQueryTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.CalciteQueryTest.testAvgDailyCountDistinct()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-08-26 16:15:08", "commitMessage": "[Backport] SQL: Finalize aggregations for inner queries when necessary. (#6229)\n\n* SQL: Finalize aggregations for inner queries when necessary. (#6221)\r\n\r\n* SQL: Finalize aggregations for inner queries when necessary.\r\n\r\nFixes #5779.\r\n\r\n* Fixed test method name.\r\n\r\n* Fix test for 0.12 branch.\r\n", "commitUser": "fjy", "commitDateTime": "2018-08-26 17:15:08", "commitParents": ["0b79e76720bb41dfc316632d1709898c8019fce8"], "commitGHEventType": "referenced", "nameRev": "98234e87741b577e451b9128dc9e2c674de41208 tags/druid-0.12.3-rc1~12", "commitHash": "98234e87741b577e451b9128dc9e2c674de41208"}], "body": "I try running this query to compute average daily active users on the wikipedia dataset:\r\n\r\n```sql\r\nSELECT AVG(\"t_unique\") AS \"AvgDAU\"\r\nFROM (SELECT\r\n      TIME_FLOOR(\"__time\", 'P1D'),\r\n      APPROX_COUNT_DISTINCT(\"user_unique\") AS \"t_unique\"  \r\n    FROM \"wikipedia\"\r\n    GROUP BY 1\r\n)\r\n```\r\n\r\nI get the error: `Unknown type[class io.druid.hll.HLLCV1] for field` probably because not finalizer is added.\r\n\r\nSame result if you substitute \"user_unique\" (an HLL column) with \"user\" (a string column)\r\n\r\nThis is the explain plan:\r\n\r\n```\r\nDruidOuterQueryRel(query=[{\"queryType\":\"timeseries\",\"dataSource\":{\"type\":\"table\",\"name\":\"__subquery__\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"descending\":false,\"virtualColumns\":[],\"filter\":null,\"granularity\":{\"type\":\"all\"},\"aggregations\":[{\"type\":\"longSum\",\"name\":\"a0:sum\",\"fieldName\":\"t_unique\",\"expression\":null},{\"type\":\"count\",\"name\":\"a0:count\"}],\"postAggregations\":[{\"type\":\"arithmetic\",\"name\":\"a0\",\"fn\":\"quotient\",\"fields\":[{\"type\":\"fieldAccess\",\"name\":null,\"fieldName\":\"a0:sum\"},{\"type\":\"fieldAccess\",\"name\":null,\"fieldName\":\"a0:count\"}],\"ordering\":null}],\"context\":{\"skipEmptyBuckets\":true,\"timeout\":300000}}], signature=[{a0:LONG}])\r\n     DruidQueryRel(query=[{\"queryType\":\"timeseries\",\"dataSource\":{\"type\":\"table\",\"name\":\"wikipedia\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"descending\":false,\"virtualColumns\":[],\"filter\":null,\"granularity\":\"DAY\",\"aggregations\":[{\"type\":\"hyperUnique\",\"name\":\"a0\",\"fieldName\":\"user_unique\",\"isInputHyperUnique\":false,\"round\":true}],\"postAggregations\":[],\"context\":{\"skipEmptyBuckets\":true,\"timeout\":300000}}], signature=[{d0:LONG, a0:LONG}])\r\n```\r\n\r\nPlease finalize"}, {"user": "gianm", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2018-05-09 02:37:45", "title": "Kafka: Reordered segment allocation causes spurious failures", "url": "https://github.com/apache/druid/issues/5761", "closed": "2018-07-02 22:09:13", "ttf": 54.000277777777775, "commitsDetails": [], "body": "In one of our test clusters we saw Kafka index task failures corresponding to `sequence_name_prev_id_sha1` unique constraint violations on the overlord. The query getting run was this one:\r\n\r\n```\r\n            handle.createStatement(\r\n                StringUtils.format(\r\n                    \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, sequence_name, sequence_prev_id, sequence_name_prev_id_sha1, payload) \"\r\n                    + \"VALUES (:id, :dataSource, :created_date, :start, :end, :sequence_name, :sequence_prev_id, :sequence_name_prev_id_sha1, :payload)\",\r\n                    dbTables.getPendingSegmentsTable(), connector.getQuoteString()\r\n                )\r\n            )\r\n                  .bind(\"id\", newIdentifier.getIdentifierAsString())\r\n                  .bind(\"dataSource\", dataSource)\r\n                  .bind(\"created_date\", DateTimes.nowUtc().toString())\r\n                  .bind(\"start\", interval.getStart().toString())\r\n                  .bind(\"end\", interval.getEnd().toString())\r\n                  .bind(\"sequence_name\", sequenceName)\r\n                  .bind(\"sequence_prev_id\", previousSegmentIdNotNull)\r\n                  .bind(\"sequence_name_prev_id_sha1\", sequenceNamePrevIdSha1)\r\n                  .bind(\"payload\", jsonMapper.writeValueAsBytes(newIdentifier))\r\n                  .execute();\r\n```\r\n\r\nThe constraint violation happens when the (sequence_name, previousSegmentIdNotNull) tuple is not unique. So in particular, it happens when the segment sequence \"forks\" and goes from the same segment X into two subsequent segments Y and Z. And from looking at the tasks involved, the events that occurred looked something like this:\r\n\r\n1. A Kafka index task allocated some segments and then failed for some reason.\r\n2. A new task was started up to re-read from the same offset.\r\n3. The new task also tried to allocate segments, and it worked for a while, but eventually it tried to allocate one in a _different order_ than the original task. This fails and so does the whole task.\r\n4. Another task starts up, but it fails for the same reason.\r\n5. The cycle of violence continues ad infinitum.\r\n\r\nI think the problem is that the allocation fails if you don't do them in the same order as they were originally done in, but, since #4815 it's no longer guaranteed that two Kafka index tasks, starting from the same partition/offsets, will create segments in the same order (due to the mixing of messages from different partitions being non-deterministic). I believe this is meant to be okay, since the hypothetical two tasks would still end up at the same place, due to the restriction that there is just one segment per interval now.\r\n\r\n(Aside: if there were more than one segment per interval, the tasks would _not_ be guaranteed to end up at the same place, since they wouldn't necessarily make the same decision about which rows to put in which segments for the same interval.)\r\n\r\nSo I am thinking we should:\r\n\r\n1. Confirm that there is meant to only be one segment per interval per sequence now, and add sanity checks to enforce this if they do not already exist.\r\n2. Confirm there is no longer any need for the previous-segment-id check.\r\n3. Remove the check: allow a sequence to allocate segments out of order, subject to there being at most one per interval per segment.\r\n\r\n/cc @pjain1 @jihoonson "}, {"user": "drcrallen", "commits": {}, "labels": ["Area - Testing", "Bug"], "created": "2018-05-08 01:58:41", "title": "Master build fails", "url": "https://github.com/apache/druid/issues/5752", "closed": "2018-05-08 02:03:35", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "```\r\n[ERROR] Failed to execute goal de.thetaphi:forbiddenapis:2.3:check (validate) on project druid-aws-common: Check for forbidden API calls failed: java.lang.ClassNotFoundException: io.druid.concurrent.Execs -> [Help 1]\r\n```"}, {"user": "silver--bullet", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2018-05-06 01:37:32", "title": "ConcurrentModificationException in KafkaIndexTask", "url": "https://github.com/apache/druid/issues/5745", "closed": "2018-07-01 00:20:42", "ttf": 55.000277777777775, "commitsDetails": [], "body": "```\r\n01:26:32.470 [publish-driver] ERROR io.druid.indexing.kafka.KafkaIndexTask - Error in publish thread, dying: {class=io.druid.indexing.kafka.KafkaIndexTask, exceptionType=class java.util.ConcurrentModificationException, exceptionMessage=null}\r\njava.util.ConcurrentModificationException\r\n\tat java.util.HashMap$HashIterator.nextNode(HashMap.java:1429) ~[?:1.8.0_91]\r\n\tat java.util.HashMap$KeyIterator.next(HashMap.java:1453) ~[?:1.8.0_91]\r\n\tat java.util.AbstractCollection.toString(AbstractCollection.java:461) ~[?:1.8.0_91]\r\n\tat java.lang.String.valueOf(String.java:2994) ~[?:1.8.0_91]\r\n\tat java.lang.StringBuilder.append(StringBuilder.java:131) ~[?:1.8.0_91]\r\n\tat io.druid.indexing.kafka.KafkaIndexTask$SequenceMetadata.toString(KafkaIndexTask.java:2175) ~[?:?]\r\n\tat java.lang.String.valueOf(String.java:2994) ~[?:1.8.0_91]\r\n\tat java.util.Arrays.toString(Arrays.java:4571) ~[?:1.8.0_91]\r\n\tat java.util.concurrent.CopyOnWriteArrayList.toString(CopyOnWriteArrayList.java:1011) ~[?:1.8.0_91]\r\n\tat java.util.Formatter$FormatSpecifier.printString(Formatter.java:2886) ~[?:1.8.0_91]\r\n\tat java.util.Formatter$FormatSpecifier.print(Formatter.java:2763) ~[?:1.8.0_91]\r\n\tat java.util.Formatter.format(Formatter.java:2520) ~[?:1.8.0_91]\r\n\tat java.util.Formatter.format(Formatter.java:2455) ~[?:1.8.0_91]\r\n\tat java.lang.String.format(String.java:2981) ~[?:1.8.0_91]\r\n\tat io.druid.java.util.common.StringUtils.nonStrictFormat(StringUtils.java:122) ~[java-util-0.12.0.jar:0.12.0]\r\n\tat io.druid.java.util.common.logger.Logger.info(Logger.java:78) ~[java-util-0.12.0.jar:0.12.0]\r\n\tat io.druid.indexing.kafka.KafkaIndexTask.persistSequences(KafkaIndexTask.java:1315) ~[?:?]\r\n\tat io.druid.indexing.kafka.KafkaIndexTask.lambda$createAndStartPublishExecutor$1(KafkaIndexTask.java:388) ~[?:?]\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_91]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_91]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_91]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_91]\r\n\tat java.lang.Thread.run(Thread.java:745) [?:1.8.0_91]\r\n```"}, {"user": "jihoonson", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2018-05-01 23:37:09", "title": "Broken contract of Appenderator in KafkaIndexTask ", "url": "https://github.com/apache/druid/issues/5729", "closed": "2018-07-03 20:31:30", "ttf": 62.000277777777775, "commitsDetails": [], "body": "#4815 introduced the incremental handoff for KafkaIndexTask. In this PR, [publishExecService](https://github.com/druid-io/druid/blob/master/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIndexTask.java#L357-L424) was added, but it breaks the below contract of `Appenderator`.\r\n\r\n> If committer is not provided, no metadata is persisted. If it's provided, the add, clear, persist, persistAll, and push methods should all be called from the same thread to keep the metadata committed by Committer in sync.\r\n\r\nOne solution is adding synchronization blocks to KafkaIndexTask when it calls `AppenderatorDriver.add()`, `AppenderatorDriver.persist()`, and `AppenderatorDriver.publish()`. However, I'm not sure why `publishExecService` is needed because `Appenderator` and `AppenderatorDriver` are already supporting background publishing."}, {"user": "JeKuOrdina", "commits": {}, "labels": ["Bug"], "created": "2018-04-18 09:13:20", "title": "No serializer found for class io.druid.client.ImmutableDruidDataSource", "url": "https://github.com/apache/druid/issues/5661", "closed": "2018-04-20 00:41:54", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "## Problem\r\n\r\nI've seem to have misconfigured the druid installation and cannot retrieve the full metadata for datasources.\r\n\r\nGet request error for `http://<host>:8081/druid/coordinator/v1/datasources/wikiticker?full=true`\r\n```\r\n com.fasterxml.jackson.databind.JsonMappingException: No serializer found for class io.druid.client.ImmutableDruidDataSource and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) )\r\n```\r\n\r\nAny pointers on where I should look to resolve this issue ?\r\n\r\n## Druid Setup\r\n\r\n- Druid Version:\r\nLatest stable version 0.12.0, installed using binaries.\r\n- System: \r\nKerberized Hadoop Cluster with druid running on 4 nodes.\r\n- Loaded plugins: \r\n[\"druid-histogram\",\"druid-datasketches\",\"druid-lookups-cached-global\",\"mysql-metadata-storage\",\"druid-kerberos\",\"druid-hdfs-storage\",\"druid-avro-extensions\",\"druid-parquet-extensions\"]\r\n- Common config\r\n```\r\n\r\ndruid.extensions.loadList=[\"druid-histogram\",\"druid-datasketches\",\"druid-lookups-cached-global\",\"mysql-metadata-storage\",\"druid-kerberos\",\"druid-hdfs-storage\",\"druid-avro-extensions\",\"druid-parquet-extensions\"]\r\ndruid.indexer.logs.directory=</hdfs/root/path/druid/logdir>\r\ndruid.indexer.logs.type=hdfs\r\ndruid.javascript.enabled=false\r\ndruid.metadata.storage.connector.connectURI=jdbc:mysql://<mysql host>:3306/druid\r\ndruid.metadata.storage.connector.password=***REDACTED***\r\ndruid.metadata.storage.connector.user=***REDACTED***\r\ndruid.metadata.storage.type=mysql\r\ndruid.sql.enable=false\r\ndruid.startup.logging.logProperties=true\r\ndruid.storage.storageDirectory=</hdfs/root/path/druid>\r\ndruid.storage.type=hdfs\r\ndruid.zk.paths.base=/druid\r\ndruid.zk.service.host=<host 1>:2181,<host 2>:2181,<host 3>:2181\r\ndruid.extensions.directory=/opt/cloudera/parcels/DRUID-0.12.0-0.0.4/extensions\r\ndruid.extensions.hadoopDependenciesDir=/opt/cloudera/parcels/DRUID-0.12.0-0.0.4/hadoop-dependencies\r\ndruid.selectors.indexing.serviceName=druid/overlord\r\ndruid.selectors.coordinator.serviceName=druid/coordinator\r\ndruid.request.logging.feed=druid_requests\r\ndruid.request.logging.type=emitter\r\ndruid.monitoring.emissionPeriod=PT1m\r\ndruid.hadoop.security.kerberos.principal=***REDACTED***\r\ndruid.hadoop.security.kerberos.keytab=***REDACTED***\r\n\r\n```\r\n## Successful http request\r\n\r\n**Request:**\r\n```\r\nhttp://<host>:8081/druid/coordinator/v1/datasources/\r\n```\r\n**Response:**\r\n```\r\n[\"cv_data\",\"wikiticker\"]\r\n```\r\n\r\n## Failed http request\r\n\r\n**Request:**\r\n```\r\nhttp://<host>:8081/druid/coordinator/v1/datasources/wikiticker?full=true\r\n```\r\n**Response:**\r\n```\r\nHTTP ERROR: 500\r\nProblem accessing /druid/coordinator/v1/datasources/wikiticker. Reason:\r\n\r\n    com.fasterxml.jackson.databind.JsonMappingException: No serializer found for class io.druid.client.ImmutableDruidDataSource and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) )\r\n```\r\n\r\n**Stacktrace**\r\n```\r\ncom.fasterxml.jackson.databind.JsonMappingException: No serializer found for class io.druid.client.ImmutableDruidDataSource and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) )\r\n\tat com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:59) ~[jackson-databind-2.4.6.jar:2.4.6]\r\n\tat com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:26) ~[jackson-databind-2.4.6.jar:2.4.6]\r\n\tat com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.6.jar:2.4.6]\r\n\tat com.fasterxml.jackson.databind.ObjectWriter.writeValue(ObjectWriter.java:602) ~[jackson-databind-2.4.6.jar:2.4.6]\r\n\tat com.fasterxml.jackson.jaxrs.base.ProviderBase.writeTo(ProviderBase.java:648) ~[jackson-jaxrs-base-2.4.6.jar:2.4.6]\r\n\tat com.sun.jersey.spi.container.ContainerResponse.write(ContainerResponse.java:302) ~[jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1510) ~[jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419) ~[jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409) ~[jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409) ~[jersey-servlet-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558) ~[jersey-servlet-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733) ~[jersey-servlet-1.19.3.jar:1.19.3]\r\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790) ~[javax.servlet-api-3.1.0.jar:3.1.0]\r\n\tat com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:286) ~[guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:276) ~[guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:181) ~[guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91) ~[guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:120) ~[guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:135) ~[guice-servlet-4.1.0.jar:?]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) ~[jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat io.druid.server.http.RedirectFilter.doFilter(RedirectFilter.java:72) ~[druid-server-0.12.0.jar:0.12.0]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) ~[jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat io.druid.server.security.PreResponseAuthorizationCheckFilter.doFilter(PreResponseAuthorizationCheckFilter.java:84) ~[druid-server-0.12.0.jar:0.12.0]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) ~[jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat io.druid.server.security.AllowAllAuthenticator$1.doFilter(AllowAllAuthenticator.java:85) ~[druid-server-0.12.0.jar:0.12.0]\r\n\tat io.druid.server.security.AuthenticationWrappingFilter.doFilter(AuthenticationWrappingFilter.java:60) ~[druid-server-0.12.0.jar:0.12.0]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) ~[jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat io.druid.server.security.SecuritySanityCheckFilter.doFilter(SecuritySanityCheckFilter.java:86) ~[druid-server-0.12.0.jar:0.12.0]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) ~[jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582) ~[jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:224) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512) ~[jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.Server.handle(Server.java:534) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283) ~[jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108) ~[jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93) ~[jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303) ~[jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148) ~[jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136) ~[jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671) ~[jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589) ~[jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]\r\n\r\n```\r\n\r\nSimilar error for cv_data"}, {"user": "Oooocean", "commits": {"638f50cb52c248f4408975d5fc7762cc9ce82d8e": {"commitGHEventType": "referenced", "commitUser": "gianm"}, "0f429298cff950c8d2dcc69d21c2e68c619f8177": {"commitGHEventType": "referenced", "commitUser": "gianm"}}, "changesInPackagesGIT": [], "labels": ["Area - Streaming Ingestion", "Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "Kafka Indexing Service task pause forever until timeout if no events in taskduration?", "numCommits": 0, "created": "2018-04-17 15:37:47", "closed": "2018-07-05 06:51:39", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/5656", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 78.00027777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java", "deletions": 0, "insertions": 81, "lines": 81}, {"filePath": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java", "deletions": 6, "insertions": 5, "lines": 11}], "commitSpoonAstDiffStats": [{"spoonFilePath": "KafkaSupervisorTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisorTest.testNoDataIngestionTasks()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "KafkaSupervisor.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 1, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.checkpointTaskGroup(int,boolean).8.apply(java.util.List)", "MOV": 0, "TOT": 2}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-07-09 11:22:16", "commitMessage": "Fix Kafka Indexing task pause forever if no events in taskDuration (#5656) (#5899) (#5971)\n\n* Fix Kafka Indexing task pause forever (#5656)\r\n\r\n* Fix Nullpointer Exception in overlord if taskGroups does not contain the groupId\r\n* If the endOffset is same as startOffset, still let the task resume instead of returning\r\n   endOffsets early which causes the tasks to pause forever and ultimately fail on timeout\r\n\r\n* Address PR comment\r\n\r\n*Remove the null check and do not return null from generateSequenceName", "commitUser": "gianm", "commitDateTime": "2018-07-09 11:22:16", "commitParents": ["a6eaaa5be95dc95bc6a1a391021e65afc94d97e1"], "commitGHEventType": "referenced", "nameRev": "638f50cb52c248f4408975d5fc7762cc9ce82d8e tags/druid-0.12.2-rc1~27", "commitHash": "638f50cb52c248f4408975d5fc7762cc9ce82d8e"}, {"commitGitStats": [{"filePath": "extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java", "deletions": 0, "insertions": 81, "lines": 81}, {"filePath": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java", "deletions": 3, "insertions": 2, "lines": 5}], "commitSpoonAstDiffStats": [{"spoonFilePath": "KafkaSupervisorTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisorTest.testNoDataIngestionTasks()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "KafkaSupervisor.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 1, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.checkpointTaskGroup(int,boolean).8.apply(java.util.List)", "MOV": 0, "TOT": 2}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-06-25 19:29:36", "commitMessage": "Fix Kafka Indexing task pause forever if no events in taskDuration (#5656) (#5899)\n\n* Fix Kafka Indexing task pause forever (#5656)\r\n\r\n* Fix Nullpointer Exception in overlord if taskGroups does not contain the groupId\r\n* If the endOffset is same as startOffset, still let the task resume instead of returning\r\n   endOffsets early which causes the tasks to pause forever and ultimately fail on timeout\r\n\r\n* Address PR comment\r\n\r\n*Remove the null check and do not return null from generateSequenceName\r\n", "commitUser": "gianm", "commitDateTime": "2018-06-25 19:29:36", "commitParents": ["7649742943a4a355e223be9284716e3fd177c14c"], "commitGHEventType": "referenced", "nameRev": "0f429298cff950c8d2dcc69d21c2e68c619f8177 tags/druid-0.13.0-incubating-rc1~240", "commitHash": "0f429298cff950c8d2dcc69d21c2e68c619f8177"}], "body": " We wanna use kafka indexing serivce for streaming events. But I  encounter some problem even in very simple case.\r\n1. Install druid on one machine. (use imply-2.5.8.tar.gz)\r\n2. Install kafka on same machine.\r\n3. create topic, and submit a spec:\r\n{\r\n  \"type\": \"kafka\",\r\n  \"dataSchema\": {\r\n    \"dataSource\": \"metrics-ocean4\",\r\n    \"parser\": {\r\n      \"type\": \"string\",\r\n      \"parseSpec\": {\r\n        \"format\": \"json\",\r\n        \"timestampSpec\": {\r\n          \"column\": \"timestamp\",\r\n          \"format\": \"auto\"\r\n        },\r\n        \"dimensionsSpec\": {\r\n          \"dimensions\": [],\r\n          \"dimensionExclusions\": [\r\n            \"timestamp\",\r\n            \"value\"\r\n          ]\r\n        }\r\n      }\r\n    },\r\n    \"metricsSpec\": [\r\n      {\r\n        \"name\": \"count\",\r\n        \"type\": \"count\"\r\n      },\r\n      {\r\n        \"name\": \"value_sum\",\r\n        \"fieldName\": \"value\",\r\n        \"type\": \"doubleSum\"\r\n      },\r\n      {\r\n        \"name\": \"value_min\",\r\n        \"fieldName\": \"value\",\r\n        \"type\": \"doubleMin\"\r\n      },\r\n      {\r\n        \"name\": \"value_max\",\r\n        \"fieldName\": \"value\",\r\n        \"type\": \"doubleMax\"\r\n      }\r\n    ],\r\n    \"granularitySpec\": {\r\n      \"type\": \"uniform\",\r\n      \"segmentGranularity\": \"MINUTE\",\r\n      \"queryGranularity\": \"NONE\"\r\n    }\r\n  },\r\n  \"tuningConfig\": {\r\n    \"type\": \"kafka\",\r\n    \"maxRowsPerSegment\": 5000000,\r\n    \"intermediatePersistPeriod\": \"PT5M\",\r\n    \"basePersistDirectory\": \"/home/qspace/data/mmgamedruiddata/tmp\"\r\n  },\r\n  \"ioConfig\": {\r\n    \"topic\": \"oceantopic4\",\r\n    \"consumerProperties\": {\r\n      \"bootstrap.servers\": \"10.242.20.145:9092\"\r\n    },\r\n    \"taskCount\": 1,\r\n    \"replicas\": 1,\r\n    \"taskDuration\": \"PT10M\",\r\n    \"completionTimeout\": \"PT10M\"\r\n  }\r\n}\r\nNote that I set task duration to 10M, because I wanna hava a quick test. But my tasks mostly failed in publishing status.\r\n\r\n![image](https://user-images.githubusercontent.com/13477599/38879963-2123cb68-4297-11e8-954c-cef2491781bc.png)\r\nFailed task log seems that stop at \r\n\"2018-04-17T11:25:33,184 INFO [task-runner-0-priority-0] io.druid.indexing.kafka.KafkaIndexTask - Pausing ingestion until resumed\" and never resume.\r\n\r\nI set process overlord log to debug. When index task(middleMgr) paused, overload will send offset/end to index task to publish segments.\r\nIf there aren't any envets, offsets would not changed. So overload will not send \"offset/end\" any more? \r\nIf index task(middleMgr) cannot receive resume, it will pause forever until taskCompleteTimeout.\r\nAm i miss something? Could anyone help. Thx advance!\r\n \r\nSUCC index task overlord: ![ok_overlord](https://user-images.githubusercontent.com/13477599/38880203-b55934ee-4297-11e8-8241-76fa0cb79965.png)\r\nFAIL index task overload log: \r\n![fail_overlord](https://user-images.githubusercontent.com/13477599/38880268-e074215c-4297-11e8-8112-7701ce8d2649.png)\r\n\r\n\r\n"}, {"user": "pdeva", "commits": {}, "labels": ["Bug"], "created": "2018-04-09 20:09:45", "title": "compact task throws NullPointerException", "url": "https://github.com/apache/druid/issues/5611", "closed": "2018-04-13 04:11:04", "ttf": 3.000277777777778, "commitsDetails": [], "body": "```\r\n2018-04-09 20:05:03,829 ERROR i.d.i.o.ThreadPoolTaskRunner [task-runner-0-priority-0] Exception while running task[CompactionTask{id=compact_pctile-hour_2018-04-09T20:04:50.734Z, type=compact, dataSource=pctile-hour}]\r\njava.lang.NullPointerException\r\n\tat io.druid.indexing.common.task.CompactionTask.lambda$createDataSchema$1(CompactionTask.java:299) ~[druid-indexing-service-0.12.0.jar:0.12.0]\r\n\tat java.util.stream.MatchOps$1MatchSink.accept(MatchOps.java:90) ~[?:1.8.0_161]\r\n\tat java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1359) ~[?:1.8.0_161]\r\n\tat java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126) ~[?:1.8.0_161]\r\n\tat java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498) ~[?:1.8.0_161]\r\n\tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485) ~[?:1.8.0_161]\r\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[?:1.8.0_161]\r\n\tat java.util.stream.MatchOps$MatchOp.evaluateSequential(MatchOps.java:230) ~[?:1.8.0_161]\r\n\tat java.util.stream.MatchOps$MatchOp.evaluateSequential(MatchOps.java:196) ~[?:1.8.0_161]\r\n\tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_161]\r\n\tat java.util.stream.ReferencePipeline.allMatch(ReferencePipeline.java:454) ~[?:1.8.0_161]\r\n\tat io.druid.indexing.common.task.CompactionTask.createDataSchema(CompactionTask.java:299) ~[druid-indexing-service-0.12.0.jar:0.12.0]\r\n\tat io.druid.indexing.common.task.CompactionTask.createIngestionSchema(CompactionTask.java:232) ~[druid-indexing-service-0.12.0.jar:0.12.0]\r\n\tat io.druid.indexing.common.task.CompactionTask.run(CompactionTask.java:184) ~[druid-indexing-service-0.12.0.jar:0.12.0]\r\n\tat io.druid.indexing.overlord.ThreadPoolTaskRunner$ThreadPoolTaskRunnerCallable.call(ThreadPoolTaskRunner.java:444) [druid-indexing-service-0.12.0.jar:0.12.0]\r\n\tat io.druid.indexing.overlord.ThreadPoolTaskRunner$ThreadPoolTaskRunnerCallable.call(ThreadPoolTaskRunner.java:416) [druid-indexing-service-0.12.0.jar:0.12.0]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_161]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_161]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_161]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_161]\r\n```\r\n\r\nState of segments:\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/3190176/38520252-1e0ed022-3bf7-11e8-823b-7a5b3dbdf6c3.png)\r\n\r\n\r\nTask specification:\r\n\r\n```\r\n{\r\n  \"type\": \"compact\",\r\n  \"dataSource\": \"pctile-hour\",\r\n  \"interval\": \"2018-04-08T00:00:00.000Z/2018-04-09T00:00:00.000Z\"\r\n}\r\n```\r\nPayload:\r\n\r\n```\r\n{\r\n  \"task\": \"compact_pctile-hour_2018-04-09T20:04:50.734Z\",\r\n  \"payload\": {\r\n    \"id\": \"compact_pctile-hour_2018-04-09T20:04:50.734Z\",\r\n    \"resource\": {\r\n      \"availabilityGroup\": \"compact_pctile-hour_2018-04-09T20:04:50.734Z\",\r\n      \"requiredCapacity\": 1\r\n    },\r\n    \"dataSource\": \"pctile-hour\",\r\n    \"interval\": \"2018-04-08T00:00:00.000Z/2018-04-09T00:00:00.000Z\",\r\n    \"segments\": null,\r\n    \"tuningConfig\": null,\r\n    \"context\": null,\r\n    \"groupId\": \"compact_pctile-hour_2018-04-09T20:04:50.734Z\",\r\n    \"dimensionsSpec\": null\r\n  }\r\n}\r\n```\r\nDruid 0.12.0"}, {"user": "pguzik", "commits": {}, "labels": ["Bug", "Security"], "created": "2018-04-06 11:11:17", "title": "In Druid 0.12 there is no endpoint for OPTIONS queries. Thus it is impossible to run CORS queries.", "url": "https://github.com/apache/druid/issues/5588", "closed": "2018-04-12 21:02:21", "ttf": 6.000277777777778, "commitsDetails": [], "body": "We receive such HTTP query errors (403 return code):\r\n\r\n`Request did not have an authorization check performed.: {class=io.druid.server.security.PreResponseAuthorizationCheckFilter, uri=/druid/v2/, method=OPTIONS)`\r\n\r\nWhile running CORS HTTP POST query, there is a pre-flight request which is of OPTION type.\r\n\r\nIs is missing @OPTIONS endpoint in QueryResource class"}, {"user": "pdeva", "commits": {}, "labels": ["Bug"], "created": "2018-04-03 19:41:40", "title": "exception on node shutdown", "url": "https://github.com/apache/druid/issues/5568", "closed": "2019-01-21 23:17:10", "ttf": 293.0002777777778, "commitsDetails": [], "body": "Happens in 0.12.0, but i am sure i noticed in 0.11.0 too\r\n\r\n```\r\n2018-04-03 19:40:38,809 Thread-49 ERROR Unable to register shutdown hook because JVM is shutting down. java.lang.IllegalStateException: Not started\r\n\tat io.druid.common.config.Log4jShutdown.addShutdownCallback(Log4jShutdown.java:47)\r\n\tat org.apache.logging.log4j.core.impl.Log4jContextFactory.addShutdownCallback(Log4jContextFactory.java:273)\r\n\tat org.apache.logging.log4j.core.LoggerContext.setUpShutdownHook(LoggerContext.java:256)\r\n\tat org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:216)\r\n\tat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:145)\r\n\tat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41)\r\n\tat org.apache.logging.log4j.LogManager.getContext(LogManager.java:182)\r\n\tat org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:103)\r\n\tat org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:43)\r\n\tat org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:42)\r\n\tat org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:29)\r\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:253)\r\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:265)\r\n\tat org.apache.curator.RetryLoop.<init>(RetryLoop.java:65)\r\n\tat org.apache.curator.CuratorZookeeperClient.newRetryLoop(CuratorZookeeperClient.java:151)\r\n\tat org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:59)\r\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100)\r\n\tat org.apache.curator.framework.imps.CuratorTransactionImpl.commit(CuratorTransactionImpl.java:123)\r\n\tat io.druid.curator.announcement.Announcer.stop(Announcer.java:170)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat io.druid.java.util.common.lifecycle.Lifecycle$AnnotationBasedHandler.stop(Lifecycle.java:434)\r\n\tat io.druid.java.util.common.lifecycle.Lifecycle.stop(Lifecycle.java:335)\r\n\tat io.druid.java.util.common.lifecycle.Lifecycle$1.run(Lifecycle.java:366)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n```"}, {"user": "pdeva", "commits": {}, "labels": ["Bug"], "created": "2018-03-31 18:37:08", "title": "exception when browsing the overlord console in 0.12.0", "url": "https://github.com/apache/druid/issues/5559", "closed": "2018-04-05 21:16:07", "ttf": 5.000277777777778, "commitsDetails": [], "body": "exception:\r\n\r\n```\r\n2018-03-31 18:33:15,345 WARN o.e.j.s.HttpChannel [qtp2020602315-146] //<coordinatorhost>:8080/js/console-0.0.1.js\r\nio.druid.java.util.common.ISE: Request did not have an authorization check performed.\r\n\tat io.druid.server.security.PreResponseAuthorizationCheckFilter.handleAuthorizationCheckError(PreResponseAuthorizationCheckFilter.java:158) ~[druid-server-0.12.0.jar:0.12.0]\r\n\tat io.druid.server.security.PreResponseAuthorizationCheckFilter.doFilter(PreResponseAuthorizationCheckFilter.java:91) ~[druid-server-0.12.0.jar:0.12.0]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) ~[jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat io.druid.server.security.AllowAllAuthenticator$1.doFilter(AllowAllAuthenticator.java:85) ~[druid-server-0.12.0.jar:0.12.0]\r\n\tat io.druid.server.security.AuthenticationWrappingFilter.doFilter(AuthenticationWrappingFilter.java:60) ~[druid-server-0.12.0.jar:0.12.0]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) ~[jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat io.druid.server.security.SecuritySanityCheckFilter.doFilter(SecuritySanityCheckFilter.java:86) ~[druid-server-0.12.0.jar:0.12.0]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) ~[jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582) ~[jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:224) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512) ~[jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.Server.handle(Server.java:534) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat java.lang.Thread.run(Thread.java:745) [?:1.8.0_101]\r\n```\r\n\r\nDruid version: 0.12.0\r\n\r\nCoorindator as overlord enabled\r\n(`druid.coordinator.asOverlord.enabled=true`)\r\n\r\nNo middle managers connected.\r\n\r\nTo repro, just navigate to http://coordinatorhost:8080/console.html\r\n\r\nview of console:\r\n(seems to be stuck on loading autoscale activities)\r\n\r\n![image](https://user-images.githubusercontent.com/3190176/38166372-b74e95fa-34d7-11e8-9d19-b7851b9dcaf6.png)\r\n\r\n"}, {"user": "clintropolis", "commits": {"30fc4d3ba0c43409ea9b0ef228c017345c02ef0b": {"commitGHEventType": "referenced", "commitUser": "jihoonson"}, "6ff329bd6f7f6c36c1afe2fef928c1856303494b": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Area - Segment Balancing/Coordination", "Bug"], "spoonStatsSummary": {}, "title": "Coordinator rule runner and balancer can trample on each other, often producing unstable/undefined results, particularly when cluster is heavily imbalanced", "numCommits": 0, "created": "2018-03-22 19:01:11", "closed": "2018-08-06 21:21:10", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "filteredCommits": [], "url": "https://github.com/apache/druid/issues/5521", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 137.00027777777777, "commitsDetails": [{"commitGitStats": [{"filePath": "server/src/main/java/io/druid/server/coordinator/DruidCoordinator.java", "deletions": 79, "insertions": 49, "lines": 128}, {"filePath": "server/src/main/java/io/druid/server/coordinator/helper/DruidCoordinatorRuleRunner.java", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "server/src/main/java/io/druid/server/coordinator/CuratorLoadQueuePeon.java", "deletions": 47, "insertions": 23, "lines": 70}, {"filePath": "server/src/test/java/io/druid/curator/CuratorTestBase.java", "deletions": 0, "insertions": 51, "lines": 51}, {"filePath": "server/src/main/java/io/druid/client/AbstractCuratorServerInventoryView.java", "deletions": 49, "insertions": 14, "lines": 63}, {"filePath": "server/src/test/java/io/druid/server/coordinator/CuratorDruidCoordinatorTest.java", "deletions": 0, "insertions": 534, "lines": 534}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "tooManyChanges", "authoredDateTime": "2018-03-29 10:30:12", "commitMessage": "Coordinator balancer move then drop fix (#5528)\n\n* #5521 part 1\r\n\r\n* formatting\r\n\r\n* oops\r\n\r\n* less magic tests\r\n", "commitUser": "jihoonson", "commitDateTime": "2018-03-29 10:30:12", "commitParents": ["8878a7ff943bf2ce6f15339d330bf4adf8d28472"], "commitGHEventType": "referenced", "nameRev": "30fc4d3ba0c43409ea9b0ef228c017345c02ef0b tags/druid-0.13.0-incubating-rc1~380", "commitHash": "30fc4d3ba0c43409ea9b0ef228c017345c02ef0b"}, {"commitGitStats": [{"filePath": "server/src/main/java/io/druid/server/coordinator/DruidCoordinator.java", "deletions": 79, "insertions": 49, "lines": 128}, {"filePath": "server/src/main/java/io/druid/server/coordinator/helper/DruidCoordinatorRuleRunner.java", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "server/src/main/java/io/druid/server/coordinator/CuratorLoadQueuePeon.java", "deletions": 48, "insertions": 24, "lines": 72}, {"filePath": "server/src/test/java/io/druid/curator/CuratorTestBase.java", "deletions": 0, "insertions": 51, "lines": 51}, {"filePath": "server/src/main/java/io/druid/client/AbstractCuratorServerInventoryView.java", "deletions": 49, "insertions": 14, "lines": 63}, {"filePath": "server/src/test/java/io/druid/server/coordinator/CuratorDruidCoordinatorTest.java", "deletions": 0, "insertions": 527, "lines": 527}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "tooManyChanges", "authoredDateTime": "2018-07-06 08:09:19", "commitMessage": "Coordinator balancer move then drop fix (#5528) (#5946)\n\n* #5521 part 1\r\n\r\n* formatting\r\n\r\n* oops\r\n\r\n* less magic tests", "commitUser": "fjy", "commitDateTime": "2018-07-06 08:09:19", "commitParents": ["edff62a43befed4e6b33a15f6e287cd18d2fa01b"], "commitGHEventType": "referenced", "nameRev": "6ff329bd6f7f6c36c1afe2fef928c1856303494b tags/druid-0.12.2-rc1~32", "commitHash": "6ff329bd6f7f6c36c1afe2fef928c1856303494b"}], "body": "TL;DR The deeper the load queue, the less likely a historical is to actually retain segments it is told to load because of some bugs and competing coordinator logic. Assigning and moving segments is done with the balancer, which makes segment placement decisions taking into account the load and drop queues of servers. Dropping segments is done considering solely the ideal state of all historicals (currently size loaded + load queue size) producing very quirky undefined behavior when trying to balance segments in a very imbalanced cluster (e.g. when replacing a failed historical or adding a new historical to the cluster) if this value surpasses the size of the 'typical' loaded historicals.\r\n\r\nA potential workaround is to set `maxSegmentsInNodeLoadingQueue` on coordinator config to be small enough that size actually loaded + size of load queue is less likely to be larger than the size of a 'typical' historical. Reducing `maxSegmentsToMove` can also have a positive effect.\r\n\r\nThis was a fun one to hunt down! We ran into an issue where a cluster had replaced a historical server, but after a time began to present with 'stuck segment' log messages. This was not a production cluster so we had the rare luxury to investigate a bit deeper than might have otherwise been normally available, and noticed that this historical was constantly being told by the coordinator to load and then nearly immediately drop the same segments it had just loaded every cycle. \r\n\r\nThe basic pattern played out like this in the coordinator logs:\r\n```\r\n2018-03-22T01:22:35,505 INFO [Coordinator-Exec--0] io.druid.server.coordinator.helper.DruidCoordinatorBalancer - Moving [some_segment] from [happy.historical.internal:8283] to [unlucky.historical.internal:8283]\r\n2018-03-22T01:22:35,505 INFO [Coordinator-Exec--0] io.druid.server.coordinator.LoadQueuePeon - Asking server peon[/druid/loadQueue/unlucky.historical.internal:8283] to load segment[some_segment]\r\n...\r\n2018-03-22T01:22:55,646 INFO [Master-PeonExec--0] io.druid.server.coordinator.LoadQueuePeon - Server[/druid/loadQueue/unlucky.historical.internal:8283] loading [some_segment]\r\n2018-03-22T01:22:55,646 INFO [Master-PeonExec--0] io.druid.server.coordinator.LoadQueuePeon - Server[/druid/loadQueue/unlucky.historical.internal:8283] processing segment[some_segment]\r\n...\r\n2018-03-22T01:22:58,422 INFO [main-EventThread] io.druid.server.coordinator.LoadQueuePeon - Server[/druid/loadQueue/unlucky.historical.internal:8283] done processing [/druid/loadQueue/unlucky.historical.internal:8283/some_segment]\r\n```\r\nThen the next coordinator run:\r\n```\r\n...\r\n2018-03-22T01:23:05,542 INFO [Coordinator-Exec--0] io.druid.server.coordinator.LoadQueuePeon - Asking server peon[/druid/loadQueue/unlucky.historical.internal:8283] to drop segment[some_segment]\r\n2018-03-22T01:23:08,327 INFO [Master-PeonExec--0] io.druid.server.coordinator.LoadQueuePeon - Server[/druid/loadQueue/unlucky.historical.internal:8283] dropping [some_segment]\r\n2018-03-22T01:23:08,327 INFO [Master-PeonExec--0] io.druid.server.coordinator.LoadQueuePeon - Server[/druid/loadQueue/unlucky.historical.internal:8283] processing segment[some_segment]\r\n```\r\n\r\nThis happened for many segments. Looking deeper, the first issue we noticed is that the segment move code in the cost balancer is partially broken, in that segments fail to be dropped from the source server after successfully loading on the destination server as the design intends. A flaw in the play between how [`BatchDataSegmentAnnouncer` announces a set of segments under a single partially random zk node path](https://github.com/druid-io/druid/blob/693e3575f9ff878d94770948f468810f91552b32/server/src/main/java/io/druid/server/coordination/BatchDataSegmentAnnouncer.java#L308), and an assumption [`AbstractCuratorServerInventoryView.isSegmentLoadedByServer` is making that the segment is advertised in a zk node by identifier](https://github.com/druid-io/druid/blob/80419752b53b1fe27cc59e6e07a0e8e89cfe5869/server/src/main/java/io/druid/client/AbstractCuratorServerInventoryView.java#L335) means that it [never in fact makes it to the logic that drops the segment after moving](https://github.com/druid-io/druid/blob/2099b43e5fe21e10f92d9703224206275c79c8ce/server/src/main/java/io/druid/server/coordinator/DruidCoordinator.java#L432). So, the add happens, but the drop can not. \r\n\r\nOn the next coordinator run, the rule runner will notice that this segment is now over replicated, and decide that one of them needs dropped.\r\n\r\nHowever, [the logic to decide which server to drop from](https://github.com/druid-io/druid/blob/80419752b53b1fe27cc59e6e07a0e8e89cfe5869/server/src/main/java/io/druid/server/coordinator/rules/LoadRule.java#L355) chooses [the server with the least amount of space available](https://github.com/druid-io/druid/blob/26fd2b3a8ebd16d47cb4291405cea1777a7ad0f2/server/src/main/java/io/druid/server/coordinator/ServerHolder.java#L118) but unfortunately for us [_this includes the load queue_.](https://github.com/druid-io/druid/blob/26fd2b3a8ebd16d47cb4291405cea1777a7ad0f2/server/src/main/java/io/druid/server/coordinator/ServerHolder.java#L81) If the cluster is in an ideal state this isn't as painful, but when trying to load a historical with a small number of segments, this means that if the actual segment size + load queue size of the mostly empty server adds up to be larger than the historical that the segment was moved from, the segment will be dropped from the mostly empty server. We confirmed that this was indeed the case with the sizes reported in the logs:\r\n\r\n```\r\n2018-03-22T01:23:05,569 INFO [Coordinator-Exec--0] io.druid.server.coordinator.helper.DruidCoordinatorLogger - Server[happy.historical.internal:8283, historical, _default_tier] has 0 left to load, 0 left to drop, 0 bytes queued, 517,477,739,359 bytes served.\r\n2018-03-22T01:23:05,569 INFO [Coordinator-Exec--0] io.druid.server.coordinator.helper.DruidCoordinatorLogger - Server[unlucky.historical.internal:8283, historical, _default_tier] has 1,957 left to load, 10 left to drop, 545,943,116,991 bytes queued, 28,668,295,253 bytes served.\r\n```\r\n\r\nNow of course, the balancer, wanting to balance the cluster and fill up our still empty guy, will decide to move that or some other segments back to the imbalanced historical, likely repeating the process, perhaps with different segments this time. The number of segments affected per pair of coordinator runs is maximally bound by the `maxSegmentsToMove` configuration.\r\n\r\nThis is very likely the cause of a wide range of totally strange coordinator behavior, from stuck and slow balancing, to overly large load queues, etc.\r\n\r\nThe assign segments part of the rule runner delegates the decision of where to add the segment to the balancer, so too should the logic which decides which server should drop over replicated segments. Additionally, the segment drop at the end of a move should either work, or be removed and use a two pass strategy like what is effectively happening in the current state."}, {"user": "niketh", "commits": {}, "labels": ["Area - Metrics/Event Emitting", "Bug"], "created": "2018-03-14 00:03:38", "title": "Query fails because of potential bug in HTTPPostEmitter", "url": "https://github.com/apache/druid/issues/5485", "closed": "2018-04-13 04:07:12", "ttf": 30.00027777777778, "commitsDetails": [], "body": "Queries fail will the following stacktrace @leventov \r\n\r\n```\r\n2018-03-13T23:05:59,740 ERROR [groupBy_xxxx_[2018-03-08T00:00:00.000Z/2018-03-08T08:00:00.000Z]] com.metamx.emitter.core.HttpPostEmitter - Serious error during onSealExclusive(), set currentBatch to the failed Batch.batchNumber\r\njava.lang.IllegalStateException: Attempted arrival of unregistered party for java.util.concurrent.Phaser@18dcd7c9[phase = 34658 parties = 1 arrived = 1]\r\n        at java.util.concurrent.Phaser.doArrive(Phaser.java:391) ~[?:1.8.0_112]\r\n        at java.util.concurrent.Phaser.arrive(Phaser.java:634) ~[?:1.8.0_112]\r\n        at com.metamx.emitter.core.EmittedBatchCounter.batchEmitted(EmittedBatchCounter.java:40) ~[java-util-1.3.4-SNAPSHOT.jar:?]\r\n        at com.metamx.emitter.core.HttpPostEmitter.batchFinalized(HttpPostEmitter.java:383) ~[java-util-1.3.4-SNAPSHOT.jar:?]\r\n        at com.metamx.emitter.core.HttpPostEmitter.limitBuffersToEmitSize(HttpPostEmitter.java:368) ~[java-util-1.3.4-SNAPSHOT.jar:?]\r\n        at com.metamx.emitter.core.HttpPostEmitter.addBatchToEmitQueue(HttpPostEmitter.java:357) ~[java-util-1.3.4-SNAPSHOT.jar:?]\r\n        at com.metamx.emitter.core.HttpPostEmitter.doOnSealExclusive(HttpPostEmitter.java:328) ~[java-util-1.3.4-SNAPSHOT.jar:?]\r\n        at com.metamx.emitter.core.HttpPostEmitter.onSealExclusive(HttpPostEmitter.java:304) [java-util-1.3.4-SNAPSHOT.jar:?]\r\n        at com.metamx.emitter.core.Batch.tryReleaseShared(Batch.java:278) [java-util-1.3.4-SNAPSHOT.jar:?]\r\n        at java.util.concurrent.locks.AbstractQueuedLongSynchronizer.releaseShared(AbstractQueuedLongSynchronizer.java:1119) [?:1.8.0_112]\r\n        at com.metamx.emitter.core.Batch.unlockAndSeal(Batch.java:244) [java-util-1.3.4-SNAPSHOT.jar:?]\r\n        at com.metamx.emitter.core.Batch.unlockAndSealIfNeeded(Batch.java:213) [java-util-1.3.4-SNAPSHOT.jar:?]\r\n        at com.metamx.emitter.core.Batch.tryAddNonFirstEvent(Batch.java:181) [java-util-1.3.4-SNAPSHOT.jar:?]\r\n        at com.metamx.emitter.core.Batch.tryAddEvent(Batch.java:132) [java-util-1.3.4-SNAPSHOT.jar:?]\r\n        at com.metamx.emitter.core.HttpPostEmitter.emitAndReturnBatch(HttpPostEmitter.java:261) [java-util-1.3.4-SNAPSHOT.jar:?]\r\n        at com.metamx.emitter.core.HttpPostEmitter.emit(HttpPostEmitter.java:226) [java-util-1.3.4-SNAPSHOT.jar:?]\r\n        at com.metamx.emitter.service.ServiceEmitter.emit(ServiceEmitter.java:72) [java-util-1.3.4-SNAPSHOT.jar:?]\r\n        at com.metamx.emitter.service.ServiceEmitter.emit(ServiceEmitter.java:77) [java-util-1.3.4-SNAPSHOT.jar:?]\r\n        at io.druid.query.DefaultQueryMetrics.emit(DefaultQueryMetrics.java:290) [druid-processing-0.11.0.jar:0.11.0]\r\n        at io.druid.query.MetricsEmittingQueryRunner$1.after(MetricsEmittingQueryRunner.java:120) [druid-processing-0.11.0.jar:0.11.0]\r\n        at io.druid.java.util.common.guava.WrappingSequence.accumulate(WrappingSequence.java:66) [java-util-0.11.0.jar:0.11.0]\r\n        at io.druid.query.spec.SpecificSegmentQueryRunner$2.accumulate(SpecificSegmentQueryRunner.java:86) [druid-processing-0.11.0.jar:0.11.0]\r\n        at io.druid.java.util.common.guava.WrappingSequence$1.get(WrappingSequence.java:50) [java-util-0.11.0.jar:0.11.0]\r\n        at io.druid.query.spec.SpecificSegmentQueryRunner.doNamed(SpecificSegmentQueryRunner.java:172) [druid-processing-0.11.0.jar:0.11.0]\r\n        at io.druid.query.spec.SpecificSegmentQueryRunner.access$200(SpecificSegmentQueryRunner.java:45) [druid-processing-0.11.0.jar:0.11.0]\r\n        at io.druid.query.spec.SpecificSegmentQueryRunner$3.wrap(SpecificSegmentQueryRunner.java:152) [druid-processing-0.11.0.jar:0.11.0]\r\n        at io.druid.java.util.common.guava.WrappingSequence.accumulate(WrappingSequence.java:45) [java-util-0.11.0.jar:0.11.0]\r\n        at io.druid.java.util.common.guava.WrappingSequence$1.get(WrappingSequence.java:50) [java-util-0.11.0.jar:0.11.0]\r\n        at io.druid.query.CPUTimeMetricQueryRunner$1.wrap(CPUTimeMetricQueryRunner.java:74) [druid-processing-0.11.0.jar:0.11.0]\r\n        at io.druid.java.util.common.guava.WrappingSequence.accumulate(WrappingSequence.java:45) [java-util-0.11.0.jar:0.11.0]\r\n```"}, {"user": "gianm", "commits": {"ff0de21fc523614e2ede451328ddc9f16e665b72": {"commitGHEventType": "referenced", "commitUser": "fjy"}, "f5d83a452a20ccc8b9320e7e0b45b12e6bb24ed2": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Area - SQL", "Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "SQL: Fix assumption that AND, OR operators are binary", "numCommits": 0, "created": "2018-03-05 21:55:32", "closed": "2018-03-06 02:56:36", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/5468", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 0.0002777777777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "sql/src/test/java/io/druid/sql/calcite/CalciteQueryTest.java", "deletions": 0, "insertions": 33, "lines": 33}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/BinaryOperatorConversion.java", "deletions": 7, "insertions": 12, "lines": 19}], "commitSpoonAstDiffStats": [{"spoonFilePath": "BinaryOperatorConversion.java", "spoonMethods": [{"INS": 1, "UPD": 5, "DEL": 2, "spoonMethodName": "io.druid.sql.calcite.expression.BinaryOperatorConversion.toDruidExpression(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexNode)", "MOV": 2, "TOT": 10}, {"INS": 1, "UPD": 3, "DEL": 1, "spoonMethodName": "io.druid.sql.calcite.expression.BinaryOperatorConversion", "MOV": 0, "TOT": 5}]}, {"spoonFilePath": "CalciteQueryTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.CalciteQueryTest.testGroupByCaseWhenOfTripleAnd()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-03-05 21:56:35", "commitMessage": "SQL: Fix assumption that AND, OR have two arguments. (#5470)\n\nCalcite can deliver an AND or OR operator with > 2 arguments.\r\nFixes #5468.", "commitUser": "fjy", "commitDateTime": "2018-03-05 18:56:35", "commitParents": ["c9b12e7813aa151e7efa16a8fa862c807ab6fedb"], "commitGHEventType": "referenced", "nameRev": "ff0de21fc523614e2ede451328ddc9f16e665b72 tags/druid-0.13.0-incubating-rc1~416", "commitHash": "ff0de21fc523614e2ede451328ddc9f16e665b72"}, {"commitGitStats": [{"filePath": "sql/src/test/java/io/druid/sql/calcite/CalciteQueryTest.java", "deletions": 0, "insertions": 33, "lines": 33}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/BinaryOperatorConversion.java", "deletions": 7, "insertions": 12, "lines": 19}], "commitSpoonAstDiffStats": [{"spoonFilePath": "BinaryOperatorConversion.java", "spoonMethods": [{"INS": 1, "UPD": 5, "DEL": 2, "spoonMethodName": "io.druid.sql.calcite.expression.BinaryOperatorConversion.toDruidExpression(io.druid.sql.calcite.planner.PlannerContext,io.druid.sql.calcite.table.RowSignature,org.apache.calcite.rex.RexNode)", "MOV": 2, "TOT": 10}, {"INS": 1, "UPD": 3, "DEL": 1, "spoonMethodName": "io.druid.sql.calcite.expression.BinaryOperatorConversion", "MOV": 0, "TOT": 5}]}, {"spoonFilePath": "CalciteQueryTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.CalciteQueryTest.testGroupByCaseWhenOfTripleAnd()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-08-26 18:44:46", "commitMessage": "SQL: Fix assumption that AND, OR have two arguments. (#5470) (#6239)\n\nCalcite can deliver an AND or OR operator with > 2 arguments.\r\nFixes #5468.", "commitUser": "fjy", "commitDateTime": "2018-08-26 19:44:46", "commitParents": ["bc07320ae73a0ba03512fb4b647bf98dfdce6aef"], "commitGHEventType": "referenced", "nameRev": "f5d83a452a20ccc8b9320e7e0b45b12e6bb24ed2 tags/druid-0.12.3-rc1~10", "commitHash": "f5d83a452a20ccc8b9320e7e0b45b12e6bb24ed2"}], "body": "We'll get ones that are nonbinary inside a query like:\r\n\r\n```\r\nSELECT\r\n  CASE WHEN m1 > 1 AND m1 < 5 AND cnt > 1 THEN 'x' ELSE NULL END,\r\n  COUNT(*)\r\nFROM druid.foo\r\nGROUP BY 1\r\n```\r\n\r\nOriginally reported in https://groups.google.com/d/msg/druid-user/tPuxE_6nrHI/IaMkeOKLAAAJ."}, {"user": "bendrecm", "commits": {}, "labels": ["Area - SQL", "Bug"], "created": "2018-02-06 08:02:55", "title": "Nested SQL query throws error on latest version 0.11.0", "url": "https://github.com/apache/druid/issues/5353", "closed": "2018-09-19 18:10:46", "ttf": 225.00027777777777, "commitsDetails": [], "body": "For dataset se-druid, following query was working for 0.10.1 version but throws error on latest version 0.11.0\r\n\r\n`SELECT gender, country, count(distinct unique_users) as unique_users FROM \\\"se-druid\\\" GROUP BY gender, country HAVING gender <> '' and country in (SELECT country FROM \\\"se-druid\\\" WHERE country <> '' and country is not null GROUP BY country ORDER BY count(distinct unique_users) desc LIMIT 5)`\r\n\r\nError:\r\n\r\n`{\"error\":\"Unknown exception\",\"errorMessage\":\"Cannot build plan for query: SELECT gender, country, count(distinct unique_users) as unique_users FROM \\\"se-druid{\"query\":\"SELECT gender, country, count(distinct unique_users) as unique_users FROM \\\"se-druid\\\" GROUP BY gender, country HAVING gender <> '' and country in (SELECT country FROM \\\"se-druid\\\" WHERE country <> '' and country is not null GROUP BY country ORDER BY count(distinct unique_users) desc LIMIT 5)\",\"errorClass\":\"io.druid.java.util.common.ISE\",\"host\":null}`\r\n\r\nHow to reproduce ->\r\n\r\n1. Run any nested query like above on calcite SQL adapter on 0.11.0 version. The nested query can be in where clause or having clause.\r\n"}, {"user": "pjain1", "commits": {}, "labels": ["Bug"], "created": "2018-02-02 23:03:20", "title": "Potential bug in HttpPostEmitter causing high CPU usage", "url": "https://github.com/apache/druid/issues/5338", "closed": "2018-02-14 01:00:59", "ttf": 11.000277777777777, "commitsDetails": [], "body": "After upgrading to 0.11.0, some of the deployments are facing high CPU usage issue.\r\n\r\nAfter taking a thread dump, it was a suspicion that emitter thread might be causing it.\r\n\r\n```\r\nThread 65688: (state = IN_JAVA)\r\n - com.metamx.emitter.core.HttpPostEmitter.emitAndReturnBatch(com.metamx.emitter.core.Event) @bci=111, line=249 (Compiled frame; information may be imprecise)\r\n - com.metamx.emitter.core.HttpPostEmitter.emit(com.metamx.emitter.core.Event) @bci=2, line=214 (Compiled frame)\r\n - com.metamx.emitter.core.ComposingEmitter.emit(com.metamx.emitter.core.Event) @bci=31, line=57 (Compiled frame)\r\n - com.metamx.emitter.service.ServiceEmitter.emit(com.metamx.emitter.core.Event) @bci=5, line=72 (Compiled frame)\r\n - com.metamx.emitter.service.ServiceEmitter.emit(com.metamx.emitter.service.ServiceEventBuilder) @bci=9, line=77 (Compiled frame)\r\n```\r\n\r\nTo verify the issue, we added an executor in DruidCoordinator class which just keeps on emitting events in while(true) loop like this - \r\n```\r\nwhile (true) {\r\n   emitter.emit(ServiceMetricEvent.builder().setDimension(\"dataSource\", \"try\").build(\"test\", 10));\r\n}\r\n```\r\nWe found that after some time, [batch.tryAddEvents](https://github.com/metamx/java-util/blob/java-util-1.3.2/src/main/java/com/metamx/emitter/core/HttpPostEmitter.java#L244) method always return false and the reference in `concurrentBatch` never changes and the `while(true)` loop just keeps on spinning without sending anything or creating new batch. \r\n\r\nStill not sure why it is happening as its not happening for all deployments, might be some concurrency issue.\r\n\r\n@leventov "}, {"user": "TwojaWina", "commits": {"0f03ab0c743c46e802d2d7f05bf3d251256c6d23": {"commitGHEventType": "referenced", "commitUser": "fjy"}, "3a63540e9e6c1201e8a355bc22da733a8542753d": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Area - SQL", "Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "SQL filtering with \"__Time =\" does not work in Pivot", "numCommits": 0, "created": "2018-02-02 22:26:59", "closed": "2018-03-06 02:56:53", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/5337", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 31.00027777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "sql/src/main/java/io/druid/sql/calcite/rule/CaseFilteredAggregatorRule.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/table/RowSignature.java", "deletions": 17, "insertions": 6, "lines": 23}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/OperatorConversions.java", "deletions": 6, "insertions": 5, "lines": 11}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/planner/DruidTypeSystem.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "sql/src/test/java/io/druid/sql/calcite/CalciteQueryTest.java", "deletions": 0, "insertions": 27, "lines": 27}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/planner/DruidPlanner.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/planner/Calcites.java", "deletions": 0, "insertions": 43, "lines": 43}], "commitSpoonAstDiffStats": [{"spoonFilePath": "CaseFilteredAggregatorRule.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rule.CaseFilteredAggregatorRule.onMatch(org.apache.calcite.plan.RelOptRuleCall)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "DruidTypeSystem.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.planner.DruidTypeSystem.deriveSumType(org.apache.calcite.rel.type.RelDataTypeFactory,org.apache.calcite.rel.type.RelDataType)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "Calcites.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.planner.Calcites.createSqlTypeWithNullability(org.apache.calcite.rel.type.RelDataTypeFactory,org.apache.calcite.sql.type.SqlTypeName,boolean)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.planner.Calcites.createSqlType(org.apache.calcite.rel.type.RelDataTypeFactory,org.apache.calcite.sql.type.SqlTypeName)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "DruidPlanner.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.planner.DruidPlanner.planExplanation(org.apache.calcite.rel.RelNode,org.apache.calcite.sql.SqlExplain)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "RowSignature.java", "spoonMethods": [{"INS": 4, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.table.RowSignature.getRelDataType(org.apache.calcite.rel.type.RelDataTypeFactory)", "MOV": 2, "TOT": 8}]}, {"spoonFilePath": "OperatorConversions.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.expression.OperatorConversions.OperatorBuilder.returnType(org.apache.calcite.sql.type.SqlTypeName)", "MOV": 1, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.expression.OperatorConversions.OperatorBuilder.nullableReturnType(org.apache.calcite.sql.type.SqlTypeName)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "CalciteQueryTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.CalciteQueryTest.testCountStarWithTimeMillisecondFilters()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-03-05 21:56:52", "commitMessage": "SQL: Fix precision of TIMESTAMP types. (#5464)\n\nDruid stores timestamps down to the millisecond, so we should use\r\nprecision = 3. Setting this wrong sometimes caused milliseconds\r\nto be ignored in timestamp literals.\r\n\r\nFixes #5337.", "commitUser": "fjy", "commitDateTime": "2018-03-05 18:56:52", "commitParents": ["ff0de21fc523614e2ede451328ddc9f16e665b72"], "commitGHEventType": "referenced", "nameRev": "0f03ab0c743c46e802d2d7f05bf3d251256c6d23 tags/druid-0.13.0-incubating-rc1~415", "commitHash": "0f03ab0c743c46e802d2d7f05bf3d251256c6d23"}, {"commitGitStats": [{"filePath": "sql/src/main/java/io/druid/sql/calcite/rule/CaseFilteredAggregatorRule.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/table/RowSignature.java", "deletions": 17, "insertions": 6, "lines": 23}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/OperatorConversions.java", "deletions": 6, "insertions": 5, "lines": 11}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/planner/DruidTypeSystem.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "sql/src/test/java/io/druid/sql/calcite/CalciteQueryTest.java", "deletions": 0, "insertions": 27, "lines": 27}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/planner/DruidPlanner.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/planner/Calcites.java", "deletions": 0, "insertions": 43, "lines": 43}], "commitSpoonAstDiffStats": [{"spoonFilePath": "CaseFilteredAggregatorRule.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.rule.CaseFilteredAggregatorRule.onMatch(org.apache.calcite.plan.RelOptRuleCall)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "DruidTypeSystem.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.planner.DruidTypeSystem.deriveSumType(org.apache.calcite.rel.type.RelDataTypeFactory,org.apache.calcite.rel.type.RelDataType)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "Calcites.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.planner.Calcites.createSqlTypeWithNullability(org.apache.calcite.rel.type.RelDataTypeFactory,org.apache.calcite.sql.type.SqlTypeName,boolean)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.planner.Calcites.createSqlType(org.apache.calcite.rel.type.RelDataTypeFactory,org.apache.calcite.sql.type.SqlTypeName)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "DruidPlanner.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.planner.DruidPlanner.planExplanation(org.apache.calcite.rel.RelNode,org.apache.calcite.sql.SqlExplain)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "RowSignature.java", "spoonMethods": [{"INS": 4, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.table.RowSignature.getRelDataType(org.apache.calcite.rel.type.RelDataTypeFactory)", "MOV": 2, "TOT": 8}]}, {"spoonFilePath": "OperatorConversions.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.expression.OperatorConversions.OperatorBuilder.returnType(org.apache.calcite.sql.type.SqlTypeName)", "MOV": 1, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.expression.OperatorConversions.OperatorBuilder.nullableReturnType(org.apache.calcite.sql.type.SqlTypeName)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "CalciteQueryTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.CalciteQueryTest.testCountStarWithTimeMillisecondFilters()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-08-26 22:00:25", "commitMessage": "SQL: Fix precision of TIMESTAMP types. (#5464) (#6240)\n\nDruid stores timestamps down to the millisecond, so we should use\r\nprecision = 3. Setting this wrong sometimes caused milliseconds\r\nto be ignored in timestamp literals.\r\n\r\nFixes #5337.", "commitUser": "fjy", "commitDateTime": "2018-08-26 23:00:25", "commitParents": ["130a6011b36e3ba1811369350e7929607e33d5ad"], "commitGHEventType": "referenced", "nameRev": "3a63540e9e6c1201e8a355bc22da733a8542753d tags/druid-0.12.3-rc1~6", "commitHash": "3a63540e9e6c1201e8a355bc22da733a8542753d"}], "body": "SQL filtering with \"__Time =\" returns error, while \"__Time <\" or \"__Time >\" work correctly.  Please find attached screen shots. \r\n\r\n**Case where __Time filter works :** \r\n<img width=\"1260\" alt=\"screen shot 2018-02-02 at 1 59 36 pm\" src=\"https://user-images.githubusercontent.com/9752231/35757671-1b5c611a-0825-11e8-95f4-b344de631085.png\">\r\n\r\n**Case where __Time filter does not work**\r\n<img width=\"1180\" alt=\"screen shot 2018-02-02 at 2 19 28 pm\" src=\"https://user-images.githubusercontent.com/9752231/35757721-4c41e62e-0825-11e8-9527-9d0f2f822201.png\">\r\n<img width=\"1269\" alt=\"screen shot 2018-02-02 at 2 19 58 pm\" src=\"https://user-images.githubusercontent.com/9752231/35757722-4c58864a-0825-11e8-8f10-6d642d4aebf6.png\">\r\n"}, {"user": "gianm", "commits": {}, "labels": ["Bug"], "created": "2018-02-02 10:34:49", "title": "HttpServerInventoryView: startup delayed unexpectedly", "url": "https://github.com/apache/druid/issues/5331", "closed": "2018-03-13 05:13:52", "ttf": 38.000277777777775, "commitsDetails": [], "body": "I just tried the http based segment view stuff today, and noticed that my broker took a long time to start up. It seems to be related to timing out waiting for servers to initialize. However the initialization seems fine, in that when I query `/druid-internal/v1/httpServerInventoryView` after the broker is done starting, the server \"ip-172-31-0-22.ec2.internal:8303\" does appear in the list and looks good:\r\n\r\n```\r\n  \"ip-172-31-0-22.ec2.internal:8303\": {\r\n    \"notSyncedForSecs\": 5,\r\n    \"notSuccessfullySyncedFor\": 5,\r\n    \"consecutiveFailedAttemptCount\": 0,\r\n    \"started\": true\r\n  }\r\n```\r\n\r\nThere are two different issues here, I think (we could split them but I decided it was simpler to put them in one github issue for now\u2026)\r\n\r\n1. There were some problems contacting this server at first (connection refused), but you can see from the logs that these errors stopped getting logged many minutes before the initialization timeout. There wasn't any log message saying when this server first synced, but I guess it did sync since it shows up ok in the debug info. I wonder if something is wrong with the latch that is used to check for initialization?\r\n\r\n2. There is a loop in `serverInventoryInitialized` where `server.awaitInitialization()` is called for each server in a row. That means it will wait the full timeout (default 3 minutes) for each timing-out server. In my case, two different servers timed out so the broker startup took over 6 minutes. I think it'd be better for there to be an overall initialization, since with the current mechanism, in a big cluster the effective timeout could be huge.\r\n\r\n```\r\n2018-02-02T10:09:46,116 INFO [CuratorDruidNodeDiscoveryProvider-ListenerExecutor] io.druid.client.HttpServerInventoryView - Server[ip-172-31-0-22.ec2.internal:8303] appeared.\r\n2018-02-02T10:09:46,116 INFO [CuratorDruidNodeDiscoveryProvider-ListenerExecutor] io.druid.server.coordination.ChangeRequestHttpSyncer - Starting ChangeRequestHttpSyncer[https://ip-172-31-0-22.ec2.internal:8303/_1517566186116].\r\n2018-02-02T10:09:46,118 INFO [HttpServerInventoryView-3] io.druid.server.coordination.ChangeRequestHttpSyncer - Temporary Failure. failed to get sync response from [https://ip-172-31-0-22.ec2.internal:8303/_1517566186116]. Return code [0], Reason: [null]\r\n2018-02-02T10:09:46,118 INFO [HttpServerInventoryView-3] io.druid.server.coordination.ChangeRequestHttpSyncer - Scheduling next syncup in [1349] millis for server[https://ip-172-31-0-22.ec2.internal:8303/_1517566186116].\r\n2018-02-02T10:09:47,468 INFO [HttpServerInventoryView-2] io.druid.server.coordination.ChangeRequestHttpSyncer - Temporary Failure. failed to get sync response from [https://ip-172-31-0-22.ec2.internal:8303/_1517566186116]. Return code [0], Reason: [null]\r\n2018-02-02T10:09:47,468 INFO [HttpServerInventoryView-2] io.druid.server.coordination.ChangeRequestHttpSyncer - Scheduling next syncup in [2198] millis for server[https://ip-172-31-0-22.ec2.internal:8303/_1517566186116].\r\n2018-02-02T10:09:49,668 INFO [HttpServerInventoryView-0] io.druid.server.coordination.ChangeRequestHttpSyncer - Temporary Failure. failed to get sync response from [https://ip-172-31-0-22.ec2.internal:8303/_1517566186116]. Return code [0], Reason: [null]\r\n2018-02-02T10:09:49,668 INFO [HttpServerInventoryView-0] io.druid.server.coordination.ChangeRequestHttpSyncer - Scheduling next syncup in [3055] millis for server[https://ip-172-31-0-22.ec2.internal:8303/_1517566186116].\r\n2018-02-02T10:09:52,725 INFO [HttpServerInventoryView-2] io.druid.server.coordination.ChangeRequestHttpSyncer - Temporary Failure. failed to get sync response from [https://ip-172-31-0-22.ec2.internal:8303/_1517566186116]. Return code [0], Reason: [null]\r\n2018-02-02T10:09:52,725 INFO [HttpServerInventoryView-2] io.druid.server.coordination.ChangeRequestHttpSyncer - Scheduling next syncup in [8645] millis for server[https://ip-172-31-0-22.ec2.internal:8303/_1517566186116].\r\n2018-02-02T10:16:56,250 WARN [HttpServerInventoryView-1] io.druid.client.HttpServerInventoryView - Await initialization timed out for server [ip-172-31-0-22.ec2.internal:8303].\r\n```"}, {"user": "gianm", "commits": {}, "labels": ["Bug"], "created": "2018-02-02 08:20:15", "title": "HTTP segment load/drop peons getting unauthorized", "url": "https://github.com/apache/druid/issues/5325", "closed": "2018-02-02 16:44:33", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "Noticed this when testing #4874 on a cluster with authentication enabled. Perhaps it needs to use an escalated http client?\r\n\r\n```\r\n2018-02-02T08:15:28,336 ERROR [Master-PeonExec--0] io.druid.server.coordinator.HttpLoadQueuePeon - Request[https://ip-172-31-3-17.ec2.internal:8283/druid-internal/v1/segments/changeRequests?timeout=300000] Failed with status[401]. Reason[Unauthorized].\r\nio.druid.java.util.common.RE: Unexpected Response Status.\r\n        at io.druid.server.coordinator.HttpLoadQueuePeon$3.onSuccess(HttpLoadQueuePeon.java:248) [druid-server-0.12.0-SNAPSHOT.jar:0.12.0-SNAPSHOT]\r\n        at io.druid.server.coordinator.HttpLoadQueuePeon$3.onSuccess(HttpLoadQueuePeon.java:205) [druid-server-0.12.0-SNAPSHOT.jar:0.12.0-SNAPSHOT]\r\n        at com.google.common.util.concurrent.Futures$4.run(Futures.java:1181) [guava-16.0.1.jar:?]\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_152]\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_152]\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_152]\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_152]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_152]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_152]\r\n        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_152]\r\n```\r\n\r\n/cc @himanshug"}, {"user": "quenlang", "commits": {}, "labels": ["Bug"], "created": "2017-12-27 08:54:01", "title": "druid 0.11.0 kafka-emitter extension error ", "url": "https://github.com/apache/druid/issues/5198", "closed": "2018-01-03 22:39:26", "ttf": 7.000277777777778, "commitsDetails": [], "body": "## I already uploaded kafka-emitter jars to the extensions directory and loaded the extension.\r\n```\r\n[root@gpsegment1 druid-0.11.0]# ls extensions/kafka-emitter/\r\nkafka-clients-0.10.2.0.jar  kafka-emitter-0.11.0.jar  lz4-1.3.0.jar  slf4j-api-1.6.4.jar  snappy-java-1.1.2.6.jar\r\n\r\n[root@gpsegment1 druid-0.11.0]# grep \"kafka-emitter\" /opt/druid-0.11.0/conf/druid/_common/common.runtime.properties \r\ndruid.extensions.loadList=[\"druid-hdfs-storage\", \"kafka-emitter\", \"druid-caffeine-cache\", \"druid-kafka-indexing-service\", \"mysql-metadata-storage\", \"druid-datasketches\", \"druid-histogram\"]\r\n\r\n[root@gpsegment1 druid-0.11.0]# grep \"emitter.kafka\" /opt/druid-0.11.0/conf/druid/_common/common.runtime.properties \r\ndruid.emitter=kafka\r\ndruid.emitter.logging.logLevel=info\r\ndruid.emitter.kafka.bootstrap.servers=192.168.1.115:9092\r\ndruid.emitter.kafka.metric.topic=druid-metrics-monitor\r\ndruid.emitter.kafka.alert.topic=druid-metrics-alert\r\ndruid.emitter.kafka.producer.config={\"max.block.ms\":10000}\r\n```\r\n\r\n## When i start the service ,error occur.but under version 0.10.0, kafka-emitter works normally.my kafka version is 0.10.2. Is this a bug?\r\n2017-12-27T15:55:43,216 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.curator.ExhibitorConfig] from props[druid.exhibitor.service.] as [io.druid.curator.ExhibitorConfig@20a7953c]\r\n2017-12-27T15:55:43,337 INFO [main] org.apache.curator.utils.Compatibility - Running in ZooKeeper 3.4.x compatibility mode\r\n2017-12-27T15:55:43,340 WARN [main] org.apache.curator.retry.ExponentialBackoffRetry - maxRetries too large (30). Pinning to 29\r\n2017-12-27T15:55:43,416 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.server.initialization.ZkPathsConfig] from props[druid.zk.paths.] as [io.druid.server.initialization.ZkPathsConfig@22e2266d]\r\n2017-12-27T15:55:43,460 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[interface io.druid.server.security.Escalator] from props[druid.escalator.] as [io.druid.server.security.NoopEscalator@78422efb]\r\n2017-12-27T15:55:43,480 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.server.security.AuthConfig] from props[druid.auth.] as [AuthConfig{authenticatorChain='null', authorizers='null'}]\r\nException in thread \"main\" com.google.inject.CreationException: Unable to create injector, see the following errors:\r\n\r\n1) Problem parsing object at prefix[druid.emitter.kafka]: Instantiation of [simple type, class io.druid.emitter.kafka.KafkaEmitterConfig] value failed: bootstrap.servers can not be null.\r\n  at io.druid.guice.JsonConfigProvider.bind(JsonConfigProvider.java:133) (via modules: com.google.inject.util.Modules$OverrideModule -> io.druid.emitter.kafka.KafkaEmitterModule)\r\n  at io.druid.guice.JsonConfigProvider.bind(JsonConfigProvider.java:133) (via modules: com.google.inject.util.Modules$OverrideModule -> io.druid.emitter.kafka.KafkaEmitterModule)\r\n  while locating com.google.common.base.Supplier<io.druid.emitter.kafka.KafkaEmitterConfig>\r\n  at io.druid.guice.JsonConfigProvider.bind(JsonConfigProvider.java:134) (via modules: com.google.inject.util.Modules$OverrideModule -> io.druid.emitter.kafka.KafkaEmitterModule)\r\n  while locating io.druid.emitter.kafka.KafkaEmitterConfig\r\n    for the 1st parameter of io.druid.emitter.kafka.KafkaEmitterModule.getEmitter(KafkaEmitterModule.java:56)\r\n  at io.druid.emitter.kafka.KafkaEmitterModule.getEmitter(KafkaEmitterModule.java:56) (via modules: com.google.inject.util.Modules$OverrideModule -> io.druid.emitter.kafka.KafkaEmitterModule)\r\n  while locating com.metamx.emitter.core.Emitter annotated with @com.google.inject.name.Named(value=kafka)\r\n  at io.druid.server.emitter.EmitterModule$EmitterProvider.inject(EmitterModule.java:118)\r\n  at io.druid.server.emitter.EmitterModule.configure(EmitterModule.java:78) (via modules: com.google.inject.util.Modules$OverrideModule -> com.google.inject.util.Modules$OverrideModule -> io.druid.server.emitter.EmitterModule)\r\n\r\n1 error\r\n        at com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:470)\r\n        at com.google.inject.internal.InternalInjectorCreator.injectDynamically(InternalInjectorCreator.java:176)\r\n        at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:110)\r\n        at com.google.inject.Guice.createInjector(Guice.java:99)\r\n        at com.google.inject.Guice.createInjector(Guice.java:73)\r\n        at com.google.inject.Guice.createInjector(Guice.java:62)\r\n        at io.druid.initialization.Initialization.makeInjectorWithModules(Initialization.java:401)\r\n        at io.druid.cli.GuiceRunnable.makeInjector(GuiceRunnable.java:62)\r\n        at io.druid.cli.ServerRunnable.run(ServerRunnable.java:49)\r\n        at io.druid.cli.Main.main(Main.java:108)\r\nCaused by: java.lang.IllegalArgumentException: Instantiation of [simple type, class io.druid.emitter.kafka.KafkaEmitterConfig] value failed: bootstrap.servers can not be null\r\n        at com.fasterxml.jackson.databind.ObjectMapper._convert(ObjectMapper.java:2774)\r\n        at com.fasterxml.jackson.databind.ObjectMapper.convertValue(ObjectMapper.java:2700)\r\n        at io.druid.guice.JsonConfigurator.configurate(JsonConfigurator.java:103)\r\n        at io.druid.guice.JsonConfigProvider.get(JsonConfigProvider.java:200)\r\n        at io.druid.guice.JsonConfigProvider.get(JsonConfigProvider.java:80)\r\n        at com.google.inject.internal.ProviderInternalFactory.provision(ProviderInternalFactory.java:81)\r\n        at com.google.inject.internal.InternalFactoryToInitializableAdapter.provision(InternalFactoryToInitializableAdapter.java:53)\r\n        at com.google.inject.internal.ProviderInternalFactory.circularGet(ProviderInternalFactory.java:61)\r\n        at com.google.inject.internal.InternalFactoryToInitializableAdapter.get(InternalFactoryToInitializableAdapter.java:45)\r\n        at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46)\r\n        at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1092)\r\n        at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)\r\n        at com.google.inject.internal.SingletonScope$1.get(SingletonScope.java:194)\r\n        at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:41)\r\n        at com.google.inject.internal.InjectorImpl$2$1.call(InjectorImpl.java:1019)\r\n        at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1092)\r\n        at com.google.inject.internal.InjectorImpl$2.get(InjectorImpl.java:1015)\r\n        at io.druid.guice.SupplierProvider.get(SupplierProvider.java:52)\r\n        at com.google.inject.internal.ProviderInternalFactory.provision(ProviderInternalFactory.java:81)\r\n        at com.google.inject.internal.InternalFactoryToInitializableAdapter.provision(InternalFactoryToInitializableAdapter.java:53)\r\n        at com.google.inject.internal.ProviderInternalFactory.circularGet(ProviderInternalFactory.java:61)\r\n        at com.google.inject.internal.InternalFactoryToInitializableAdapter.get(InternalFactoryToInitializableAdapter.java:45)\r\n        at com.google.inject.internal.SingleParameterInjector.inject(SingleParameterInjector.java:38)\r\n        at com.google.inject.internal.SingleParameterInjector.getAll(SingleParameterInjector.java:62)\r\n        at com.google.inject.internal.ProviderMethod$Factory.provision(ProviderMethod.java:402)\r\n        at com.google.inject.internal.ProviderMethod$Factory.get(ProviderMethod.java:376)\r\n        at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46)\r\n        at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1092)\r\n        at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)\r\n        at io.druid.guice.LifecycleScope$1.get(LifecycleScope.java:68)\r\n        at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:41)\r\n        at com.google.inject.internal.InjectorImpl$2$1.call(InjectorImpl.java:1019)\r\n        at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1092)\r\n        at com.google.inject.internal.InjectorImpl$2.get(InjectorImpl.java:1015)\r\n        at io.druid.server.emitter.EmitterModule$EmitterProvider.findEmitter(EmitterModule.java:142)\r\n        at io.druid.server.emitter.EmitterModule$EmitterProvider.inject(EmitterModule.java:120)\r\n        at io.druid.server.emitter.EmitterModule$EmitterProvider$$FastClassByGuice$$622b285e.invoke(<generated>)\r\n        at com.google.inject.internal.SingleMethodInjector$1.invoke(SingleMethodInjector.java:54)\r\n        at com.google.inject.internal.SingleMethodInjector.inject(SingleMethodInjector.java:89)\r\n        at com.google.inject.internal.MembersInjectorImpl.injectMembers(MembersInjectorImpl.java:132)\r\n        at com.google.inject.internal.MembersInjectorImpl$1.call(MembersInjectorImpl.java:93)\r\n        at com.google.inject.internal.MembersInjectorImpl$1.call(MembersInjectorImpl.java:80)\r\n        at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1085)\r\n        at com.google.inject.internal.MembersInjectorImpl.injectAndNotify(MembersInjectorImpl.java:80)\r\n        at com.google.inject.internal.Initializer$InjectableReference.get(Initializer.java:223)\r\n        at com.google.inject.internal.Initializer.injectAll(Initializer.java:132)\r\n        at com.google.inject.internal.InternalInjectorCreator.injectDynamically(InternalInjectorCreator.java:174)\r\n        ... 8 more\r\n```"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2017-12-21 16:16:19", "title": "Druid does not work with java 9", "url": "https://github.com/apache/druid/issues/5190", "closed": "2018-04-10 14:41:55", "ttf": 109.00027777777778, "commitsDetails": [], "body": "```\r\nJAVA_HOME=\"$(/usr/libexec/java_home -v 1.9)\" mvn clean package\r\n```\r\n\r\nResults in the following:\r\n\r\n```\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.2:compile (default-compile) on project java-util: Compilation failure: Compilation failure: \r\n[ERROR] /Users/charles.allen/src/druid/java-util/src/main/java/io/druid/java/util/common/ByteBufferUtils.java:[22,16] cannot find symbol\r\n[ERROR]   symbol:   class Cleaner\r\n[ERROR]   location: package sun.misc\r\n[ERROR] /Users/charles.allen/src/druid/java-util/src/main/java/io/druid/java/util/common/ByteBufferUtils.java:[57,11] cannot find symbol\r\n[ERROR]   symbol:   class Cleaner\r\n[ERROR]   location: class io.druid.java.util.common.ByteBufferUtils\r\n```"}, {"user": "dclim", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2017-12-14 00:10:35", "title": "Committed datasource metadata can be inconsistent with generated segments", "url": "https://github.com/apache/druid/issues/5161", "closed": "2018-01-04 00:13:21", "ttf": 21.00027777777778, "commitsDetails": [], "body": "In the following scenario, the committed datasource metadata will not match the segments:\r\n\r\n1) task starts at offset X\r\n2) supervisor tells it to stop at Y\r\n3) task uploads some of its segments but fails halfway through\r\n4) new task starts at X again\r\n5) supervisor tells new task to end at Z\r\n\r\nSince this task will have the same sequence name as the previous failed one, it will reuse the same segment IDs. The content of these segments will not be the same as the ones previously uploaded before the failure (read to offset Z vs offset Y).\r\n\r\nThe issue arises when the task goes to push the segment to deep storage and commit metadata. Because some of the segments were already written to deep storage, these ones don't get written again and the old segments (that stopped at Y) remain. This operation succeeds so the datasource metadata is written with the ending offset as Z which doesn't match what is contained in the segments.\r\n\r\nA possible solution is to allow the deep storage pushers to be configured to overwrite existing segments. This needs to be a configurable option because in the case of Tranquility replicas, generated segments are not guaranteed to be identical and you want to avoid a situation where historicals have loaded different versions of the same segment.\r\n\r\nIn the case of exactly-once ingestion using the datasource table for tracking stream position, segments generated by replicas will be the same so overriding will not be problematic and will prevent this issue from happening."}, {"user": "gianm", "commits": {}, "labels": ["Bug"], "created": "2017-12-01 16:52:59", "title": "Some topN specializations clone offsets, which can fail", "url": "https://github.com/apache/druid/issues/5132", "closed": "2017-12-02 01:04:23", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "HistoricalSingleValueDimSelector1SimpleDoubleAggPooledTopNScannerPrototype and Historical1SimpleDoubleAggPooledTopNScannerPrototype both call `TopNUtils.copyOffset(cursor)`. It looks like the intent is to try to get the jvm to allocate the object on the stack. (see https://github.com/druid-io/druid/pull/4710#discussion_r154273264 and [comment in the code](https://github.com/druid-io/druid/blob/master/processing/src/main/java/io/druid/query/topn/TopNUtils.java))\r\n\r\nHowever, FilteredOffset, which is used when there is a \"postFilter\" (scan-based rather than index-based) cannot be cloned. This means queries will throw exceptions if they are topNs, filtering on non-string columns, with 1 simple double aggregator.\r\n\r\nBased on the uncertainty expressed in https://github.com/druid-io/druid/pull/4710#discussion_r154273264 that the cloning is actually useful, I suggest fixing this by removing the cloning step.\r\n\r\n@leventov what do you think?"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2017-11-29 22:58:28", "title": "0.11.0 branch is failing builds", "url": "https://github.com/apache/druid/issues/5126", "closed": "2017-11-29 22:58:54", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "`indexing-service/src/test/java/io/druid/indexing/common/task/RealtimeIndexTaskTest.java:35: Line matches the illegal pattern 'Use io.druid.java.util.common.logger.Logger instead'. [Regexp]` is what I'm getting when building the 0.11.0 branch suddenly"}, {"user": "drcrallen", "commits": {}, "labels": ["Area - Querying", "Area - Zookeeper/Curator", "Bug"], "created": "2017-11-28 18:13:40", "title": "Http broker filters probably do not work as expected.", "url": "https://github.com/apache/druid/issues/5121", "closed": "2017-12-21 02:56:01", "ttf": 22.00027777777778, "commitsDetails": [], "body": "https://github.com/druid-io/druid/blob/druid-0.11.0-rc2/server/src/main/java/io/druid/client/FilteredHttpServerInventoryViewProvider.java#L63 sets an alwaysTrue whereas https://github.com/druid-io/druid/blob/druid-0.11.0-rc2/server/src/main/java/io/druid/client/FilteredBatchServerInventoryViewProvider.java#L55 sets an alwaysFalse.\r\n\r\nThis comes into play in the OR statement creation at https://github.com/druid-io/druid/blob/druid-0.11.0-rc2/server/src/main/java/io/druid/client/HttpServerInventoryView.java#L269-L272 and https://github.com/druid-io/druid/blob/druid-0.11.0-rc2/server/src/main/java/io/druid/client/BatchServerInventoryView.java#L97-L100 respectively"}, {"user": "pjain1", "commits": {}, "labels": ["Bug"], "created": "2017-11-06 22:31:10", "title": "Potential deadlock condition in Appenderator", "url": "https://github.com/apache/druid/issues/5046", "closed": "2017-11-17 23:08:35", "ttf": 11.000277777777777, "commitsDetails": [], "body": "-  `persistExecutor` submits mergeAndPush job to `pushExecutor` [here](https://github.com/druid-io/druid/blob/master/server/src/main/java/io/druid/segment/realtime/appenderator/AppenderatorImpl.java#L482) after completing the persist. \r\n- `pushExecutor` submits abandon segment job to `persistExecutor` [here](https://github.com/druid-io/druid/blob/master/server/src/main/java/io/druid/segment/realtime/appenderator/AppenderatorImpl.java#L908) \r\n\r\n`pushExecutor` uses a blocking queue with 1 capacity and if `persistExecutor` either uses Synchronous Queue (which it does in case of KafkaIndexTask) or queue with capacity 1 then deadlock can happen. This would be more common in the new KafkaIndexingService which will support incremental handoff as handoffs (which leads to abandoning of segments) for a sequence can interleave with pushing of segments for another sequence.\r\n\r\nThis can probably be fixed by not using [pushBarrier](https://github.com/druid-io/druid/blob/master/server/src/main/java/io/druid/segment/realtime/appenderator/AppenderatorImpl.java#L909) instead using some form of Atomic counter to keep track of existing pushes before abandoning segments. Any thoughts ?"}, {"user": "himanshug", "commits": {}, "labels": ["Bug"], "created": "2017-11-02 21:12:23", "title": "Coordinator to Overlord proxied requests always fail with \"403 Forbidden\"", "url": "https://github.com/apache/druid/issues/5038", "closed": "2017-11-03 20:54:42", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "Coordinator accepts `/druid/indexer/v1/*` requests and forwards them to overlord leader via `OverlordProxyServlet`. However, at coordinator, `PreResponseAuthorizationCheckFilter` fails the request because it does not have authorization attributes set."}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2017-10-27 18:23:40", "title": "HTTP Coordinator lookup thunders coordinator with `enableLookupSyncOnStartup`", "url": "https://github.com/apache/druid/issues/5018", "closed": "2017-11-09 05:30:37", "ttf": 12.000277777777777, "commitsDetails": [], "body": "`io.druid.query.lookup.LookupReferencesManager#loadAllLookupsAndInitStateRef` thunders the living hell out of the coordinator on a rolling restart of a bunch of nodes and there is no retry in `io.druid.query.lookup.LookupReferencesManager#fetchLookupsForTier`\r\n\r\nThis is when `enableLookupSyncOnStartup` is enabled\r\n"}, {"user": "elloooooo", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2017-10-26 06:22:00", "title": "NPE when KafkaSupervisor getLagPerPartition()", "url": "https://github.com/apache/druid/issues/5009", "closed": "2017-12-27 16:50:16", "ttf": 62.000277777777775, "commitsDetails": [], "body": "> 2017-10-25T14:46:56,201 ERROR [KafkaSupervisor-m2_sfc_beatles_order_push_public_fmt] io.druid.indexing.kafka.supervisor.KafkaSupervisor - KafkaSupervisor[m2_sfc_beatles_order_push_public_fmt] failed to handle notice: {class=io.druid.indexing.kafka.supervisor.KafkaSuperv\r\nisor, exceptionType=class java.lang.NullPointerException, exceptionMessage=null, noticeClass=RunNotice}\r\njava.lang.NullPointerException\r\n        at java.util.HashMap.merge(HashMap.java:1216) ~[?:1.8.0_77]\r\n        at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320) ~[?:1.8.0_77]\r\n        at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ~[?:1.8.0_77]\r\n        at java.util.HashMap$EntrySpliterator.forEachRemaining(HashMap.java:1683) ~[?:1.8.0_77]\r\n        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ~[?:1.8.0_77]\r\n        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[?:1.8.0_77]\r\n        at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ~[?:1.8.0_77]\r\n        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_77]\r\n        at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ~[?:1.8.0_77]\r\n        at io.druid.indexing.kafka.supervisor.KafkaSupervisor.getLagPerPartition(KafkaSupervisor.java:1746) ~[druid-kafka-indexing-service-release-0.10.1-101.jar:release-0.10.1-101]\r\n        at io.druid.indexing.kafka.supervisor.KafkaSupervisor.generateReport(KafkaSupervisor.java:1625) ~[druid-kafka-indexing-service-release-0.10.1-101.jar:release-0.10.1-101]\r\n        at io.druid.indexing.kafka.supervisor.KafkaSupervisor.runInternal(KafkaSupervisor.java:691) ~[druid-kafka-indexing-service-release-0.10.1-101.jar:release-0.10.1-101]\r\n        at io.druid.indexing.kafka.supervisor.KafkaSupervisor$RunNotice.handle(KafkaSupervisor.java:518) ~[druid-kafka-indexing-service-release-0.10.1-101.jar:release-0.10.1-101]\r\n        at io.druid.indexing.kafka.supervisor.KafkaSupervisor$2.run(KafkaSupervisor.java:336) [druid-kafka-indexing-service-release-0.10.1-101.jar:release-0.10.1-101]\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_77]\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_77]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_77]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_77]\r\n        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]\r\n\r\n```\r\nCollectors.toMap(\r\n                Map.Entry::getKey,\r\n                e -> latestOffsetsFromKafka != null\r\n                     && latestOffsetsFromKafka.get(e.getKey()) != null\r\n                     && e.getValue() != null\r\n                     ? latestOffsetsFromKafka.get(e.getKey()) - e.getValue()\r\n                     : null\r\n            )\r\n``` \r\nCollectors.toMap will throw NPE when the value is null because the HashMap.merge don't support null value. How about exchange the null with -1?"}, {"user": "leventov", "commits": {}, "labels": ["Bug"], "created": "2017-10-20 21:50:05", "title": "Issue with 0.11 router - it doesn't see brokers", "url": "https://github.com/apache/druid/issues/4992", "closed": "2017-11-01 19:00:56", "ttf": 11.000277777777777, "commitsDetails": [], "body": "After updating Router from 0.10.1 to 0.11 it stops to see brokers.\r\n\r\nMight be related to recent changes in `TieredBrokerHostSelector`, e. g. https://github.com/druid-io/druid/commit/74538c32887aea0d7180857ef7edfad46e17d105#diff-09401bb9b0565d15079a30fd8683695e\r\n\r\ncc @clintropolis \r\n\r\nFYI @himanshug "}, {"user": "gianm", "commits": {}, "labels": ["Area - Segment Balancing/Coordination", "Bug"], "created": "2017-10-20 02:00:07", "title": "Callback race on start in cachingCost balancer", "url": "https://github.com/apache/druid/issues/4984", "closed": "2017-10-20 23:53:52", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "CachingCostBalancerStrategyFactory registers callbacks in its `start()` method, which is problematic since any callbacks that happen between start of ServerInventoryView and BalancerStrategyFactory will be missed. Inventory view callbacks should generally be registered in constructors, not in start methods.\r\n\r\nI discovered this when trying to test the cachingCost balancer on a cluster that has a small number of segments, and found that it missed the initialization callback (happened too fast) so it refused to create itself.\r\n\r\ncc: @dgolitsyn @leventov"}, {"user": "hellobabygogo", "commits": {"6c725a7e0621b45586a6e1a304929ebc0ceb6e4d": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Area - SQL", "Bug"], "spoonStatsSummary": {}, "title": "druid sql bug of having ", "numCommits": 0, "created": "2017-10-13 08:35:04", "closed": "2017-11-01 16:58:09", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "filteredCommits": [], "url": "https://github.com/apache/druid/issues/4957", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 19.00027777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "{server/src/main/java/io/druid/segment/indexing => processing/src/main/java/io/druid/segment/transform}/TransformSpec.java", "deletions": 2, "insertions": 17, "lines": 19}, {"filePath": "indexing-service/src/test/java/io/druid/indexing/common/task/RealtimeIndexTaskTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/test/java/io/druid/query/groupby/GroupByQueryRunnerTest.java", "deletions": 3, "insertions": 6, "lines": 9}, {"filePath": "extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "server/src/test/java/io/druid/segment/indexing/TransformSpecTest.java", "deletions": 0, "insertions": 2, "lines": 2}, {"filePath": "processing/src/main/java/io/druid/query/groupby/having/DimFilterHavingSpec.java", "deletions": 17, "insertions": 162, "lines": 179}, {"filePath": "server/src/test/java/io/druid/segment/indexing/DataSchemaTest.java", "deletions": 0, "insertions": 2, "lines": 2}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/rel/DruidQuery.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "{server/src/main/java/io/druid/segment/indexing => processing/src/main/java/io/druid/segment/transform}/Transformer.java", "deletions": 3, "insertions": 4, "lines": 7}, {"filePath": "{server/src/main/java/io/druid/segment/indexing => processing/src/main/java/io/druid/segment/transform}/ExpressionTransform.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "processing/src/test/java/io/druid/query/groupby/having/DimFilterHavingSpecTest.java", "deletions": 4, "insertions": 4, "lines": 8}, {"filePath": "indexing-service/src/test/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactoryTimelineTest.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "indexing-service/src/test/java/io/druid/indexing/common/task/IndexTaskTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "indexing-hadoop/src/test/java/io/druid/indexer/BatchDeltaIngestionTest.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "server/src/main/java/io/druid/segment/indexing/DataSchema.java", "deletions": 0, "insertions": 1, "lines": 1}, {"filePath": "server/src/main/java/io/druid/segment/realtime/firehose/IngestSegmentFirehose.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "indexing-service/src/test/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactoryTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "sql/src/test/java/io/druid/sql/calcite/CalciteQueryTest.java", "deletions": 6, "insertions": 110, "lines": 116}, {"filePath": "indexing-hadoop/src/test/java/io/druid/indexer/HadoopDruidIndexerMapperTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "{server/src/main/java/io/druid/segment/indexing => processing/src/main/java/io/druid/segment/transform}/RowFunction.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "server/src/test/java/io/druid/segment/realtime/firehose/IngestSegmentFirehoseTest.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "{server/src/main/java/io/druid/segment/indexing => processing/src/main/java/io/druid/segment/transform}/TransformingInputRowParser.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "{server/src/main/java/io/druid/segment/indexing => processing/src/main/java/io/druid/segment/transform}/TransformingStringInputRowParser.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "indexing-hadoop/src/main/java/io/druid/indexer/hadoop/DatasourceIngestionSpec.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "{server/src/main/java/io/druid/segment/indexing => processing/src/main/java/io/druid/segment/transform}/Transform.java", "deletions": 1, "insertions": 1, "lines": 2}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "tooManyFiles", "authoredDateTime": "2017-11-01 09:58:08", "commitMessage": "Fix havingSpec on complex aggregators. (#5024)\n\n* Fix havingSpec on complex aggregators.\r\n\r\n- Uses the technique from #4883 on DimFilterHavingSpec too.\r\n- Also uses Transformers from #4890, necessitating a move of that and other\r\n  related classes from druid-server to druid-processing. They probably make\r\n  more sense there anyway.\r\n- Adds a SQL query test.\r\n\r\nFixes #4957.\r\n\r\n* Remove unused import.\r\n", "commitUser": "fjy", "commitDateTime": "2017-11-01 12:58:08", "commitParents": ["e96daa2593808d7e10d833525066d4b9e455de4f"], "commitGHEventType": "referenced", "nameRev": "6c725a7e0621b45586a6e1a304929ebc0ceb6e4d tags/druid-0.12.0-rc1~90", "commitHash": "6c725a7e0621b45586a6e1a304929ebc0ceb6e4d"}], "body": "I found a bug of druid sql about having. when I add having condition of \"count(distinct uid)\". It will return []. if I replace it for \"having count(uid)\", it will return right result.for example:\r\n{\r\n    \"query\": \"select (count(distinct(\\\"uid\\\")) filter (where binlog_event='i')) as __y,Floor(__time to MINUTE) as __time from \\\"test_dataSource\\\" where  __time > TIMESTAMP '2017-10-12 00:00:00' and __time < TIMESTAMP '2017-10-12 23:59:59' group by Floor(__time to MINUTE) having (count(\\\"uid\\\") filter (where binlog_event='i')) >0\",\r\n    \"context\": {\r\n        \"sqlTimeZone\": \"Asia/Shanghai\",\r\n        \"skipEmptyBuckets\": \"false\"\r\n    }\r\n}\r\nthe reasult is:\r\n[\r\n    {\r\n        \"__y\": 1,\r\n        \"__time\": \"2017-10-12T01:54:00.000+08:00\"\r\n    },\r\n    {\r\n        \"__y\": 2,\r\n        \"__time\": \"2017-10-12T02:23:00.000+08:00\"\r\n    }\r\n]\r\n\r\nbut when I used having (count(distinct(\\\"uid\\\")) filter (where binlog_event='i')) >0\r\nthe result is:\r\n[]\r\n"}, {"user": "gianm", "commits": {}, "labels": ["Area - SQL", "Bug"], "created": "2017-10-12 00:13:37", "title": "SQL: Incorrect filter simplification", "url": "https://github.com/apache/druid/issues/4945", "closed": "2018-01-09 18:30:07", "ttf": 89.00027777777778, "commitsDetails": [], "body": "The filter in `SELECT COUNT(*) FROM druid.foo WHERE 1 < m2 AND m2 < 4` is incorrectly simplified to just `m2 < 4`. The root cause is https://issues.apache.org/jira/browse/CALCITE-2007 so this will require a Calcite fix and upgrade."}, {"user": "himanshug", "commits": {"609da018827697b7ae5ce6cc464762041e9a865e": {"commitGHEventType": "referenced", "commitUser": "leventov"}}, "changesInPackagesGIT": ["processing/src/main/java/org/apache/druid/segment/incremental"], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 1, "spoonMethodsChanged": 1, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 1, "TOT": 1}, "title": "ArrayIndexOutOfBoundsException  in TopN query", "numCommits": 1, "created": "2017-10-10 19:53:47", "closed": "2018-09-19 15:56:05", "gitStatsSummary": {"deletions": 1, "insertions": 2, "lines": 3, "gitFilesChange": 1}, "statsSkippedReason": "", "changesInPackagesSPOON": ["org.apache.druid.segment.incremental.IncrementalIndexStorageAdapter.IncrementalIndexCursor"], "filteredCommits": ["609da018827697b7ae5ce6cc464762041e9a865e"], "url": "https://github.com/apache/druid/issues/4937", "filteredCommitsReason": {"multipleIssueFixes": 0, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 343.0002777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "processing/src/main/java/org/apache/druid/segment/incremental/IncrementalIndexStorageAdapter.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexStorageAdapterTest.java", "deletions": 0, "insertions": 159, "lines": 159}], "commitSpoonAstDiffStats": [{"spoonFilePath": "IncrementalIndexStorageAdapter.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.segment.incremental.IncrementalIndexStorageAdapter.IncrementalIndexCursor", "MOV": 1, "TOT": 1}]}, {"spoonFilePath": "IncrementalIndexStorageAdapterTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.segment.incremental.IncrementalIndexStorageAdapterTest.testCursorDictionaryRaceConditionFix()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.segment.incremental.IncrementalIndexStorageAdapterTest.DictionaryRaceTestFilter", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-09-17 23:43:29", "commitMessage": "Fix dictionary ID race condition in IncrementalIndexStorageAdapter (#6340)\n\nPossibly related to https://github.com/apache/incubator-druid/issues/4937\r\n\r\n--------\r\n\r\nThere is currently a race condition in IncrementalIndexStorageAdapter that can lead to exceptions like the following, when running queries with filters on String dimensions that hit realtime tasks: \r\n\r\n```\r\norg.apache.druid.java.util.common.ISE: id[5] >= maxId[5]\r\n\tat org.apache.druid.segment.StringDimensionIndexer$1IndexerDimensionSelector.lookupName(StringDimensionIndexer.java:591)\r\n\tat org.apache.druid.segment.StringDimensionIndexer$1IndexerDimensionSelector$2.matches(StringDimensionIndexer.java:562)\r\n\tat org.apache.druid.segment.incremental.IncrementalIndexStorageAdapter$IncrementalIndexCursor.advance(IncrementalIndexStorageAdapter.java:284)\r\n```\r\n\r\nWhen the `filterMatcher` is created in the constructor of `IncrementalIndexStorageAdapter.IncrementalIndexCursor`, `StringDimensionIndexer.makeDimensionSelector` gets called eventually, which calls:\r\n\r\n```\r\nfinal int maxId = getCardinality();\r\n...\r\n\r\n @Override\r\n  public int getCardinality()\r\n  {\r\n    return dimLookup.size();\r\n  }\r\n```\r\n\r\nSo `maxId` is set to the size of the dictionary at the time that the `filterMatcher` is created.\r\n\r\nHowever, the `maxRowIndex` which is meant to prevent the Cursor from returning rows that were added after the Cursor was created (see https://github.com/apache/incubator-druid/pull/4049) is set after the `filterMatcher` is created.\r\n\r\nIf rows with new dictionary values are added after the `filterMatcher` is created but before `maxRowIndex` is set, then it is possible for the Cursor to return rows that contain the new values, which will have `id >= maxId`.\r\n\r\nThis PR sets `maxRowIndex` before creating the `filterMatcher` to prevent rows with unknown dictionary IDs from being passed to the `filterMatcher`.\r\n\r\n-----------\r\n\r\nThe included test triggers the error with a custom Filter + DruidPredicateFactory.\r\n\r\nThe DimensionSelector for predicate-based filter matching is created here in `Filters.makeValueMatcher`:\r\n\r\n```\r\n  public static ValueMatcher makeValueMatcher(\r\n      final ColumnSelectorFactory columnSelectorFactory,\r\n      final String columnName,\r\n      final DruidPredicateFactory predicateFactory\r\n  )\r\n  {\r\n    final ColumnCapabilities capabilities = columnSelectorFactory.getColumnCapabilities(columnName);\r\n\r\n    // This should be folded into the ValueMatcherColumnSelectorStrategy once that can handle LONG typed columns.\r\n    if (capabilities != null && capabilities.getType() == ValueType.LONG) {\r\n      return getLongPredicateMatcher(\r\n          columnSelectorFactory.makeColumnValueSelector(columnName),\r\n          predicateFactory.makeLongPredicate()\r\n      );\r\n    }\r\n\r\n    final ColumnSelectorPlus<ValueMatcherColumnSelectorStrategy> selector =\r\n        DimensionHandlerUtils.createColumnSelectorPlus(\r\n            ValueMatcherColumnSelectorStrategyFactory.instance(),\r\n            DefaultDimensionSpec.of(columnName),\r\n            columnSelectorFactory\r\n        );\r\n\r\n    return selector.getColumnSelectorStrategy().makeValueMatcher(selector.getSelector(), predicateFactory);\r\n  }\r\n```\r\n\r\nThe test Filter adds a row to the IncrementalIndex in the test when the predicateFactory creates a new String predicate, after `DimensionHandlerUtils.createColumnSelectorPlus` is called.\r\n", "commitUser": "leventov", "commitDateTime": "2018-09-18 10:43:29", "commitParents": ["edf0c13807e7d65db458215feab500fc4c42bd8e"], "commitGHEventType": "referenced", "nameRev": "609da018827697b7ae5ce6cc464762041e9a865e tags/druid-0.13.0-incubating-rc1~84", "commitHash": "609da018827697b7ae5ce6cc464762041e9a865e"}], "body": "I'm seeing following in KafkaIndexTask peons time to time, looks like a bug.\r\n\r\n```\r\n2017-10-10T18:19:35,062 ERROR [topN_xx_metrics_cluster_[2017-10-10T00:00:00.000Z/2017-10-10T18:20:00.000Z]] io.druid.query.ChainedExecutionQueryRunner - Exception with one of the sequences!\r\njava.lang.ArrayIndexOutOfBoundsException: 67\r\n        at io.druid.query.topn.Generic1AggPooledTopNScannerPrototype$Copy0.scanAndAggregate(Generic1AggPooledTopNScannerPrototype.java:48) ~[druid-processing-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.query.topn.PooledTopNAlgorithm.scanAndAggregateGeneric1Agg(PooledTopNAlgorithm.java:372) ~[druid-processing-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.query.topn.PooledTopNAlgorithm.lambda$computeSpecializedScanAndAggregateImplementations$2(PooledTopNAlgorithm.java:171) ~[druid-processing-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.query.topn.PooledTopNAlgorithm.scanAndAggregate(PooledTopNAlgorithm.java:319) ~[druid-processing-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.query.topn.PooledTopNAlgorithm.scanAndAggregate(PooledTopNAlgorithm.java:54) ~[druid-processing-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.query.topn.BaseTopNAlgorithm.runWithCardinalityKnown(BaseTopNAlgorithm.java:118) ~[druid-processing-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.query.topn.BaseTopNAlgorithm.run(BaseTopNAlgorithm.java:81) ~[druid-processing-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.query.topn.TopNMapFn.apply(TopNMapFn.java:105) ~[druid-processing-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.query.topn.TopNQueryEngine$1.apply(TopNQueryEngine.java:100) ~[druid-processing-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.query.topn.TopNQueryEngine$1.apply(TopNQueryEngine.java:93) ~[druid-processing-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.java.util.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:40) ~[java-util-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.java.util.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:40) ~[java-util-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.java.util.common.guava.BaseSequence.accumulate(BaseSequence.java:46) ~[java-util-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.java.util.common.guava.MappedSequence.accumulate(MappedSequence.java:43) ~[java-util-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.java.util.common.guava.MappedSequence.accumulate(MappedSequence.java:43) ~[java-util-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.java.util.common.guava.FilteredSequence.accumulate(FilteredSequence.java:45) ~[java-util-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.java.util.common.guava.WrappingSequence$1.get(WrappingSequence.java:50) ~[java-util-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.java.util.common.guava.SequenceWrapper.wrap(SequenceWrapper.java:55) ~[java-util-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.java.util.common.guava.WrappingSequence.accumulate(WrappingSequence.java:45) ~[java-util-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.java.util.common.guava.Sequences.toList(Sequences.java:150) ~[java-util-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:130) [druid-processing-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:120) [druid-processing-0.11.1-1507587363-0b733f3-76.jar:0.11.1-1507587363-0b733f3-76]\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_131]\r\n```"}, {"user": "NicolasBielza", "commits": {}, "labels": ["Area - Querying", "Bug"], "created": "2017-10-06 19:52:59", "title": "TopN query on multi-value dimension ignores null values", "url": "https://github.com/apache/druid/issues/4915", "closed": "2020-01-10 14:48:22", "ttf": 825.0002777777778, "commitsDetails": [], "body": "Using Druid 0.10.1,\r\n\r\nWhen running a topN query on a multi-valued dimension, Druid only returns the aggregation results for non-null values.\r\n\r\nThe following TopN query:\r\n\r\n```\r\n{\r\n  \"queryType\": \"topN\",\r\n  \"dataSource\": \"audience-breakdown\",\r\n  \"intervals\": \"2017-09-07T00Z/2017-09-08T00Z\",\r\n  \"granularity\": \"all\",\r\n  \"dimension\": {\r\n    \"type\": \"default\",\r\n    \"dimension\": \"listenerDma\",\r\n    \"outputName\": \"DMA\"\r\n  },\r\n  \"aggregations\": [\r\n    {\r\n      \"name\": \"TLS\",\r\n      \"type\": \"longSum\",\r\n      \"fieldName\": \"tls\"\r\n    }\r\n  ],\r\n  \"metric\": \"TLS\",\r\n  \"threshold\": 3\r\n}\r\n```\r\n\r\nProduces this response:\r\n\r\n```\r\n[\r\n    {\r\n        \"timestamp\": \"2017-09-07T00:00:00.000Z\",\r\n        \"result\": [\r\n            {\r\n                \"DMA\": \"803\",\r\n                \"TLS\": 2798577\r\n            },\r\n            {\r\n                \"DMA\": \"501\",\r\n                \"TLS\": 2509147\r\n            },\r\n            {\r\n                \"DMA\": \"602\",\r\n                \"TLS\": 1779172\r\n            }\r\n        ]\r\n    }\r\n]\r\n```\r\nWhereas using a similar groupBy query:\r\n\r\n```\r\n{\r\n  \"queryType\": \"groupBy\",\r\n  \"dataSource\": \"audience-breakdown\",\r\n  \"intervals\": \"2017-09-07T00Z/2017-09-08T00Z\",\r\n  \"granularity\": \"all\",\r\n  \"dimensions\": [\r\n    {\r\n      \"type\": \"default\",\r\n      \"dimension\": \"listenerDma\",\r\n      \"outputName\": \"DMA\"\r\n    }\r\n  ],\r\n  \"aggregations\": [\r\n    {\r\n      \"name\": \"TLS\",\r\n      \"type\": \"longSum\",\r\n      \"fieldName\": \"tls\"\r\n    }\r\n  ],\r\n  \"limitSpec\": {\r\n    \"type\": \"default\",\r\n    \"columns\": [\r\n      {\r\n        \"dimension\": \"TLS\",\r\n        \"direction\": \"descending\"\r\n      }\r\n    ],\r\n    \"limit\": 3\r\n  }\r\n}\r\n```\r\n\r\nWe can see that the largest group is actually the one having null in the listenerDma dimension:\r\n\r\n```\r\n[\r\n    {\r\n        \"version\": \"v1\",\r\n        \"timestamp\": \"2017-09-07T00:00:00.000Z\",\r\n        \"event\": {\r\n            \"DMA\": null,\r\n            \"TLS\": 8770694\r\n        }\r\n    },\r\n    {\r\n        \"version\": \"v1\",\r\n        \"timestamp\": \"2017-09-07T00:00:00.000Z\",\r\n        \"event\": {\r\n            \"DMA\": \"803\",\r\n            \"TLS\": 2798577\r\n        }\r\n    },\r\n    {\r\n        \"version\": \"v1\",\r\n        \"timestamp\": \"2017-09-07T00:00:00.000Z\",\r\n        \"event\": {\r\n            \"DMA\": \"501\",\r\n            \"TLS\": 2509147\r\n        }\r\n    }\r\n]\r\n```\r\n\r\nIs this the expected behavior of topN? If so, then why does it properly return the null group when I run my query on a single valued dimension?\r\n\r\nFor instance, if I use the city dimension (which is single valued):\r\n\r\n```\r\n{\r\n  \"queryType\": \"topN\",\r\n  \"dataSource\": \"audience-breakdown\",\r\n  \"intervals\": \"2017-09-07T00Z/2017-09-08T00Z\",\r\n  \"granularity\": \"all\",\r\n  \"dimension\": {\r\n    \"type\": \"default\",\r\n    \"dimension\": \"city\",\r\n    \"outputName\": \"City\"\r\n  },\r\n  \"aggregations\": [\r\n    {\r\n      \"name\": \"TLS\",\r\n      \"type\": \"longSum\",\r\n      \"fieldName\": \"tls\"\r\n    }\r\n  ],\r\n  \"metric\": \"TLS\",\r\n  \"threshold\": 3\r\n}\r\n```\r\n\r\nDruid does return an aggregated value for null\r\n\r\n```\r\n[\r\n    {\r\n        \"timestamp\": \"2017-09-07T00:00:00.000Z\",\r\n        \"result\": [\r\n            {\r\n                \"TLS\": 2412533,\r\n                \"City\": null\r\n            },\r\n            {\r\n                \"TLS\": 634690,\r\n                \"City\": \"Los Angeles\"\r\n            },\r\n            {\r\n                \"TLS\": 609145,\r\n                \"City\": \"Houston\"\r\n            }\r\n        ]\r\n    }\r\n]\r\n```\r\n\r\nThanks,\r\nNicolas"}, {"user": "gianm", "commits": {"291d9c5830a253069d920ee214519d8ccad2a387": {"commitGHEventType": "referenced", "commitUser": "jon-wei"}, "797b54d283a618ead581aa8cc100eeaeb39e4338": {"commitGHEventType": "referenced", "commitUser": "himanshug"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "[0.11] ISE in DruidLeaderClient foils RTAC retries", "numCommits": 0, "created": "2017-10-06 17:14:59", "closed": "2017-10-06 20:12:10", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/4911", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 0.0002777777777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "indexing-service/src/main/java/io/druid/indexing/common/actions/RemoteTaskActionClient.java", "deletions": 11, "insertions": 7, "lines": 18}, {"filePath": "server/src/test/java/io/druid/discovery/DruidLeaderClientTest.java", "deletions": 0, "insertions": 31, "lines": 31}, {"filePath": "server/src/main/java/io/druid/discovery/DruidLeaderClient.java", "deletions": 18, "insertions": 47, "lines": 65}], "commitSpoonAstDiffStats": [{"spoonFilePath": "DruidLeaderClient.java", "spoonMethods": [{"INS": 3, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.discovery.DruidLeaderClient.getCurrentKnownLeader(boolean)", "MOV": 1, "TOT": 5}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.discovery.DruidLeaderClient", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.discovery.DruidLeaderClient.makeRequest(org.jboss.netty.handler.codec.http.HttpMethod,java.lang.String)", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.discovery.DruidLeaderClient.pickOneHost()", "MOV": 0, "TOT": 3}, {"INS": 2, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.discovery.DruidLeaderClient.go(com.metamx.http.client.Request)", "MOV": 1, "TOT": 4}]}, {"spoonFilePath": "RemoteTaskActionClient.java", "spoonMethods": [{"INS": 1, "UPD": 1, "DEL": 4, "spoonMethodName": "io.druid.indexing.common.actions.RemoteTaskActionClient.submit(io.druid.indexing.common.actions.TaskAction)", "MOV": 2, "TOT": 8}]}, {"spoonFilePath": "DruidLeaderClientTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.discovery.DruidLeaderClientTest.testNoLeaderFound()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.discovery.DruidLeaderClientTest", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2017-10-06 13:12:09", "commitMessage": "DruidLeaderClient: Throw IOException on retryable errors. (#4913)\n\n* DruidLeaderClient: Throw IOException on retryable errors.\r\n\r\nFixes #4911.\r\n\r\n* Adjustments.\r\n", "commitUser": "himanshug", "commitDateTime": "2017-10-06 15:12:09", "commitParents": ["535c034c06e7b62dfc516908d20488490da68a7f"], "commitGHEventType": "referenced", "nameRev": "797b54d283a618ead581aa8cc100eeaeb39e4338 tags/druid-0.12.0-rc1~133", "commitHash": "797b54d283a618ead581aa8cc100eeaeb39e4338"}, {"commitGitStats": [{"filePath": "indexing-service/src/main/java/io/druid/indexing/common/actions/RemoteTaskActionClient.java", "deletions": 11, "insertions": 7, "lines": 18}, {"filePath": "server/src/test/java/io/druid/discovery/DruidLeaderClientTest.java", "deletions": 0, "insertions": 31, "lines": 31}, {"filePath": "server/src/main/java/io/druid/discovery/DruidLeaderClient.java", "deletions": 18, "insertions": 47, "lines": 65}], "commitSpoonAstDiffStats": [{"spoonFilePath": "DruidLeaderClient.java", "spoonMethods": [{"INS": 3, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.discovery.DruidLeaderClient.getCurrentKnownLeader(boolean)", "MOV": 1, "TOT": 5}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.discovery.DruidLeaderClient", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.discovery.DruidLeaderClient.makeRequest(org.jboss.netty.handler.codec.http.HttpMethod,java.lang.String)", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.discovery.DruidLeaderClient.pickOneHost()", "MOV": 0, "TOT": 3}, {"INS": 2, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.discovery.DruidLeaderClient.go(com.metamx.http.client.Request)", "MOV": 1, "TOT": 4}]}, {"spoonFilePath": "RemoteTaskActionClient.java", "spoonMethods": [{"INS": 1, "UPD": 1, "DEL": 4, "spoonMethodName": "io.druid.indexing.common.actions.RemoteTaskActionClient.submit(io.druid.indexing.common.actions.TaskAction)", "MOV": 2, "TOT": 8}]}, {"spoonFilePath": "DruidLeaderClientTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.discovery.DruidLeaderClientTest.testNoLeaderFound()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.discovery.DruidLeaderClientTest", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2017-10-10 09:28:21", "commitMessage": "DruidLeaderClient: Throw IOException on retryable errors. (#4913) (#4928)\n\n* DruidLeaderClient: Throw IOException on retryable errors.\r\n\r\nFixes #4911.\r\n\r\n* Adjustments.", "commitUser": "jon-wei", "commitDateTime": "2017-10-10 09:28:21", "commitParents": ["d676f530d6b2c2652e1c14bd2836fa945aa32649"], "commitGHEventType": "referenced", "nameRev": "291d9c5830a253069d920ee214519d8ccad2a387 tags/druid-0.11.0-rc1~10", "commitHash": "291d9c5830a253069d920ee214519d8ccad2a387"}], "body": "RemoteTaskActionClient is built to retry any IOExceptions or ChannelExceptions, but consider other exceptions as fatal and give up. Generally, this will bubble up and fail the task as well.\r\n\r\nDruidLeaderClient (introduced in 0.11.0) will however throw ISE for some things that should be IOExceptions, such as `ISE(\"Couldn't find any servers.\")` in `pickOneHost()`. It causes tasks to fail when a leader briefly goes offline, when they really should just retry."}, {"user": "gianm", "commits": {}, "labels": ["Bug"], "created": "2017-10-06 16:52:15", "title": "[0.11] Overlord-first upgrade breaks task log streaming", "url": "https://github.com/apache/druid/issues/4910", "closed": "2017-10-06 19:50:49", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "The overlord cannot construct a proper worker url; it tries to call `worker.getScheme()` but that returns null. It should assume that a null scheme is http."}, {"user": "gianm", "commits": {}, "labels": ["Bug", "Security"], "created": "2017-09-26 20:59:01", "title": "/druid/coordinator/v1/datasources authorization error", "url": "https://github.com/apache/druid/issues/4860", "closed": "2017-10-02 18:38:46", "ttf": 5.000277777777778, "commitsDetails": [], "body": "Some other APIs use the same helper methods and may be affected too.\r\n\r\nError log:\r\n\r\n```\r\n2017-09-26T20:20:18,740 WARN [qtp1115349385-90] org.eclipse.jetty.server.HttpChannel - //localhost:8081/druid/coordinator/v1/datasources\r\nio.druid.java.util.common.ISE: Request did not have an authorization check performed.\r\n        at io.druid.server.security.PreResponseAuthorizationCheckFilter.handleAuthorizationCheckError(PreResponseAuthorizationCheckFilter.java:156) ~[druid-server-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSH\r\nOT]\r\n        at io.druid.server.security.PreResponseAuthorizationCheckFilter.doFilter(PreResponseAuthorizationCheckFilter.java:91) ~[druid-server-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) ~[jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at io.druid.server.security.AllowAllAuthenticator$1.doFilter(AllowAllAuthenticator.java:83) ~[druid-server-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n        at io.druid.server.security.AuthenticationWrappingFilter.doFilter(AuthenticationWrappingFilter.java:60) ~[druid-server-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) ~[jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at io.druid.server.security.SecuritySanityCheckFilter.doFilter(SecuritySanityCheckFilter.java:86) ~[druid-server-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) ~[jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582) ~[jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:224) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512) ~[jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.Server.handle(Server.java:534) ~[jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_101]\r\n```"}, {"user": "jon-wei", "commits": {}, "labels": ["Bug"], "created": "2017-09-26 18:56:58", "title": "Incorrect query results with limit push down optimization", "url": "https://github.com/apache/druid/issues/4856", "closed": "2017-10-02 18:44:24", "ttf": 5.000277777777778, "commitsDetails": [], "body": "I saw the following query return improperly aggregated results with the limit push down optimization for GroupBy enabled:\r\n\r\n```\r\nselect host, floor(__time to hour), remoteAddress, sum(\"count\") from datasource where user = 'user' and metric = 'query/time' and host =  'host' and __time >= timestamp '2017-09-22 00:00:00' and __time < timestamp '2017-09-22 03:00:00' group by host, floor(__time to hour), remoteAddress order by 2, 3 limit 1000;\r\n```\r\n\r\nNot sure on the exact cause, still investigating."}, {"user": "himanshug", "commits": {}, "labels": ["Bug"], "created": "2017-09-26 17:44:34", "title": "ServerConfig dependency needs to be removed from DruidNode", "url": "https://github.com/apache/druid/issues/4850", "closed": "2017-09-28 15:41:00", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "or else things like tls or plainText enable/disable are not present in serialized form of DruidNode and following kind of scenarios break....\r\n\r\n(1)  tls is not enabled on historical, it serializes its DruidNode object and puts in to zk for discovery.\r\n(2) tls is enabled on broker, when broker discovers historical and tries to deserialize with the broker local ServerConfig object (which has tls enabled), then deserialization fails because serialized DruidNode object from historical doesn't have tls port set.\r\n"}, {"user": "jihoonson", "commits": {}, "labels": ["Area - SQL", "Bug"], "created": "2017-09-26 08:44:59", "title": "replaceInput should be implemented for DruidOuterQueryRel and DruidSemiJoin", "url": "https://github.com/apache/druid/issues/4847", "closed": "2017-10-03 15:00:59", "ttf": 7.000277777777778, "commitsDetails": [], "body": "```sql\r\nselect\r\n    floor(__time to hour) as __time,\r\n    l_linestatus,\r\n    count(l_partkey)\r\nfrom (\r\n    select \r\n        max(__time) as __time,\r\n        l_partkey,\r\n        l_linestatus\r\n    from lineitem_1g_split_year\r\n    where 1=1\r\n        and l_returnflag = 'R'\r\n    group by l_partkey, l_linestatus\r\n)\r\ngroup by floor(__time to hour), l_linestatus\r\n```\r\n\r\nThe above query causes the following error.\r\n\r\n```\r\njava.lang.RuntimeException: Error while applying rule DruidFilterRule, args [rel#337:LogicalFilter.NONE.[[]](input=rel#306:Subset#0.NONE.[],condition==($12, 'R')), rel#371:DruidQueryRel.NONE.[](query={\"queryType\":\"scan\",\"dataSource\":{\"type\":\"table\",\"name\":\"lineitem_1g_split_year\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"virtualColumns\":[],\"resultFormat\":\"compactedList\",\"batchSize\":20480,\"limit\":9223372036854775807,\"filter\":null,\"columns\":[\"__time\",\"count\",\"l_comment\",\"l_commitdate\",\"l_discount\",\"l_extendedprice\",\"l_linenumber\",\"l_linestatus\",\"l_orderkey\",\"l_partkey\",\"l_quantity\",\"l_receiptdate\",\"l_returnflag\",\"l_shipdate\",\"l_shipinstruct\",\"l_shipmode\",\"l_suppkey\",\"l_tax\"],\"legacy\":false,\"context\":{},\"descending\":false})]\r\n\tat org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:235) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:650) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:368) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat org.apache.calcite.prepare.PlannerImpl.transform(PlannerImpl.java:313) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat io.druid.sql.calcite.planner.DruidPlanner.planWithDruidConvention(DruidPlanner.java:148) ~[druid-sql-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.sql.calcite.planner.DruidPlanner.plan(DruidPlanner.java:116) ~[druid-sql-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.sql.http.SqlResource.doPost(SqlResource.java:88) [druid-sql-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_144]\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_144]\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]\r\n\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]\r\n\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409) [jersey-server-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409) [jersey-servlet-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558) [jersey-servlet-1.19.3.jar:1.19.3]\r\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733) [jersey-servlet-1.19.3.jar:1.19.3]\r\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790) [javax.servlet-api-3.1.0.jar:3.1.0]\r\n\tat com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:286) [guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:276) [guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:181) [guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91) [guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85) [guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:120) [guice-servlet-4.1.0.jar:?]\r\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:135) [guice-servlet-4.1.0.jar:?]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat io.druid.server.security.PreResponseAuthorizationCheckFilter.doFilter(PreResponseAuthorizationCheckFilter.java:84) [druid-server-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat io.druid.server.security.AllowAllAuthenticator$1.doFilter(AllowAllAuthenticator.java:83) [druid-server-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.server.security.AuthenticationWrappingFilter.doFilter(AuthenticationWrappingFilter.java:60) [druid-server-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat io.druid.server.security.SecuritySanityCheckFilter.doFilter(SecuritySanityCheckFilter.java:86) [druid-server-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:224) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512) [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.Server.handle(Server.java:534) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251) [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93) [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589) [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]\r\nCaused by: java.lang.RuntimeException: Error occurred while applying rule DruidFilterRule\r\n\tat org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:148) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:225) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat io.druid.sql.calcite.rule.DruidFilterRule.onMatch(DruidFilterRule.java:58) ~[druid-sql-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:211) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\t... 62 more\r\nCaused by: java.lang.UnsupportedOperationException: replaceInput called on rel#374:DruidOuterQueryRel.NONE.[](innerQuery=rel#358:Subset#11.NONE.[],query={\"queryType\":\"groupBy\",\"dataSource\":{\"type\":\"table\",\"name\":\"__subquery__\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"virtualColumns\":[],\"filter\":null,\"granularity\":{\"type\":\"all\"},\"dimensions\":[{\"type\":\"extraction\",\"dimension\":\"a0\",\"outputName\":\"d0\",\"outputType\":\"LONG\",\"extractionFn\":{\"type\":\"timeFormat\",\"format\":null,\"timeZone\":null,\"locale\":null,\"granularity\":\"HOUR\",\"asMillis\":true}},{\"type\":\"default\",\"dimension\":\"d0\",\"outputName\":\"d1\",\"outputType\":\"STRING\"}],\"aggregations\":[{\"type\":\"filtered\",\"aggregator\":{\"type\":\"count\",\"name\":\"a0\"},\"filter\":{\"type\":\"not\",\"field\":{\"type\":\"selector\",\"dimension\":\"d1\",\"value\":\"\",\"extractionFn\":null}},\"name\":\"a0\"}],\"postAggregations\":[],\"having\":null,\"limitSpec\":{\"type\":\"NoopLimitSpec\"},\"context\":{},\"descending\":false})\r\n\tat org.apache.calcite.rel.AbstractRelNode.replaceInput(AbstractRelNode.java:372) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat org.apache.calcite.plan.volcano.VolcanoPlanner.fixUpInputs(VolcanoPlanner.java:1379) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat org.apache.calcite.plan.volcano.VolcanoPlanner.rename(VolcanoPlanner.java:1249) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat org.apache.calcite.plan.volcano.RelSet.mergeWith(RelSet.java:345) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat org.apache.calcite.plan.volcano.VolcanoPlanner.merge(VolcanoPlanner.java:1413) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat org.apache.calcite.plan.volcano.VolcanoPlanner.rename(VolcanoPlanner.java:1291) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat org.apache.calcite.plan.volcano.RelSet.mergeWith(RelSet.java:345) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat org.apache.calcite.plan.volcano.VolcanoPlanner.merge(VolcanoPlanner.java:1413) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat org.apache.calcite.plan.volcano.VolcanoPlanner.registerSubset(VolcanoPlanner.java:1669) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1530) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:863) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:883) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:1769) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:135) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:225) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\tat io.druid.sql.calcite.rule.DruidFilterRule.onMatch(DruidFilterRule.java:58) ~[druid-sql-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:211) ~[calcite-core-1.12.0.jar:1.12.0]\r\n\t... 62 more\r\n```"}, {"user": "KenjiTakahashi", "commits": {}, "labels": ["Bug"], "created": "2017-09-22 00:01:38", "title": "Error in coordinator/overlord when loading scan-query", "url": "https://github.com/apache/druid/issues/4835", "closed": "2018-01-09 18:35:58", "ttf": 109.00027777777778, "commitsDetails": [], "body": "I'm getting errors like this\r\n\r\n```bash\r\n2017-09-21T21:17:34.025034+00:00 monitowl-dev wh_druid_overlord[27859]: Exception in thread \"main\" com.google.inject.CreationException: Unable to create injector, see the following errors:                                      \r\n2017-09-21T21:17:34.025617+00:00 monitowl-dev wh_druid_overlord[27859]: 1) No implementation for io.druid.query.GenericQueryMetricsFactory was bound.                                \r\n2017-09-21T21:17:34.026017+00:00 monitowl-dev wh_druid_overlord[27859]:   while locating io.druid.query.GenericQueryMetricsFactory                                                   \r\n2017-09-21T21:17:34.026441+00:00 monitowl-dev wh_druid_overlord[27859]:     for the 1st parameter of io.druid.query.scan.ScanQueryQueryToolChest.<init>(ScanQueryQueryToolChest.java:48)                                          \r\n2017-09-21T21:17:34.026811+00:00 monitowl-dev wh_druid_overlord[27859]:   at io.druid.query.scan.ScanQueryDruidModule.configure(ScanQueryDruidModule.java:36) (via modules: com.google.inject.util.Modules$OverrideModule -> io.druid.query.scan.ScanQueryDruidModule)\r\n2017-09-21T21:17:34.027141+00:00 monitowl-dev wh_druid_overlord[27859]: 2) No implementation for io.druid.query.GenericQueryMetricsFactory was bound.\r\n2017-09-21T21:17:34.027372+00:00 monitowl-dev wh_druid_overlord[27859]:   while locating io.druid.query.GenericQueryMetricsFactory                                                   \r\n2017-09-21T21:17:34.027621+00:00 monitowl-dev wh_druid_overlord[27859]:     for the 1st parameter of io.druid.query.scan.ScanQueryQueryToolChest.<init>(ScanQueryQueryToolChest.java:48)\r\n2017-09-21T21:17:34.027988+00:00 monitowl-dev wh_druid_overlord[27859]:   while locating io.druid.query.scan.ScanQueryQueryToolChest\r\n2017-09-21T21:17:34.028260+00:00 monitowl-dev wh_druid_overlord[27859]:     for the 1st parameter of io.druid.query.scan.ScanQueryRunnerFactory.<init>(ScanQueryRunnerFactory.java:52)\r\n2017-09-21T21:17:34.028492+00:00 monitowl-dev wh_druid_overlord[27859]:   at io.druid.query.scan.ScanQueryDruidModule.configure(ScanQueryDruidModule.java:41) (via modules: com.google.inject.util.Modules$OverrideModule -> io.druid.query.scan.ScanQueryDruidModule)\r\n2017-09-21T21:17:34.028749+00:00 monitowl-dev wh_druid_overlord[27859]: 2 errors\r\n2017-09-21T21:17:34.028975+00:00 monitowl-dev wh_druid_overlord[27859]: #011at com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:470)\r\n2017-09-21T21:17:34.029211+00:00 monitowl-dev wh_druid_overlord[27859]: #011at com.google.inject.internal.InternalInjectorCreator.initializeStatically(InternalInjectorCreator.java:155)\r\n2017-09-21T21:17:34.029438+00:00 monitowl-dev wh_druid_overlord[27859]: #011at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:107)            \r\n2017-09-21T21:17:34.029665+00:00 monitowl-dev wh_druid_overlord[27859]: #011at com.google.inject.Guice.createInjector(Guice.java:99)\r\n2017-09-21T21:17:34.029944+00:00 monitowl-dev wh_druid_overlord[27859]: #011at com.google.inject.Guice.createInjector(Guice.java:73)                                                 \r\n2017-09-21T21:17:34.030185+00:00 monitowl-dev wh_druid_overlord[27859]: #011at com.google.inject.Guice.createInjector(Guice.java:62)\r\n2017-09-21T21:17:34.030399+00:00 monitowl-dev wh_druid_overlord[27859]: #011at io.druid.initialization.Initialization.makeInjectorWithModules(Initialization.java:390)               \r\n2017-09-21T21:17:34.030678+00:00 monitowl-dev wh_druid_overlord[27859]: #011at io.druid.cli.GuiceRunnable.makeInjector(GuiceRunnable.java:63)\r\n2017-09-21T21:17:34.030892+00:00 monitowl-dev wh_druid_overlord[27859]: #011at io.druid.cli.ServerRunnable.run(ServerRunnable.java:40)                                               \r\n2017-09-21T21:17:34.031104+00:00 monitowl-dev wh_druid_overlord[27859]: #011at io.druid.cli.Main.main(Main.java:108)\r\n```\r\n\r\nwhile having `scan-query` extension in `druid.extensions.loadList`, which is in the `_common/common.runtime.properties` file.\r\n\r\nI've confirmed that this does not affect historicals, nor brokers and that removing `scan-query` from extensions fixes the problem.\r\n\r\nIs this expected? Should I be splitting the extensions definitions for different nodes now? What I'm worried about that our local tasks [or middle managers, or whatever actually serves incoming data :-)] (spawned by overlord) will not see it, then, which will be no good."}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2017-09-15 03:56:35", "title": "0.9.2 coordinator with 0.10.1 historicals \"There are too many [10001] pendingNotices.\"", "url": "https://github.com/apache/druid/issues/4804", "closed": "2017-09-15 18:40:57", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "Running mis-matched coordinator and historicals causes a bunch of notices in the logs like `There are too many [10001] pendingNotices.` in certain scenarios."}, {"user": "himanshug", "commits": {}, "labels": ["Bug", "Security"], "created": "2017-09-14 21:18:30", "title": "[security] jackson-databind dependency needs to be upgraded", "url": "https://github.com/apache/druid/issues/4798", "closed": "2019-11-19 17:14:34", "ttf": 795.0002777777778, "commitsDetails": [], "body": "jackson-databind security vulnerability published: https://bugzilla.redhat.com/show_bug.cgi?id=1462702 \r\n\r\njackson-databind issue: https://github.com/FasterXML/jackson-databind/issues/1599\r\n\r\nversion < 2.8.9  seems problematic."}, {"user": "gaodayue", "commits": {}, "labels": ["Bug"], "created": "2017-09-13 03:58:22", "title": "Occasional \"Null timestamp in input\" during realtime ingestion", "url": "https://github.com/apache/druid/issues/4790", "closed": "2017-09-13 17:04:04", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "We use druid 0.10.0 and tranquility-kafka to do realtime ingestion. Occasionally, we see NPE in realtime task log, and the error message looks fairly strange.\r\n\r\n```\r\nio.druid.java.util.common.parsers.ParseException: Unparseable timestamp found!\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at io.druid.data.input.impl.MapInputRowParser.parse(MapInputRowParser.java:75) ~[druid-api-0.10.0.jar:0.10.0]\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at io.druid.segment.realtime.firehose.EventReceiverFirehoseFactory$EventReceiverFirehose.addAll(EventReceiverFirehoseFactory.java:192) ~[druid-server-0.10.0.jar:0.10.0]\r\nCaused by: java.lang.NullPointerException: Null timestamp in input: {timestamp=1505203571000, reach=0, passImpression=1, passReach=0, frequencyFilter=0, click=0, impres...\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at io.druid.data.input.impl.MapInputRowParser.parse(MapInputRowParser.java:67) ~[druid-api-0.10.0.jar:0.10.0]\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0... 54 more\r\n```\r\n\r\nFrom the error message, it's obvious that the timestamp column \"timestamp\" does exist and has a valid value, but Druid says \"Null timestamp in input\" anyway."}, {"user": "b-slim", "commits": {}, "labels": ["Bug"], "created": "2017-09-01 18:18:39", "title": "Sometimes we hit this  java.lang.ArrayIndexOutOfBoundsException'", "url": "https://github.com/apache/druid/issues/4741", "closed": "2018-10-02 16:15:36", "ttf": 395.0002777777778, "commitsDetails": [], "body": "This issue seem to be a known bug of ArrayIndexOutOfBoundsException: 128 when repeatedly serializing to a byte array  https://github.com/FasterXML/jackson-core/issues/216\r\n```\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException\r\n java.lang.ArrayIndexOutOfBoundsException: 129\r\n\tat com.fasterxml.jackson.core.sym.ByteQuadsCanonicalizer.addName(ByteQuadsCanonicalizer.java:870)\r\n\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.addName(UTF8StreamJsonParser.java:2340)\r\n\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.findName(UTF8StreamJsonParser.java:2224)\r\n\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.parseLongName(UTF8StreamJsonParser.java:1804)\r\n\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.parseMediumName2(UTF8StreamJsonParser.java:1786)\r\n\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.parseMediumName(UTF8StreamJsonParser.java:1743)\r\n\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1678)\r\n\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:740)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:231)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:118)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3051)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2215)\r\n\tat io.druid.segment.IndexIO$V9IndexLoader.load(IndexIO.java:1008)\r\n\tat io.druid.segment.IndexIO.loadIndex(IndexIO.java:222)\r\n\tat io.druid.segment.realtime.appenderator.AppenderatorImpl.persistHydrant(AppenderatorImpl.java:1003)\r\n\tat io.druid.segment.realtime.appenderator.AppenderatorImpl.access$200(AppenderatorImpl.java:93)\r\n\tat io.druid.segment.realtime.appenderator.AppenderatorImpl$2.doCall(AppenderatorImpl.java:385)\r\n\tat io.druid.common.guava.ThreadRenamingCallable.call(ThreadRenamingCallable.java:44)\r\n\t... 4 more\r\n```"}, {"user": "jon-wei", "commits": {}, "labels": ["Bug"], "created": "2017-08-31 22:04:25", "title": "GroupBy V2 query fails when using a large limit", "url": "https://github.com/apache/druid/issues/4739", "closed": "2017-09-12 19:34:51", "ttf": 11.000277777777777, "commitsDetails": [], "body": "0.10.1 added limit push down optimization to GroupBy V2 in PR #3873. \r\n\r\nWhen the optimization is on, GroupBy V2 requires a buffer with a minimum size depending on the limit.\r\n\r\nIf the user submits a query with a limit that's large enough such that the configured processing buffers are too small to support this optimization, the query fails with an error like:\r\n\r\n```\r\n { \"errorClass\": \"io.druid.java.util.common.IAE\", \"host\": \"localhost:8083\", \"errorMessage\": \"Buffer capacity [34636833] is too small for limit[500000000] with load factor[0.700000], minimum bytes needed: [1666887684]\", \"error\": \"Unknown exception\" }\r\n```\r\n\r\nIt would be better to disable the push down optimization in that situation instead of failing the query."}, {"user": "wysstartgo", "commits": {}, "labels": ["Bug"], "created": "2017-08-31 10:32:22", "title": "DoubleFirstAggregatorFactory java.lang.ClassCastException", "url": "https://github.com/apache/druid/issues/4734", "closed": "2018-09-25 00:54:21", "ttf": 389.0002777777778, "commitsDetails": [], "body": "my code is:\r\n \r\n private static void executeTopNQuery(DruidClient druidClient) {\r\n        // Create a simple select query using the Druids query builder.\r\n        final int threshold = 1500;\r\n        // final Query query = getSelectQuery(threshold);\r\n        // TopNQuery topNQuery = new TopNQueryBuilder()\r\n        AggregatorFactory aggregatorFactory = new DoubleMaxAggregatorFactory(\"highPx\", \"mostHighPx\");\r\n        CountAggregatorFactory countAggregatorFactory = new CountAggregatorFactory(\"count\");\r\n        DoubleMinAggregatorFactory doubleMinAggregatorFactory = new DoubleMinAggregatorFactory(\"lowPx\", \"worstLowPx\");\r\n        DoubleFirstAggregatorFactory doubleFirstAggregatorFactory = new DoubleFirstAggregatorFactory(\"openPxs\", \"openPx\");\r\n        DoubleLastAggregatorFactory doubleLastAggregatorFactory = new DoubleLastAggregatorFactory(\"closePxs\", \"lastClosePx\");\r\n\r\n        DateTime startDateTime = new DateTime(2017, 8, 31, 10, 5).toDateTime(DateTimeZone.UTC);\r\n        DateTime endDateTime = new DateTime(2017, 8, 31, 11, 10).toDateTime(DateTimeZone.UTC);\r\n        Interval interval = new Interval(startDateTime, endDateTime);\r\n        //\u8868\u660e\u662f\u6309\u7167prodCode\u6765\u67e5\u627etopN\r\n        TopNQuery topNQuery = new TopNQueryBuilder().dataSource(dataSource).dimension(\"prodCode\")\r\n                .metric(\"highPx\").threshold(threshold)\r\n                .intervals(ImmutableList.<Interval>of(interval))\r\n                .granularity(Granularities.HOUR)\r\n                //.intervals(\"2012-01-01/2018-01-01\")\r\n                .aggregators(ImmutableList.of(aggregatorFactory,\r\n                        countAggregatorFactory,\r\n                        doubleMinAggregatorFactory,\r\n                        doubleFirstAggregatorFactory,\r\n                        doubleLastAggregatorFactory\r\n                )).build();\r\n        // Fetch the results.\r\n        final long startTime = System.currentTimeMillis();\r\n        final Sequence<Result<TopNResultValue>> resultSequence = druidClient.execute(topNQuery);\r\n        System.out.println(resultSequence);\r\n        final List<Result<TopNResultValue>> resultList = Sequences.toList(resultSequence,\r\n                Lists.<Result<TopNResultValue>> newArrayList());\r\n        final long fetchTime = System.currentTimeMillis() - startTime;\r\n\r\n        // Print the results.\r\n        int resultCount = 0;\r\n        for (final Result<TopNResultValue> result : resultList) {\r\n            System.out.println(\"\u65f6\u95f4\u6233\u4e3a:\" + result.getTimestamp());\r\n            TopNResultValue topNResultValue = result.getValue();\r\n            List<DimensionAndMetricValueExtractor> dimensionAndMetricValueExtractors = topNResultValue.getValue();\r\n            System.out.println(\"DimensionAndMetricValueExtractor size is:\" + dimensionAndMetricValueExtractors.size());\r\n            for (DimensionAndMetricValueExtractor dimensionAndMetricValueExtractor : dimensionAndMetricValueExtractors) {\r\n                resultCount++;\r\n                System.out.println(dimensionAndMetricValueExtractor.getBaseObject());\r\n            }\r\n        }\r\n\r\n        // Print statistics.\r\n        System.out.println(String.format(\"Fetched %,d rows in %,dms.\", resultCount, fetchTime));\r\n    }\r\n\r\nwhen execute:\r\n\r\nException in thread \"main\" java.lang.ClassCastException: java.lang.Double cannot be cast to java.util.Map\r\n\tat io.druid.query.aggregation.first.DoubleFirstAggregatorFactory.deserialize(DoubleFirstAggregatorFactory.java:175)\r\n\tat io.druid.query.aggregation.MetricManipulatorFns$3.manipulate(MetricManipulatorFns.java:59)\r\n\tat io.druid.query.topn.TopNQueryQueryToolChest$5$1.apply(TopNQueryQueryToolChest.java:193)\r\n\tat io.druid.query.topn.TopNQueryQueryToolChest$5$1.apply(TopNQueryQueryToolChest.java:181)\r\n\tat com.google.common.collect.Iterators$8.transform(Iterators.java:794)\r\n\tat com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48)\r\n\tat com.google.common.collect.Iterators.addAll(Iterators.java:357)\r\n\tat com.google.common.collect.Lists.newArrayList(Lists.java:147)\r\n\tat com.google.common.collect.Lists.newArrayList(Lists.java:129)\r\n\tat io.druid.query.topn.TopNQueryQueryToolChest$5.apply(TopNQueryQueryToolChest.java:177)\r\n\tat io.druid.query.topn.TopNQueryQueryToolChest$5.apply(TopNQueryQueryToolChest.java:167)\r\n\tat io.druid.java.util.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:42)\r\n\tat io.druid.java.util.common.guava.BaseSequence.accumulate(BaseSequence.java:46)\r\n\tat io.druid.java.util.common.guava.MappedSequence.accumulate(MappedSequence.java:43)\r\n\tat io.druid.java.util.common.guava.Sequences.toList(Sequences.java:150)\r\n\tat io.imply.druid.query.ExampleMain.executeTopNQuery(ExampleMain.java:256)\r\n\tat io.imply.druid.query.ExampleMain.main(ExampleMain.java:78)\r\n\r\n"}, {"user": "mazorigal", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2017-08-24 11:33:02", "title": "datasource created by kafka supervisor is not presented in coordinator UI", "url": "https://github.com/apache/druid/issues/4715", "closed": "2019-06-17 19:47:51", "ttf": 662.0002777777778, "commitsDetails": [], "body": "Hi,\r\n\r\nwhen we submit kafka supervisor task, until the first time data would be persisted in the deep storage, the coordinator UI do not show that datasource, although the datasource is queryable almost immediately after the supervisor starts.\r\n\r\nThanks,  "}, {"user": "gianm", "commits": {}, "labels": ["Area - SQL", "Bug"], "created": "2017-08-11 18:48:50", "title": "SQL: Router is not SQL aware", "url": "https://github.com/apache/druid/issues/4683", "closed": "2017-10-11 15:35:49", "ttf": 60.000277777777775, "commitsDetails": [], "body": "The router redirects `/druid/v2/sql/` to `/druid/v2/` and so JSON-over-HTTP SQL queries do not work. I bet something similar applies to the Avatica route at `/druid/v2/sql/avatica/`.\r\n\r\nThe router should become aware of SQL. I'm not sure how this will fit in with the router's query-aware routing capabilities, since lacks the SQL metadata infrastructure that the broker uses to plan and understand SQL queries. Maybe the query-aware routing won't apply to SQL at first?"}, {"user": "scan-the-automator", "commits": {}, "labels": ["Area - SQL", "Bug"], "created": "2017-08-09 17:49:53", "title": "Druid SQL Error: Unknown exception: io.druid.query.groupby.RowBasedColumnSelectorFactory$3 cannot be cast to io.druid.segment.LongColumnSelector", "url": "https://github.com/apache/druid/issues/4670", "closed": "2017-09-26 17:11:01", "ttf": 47.000277777777775, "commitsDetails": [], "body": "I have two queries that seem similar in nature, but one runs successfully while the other gives me the following error:\r\n\r\n`Unknown exception: io.druid.query.groupby.RowBasedColumnSelectorFactory$3 cannot be cast to io.druid.segment.LongColumnSelector`\r\n\r\nAs far as I can tell the time column in query 2 is causing the error, however the same data types exist in query 1 (which runs) with the one difference being the case statement.  To make this more confusing, when I construct a similar case statement for query 2 it will run successfully.  Any ideas what might be the issue?  Thanks!\r\n\r\nQuery 1 (runs successfully)\r\n\r\n```\r\nselect \r\n    floor(__time to day) as __time,\r\n    fail_reason,\r\n    count(primary_id)\r\nfrom (\r\n    select \r\n        max(__time) as __time,\r\n        primary_id,\r\n        case \r\n            when event_name = 'event_name_1' then 'failure'\r\n            when event_name = 'event_name_2' and details_failed_reason = 'failed_reason_1' then 'failed_reason'\r\n            else null\r\n        end as fail_reason\r\n    from data_source\r\n    where 1=1\r\n        and (event_name = 'event_name_1' or details_failed_reason = 'failed_reason_1')\r\n        and floor(__time to day) > current_timestamp - interval '30' day\r\n    group by primary_id, event_name, details_failed_reason\r\n)\r\ngroup by floor(__time to day), fail_reason\r\n```\r\n\r\nQuery 2 (Does not run)\r\n\r\n```\r\nselect\r\n    floor(__time to hour) as __time,\r\n    details_result,\r\n    count(primary_id)\r\nfrom (\r\n    select \r\n        max(__time) as __time,\r\n        primary_id,\r\n        details_result\r\n    from data_soure\r\n    where 1=1\r\n        and event_name = 'event_name_3'\r\n        and floor(__time to day) > current_timestamp - interval '30' day\r\n    group by primary_id,details_result\r\n)\r\ngroup by floor(__time to hour),details_result\r\n```"}, {"user": "xanec", "commits": {}, "labels": ["Bug"], "created": "2017-08-08 23:08:06", "title": "Possible unhandled null value in `ExpressionSelectors`", "url": "https://github.com/apache/druid/issues/4666", "closed": "2017-08-09 18:48:08", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "https://github.com/druid-io/druid/blob/f18cc5df97e5826c2dd8ffafba9fcb69d10a4d44/processing/src/main/java/io/druid/segment/virtual/ExpressionSelectors.java#L108-L112\r\n\r\nThe above call `baseSelector.get().asDouble()` in `ExpressionSelectors.makeDoubleColumnSelector` may result in `NullPointerException` if the returned `ExprEval` contains `null` value.\r\n\r\nNote that in comparison, a similar call in `ExpressionSelectors.makeFloatColumnSelector` is handled as follows:\r\n\r\nhttps://github.com/druid-io/druid/blob/f18cc5df97e5826c2dd8ffafba9fcb69d10a4d44/processing/src/main/java/io/druid/segment/virtual/ExpressionSelectors.java#L83-L87\r\n\r\nI believe this is introduced in #4491 (@b-slim)"}, {"user": "leventov", "commits": {}, "labels": ["Bug"], "created": "2017-08-07 21:38:53", "title": "Bug in TimestampMinAggregatorFactory.combine()", "url": "https://github.com/apache/druid/issues/4658", "closed": "2018-09-18 04:23:22", "ttf": 406.0002777777778, "commitsDetails": [], "body": "It works the same as `TimestampMaxAggregatorFactory`, however it should be the opposite. FYI @sirpkt. Introduced in #3299."}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2017-08-07 16:26:36", "title": "Wrong time report in DruidRequest log", "url": "https://github.com/apache/druid/issues/4655", "closed": "2017-08-07 23:38:19", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "https://github.com/druid-io/druid/pull/3954/files#diff-0df117b4522295642fb19e7684e38a67R354  introduced a change in the timing mechanism of druid request results fixed in https://github.com/druid-io/druid/pull/4561#issuecomment-320686484 . The result is that the reported time is nanoseconds since `<<arbitrary time point>>` applied as nanoseconds since epoch.\r\n\r\nFor most systems this makes a query report that it was issued sometime in the 1970's."}, {"user": "gianm", "commits": {"4ff12e4394b8634b60a927ad4f1d4614b6b598c4": {"commitGHEventType": "referenced", "commitUser": "jon-wei"}, "72b436fc454079962eecf3e843c82cd3854517f5": {"commitGHEventType": "referenced", "commitUser": "jon-wei"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "Hadoop indexing: NPE when intervals not provided", "numCommits": 0, "created": "2017-08-03 21:20:44", "closed": "2017-10-06 05:46:08", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/4647", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 63.000277777777775, "commitsDetails": [{"commitGitStats": [{"filePath": "indexing-hadoop/src/main/java/io/druid/indexer/DetermineHashedPartitionsJob.java", "deletions": 5, "insertions": 15, "lines": 20}, {"filePath": "indexing-hadoop/src/test/java/io/druid/indexer/DetermineHashedPartitionsJobTest.java", "deletions": 7, "insertions": 39, "lines": 46}, {"filePath": "indexing-hadoop/src/test/resources/druid.test.data.with.rows.in.timezone.tsv", "deletions": 0, "insertions": 16, "lines": 16}, {"filePath": "server/src/main/java/io/druid/segment/indexing/granularity/UniformGranularitySpec.java", "deletions": 1, "insertions": 5, "lines": 6}], "commitSpoonAstDiffStats": [{"spoonFilePath": "UniformGranularitySpec.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.indexing.granularity.UniformGranularitySpec.bucketInterval(org.joda.time.DateTime)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "DetermineHashedPartitionsJobTest.java", "spoonMethods": [{"INS": 5, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexer.DetermineHashedPartitionsJobTest.data()", "MOV": 0, "TOT": 6}, {"INS": 5, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexer.DetermineHashedPartitionsJobTest", "MOV": 2, "TOT": 7}]}, {"spoonFilePath": "DetermineHashedPartitionsJob.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexer.DetermineHashedPartitionsJob.DetermineCardinalityReducer.reduce(org.apache.hadoop.io.LongWritable,java.lang.Iterable,io.druid.indexer.Context)", "MOV": 3, "TOT": 4}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexer.DetermineHashedPartitionsJob.DetermineCardinalityReducer", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexer.DetermineHashedPartitionsJob.DetermineCardinalityReducer.run(io.druid.indexer.Context)", "MOV": 1, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexer.DetermineHashedPartitionsJob.DetermineCardinalityReducer.setup(io.druid.indexer.Context)", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2017-10-05 22:46:07", "commitMessage": "Hadoop indexing: Fix NPE when intervals not provided (#4686)\n\n* Fix #4647\r\n\r\n* NPE protect bucketInterval as well\r\n\r\n* Add test to verify timezone as well\r\n\r\n* Also handle case when intervals are already present\r\n\r\n* Fix checkstyle error\r\n\r\n* Use factory method instead for Datetime\r\n\r\n* Use Intervals factory method\r\n", "commitUser": "jon-wei", "commitDateTime": "2017-10-05 22:46:07", "commitParents": ["716a5ec1a8bb66ca480eaf0eb8e3cc68a234d1cc"], "commitGHEventType": "referenced", "nameRev": "4ff12e4394b8634b60a927ad4f1d4614b6b598c4 tags/druid-0.12.0-rc1~138", "commitHash": "4ff12e4394b8634b60a927ad4f1d4614b6b598c4"}, {"commitGitStats": [{"filePath": "indexing-hadoop/src/main/java/io/druid/indexer/DetermineHashedPartitionsJob.java", "deletions": 5, "insertions": 15, "lines": 20}, {"filePath": "indexing-hadoop/src/test/java/io/druid/indexer/DetermineHashedPartitionsJobTest.java", "deletions": 7, "insertions": 39, "lines": 46}, {"filePath": "indexing-hadoop/src/test/resources/druid.test.data.with.rows.in.timezone.tsv", "deletions": 0, "insertions": 16, "lines": 16}, {"filePath": "server/src/main/java/io/druid/segment/indexing/granularity/UniformGranularitySpec.java", "deletions": 1, "insertions": 5, "lines": 6}], "commitSpoonAstDiffStats": [{"spoonFilePath": "UniformGranularitySpec.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.indexing.granularity.UniformGranularitySpec.bucketInterval(org.joda.time.DateTime)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "DetermineHashedPartitionsJobTest.java", "spoonMethods": [{"INS": 5, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexer.DetermineHashedPartitionsJobTest.data()", "MOV": 0, "TOT": 6}, {"INS": 5, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexer.DetermineHashedPartitionsJobTest", "MOV": 2, "TOT": 7}]}, {"spoonFilePath": "DetermineHashedPartitionsJob.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexer.DetermineHashedPartitionsJob.DetermineCardinalityReducer.reduce(org.apache.hadoop.io.LongWritable,java.lang.Iterable,io.druid.indexer.Context)", "MOV": 3, "TOT": 4}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexer.DetermineHashedPartitionsJob.DetermineCardinalityReducer", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexer.DetermineHashedPartitionsJob.DetermineCardinalityReducer.run(io.druid.indexer.Context)", "MOV": 1, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexer.DetermineHashedPartitionsJob.DetermineCardinalityReducer.setup(io.druid.indexer.Context)", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2017-10-10 09:28:36", "commitMessage": "Hadoop indexing: Fix NPE when intervals not provided (#4686) (#4929)\n\n* Fix #4647\r\n\r\n* NPE protect bucketInterval as well\r\n\r\n* Add test to verify timezone as well\r\n\r\n* Also handle case when intervals are already present\r\n\r\n* Fix checkstyle error\r\n\r\n* Use factory method instead for Datetime\r\n\r\n* Use Intervals factory method", "commitUser": "jon-wei", "commitDateTime": "2017-10-10 09:28:36", "commitParents": ["291d9c5830a253069d920ee214519d8ccad2a387"], "commitGHEventType": "referenced", "nameRev": "72b436fc454079962eecf3e843c82cd3854517f5 tags/druid-0.11.0-rc1~9", "commitHash": "72b436fc454079962eecf3e843c82cd3854517f5"}], "body": "The \"determineIntervals\" functionality that gets activated when your granularitySpec doesn't provide intervals, doesn't seem to work anymore. What happens is that in DetermineCardinalityReducer, this line NPEs:\r\n\r\n```\r\n      Optional<Interval> intervalOptional = config.getGranularitySpec().bucketInterval(new DateTime(key.get()));\r\n```\r\n\r\nBecause in UniformGranularitySpec, `bucketInterval` dereferences `wrappedSpec`, which is null when intervals aren't provided. This appears to have been broken since #3674 (0.10.0)."}, {"user": "AlexCharlton", "commits": {"f537c0069a724c55161168ed8c7fc1e1d4b52293": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "SQL: Selecting certain columns results in an exception", "numCommits": 0, "created": "2017-08-02 20:56:17", "closed": "2017-08-22 17:51:08", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/4637", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 19.00027777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "sql/src/test/java/org/apache/druid/sql/avatica/DruidStatementTest.java", "deletions": 0, "insertions": 1, "lines": 1}, {"filePath": "sql/src/test/java/org/apache/druid/sql/calcite/util/CalciteTests.java", "deletions": 7, "insertions": 47, "lines": 54}, {"filePath": "sql/src/test/java/org/apache/druid/sql/avatica/DruidAvaticaHandlerTest.java", "deletions": 0, "insertions": 8, "lines": 8}, {"filePath": "sql/src/test/java/org/apache/druid/sql/calcite/CalciteQueryTest.java", "deletions": 55, "insertions": 47, "lines": 102}, {"filePath": "sql/src/test/java/org/apache/druid/sql/calcite/http/SqlResourceTest.java", "deletions": 15, "insertions": 27, "lines": 42}, {"filePath": "sql/src/main/java/org/apache/druid/sql/calcite/rel/QueryMaker.java", "deletions": 0, "insertions": 15, "lines": 15}], "commitSpoonAstDiffStats": [{"spoonFilePath": "DruidStatementTest.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.avatica.DruidStatementTest.testSignature()", "MOV": 1, "TOT": 3}]}, {"spoonFilePath": "CalciteTests.java", "spoonMethods": [{"INS": 7, "UPD": 6, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.util.CalciteTests", "MOV": 58, "TOT": 71}]}, {"spoonFilePath": "QueryMaker.java", "spoonMethods": [{"INS": 3, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.sql.calcite.rel.QueryMaker.coerce(java.lang.Object,org.apache.calcite.sql.type.SqlTypeName)", "MOV": 7, "TOT": 11}]}, {"spoonFilePath": "SqlResourceTest.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testArrayResultFormat()", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 3, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testCsvResultFormatWithHeaders()", "MOV": 0, "TOT": 3}, {"INS": 2, "UPD": 17, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testObjectResultFormat()", "MOV": 2, "TOT": 21}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testArrayLinesResultFormat()", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testCsvResultFormat()", "MOV": 0, "TOT": 2}, {"INS": 3, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testArrayResultFormatWithHeader()", "MOV": 0, "TOT": 4}, {"INS": 3, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testArrayLinesResultFormatWithHeader()", "MOV": 0, "TOT": 4}, {"INS": 2, "UPD": 19, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testObjectLinesResultFormat()", "MOV": 2, "TOT": 23}, {"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.http.SqlResourceTest.testCannotValidate()", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "DruidAvaticaHandlerTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.avatica.DruidAvaticaHandlerTest.testDatabaseMetaDataColumns()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "CalciteQueryTest.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.CalciteQueryTest", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.CalciteQueryTest.testExplainSelectStar()", "MOV": 1, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.sql.calcite.CalciteQueryTest.testSelectStarOnForbiddenTable()", "MOV": 0, "TOT": 2}, {"INS": 3, "UPD": 0, "DEL": 4, "spoonMethodName": "org.apache.druid.sql.calcite.CalciteQueryTest.testTopNWithSelectAndOrderByProjections()", "MOV": 0, "TOT": 7}, {"INS": 1, "UPD": 0, "DEL": 2, "spoonMethodName": "org.apache.druid.sql.calcite.CalciteQueryTest.testSelectWithProjection()", "MOV": 0, "TOT": 3}, {"INS": 16, "UPD": 0, "DEL": 9, "spoonMethodName": "org.apache.druid.sql.calcite.CalciteQueryTest.testSelectStarWithoutLimitTimeAscending()", "MOV": 0, "TOT": 25}, {"INS": 3, "UPD": 0, "DEL": 4, "spoonMethodName": "org.apache.druid.sql.calcite.CalciteQueryTest.testTopNWithSelectProjections()", "MOV": 0, "TOT": 7}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.sql.calcite.CalciteQueryTest.testInformationSchemaColumnsOnTable()", "MOV": 0, "TOT": 1}, {"INS": 6, "UPD": 0, "DEL": 4, "spoonMethodName": "org.apache.druid.sql.calcite.CalciteQueryTest.testSelectStarWithLimit()", "MOV": 1, "TOT": 11}, {"INS": 7, "UPD": 0, "DEL": 3, "spoonMethodName": "org.apache.druid.sql.calcite.CalciteQueryTest.testSelectStarWithDimFilter()", "MOV": 0, "TOT": 10}, {"INS": 3, "UPD": 0, "DEL": 4, "spoonMethodName": "org.apache.druid.sql.calcite.CalciteQueryTest.testGroupByWithSelectAndOrderByProjections()", "MOV": 0, "TOT": 7}, {"INS": 10, "UPD": 0, "DEL": 3, "spoonMethodName": "org.apache.druid.sql.calcite.CalciteQueryTest.testSelectStar()", "MOV": 2, "TOT": 15}, {"INS": 2, "UPD": 0, "DEL": 3, "spoonMethodName": "org.apache.druid.sql.calcite.CalciteQueryTest.testSelectSingleColumnTwice()", "MOV": 0, "TOT": 5}, {"INS": 6, "UPD": 0, "DEL": 4, "spoonMethodName": "org.apache.druid.sql.calcite.CalciteQueryTest.testSelectStarWithLimitTimeDescending()", "MOV": 0, "TOT": 10}, {"INS": 3, "UPD": 0, "DEL": 4, "spoonMethodName": "org.apache.druid.sql.calcite.CalciteQueryTest.testGroupByWithSelectProjections()", "MOV": 0, "TOT": 7}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-10-15 14:01:21", "commitMessage": "SQL: Support for selecting multi-value dimensions. (#6462)\n\n* SQL: Support for selecting multi-value dimensions.\r\n\r\nFixes #4637. Doesn't completely address everything mentioned in #4638,\r\nbut at least fixes one issue on the way there.\r\n\r\n* Fix null cases in tests.\r\n", "commitUser": "fjy", "commitDateTime": "2018-10-15 14:01:21", "commitParents": ["b57d5df07479174f9a8bbbab7b131040457ce6c6"], "commitGHEventType": "referenced", "nameRev": "f537c0069a724c55161168ed8c7fc1e1d4b52293 tags/druid-0.14.0-incubating~275", "commitHash": "f537c0069a724c55161168ed8c7fc1e1d4b52293"}], "body": "Selecting certain columns from certain SQL queries frequently returns the following error: `Unknown exception: Cannot coerce[java.util.ArrayList] to VARCHAR`\r\n\r\nWhich columns/queries that are affected seems to be non-deterministic, though I can always make a given query work by excluding particular columns. Eg. for one query, I cannot select column X without having an error returned, but for a different query I cannot select column Y. Sometimes a query can work for some time, and then start failing, despite pulling in the exact same set of data. In the case of this error, the columns in question seem to be always empty (determined by cross-referencing the column/query in question over the JSON query API).\r\n\r\nDoing a `select *` very frequently returns this error for me."}, {"user": "gianm", "commits": {}, "labels": ["Area - SQL", "Bug"], "created": "2017-08-02 17:22:35", "title": "SQL: NPE on filtered aggregation with \"NOT IN\"", "url": "https://github.com/apache/druid/issues/4635", "closed": "2018-01-04 20:11:25", "ttf": 155.00027777777777, "commitsDetails": [], "body": "This query throws an NPE during planning.\r\n\r\n```sql\r\nSELECT\r\n  COUNT(*) filter(WHERE dim1 NOT IN ('1'))\r\n  COUNT(dim2) filter(WHERE dim1 NOT IN ('1'))\r\nFROM druid.foo\r\n```\r\n\r\nSeems to be a Calcite bug, https://issues.apache.org/jira/browse/CALCITE-1910.\r\n\r\nThe stack trace:\r\n\r\n```\r\njava.lang.NullPointerException\r\n        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:212)\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:4426)\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$AggConverter.translateAgg(SqlToRelConverter.java:4888)\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$AggConverter.visit(SqlToRelConverter.java:4819)\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$AggConverter.visit(SqlToRelConverter.java:4663)\r\n        at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:137)\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$AggConverter.visit(SqlToRelConverter.java:4795)\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter$AggConverter.visit(SqlToRelConverter.java:4663)\r\n        at org.apache.calcite.sql.SqlNodeList.accept(SqlNodeList.java:153)\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.createAggImpl(SqlToRelConverter.java:2715)\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertAgg(SqlToRelConverter.java:2656)\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:658)\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:620)\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3066)\r\n        at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:556)\r\n```"}, {"user": "jon-wei", "commits": {"a5735bafbcaefd60ea64acdc67665f2f967ddd7d": {"commitGHEventType": "referenced", "commitUser": "leventov"}, "5155b64fa5f080cb6f59e9ea705f27188833c9d2": {"commitGHEventType": "referenced", "commitUser": "jon-wei"}, "7a005088d9878eec4642897ec532e3aea4005c44": {"commitGHEventType": "referenced", "commitUser": "gianm"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0.0, "UPD": 0.0, "DEL": 0.0, "MOV": 0.0, "TOT": 0.0}, "title": "Stack overflow in StringRuntimeShapeInspector", "numCommits": 0, "created": "2017-08-02 01:37:35", "closed": "2017-08-04 19:51:49", "gitStatsSummary": {"deletions": 0.0, "insertions": 0.0, "lines": 0.0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/4628", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 1, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 2.000277777777778, "commitsDetails": [{"commitGitStats": [], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "", "authoredDateTime": "", "commitMessage": "", "commitUser": "leventov", "commitDateTime": "", "commitParents": [], "commitGHEventType": "referenced", "nameRev": "", "commitHash": "a5735bafbcaefd60ea64acdc67665f2f967ddd7d"}, {"commitGitStats": [{"filePath": "processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java", "deletions": 0, "insertions": 41, "lines": 41}, {"filePath": "processing/src/main/java/io/druid/segment/column/SimpleDictionaryEncodedColumn.java", "deletions": 4, "insertions": 4, "lines": 8}, {"filePath": "processing/src/main/java/io/druid/segment/historical/OffsetHolder.java", "deletions": 0, "insertions": 8, "lines": 8}, {"filePath": "processing/src/main/java/io/druid/segment/QueryableIndexStorageAdapter.java", "deletions": 11, "insertions": 29, "lines": 40}, {"filePath": "processing/src/main/java/io/druid/segment/FilteredOffset.java", "deletions": 2, "insertions": 4, "lines": 6}], "commitSpoonAstDiffStats": [{"spoonFilePath": "TopNQueryRunnerTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNQueryRunnerTest.testFullOnTopNBoundFilterAndLongSumMetric()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "FilteredOffset.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.FilteredOffset.CursorOffsetHolderRowOffsetMatcherFactory.makeRowOffsetMatcher(io.druid.collections.bitmap.ImmutableBitmap).1.matches()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.FilteredOffset.CursorOffsetHolderRowOffsetMatcherFactory.makeRowOffsetMatcher(io.druid.collections.bitmap.ImmutableBitmap).2.matches()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.FilteredOffset.CursorOffsetHolderRowOffsetMatcherFactory.makeRowOffsetMatcher(io.druid.collections.bitmap.ImmutableBitmap).2.inspectRuntimeShape(io.druid.query.monomorphicprocessing.RuntimeShapeInspector)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.FilteredOffset.CursorOffsetHolderRowOffsetMatcherFactory.makeRowOffsetMatcher(io.druid.collections.bitmap.ImmutableBitmap).1.inspectRuntimeShape(io.druid.query.monomorphicprocessing.RuntimeShapeInspector)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "OffsetHolder.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.historical.getReadableOffset()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "SimpleDictionaryEncodedColumn.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.column.SimpleDictionaryEncodedColumn.makeDimensionSelector(io.druid.segment.historical.OffsetHolder,io.druid.query.extraction.ExtractionFn).1MultiValueDimensionSelector.inspectRuntimeShape(io.druid.query.monomorphicprocessing.RuntimeShapeInspector)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.column.SimpleDictionaryEncodedColumn.makeDimensionSelector(io.druid.segment.historical.OffsetHolder,io.druid.query.extraction.ExtractionFn).1MultiValueDimensionSelector.getRow()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.column.SimpleDictionaryEncodedColumn.makeDimensionSelector(io.druid.segment.historical.OffsetHolder,io.druid.query.extraction.ExtractionFn).1SingleValueQueryableDimensionSelector.inspectRuntimeShape(io.druid.query.monomorphicprocessing.RuntimeShapeInspector)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.column.SimpleDictionaryEncodedColumn.makeDimensionSelector(io.druid.segment.historical.OffsetHolder,io.druid.query.extraction.ExtractionFn).1SingleValueQueryableDimensionSelector.getRowValue()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "QueryableIndexStorageAdapter.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeObjectColumnSelector(java.lang.String).3.get()", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeLongColumnSelector(java.lang.String).2.get()", "MOV": 1, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.getReadableOffset()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeLongColumnSelector(java.lang.String).2.inspectRuntimeShape(io.druid.query.monomorphicprocessing.RuntimeShapeInspector)", "MOV": 1, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).2", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeObjectColumnSelector(java.lang.String).4.get()", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeFloatColumnSelector(java.lang.String).1.inspectRuntimeShape(io.druid.query.monomorphicprocessing.RuntimeShapeInspector)", "MOV": 0, "TOT": 2}, {"INS": 2, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeObjectColumnSelector(java.lang.String).6.get()", "MOV": 0, "TOT": 3}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).2.reset()", "MOV": 1, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).2.getReadableOffset()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeObjectColumnSelector(java.lang.String).8.get()", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeObjectColumnSelector(java.lang.String).5.get()", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeFloatColumnSelector(java.lang.String).1.get()", "MOV": 1, "TOT": 2}, {"INS": 2, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeObjectColumnSelector(java.lang.String).7.get()", "MOV": 0, "TOT": 3}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2017-08-04 14:44:22", "commitMessage": "Add HistoricalCursor.getReadableOffset() to access unwrapped offset in selectors (#4633) (#4649)\n\n* Add HistoricalCursor.getReadableOffset() to access unwrapped offset in selectors, when the 'main' offset if FilteredOffset (fixes #4628)\r\n\r\n* Stack overflow test", "commitUser": "jon-wei", "commitDateTime": "2017-08-04 14:44:22", "commitParents": ["b139fe9be64f5efde9e6df7d4116e44659cf3a6b"], "commitGHEventType": "referenced", "nameRev": "5155b64fa5f080cb6f59e9ea705f27188833c9d2 tags/druid-0.10.1-rc3~3", "commitHash": "5155b64fa5f080cb6f59e9ea705f27188833c9d2"}, {"commitGitStats": [{"filePath": "processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java", "deletions": 0, "insertions": 41, "lines": 41}, {"filePath": "processing/src/main/java/io/druid/segment/column/SimpleDictionaryEncodedColumn.java", "deletions": 4, "insertions": 4, "lines": 8}, {"filePath": "processing/src/main/java/io/druid/segment/historical/OffsetHolder.java", "deletions": 0, "insertions": 8, "lines": 8}, {"filePath": "processing/src/main/java/io/druid/segment/QueryableIndexStorageAdapter.java", "deletions": 14, "insertions": 32, "lines": 46}, {"filePath": "processing/src/main/java/io/druid/segment/FilteredOffset.java", "deletions": 2, "insertions": 4, "lines": 6}], "commitSpoonAstDiffStats": [{"spoonFilePath": "TopNQueryRunnerTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNQueryRunnerTest.testFullOnTopNBoundFilterAndLongSumMetric()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "FilteredOffset.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.FilteredOffset.CursorOffsetHolderRowOffsetMatcherFactory.makeRowOffsetMatcher(io.druid.collections.bitmap.ImmutableBitmap).1.matches()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.FilteredOffset.CursorOffsetHolderRowOffsetMatcherFactory.makeRowOffsetMatcher(io.druid.collections.bitmap.ImmutableBitmap).2.matches()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.FilteredOffset.CursorOffsetHolderRowOffsetMatcherFactory.makeRowOffsetMatcher(io.druid.collections.bitmap.ImmutableBitmap).2.inspectRuntimeShape(io.druid.query.monomorphicprocessing.RuntimeShapeInspector)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.FilteredOffset.CursorOffsetHolderRowOffsetMatcherFactory.makeRowOffsetMatcher(io.druid.collections.bitmap.ImmutableBitmap).1.inspectRuntimeShape(io.druid.query.monomorphicprocessing.RuntimeShapeInspector)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "OffsetHolder.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.historical.getReadableOffset()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "SimpleDictionaryEncodedColumn.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.column.SimpleDictionaryEncodedColumn.makeDimensionSelector(io.druid.segment.historical.OffsetHolder,io.druid.query.extraction.ExtractionFn).1MultiValueDimensionSelector.inspectRuntimeShape(io.druid.query.monomorphicprocessing.RuntimeShapeInspector)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.column.SimpleDictionaryEncodedColumn.makeDimensionSelector(io.druid.segment.historical.OffsetHolder,io.druid.query.extraction.ExtractionFn).1MultiValueDimensionSelector.getRow()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.column.SimpleDictionaryEncodedColumn.makeDimensionSelector(io.druid.segment.historical.OffsetHolder,io.druid.query.extraction.ExtractionFn).1SingleValueQueryableDimensionSelector.inspectRuntimeShape(io.druid.query.monomorphicprocessing.RuntimeShapeInspector)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.column.SimpleDictionaryEncodedColumn.makeDimensionSelector(io.druid.segment.historical.OffsetHolder,io.druid.query.extraction.ExtractionFn).1SingleValueQueryableDimensionSelector.getRowValue()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "QueryableIndexStorageAdapter.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeDoubleColumnSelector(java.lang.String).2.get()", "MOV": 1, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.getReadableOffset()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeLongColumnSelector(java.lang.String).3.inspectRuntimeShape(io.druid.query.monomorphicprocessing.RuntimeShapeInspector)", "MOV": 1, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeFloatColumnSelector(java.lang.String).1.inspectRuntimeShape(io.druid.query.monomorphicprocessing.RuntimeShapeInspector)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeObjectColumnSelector(java.lang.String).7.get()", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeObjectColumnSelector(java.lang.String).4.get()", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).2.getReadableOffset()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeDoubleColumnSelector(java.lang.String).2.inspectRuntimeShape(io.druid.query.monomorphicprocessing.RuntimeShapeInspector)", "MOV": 1, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeLongColumnSelector(java.lang.String).3.get()", "MOV": 1, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeObjectColumnSelector(java.lang.String).10.get()", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeObjectColumnSelector(java.lang.String).6.get()", "MOV": 0, "TOT": 2}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).2.reset()", "MOV": 1, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).2", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeObjectColumnSelector(java.lang.String).8.get()", "MOV": 0, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeObjectColumnSelector(java.lang.String).5.get()", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeFloatColumnSelector(java.lang.String).1.get()", "MOV": 1, "TOT": 2}, {"INS": 2, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.QueryableIndexStorageAdapter.CursorSequenceBuilder.build().1.apply(org.joda.time.Interval).1QueryableIndexBaseCursor.makeObjectColumnSelector(java.lang.String).9.get()", "MOV": 0, "TOT": 3}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2017-08-04 22:51:48", "commitMessage": "Add HistoricalCursor.getReadableOffset() to access unwrapped offset in selectors (#4633)\n\n* Add HistoricalCursor.getReadableOffset() to access unwrapped offset in selectors, when the 'main' offset if FilteredOffset (fixes #4628)\r\n\r\n* Stack overflow test\r\n", "commitUser": "gianm", "commitDateTime": "2017-08-04 12:51:48", "commitParents": ["8c8759da034490775f32219dff489f507550037e"], "commitGHEventType": "referenced", "nameRev": "7a005088d9878eec4642897ec532e3aea4005c44 tags/druid-0.11.0-rc1~120", "commitHash": "7a005088d9878eec4642897ec532e3aea4005c44"}], "body": "On master and 0.10.1-rc2, I'm seeing a stack overflow when I run the following query on the wikiticker data:\r\n\r\n```\r\n{\r\n  \"queryType\": \"topN\",\r\n  \"dataSource\": \"wikiticker\",\r\n  \"intervals\": \"1000/3000\",\r\n  \"granularity\": \"all\",\r\n  \"context\": {\r\n    \"timeout\": 10000,\r\n    \"useCache\": false,\r\n    \"populateCache\": false\r\n  },\r\n  \"filter\": {\r\n    \"type\": \"bound\",\r\n    \"dimension\": \"commentLength\",\r\n    \"ordering\": \"numeric\",\r\n    \"lower\": 46.64980229268867,\r\n    \"lowerStrict\": true\r\n  },\r\n  \"dimension\": {\r\n    \"type\": \"default\",\r\n    \"dimension\": \"channel\",\r\n    \"outputName\": \"Channel\"\r\n  },\r\n  \"aggregations\": [\r\n    {\r\n      \"name\": \"Count\",\r\n      \"type\": \"longSum\",\r\n      \"fieldName\": \"added\"\r\n    }\r\n  ],\r\n  \"metric\": \"Count\",\r\n  \"threshold\": 5\r\n}\r\n```\r\n\r\n```\r\njava.lang.StackOverflowError\r\n\tat java.lang.StringBuilder.append(StringBuilder.java:136) ~[?:1.8.0_51]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.visit(StringRuntimeShape.java:93) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.visit(StringRuntimeShape.java:145) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.segment.column.IndexedLongsGenericColumn.inspectRuntimeShape(IndexedLongsGenericColumn.java:107) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.appendHotLoopCalleeShape(StringRuntimeShape.java:107) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.visit(StringRuntimeShape.java:95) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.visit(StringRuntimeShape.java:145) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.visit(StringRuntimeShape.java:136) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.segment.QueryableIndexStorageAdapter$CursorSequenceBuilder$1$1QueryableIndexBaseCursor$3.inspectRuntimeShape(QueryableIndexStorageAdapter.java:656) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.appendHotLoopCalleeShape(StringRuntimeShape.java:107) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.visit(StringRuntimeShape.java:95) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.visit(StringRuntimeShape.java:145) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.visit(StringRuntimeShape.java:136) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.segment.filter.Filters$4.inspectRuntimeShape(Filters.java:464) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.appendHotLoopCalleeShape(StringRuntimeShape.java:107) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.visit(StringRuntimeShape.java:95) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.visit(StringRuntimeShape.java:145) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.visit(StringRuntimeShape.java:136) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.segment.FilteredOffset.inspectRuntimeShape(FilteredOffset.java:130) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.appendHotLoopCalleeShape(StringRuntimeShape.java:107) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.visit(StringRuntimeShape.java:95) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.visit(StringRuntimeShape.java:145) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.visit(StringRuntimeShape.java:136) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.segment.QueryableIndexStorageAdapter$CursorSequenceBuilder$1$1QueryableIndexBaseCursor$3.inspectRuntimeShape(QueryableIndexStorageAdapter.java:657) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.appendHotLoopCalleeShape(StringRuntimeShape.java:107) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.visit(StringRuntimeShape.java:95) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.visit(StringRuntimeShape.java:145) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.visit(StringRuntimeShape.java:136) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.segment.filter.Filters$4.inspectRuntimeShape(Filters.java:464) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.appendHotLoopCalleeShape(StringRuntimeShape.java:107) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.visit(StringRuntimeShape.java:95) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.visit(StringRuntimeShape.java:145) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.query.monomorphicprocessing.StringRuntimeShape$Inspector.visit(StringRuntimeShape.java:136) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.segment.FilteredOffset.inspectRuntimeShape(FilteredOffset.java:130) ~[druid-processing-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n```"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2017-07-26 14:59:58", "title": "QTL problem during upgrade to 0.10.x", "url": "https://github.com/apache/druid/issues/4603", "closed": "2017-07-27 04:32:10", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "During the course of upgrading, there is a time where historicals and the coordinator will be on different versions. When the historicals are updated to the latest version, they like to spew the following error:\r\n\r\n```\r\ngot notice to load lookup [LookupExtractorFactoryContainer{version='null', lookupExtractorFactory=io.druid.query.lookup.NamespaceLookupExtractorFactory@*}] that can't replace existing [LookupExtractorFactoryContainer{version='null', lookupExtractorFactory=io.druid.query.lookup.NamespaceLookupExtractorFactory@*\r\n```"}, {"user": "egor-ryashin", "commits": {}, "labels": ["Area - Segment Balancing/Coordination", "Bug"], "created": "2017-07-24 19:19:47", "title": "SegmentListUnusedAction sql query single column range access", "url": "https://github.com/apache/druid/issues/4594", "closed": "2017-07-26 17:54:36", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "Druid **SegmentListUnusedAction** action (**ArchiveTask**) uses a suboptimal SQL query, ie:\r\n```sql\r\nselect count(*) from prod_segments where used = false and start >= '2016-01-01T00:00:00.000Z' and end <= '2016-01-01T04:00:00.000Z' and dataSource = 'source1';\r\n```\r\nQuery specifies to get segments started and ended within a 4 hours interval from _'2016-01-01T00:00:00.000Z'_. In particular 2 range conditions are used on different columns, but not all SQL databases properly optimize it. MySQL can only use an index on one of the columns which can take minutes. Fortunately the query can be optimized by providing explicit knowledge that _'start'_ cannot be greater than _'end'_. \r\n```sql\r\nselect count(*) from prod_segments where used = false and start >= '2016-01-01T00:00:00.000Z' and  start <= '2016-01-01T04:00:00.000Z' and end <= '2016-01-01T04:00:00.000Z' and dataSource = 'source1';\r\n```\r\nWith this change a single _'start'_ column index can be used to do proper range access."}, {"user": "jon-wei", "commits": {}, "labels": ["Area - Operations", "Bug"], "created": "2017-07-21 01:36:28", "title": "Middle manager rolling upgrade issue with TLS support", "url": "https://github.com/apache/druid/issues/4583", "closed": "2017-08-24 20:19:08", "ttf": 34.000277777777775, "commitsDetails": [], "body": "Issue first reported here by @erikdubbelboer:\r\nhttps://github.com/druid-io/druid/issues/4536#issuecomment-316820130\r\n\r\nIf I have a 0.10.0 cluster running, and I upgrade only the middleManager to a version of Druid that includes #4270, batch ingestion tasks fail with the following exception:\r\n\r\n```\r\n2017-07-21T01:29:16,989 WARN [main] io.druid.indexing.common.actions.RemoteTaskActionClient - Exception submitting action for task[index_wikiticker_2017-07-21T01:28:40.434Z]\r\njava.io.IOException: Failed to locate service uri\r\n\tat io.druid.indexing.common.actions.RemoteTaskActionClient.submit(RemoteTaskActionClient.java:94) [druid-indexing-service-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.indexing.common.task.IndexTask.isReady(IndexTask.java:159) [druid-indexing-service-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.indexing.worker.executor.ExecutorLifecycle.start(ExecutorLifecycle.java:169) [druid-indexing-service-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_51]\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_51]\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_51]\r\n\tat java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_51]\r\n\tat io.druid.java.util.common.lifecycle.Lifecycle$AnnotationBasedHandler.start(Lifecycle.java:364) [java-util-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.java.util.common.lifecycle.Lifecycle.start(Lifecycle.java:263) [java-util-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.guice.LifecycleModule$2.start(LifecycleModule.java:156) [druid-api-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.cli.GuiceRunnable.initLifecycle(GuiceRunnable.java:101) [druid-services-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.cli.CliPeon.run(CliPeon.java:283) [druid-services-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.cli.Main.main(Main.java:108) [druid-services-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\nCaused by: java.lang.NullPointerException\r\n\tat io.druid.curator.discovery.ServerDiscoverySelector$1.apply(ServerDiscoverySelector.java:60) ~[druid-server-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.curator.discovery.ServerDiscoverySelector$1.apply(ServerDiscoverySelector.java:52) ~[druid-server-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.curator.discovery.ServerDiscoverySelector.pick(ServerDiscoverySelector.java:108) ~[druid-server-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.indexing.common.actions.RemoteTaskActionClient.getServiceInstance(RemoteTaskActionClient.java:164) ~[druid-indexing-service-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\tat io.druid.indexing.common.actions.RemoteTaskActionClient.submit(RemoteTaskActionClient.java:89) ~[druid-indexing-service-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]\r\n\t... 12 more\r\n```\r\n\r\nThis is caused by `instance.getSslPort()` returning null:\r\n\r\n```\r\n      final int port = instance.getSslPort() >= 0 ? instance.getSslPort() : instance.getPort();\r\n```"}, {"user": "himanshug", "commits": {}, "labels": ["Bug"], "created": "2017-07-17 15:14:38", "title": "Bug in CoordinatorDynamicConfig", "url": "https://github.com/apache/druid/issues/4555", "closed": "2017-07-17 21:34:00", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "https://github.com/druid-io/druid/pull/4141 added watcher to be added as part of CoordinatorDynamicConfig construction which can lead to a deadlock in ConfigManager due to single threaded \"exec\" pool in ConfigManager.\r\n"}, {"user": "dclim", "commits": {}, "labels": ["Bug", "Starter"], "created": "2017-07-06 17:28:13", "title": "NullPointerException when metricsSpec not specified", "url": "https://github.com/apache/druid/issues/4513", "closed": "2017-07-10 22:58:05", "ttf": 4.000277777777778, "commitsDetails": [], "body": "When rollup is disabled, a metricsSpec isn't necessary at ingestion time, but leaving it out altogether results in an NPE. Providing an empty list [] works.\r\n\r\n```\r\n2017-07-06T17:20:31,832 ERROR [task-runner-0-priority-0] io.druid.indexing.overlord.ThreadPoolTaskRunner - Exception while running task[IndexTask{id=index_data-source-rxDw-nometricsspec_2017-07-06T17:20:23.220Z, type=index, dataSource=data-source-rxDw-nometricsspec}]\r\njava.lang.NullPointerException\r\n\tat io.druid.segment.indexing.DataSchema.getParser(DataSchema.java:114) ~[druid-server-0.10.1-SNAPSHOT.jar:0.10.1-SNAPSHOT]\r\n\tat io.druid.indexing.common.task.IndexTask.determineShardSpecs(IndexTask.java:268) ~[druid-indexing-service-0.10.1-SNAPSHOT.jar:0.10.1-SNAPSHOT]\r\n\tat io.druid.indexing.common.task.IndexTask.run(IndexTask.java:187) ~[druid-indexing-service-0.10.1-SNAPSHOT.jar:0.10.1-SNAPSHOT]\r\n\tat io.druid.indexing.overlord.ThreadPoolTaskRunner$ThreadPoolTaskRunnerCallable.call(ThreadPoolTaskRunner.java:436) [druid-indexing-service-0.10.1-SNAPSHOT.jar:0.10.1-SNAPSHOT]\r\n\tat io.druid.indexing.overlord.ThreadPoolTaskRunner$ThreadPoolTaskRunnerCallable.call(ThreadPoolTaskRunner.java:408) [druid-indexing-service-0.10.1-SNAPSHOT.jar:0.10.1-SNAPSHOT]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_131]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]\r\n2017-07-06T17:20:31,856 INFO [task-runner-0-priority-0] io.druid.indexing.overlord.TaskRunnerUtils - Task [index_data-source-rxDw-nometricsspec_2017-07-06T17:20:23.220Z] status changed to [FAILED].\r\n2017-07-06T17:20:31,876 INFO [task-runner-0-priority-0] io.druid.indexing.worker.executor.ExecutorLifecycle - Task completed with status: {\r\n  \"id\" : \"index_data-source-rxDw-nometricsspec_2017-07-06T17:20:23.220Z\",\r\n  \"status\" : \"FAILED\",\r\n  \"duration\" : 229\r\n}\r\n```"}, {"user": "leventov", "commits": {}, "labels": ["Bug"], "created": "2017-06-30 21:58:22", "title": "Possible bug in StringDimensionIndexer", "url": "https://github.com/apache/druid/issues/4493", "closed": "2019-10-02 15:57:28", "ttf": 823.0002777777778, "commitsDetails": [], "body": "https://github.com/druid-io/druid/blob/master/processing/src/main/java/io/druid/segment/StringDimensionIndexer.java#L215\r\n\r\nShould probably be `encodedDimensionValues = new int[]{dimLookup.add(null)};`\r\n\r\n@jon-wei what do you think?"}, {"user": "gianm", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug", "Starter"], "created": "2017-06-29 17:47:26", "title": "Kafka indexing service confused by the distant future", "url": "https://github.com/apache/druid/issues/4485", "closed": "2018-01-08 23:40:51", "ttf": 193.00027777777777, "commitsDetails": [], "body": "Timestamps after `JodaUtils.MAX_INSTANT` cause strange behavior. Example stack trace is below. I'm not exactly sure what went wrong here (probably some kind of overflow?). I think we should simply treat these out of range timestamps as unparseable rows.\r\n\r\n```\r\n2017-06-26T19:23:43,774 INFO [qtp1146045637-133] io.druid.indexing.common.actions.LocalTaskActionClient - Performing action for task[index_kafka_XXX_14759287c3f3b7b_hjehimnb]: SegmentAllocateAction{dataSource='XXX', timestamp=213484092-10-22T10:45:15.392Z, queryGranularity={type=period, period=PT1M, timeZone=UTC, origin=null}, preferredSegmentGranularity={type=period, period=P1D, timeZone=UTC, origin=null}, sequenceName='index_kafka_XXX_14759287c3f3b7b_10', previousSegmentId='XXX_10031072-10-16T00:00:00.000Z_10031072-10-17T00:00:00.000Z_2017-06-22T22:47:52.070Z_1'}\r\n2017-06-26T19:23:43,832 ERROR [qtp1146045637-133] com.sun.jersey.spi.container.ContainerResponse - The RuntimeException could not be mapped to a response, re-throwing to the HTTP container\r\njava.lang.IllegalArgumentException: The end instant must be greater or equal to the start\r\n      at org.joda.time.base.AbstractInterval.checkInterval(AbstractInterval.java:63) ~[joda-time-2.8.2.jar:2.8.2]\r\n      at org.joda.time.base.BaseInterval.<init>(BaseInterval.java:94) ~[joda-time-2.8.2.jar:2.8.2]\r\n      at org.joda.time.Interval.<init>(Interval.java:122) ~[joda-time-2.8.2.jar:2.8.2]\r\n```"}, {"user": "dclim", "commits": {}, "labels": ["Bug"], "created": "2017-06-23 20:18:27", "title": "Compatibility issue with aws-java-sdk:1.10.77 and httpclient:4.5.3", "url": "https://github.com/apache/druid/issues/4456", "closed": "2017-06-24 02:43:49", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "Calls to com.amazonaws.http.AmazonHttpClient.execute() may fail with the following exception:\r\n\r\n```\r\njava.lang.IllegalStateException: Socket not created by this factory\r\n\tat org.apache.http.util.Asserts.check(Asserts.java:34) ~[httpcore-4.4.6.jar:4.4.6]\r\n\tat org.apache.http.conn.ssl.SSLSocketFactory.isSecure(SSLSocketFactory.java:435) ~[httpclient-4.5.3.jar:4.5.3]\r\n\tat org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:186) ~[httpclient-4.5.3.jar:4.5.3]\r\n\tat org.apache.http.impl.conn.ManagedClientConnectionImpl.open(ManagedClientConnectionImpl.java:326) ~[httpclient-4.5.3.jar:4.5.3]\r\n\tat org.apache.http.impl.client.DefaultRequestDirector.tryConnect(DefaultRequestDirector.java:610) ~[httpclient-4.5.3.jar:4.5.3]\r\n\tat org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:445) ~[httpclient-4.5.3.jar:4.5.3]\r\n\tat org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:835) ~[httpclient-4.5.3.jar:4.5.3]\r\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83) ~[httpclient-4.5.3.jar:4.5.3]\r\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56) ~[httpclient-4.5.3.jar:4.5.3]\r\n\tat com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:837) ~[aws-java-sdk-core-1.10.77.jar:?]\r\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:607) ~[aws-java-sdk-core-1.10.77.jar:?]\r\n\tat com.amazonaws.http.AmazonHttpClient.doExecute(AmazonHttpClient.java:376) ~[aws-java-sdk-core-1.10.77.jar:?]\r\n\tat com.amazonaws.http.AmazonHttpClient.executeWithTimer(AmazonHttpClient.java:338) ~[aws-java-sdk-core-1.10.77.jar:?]\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:287) ~[aws-java-sdk-core-1.10.77.jar:?]\r\n```\r\n\r\nI'm seeing issues with the aws-java-sdk in one of our extensions and believe that it might affect the S3 extension as well. I haven't yet seen any issues in S3, but it uses similar method calls so the version bump might have introduced lurking issues. The aws-java-sdk extends `SSLSocketFactory` as `SdkTLSSocketFactory`, and when the factory is asked to create a socket and calls into the incompatible httpclient, it returns a standard `Socket` instead of an `SSLSocket` which causes the assertion in `isSecure` to fail.\r\n\r\nSimilar issues between aws-java-sdk and httpclient have been reported by others:\r\n  - https://groups.google.com/forum/#!topic/dropwizard-user/5-zjo5N32Tg\r\n  - https://github.com/aws/aws-sdk-java/issues/1032\r\n\r\nI'd like to revert to httpclient:4.5.1 and httpcore:4.4.3 unless there's a particular reason these need to be upgraded."}, {"user": "leventov", "commits": {}, "labels": ["Bug"], "created": "2017-06-16 23:19:37", "title": "Remove druid.segmentCache.numLoadingThreads config", "url": "https://github.com/apache/druid/issues/4421", "closed": "2017-09-21 04:27:44", "ttf": 96.00027777777778, "commitsDetails": [], "body": "Because of https://github.com/druid-io/druid/issues/3202#issuecomment-229394962\r\n\r\ncc @drcrallen "}, {"user": "jon-wei", "commits": {}, "labels": ["Bug"], "created": "2017-06-14 20:03:25", "title": "Illegal state exception when running JDBC queries", "url": "https://github.com/apache/druid/issues/4403", "closed": "2017-06-16 23:50:34", "ttf": 2.000277777777778, "commitsDetails": [], "body": "We're seeing an exception on master when running JDBC queries that span multiple frames:\r\n\r\n```\r\n.16:34:06 [http-nio-8700-exec-1] DEBUG o.a.c.avatica.org.apache.http.wire - http-outgoing-0 << \"{\"response\":\"error\",\"exceptions\":[\"java.lang.IllegalStateException: DefaultQueryMetrics must not be modified from multiple threads. If it is needed to gather dimension or metric information from multiple threads or from an async thread, this information should explicitly be passed between threads (e. g. using Futures), or this DefaultQueryMetrics's ownerThread should be reassigned explicitly\\n\\tat io.druid.query.DefaultQueryMetrics.checkModifiedFromOwnerThread(DefaultQueryMetrics.java:56)\\n\\tat io.druid.query.DefaultQueryMetrics.reportMetric(DefaultQueryMetrics.java:232)\\n\\tat io.druid.query.DefaultQueryMetrics.reportCpuTime(DefaultQueryMetrics.java:210)\\n\\tat io.druid.query.CPUTimeMetricQueryRunner$1.after(CPUTimeMetricQueryRunner.java:86)\\n\\tat io.druid.java.util.common.guava.WrappingYielder.close(WrappingYielder.java:96)\\n\\tat io.druid.sql.avatica.DruidStatement.close(DruidStatement.java:289)\\n\\tat io.druid.sql.avatica.DruidStatement.nextFrame(DruidStatement.java:259)\\n\\tat io.druid.sql.avatica.DruidMeta.fetch(DruidMeta.java:205)\\n\\tat org.apache.calcite.avatica.remote.LocalService.apply(LocalService.java:252)\\n\\tat org.apache.calcite.avatica.remote.Service$FetchRequest.accept(Service.java:1350)\\n\\tat org.apache.calcite.avatica.remote.Service$FetchRequest.accept(Service.java:1317)\\n\\tat org.apache.calcite.avatica.remote.AbstractHandler.apply(AbstractHandler.java:95)\\n\\tat org.apache.calcite.avatica.remote.JsonHandler.apply(JsonHandler.java:52)\\n\\tat org.apache.calcite.avatica.server.AvaticaJsonHandler.handle(AvaticaJsonHandler.java:129)\\n\\tat io.druid.sql.avatica.DruidAvaticaHandler.handle(DruidAvaticaHandler.java:63)\\n\\tat org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:534)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:110)\\n\\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\\n\\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\\n\\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\\n\\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\\n\\tat org.ecl\" \r\n```\r\n\r\nPossible cause:\r\n\r\nThe Yielder generated by the ClientQuerySegmentWalker expects to be called only from a single thread ever (by way of the assumption in QueryMetrics), but it's called by multiple threads in the context of the JDBC server. This is because one logical query might be split into multiple frames and each frame could be got by a different http request.\r\n"}, {"user": "gianm", "commits": {"f97c49ba0eed975532185aa2f8f8914f38b518f7": {"commitGHEventType": "closed", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "DefaultQueryMetrics modified by netty worker threads in DirectDruidClient", "numCommits": 0, "created": "2017-05-23 02:14:42", "closed": "2017-05-23 17:07:45", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/4308", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 0.0002777777777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "server/src/main/java/io/druid/client/DirectDruidClient.java", "deletions": 8, "insertions": 17, "lines": 25}], "commitSpoonAstDiffStats": [{"spoonFilePath": "DirectDruidClient.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.client.DirectDruidClient.run(io.druid.query.QueryPlus,java.util.Map).1.acquireResponseMetrics()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.client.DirectDruidClient.run(io.druid.query.QueryPlus,java.util.Map)", "MOV": 2, "TOT": 4}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.client.DirectDruidClient.run(io.druid.query.QueryPlus,java.util.Map).1.handleResponse(org.jboss.netty.handler.codec.http.HttpResponse)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 3, "DEL": 0, "spoonMethodName": "io.druid.client.DirectDruidClient.run(io.druid.query.QueryPlus,java.util.Map).1.done(com.metamx.http.client.response.ClientResponse)", "MOV": 0, "TOT": 4}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.client.DirectDruidClient.run(io.druid.query.QueryPlus,java.util.Map).1", "MOV": 1, "TOT": 2}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2017-05-23 12:07:27", "commitMessage": "Don't use QueryMetrics from multiple threads in DirectDruidClient (fixes #4308) (#4309)\n\n* Don't use QueryMetrics from multiple threads in DirectDruidClient\r\n\r\n* reponseMetrics\r\n", "commitUser": "fjy", "commitDateTime": "2017-05-23 10:07:27", "commitParents": ["2bd4c0930f1d8bd5e4bd73af0f8bf0b62acdb0ed"], "commitGHEventType": "closed", "nameRev": "f97c49ba0eed975532185aa2f8f8914f38b518f7 tags/druid-0.10.1-rc1~62", "commitHash": "f97c49ba0eed975532185aa2f8f8914f38b518f7"}], "body": "Got this stack trace on master today. The caller, `io.druid.client.DirectDruidClient$1.handleResponse`, is running in a netty worker thread for http-client, but the \"owner thread\" of DefaultQueryMetrics is the query server handling thread.\r\n\r\n```\r\njava.lang.IllegalStateException: DefaultQueryMetrics must not be modified from multiple threads. If it is needed to gather dimension or metric information from multiple threads or from an async thread, this information should explicitly be passed between threads (e. g. using Futures), or this DefaultQueryMetrics's ownerThread should be reassigned explicitly\r\n        at io.druid.query.DefaultQueryMetrics.checkModifiedFromOwnerThread(DefaultQueryMetrics.java:56) ~[druid-processing-0.10.1-SNAPSHOT.jar:0.10.1-SNAPSHOT]\r\n        at io.druid.query.DefaultQueryMetrics.reportMetric(DefaultQueryMetrics.java:232) ~[druid-processing-0.10.1-SNAPSHOT.jar:0.10.1-SNAPSHOT]\r\n        at io.druid.query.DefaultQueryMetrics.reportMillisTimeMetric(DefaultQueryMetrics.java:227) ~[druid-processing-0.10.1-SNAPSHOT.jar:0.10.1-SNAPSHOT]\r\n        at io.druid.query.DefaultQueryMetrics.reportNodeTimeToFirstByte(DefaultQueryMetrics.java:216) ~[druid-processing-0.10.1-SNAPSHOT.jar:0.10.1-SNAPSHOT]\r\n        at io.druid.client.DirectDruidClient$1.handleResponse(DirectDruidClient.java:199) ~[druid-server-0.10.1-SNAPSHOT.jar:0.10.1-SNAPSHOT]\r\n        at com.metamx.http.client.NettyHttpClient$1.messageReceived(NettyHttpClient.java:209) [http-client-1.0.6.jar:?]\r\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.handler.timeout.ReadTimeoutHandler.messageReceived(ReadTimeoutHandler.java:184) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.handler.codec.http.HttpClientCodec.handleUpstream(HttpClientCodec.java:92) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) [netty-3.10.4.Final.jar:?]\r\n        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) [netty-3.10.4.Final.jar:?]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_101]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_101]\r\n        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_101]\r\n```"}, {"user": "jerchung", "commits": {}, "labels": ["Bug"], "created": "2017-05-18 22:07:58", "title": "HLL BufferUnderflowException querying realtime indexing tasks", "url": "https://github.com/apache/druid/issues/4296", "closed": "2017-05-23 01:33:04", "ttf": 4.000277777777778, "commitsDetails": [], "body": "I already posted in the druid-user google group about this: [https://groups.google.com/forum/#!topic/druid-user/X71sEX1Ia_8](https://groups.google.com/forum/#!topic/druid-user/X71sEX1Ia_8) but I've decided to also post here since I think it may be an actual issue.\r\n\r\nI believe I'm seeing an issue related to https://github.com/druid-io/druid/issues/3560, even though I'm running the 0.9.2 final release downloaded from http://static.druid.io/artifacts/releases/druid-0.9.2-bin.tar.gz, which should contain the fix (https://github.com/druid-io/druid/pull/3578).\r\n\r\nA little context:  I'm running indexing realtime tasks on middleManagers that have events pushed to them from applications using tranquility.  When I run timeseries queries for intervals that are being indexed on an indexing node, although the queries may initially return data, after a time, if I repeatedly run the queries I'll eventually begin receiving BufferUnderflowException errors from peon nodes. Here is the relevant query and stack trace pulled from the peon task logs\r\n\r\n**Query**\r\n```\r\n{\r\n  \"queryType\": \"timeseries\",\r\n  \"dataSource\": \"memcached_v1\",\r\n  \"intervals\": \"2017-05-18T17:20Z/2017-05-18T18:20Z\",\r\n  \"granularity\": \"all\",\r\n  \"context\": {\r\n    \"timeout\": 120000\r\n  },\r\n  \"aggregations\": [\r\n    {\r\n      \"name\": \"count\",\r\n      \"type\": \"doubleSum\",\r\n      \"fieldName\": \"count\"\r\n    },\r\n    {\r\n      \"name\": \"requests\",\r\n      \"type\": \"hyperUnique\",\r\n      \"fieldName\": \"requests_hll\"\r\n    },\r\n    {\r\n      \"name\": \"unique_keys\",\r\n      \"type\": \"hyperUnique\",\r\n      \"fieldName\": \"unique_keys_hll\"\r\n    },\r\n    {\r\n      \"name\": \"!T_0\",\r\n      \"type\": \"doubleSum\",\r\n      \"fieldName\": \"duration\"\r\n    }\r\n  ],\r\n  \"postAggregations\": [\r\n    {\r\n      \"type\": \"arithmetic\",\r\n      \"fn\": \"/\",\r\n      \"fields\": [\r\n        {\r\n          \"type\": \"fieldAccess\",\r\n          \"fieldName\": \"count\"\r\n        },\r\n        {\r\n          \"type\": \"hyperUniqueCardinality\",\r\n          \"fieldName\": \"requests\"\r\n        }\r\n      ],\r\n      \"name\": \"avg_count_request\"\r\n    },\r\n    {\r\n      \"type\": \"arithmetic\",\r\n      \"fn\": \"/\",\r\n      \"fields\": [\r\n        {\r\n          \"type\": \"fieldAccess\",\r\n          \"fieldName\": \"!T_0\"\r\n        },\r\n        {\r\n          \"type\": \"constant\",\r\n          \"value\": 1000000\r\n        }\r\n      ],\r\n      \"name\": \"duration\"\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n**Stacktrace**\r\n```\r\n2017-05-18T18:21:43,439 ERROR [processing-1] io.druid.query.ChainedExecutionQueryRunner - Exception with one of the sequences!\r\njava.nio.BufferUnderflowException\r\n\tat java.nio.Buffer.nextGetIndex(Buffer.java:506) ~[?:1.8.0_74]\r\n\tat java.nio.DirectByteBuffer.getShort(DirectByteBuffer.java:590) ~[?:1.8.0_74]\r\n\tat io.druid.query.aggregation.hyperloglog.HyperLogLogCollector.fold(HyperLogLogCollector.java:398) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.aggregation.hyperloglog.HyperUniquesAggregator.aggregate(HyperUniquesAggregator.java:48) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.timeseries.TimeseriesQueryEngine$1.apply(TimeseriesQueryEngine.java:73) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.timeseries.TimeseriesQueryEngine$1.apply(TimeseriesQueryEngine.java:57) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.QueryRunnerHelper$1.apply(QueryRunnerHelper.java:80) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.QueryRunnerHelper$1.apply(QueryRunnerHelper.java:75) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat com.metamx.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:39) ~[java-util-0.27.10.jar:?]\r\n\tat com.metamx.common.guava.FilteringAccumulator.accumulate(FilteringAccumulator.java:40) ~[java-util-0.27.10.jar:?]\r\n\tat com.metamx.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:39) ~[java-util-0.27.10.jar:?]\r\n\tat com.metamx.common.guava.BaseSequence.accumulate(BaseSequence.java:67) ~[java-util-0.27.10.jar:?]\r\n\tat com.metamx.common.guava.MappedSequence.accumulate(MappedSequence.java:40) ~[java-util-0.27.10.jar:?]\r\n\tat com.metamx.common.guava.ResourceClosingSequence.accumulate(ResourceClosingSequence.java:38) ~[java-util-0.27.10.jar:?]\r\n\tat com.metamx.common.guava.FilteredSequence.accumulate(FilteredSequence.java:42) ~[java-util-0.27.10.jar:?]\r\n\tat com.metamx.common.guava.MappedSequence.accumulate(MappedSequence.java:40) ~[java-util-0.27.10.jar:?]\r\n\tat com.metamx.common.guava.FilteredSequence.accumulate(FilteredSequence.java:42) ~[java-util-0.27.10.jar:?]\r\n\tat com.metamx.common.guava.ResourceClosingSequence.accumulate(ResourceClosingSequence.java:38) ~[java-util-0.27.10.jar:?]\r\n\tat com.metamx.common.guava.Sequences.toList(Sequences.java:113) ~[java-util-0.27.10.jar:?]\r\n\tat io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:129) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:119) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_74]\r\n\tat com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) ~[guava-16.0.1.jar:?]\r\n\tat java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:134) ~[?:1.8.0_74]\r\n\tat com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:58) ~[guava-16.0.1.jar:?]\r\n\tat io.druid.query.ChainedExecutionQueryRunner$1$1.apply(ChainedExecutionQueryRunner.java:117) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.ChainedExecutionQueryRunner$1$1.apply(ChainedExecutionQueryRunner.java:109) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat com.google.common.collect.Iterators$8.transform(Iterators.java:794) ~[guava-16.0.1.jar:?]\r\n\tat com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48) ~[guava-16.0.1.jar:?]\r\n\tat com.google.common.collect.Iterators.addAll(Iterators.java:357) ~[guava-16.0.1.jar:?]\r\n\tat com.google.common.collect.Lists.newArrayList(Lists.java:147) ~[guava-16.0.1.jar:?]\r\n\tat com.google.common.collect.Lists.newArrayList(Lists.java:129) ~[guava-16.0.1.jar:?]\r\n\tat io.druid.query.ChainedExecutionQueryRunner$1.make(ChainedExecutionQueryRunner.java:105) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat com.metamx.common.guava.BaseSequence.accumulate(BaseSequence.java:64) ~[java-util-0.27.10.jar:?]\r\n\tat io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:104) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:104) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.CPUTimeMetricQueryRunner$1.accumulate(CPUTimeMetricQueryRunner.java:81) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat com.metamx.common.guava.Sequences$1.accumulate(Sequences.java:90) ~[java-util-0.27.10.jar:?]\r\n\tat io.druid.query.spec.SpecificSegmentQueryRunner$2$1.call(SpecificSegmentQueryRunner.java:87) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.spec.SpecificSegmentQueryRunner.doNamed(SpecificSegmentQueryRunner.java:171) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.spec.SpecificSegmentQueryRunner.access$400(SpecificSegmentQueryRunner.java:41) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.spec.SpecificSegmentQueryRunner$2.doItNamed(SpecificSegmentQueryRunner.java:162) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.spec.SpecificSegmentQueryRunner$2.accumulate(SpecificSegmentQueryRunner.java:80) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat com.metamx.common.guava.Sequences.toList(Sequences.java:113) ~[java-util-0.27.10.jar:?]\r\n\tat io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:129) [druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:119) [druid-processing-0.9.2.jar:0.9.2]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_74]\r\n\tat io.druid.query.PrioritizedListenableFutureTask.run(PrioritizedExecutorService.java:271) [druid-processing-0.9.2.jar:0.9.2]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_74]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_74]\r\n\tat java.lang.Thread.run(Thread.java:745) [?:1.8.0_74]\r\n2017-05-18T18:21:43,441 WARN [qtp1259065345-100[timeseries_memcached_v1_858753f9-49f0-4fa6-9916-f60f48a42ad9]] io.druid.server.QueryResource - Exception occurred on request [TimeseriesQuery{dataSource='memcached_v1', querySegmentSpec=MultipleSpecificSegmentSpec{descriptors=[SegmentDescriptor{interval=2017-05-18T18:15:00.000Z/2017-05-18T18:20:00.000Z, version='2017-05-18T18:15:04.604Z', partitionNumber=15}]}, descending=false, dimFilter=null, granularity='AllGranularity', aggregatorSpecs=[DoubleSumAggregatorFactory{fieldName='count', name='count'}, HyperUniquesAggregatorFactory{name='requests', fieldName='requests_hll'}, HyperUniquesAggregatorFactory{name='unique_keys', fieldName='unique_keys_hll'}, DoubleSumAggregatorFactory{fieldName='duration', name='!T_0'}], postAggregatorSpecs=[ArithmeticPostAggregator{name='avg_count_request', fnName='/', fields=[FieldAccessPostAggregator{name='null', fieldName='count'}, io.druid.query.aggregation.hyperloglog.HyperUniqueFinalizingPostAggregator@7d250277], op=DIV}, ArithmeticPostAggregator{name='duration', fnName='/', fields=[FieldAccessPostAggregator{name='null', fieldName='!T_0'}, ConstantPostAggregator{name='null', constantValue=1000000}], op=DIV}], context={finalize=false, queryId=858753f9-49f0-4fa6-9916-f60f48a42ad9, timeout=120000}}]\r\njava.nio.BufferUnderflowException\r\n\tat java.nio.Buffer.nextGetIndex(Buffer.java:506) ~[?:1.8.0_74]\r\n\tat java.nio.DirectByteBuffer.getShort(DirectByteBuffer.java:590) ~[?:1.8.0_74]\r\n\tat io.druid.query.aggregation.hyperloglog.HyperLogLogCollector.fold(HyperLogLogCollector.java:398) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.aggregation.hyperloglog.HyperUniquesAggregator.aggregate(HyperUniquesAggregator.java:48) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.timeseries.TimeseriesQueryEngine$1.apply(TimeseriesQueryEngine.java:73) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.timeseries.TimeseriesQueryEngine$1.apply(TimeseriesQueryEngine.java:57) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.QueryRunnerHelper$1.apply(QueryRunnerHelper.java:80) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.QueryRunnerHelper$1.apply(QueryRunnerHelper.java:75) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat com.metamx.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:39) ~[java-util-0.27.10.jar:?]\r\n\tat com.metamx.common.guava.FilteringAccumulator.accumulate(FilteringAccumulator.java:40) ~[java-util-0.27.10.jar:?]\r\n\tat com.metamx.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:39) ~[java-util-0.27.10.jar:?]\r\n\tat com.metamx.common.guava.BaseSequence.accumulate(BaseSequence.java:67) ~[java-util-0.27.10.jar:?]\r\n\tat com.metamx.common.guava.MappedSequence.accumulate(MappedSequence.java:40) ~[java-util-0.27.10.jar:?]\r\n\tat com.metamx.common.guava.ResourceClosingSequence.accumulate(ResourceClosingSequence.java:38) ~[java-util-0.27.10.jar:?]\r\n\tat com.metamx.common.guava.FilteredSequence.accumulate(FilteredSequence.java:42) ~[java-util-0.27.10.jar:?]\r\n\tat com.metamx.common.guava.MappedSequence.accumulate(MappedSequence.java:40) ~[java-util-0.27.10.jar:?]\r\n\tat com.metamx.common.guava.FilteredSequence.accumulate(FilteredSequence.java:42) ~[java-util-0.27.10.jar:?]\r\n\tat com.metamx.common.guava.ResourceClosingSequence.accumulate(ResourceClosingSequence.java:38) ~[java-util-0.27.10.jar:?]\r\n\tat com.metamx.common.guava.Sequences.toList(Sequences.java:113) ~[java-util-0.27.10.jar:?]\r\n\tat io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:129) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:119) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_74]\r\n\tat com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) ~[guava-16.0.1.jar:?]\r\n\tat java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:134) ~[?:1.8.0_74]\r\n\tat com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:58) ~[guava-16.0.1.jar:?]\r\n\tat io.druid.query.ChainedExecutionQueryRunner$1$1.apply(ChainedExecutionQueryRunner.java:117) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.ChainedExecutionQueryRunner$1$1.apply(ChainedExecutionQueryRunner.java:109) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat com.google.common.collect.Iterators$8.transform(Iterators.java:794) ~[guava-16.0.1.jar:?]\r\n\tat com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48) ~[guava-16.0.1.jar:?]\r\n\tat com.google.common.collect.Iterators.addAll(Iterators.java:357) ~[guava-16.0.1.jar:?]\r\n\tat com.google.common.collect.Lists.newArrayList(Lists.java:147) ~[guava-16.0.1.jar:?]\r\n\tat com.google.common.collect.Lists.newArrayList(Lists.java:129) ~[guava-16.0.1.jar:?]\r\n\tat io.druid.query.ChainedExecutionQueryRunner$1.make(ChainedExecutionQueryRunner.java:105) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat com.metamx.common.guava.BaseSequence.accumulate(BaseSequence.java:64) ~[java-util-0.27.10.jar:?]\r\n\tat io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:104) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:104) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.CPUTimeMetricQueryRunner$1.accumulate(CPUTimeMetricQueryRunner.java:81) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat com.metamx.common.guava.Sequences$1.accumulate(Sequences.java:90) ~[java-util-0.27.10.jar:?]\r\n\tat io.druid.query.spec.SpecificSegmentQueryRunner$2$1.call(SpecificSegmentQueryRunner.java:87) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.spec.SpecificSegmentQueryRunner.doNamed(SpecificSegmentQueryRunner.java:171) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.spec.SpecificSegmentQueryRunner.access$400(SpecificSegmentQueryRunner.java:41) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.spec.SpecificSegmentQueryRunner$2.doItNamed(SpecificSegmentQueryRunner.java:162) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.spec.SpecificSegmentQueryRunner$2.accumulate(SpecificSegmentQueryRunner.java:80) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat com.metamx.common.guava.Sequences.toList(Sequences.java:113) ~[java-util-0.27.10.jar:?]\r\n\tat io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:129) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:119) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_74]\r\n\tat io.druid.query.PrioritizedListenableFutureTask.run(PrioritizedExecutorService.java:271) ~[druid-processing-0.9.2.jar:0.9.2]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_74]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_74]\r\n\tat java.lang.Thread.run(Thread.java:745) [?:1.8.0_74]\r\n```\r\n\r\nMy middleManger configuration is here:\r\n```\r\ndruid.worker.capacity=5\r\ndruid.indexer.task.baseTaskDir=/mnt/druid/0/task/\r\ndruid.indexer.task.restoreTasksOnRestart=true\r\ndruid.indexer.fork.property.druid.processing.numThreads=2\r\ndruid.indexer.fork.property.druid.processing.buffer.sizeBytes=536870912\r\ndruid.segmentCache.locations=[{\"path\":\"/mnt/druid/0/segment_cache\",\"maxSize\":0}]\r\ndruid.indexer.runner.javaOpts=-server -Xmx3221225472 -XX:+UseG1GC -XX:MaxGCPauseMillis=100\r\n```\r\n\r\nA fix I've found is that if I set\r\n`druid.indexer.fork.property.druid.processing.numThreads=1`\r\n\r\nThen I no longer have any issues, which leads to me to think I am running into the race condition described in the linked issue which ends up corrupting the HLL segment.\r\nSetting the processing threads to 1 is not ideal though since I would like the efficiency of having an indexing thread and a query thread per task if possible.\r\n\r\nI also tried upgrading the cluster to 0.10.0 and still ran into this issue as well."}, {"user": "leventov", "commits": {"8ec3a29af03909b81b576e4b9a289dac2c06a505": {"commitGHEventType": "referenced", "commitUser": "himanshug"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "Possible bug with concurrent modification of QueryMetrics", "numCommits": 0, "created": "2017-05-16 16:39:48", "closed": "2017-05-22 18:42:10", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/4279", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 6.000277777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "processing/src/main/java/io/druid/query/metadata/SegmentMetadataQueryRunnerFactory.java", "deletions": 2, "insertions": 6, "lines": 8}, {"filePath": "processing/src/main/java/io/druid/query/AsyncQueryRunner.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "processing/src/main/java/io/druid/query/CPUTimeMetricQueryRunner.java", "deletions": 5, "insertions": 4, "lines": 9}, {"filePath": "processing/src/main/java/io/druid/query/groupby/epinephelinae/GroupByMergingQueryRunnerV2.java", "deletions": 3, "insertions": 5, "lines": 8}, {"filePath": "processing/src/main/java/io/druid/query/topn/DefaultTopNQueryMetrics.java", "deletions": 4, "insertions": 4, "lines": 8}, {"filePath": "processing/src/main/java/io/druid/query/QueryPlus.java", "deletions": 0, "insertions": 30, "lines": 30}, {"filePath": "server/src/main/java/io/druid/segment/realtime/appenderator/SinkQuerySegmentWalker.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "processing/src/main/java/io/druid/query/DefaultQueryMetrics.java", "deletions": 32, "insertions": 58, "lines": 90}, {"filePath": "processing/src/main/java/io/druid/query/groupby/DefaultGroupByQueryMetrics.java", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "processing/src/main/java/io/druid/query/MetricsEmittingQueryRunner.java", "deletions": 11, "insertions": 11, "lines": 22}, {"filePath": "processing/src/main/java/io/druid/query/timeseries/DefaultTimeseriesQueryMetrics.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/main/java/io/druid/query/GroupByMergedQueryRunner.java", "deletions": 4, "insertions": 4, "lines": 8}, {"filePath": "processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java", "deletions": 2, "insertions": 2, "lines": 4}], "commitSpoonAstDiffStats": [{"spoonFilePath": "SinkQuerySegmentWalker.java", "spoonMethods": []}, {"spoonFilePath": "SegmentMetadataQueryRunnerFactory.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.metadata.SegmentMetadataQueryRunnerFactory.mergeRunners(java.util.concurrent.ExecutorService,java.lang.Iterable).2.apply(io.druid.query.QueryRunner).1.run(io.druid.query.QueryPlus,java.util.Map)", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.metadata.SegmentMetadataQueryRunnerFactory.mergeRunners(java.util.concurrent.ExecutorService,java.lang.Iterable).2.apply(io.druid.query.QueryRunner).1.run(io.druid.query.QueryPlus,java.util.Map).1.call()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "GroupByMergedQueryRunner.java", "spoonMethods": [{"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.query.GroupByMergedQueryRunner.run(io.druid.query.QueryPlus,java.util.Map).1.apply(io.druid.query.QueryRunner).1.call()", "MOV": 0, "TOT": 2}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.GroupByMergedQueryRunner.run(io.druid.query.QueryPlus,java.util.Map)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "DefaultGroupByQueryMetrics.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.groupby.DefaultGroupByQueryMetrics.numComplexMetrics(io.druid.query.groupby.GroupByQuery)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.groupby.DefaultGroupByQueryMetrics.numDimensions(io.druid.query.groupby.GroupByQuery)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.groupby.DefaultGroupByQueryMetrics.numMetrics(io.druid.query.groupby.GroupByQuery)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "DefaultTimeseriesQueryMetrics.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.timeseries.DefaultTimeseriesQueryMetrics.numMetrics(io.druid.query.timeseries.TimeseriesQuery)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.timeseries.DefaultTimeseriesQueryMetrics.numComplexMetrics(io.druid.query.timeseries.TimeseriesQuery)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "DefaultQueryMetrics.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.duration(io.druid.query.Query)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.queryId(io.druid.query.Query)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.chunkInterval(org.joda.time.Interval)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.status(java.lang.String)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.server(java.lang.String)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.remoteAddress(java.lang.String)", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.reportSegmentTime(long)", "MOV": 2, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.reportMetric(java.lang.String,java.lang.Number)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.reportQueryBytes(long)", "MOV": 3, "TOT": 6}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.queryType(io.druid.query.Query)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.context(io.druid.query.Query)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.dataSource(io.druid.query.Query)", "MOV": 1, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.checkModifiedFromOwnerThread()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.setDimension(java.lang.String,java.lang.String)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.defaultTimeMetric(java.lang.String,long)", "MOV": 6, "TOT": 7}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.reportNodeTime(long)", "MOV": 2, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.hasFilters(io.druid.query.Query)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.reportMillisTimeMetric(java.lang.String,long)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.emit(com.metamx.emitter.service.ServiceEmitter)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.reportNodeBytes(long)", "MOV": 5, "TOT": 7}, {"INS": 1, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.reportCpuTime(long)", "MOV": 2, "TOT": 5}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.reportNodeTimeToFirstByte(long)", "MOV": 2, "TOT": 3}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.reportSegmentAndCacheTime(long)", "MOV": 2, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.interval(io.druid.query.Query)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.reportWaitTime(long)", "MOV": 2, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.segment(java.lang.String)", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.reportQueryTime(long)", "MOV": 2, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.success(boolean)", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.DefaultQueryMetrics.reportIntervalChunkTime(long)", "MOV": 2, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.DefaultQueryMetrics", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "MetricsEmittingQueryRunner.java", "spoonMethods": []}, {"spoonFilePath": "AsyncQueryRunner.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.AsyncQueryRunner.run(io.druid.query.QueryPlus,java.util.Map)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.AsyncQueryRunner.run(io.druid.query.QueryPlus,java.util.Map).1.call()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "ChainedExecutionQueryRunner.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.ChainedExecutionQueryRunner.run(io.druid.query.QueryPlus,java.util.Map)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.ChainedExecutionQueryRunner.run(io.druid.query.QueryPlus,java.util.Map).<unknown>.make().<unknown>.apply(QueryRunner).<unknown>.call()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "DefaultTopNQueryMetrics.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.DefaultTopNQueryMetrics.threshold(io.druid.query.topn.TopNQuery)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.DefaultTopNQueryMetrics.numComplexMetrics(io.druid.query.topn.TopNQuery)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.DefaultTopNQueryMetrics.numMetrics(io.druid.query.topn.TopNQuery)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.DefaultTopNQueryMetrics.dimension(io.druid.query.topn.TopNQuery)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "QueryPlus.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.QueryPlus.withoutQueryMetrics()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.QueryPlus.withoutThreadUnsafeState()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "GroupByMergingQueryRunnerV2.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.groupby.epinephelinae.GroupByMergingQueryRunnerV2.run(io.druid.query.QueryPlus,java.util.Map)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "CPUTimeMetricQueryRunner.java", "spoonMethods": []}], "spoonStatsSkippedReason": "", "authoredDateTime": "2017-05-22 13:42:09", "commitMessage": "Don't pass QueryMetrics down in concurrent and async QueryRunners (fixes #4279) (#4288)\n\n* Don't pass QueryMetrics down in concurrent and async QueryRunners\r\n\r\n* Rename QueryPlus.threadSafe() to withoutThreadUnsafeState(); Update QueryPlus.withQueryMetrics() Javadocs; Fix generics in MetricsEmittingQueryRunner and CpuTimeMetricQueryRunner; Make DefaultQueryMetrics to fail fast on modifications from concurrent threads\r\n", "commitUser": "himanshug", "commitDateTime": "2017-05-22 13:42:09", "commitParents": ["000b0ffed7ceded2361917918727f78485489d7e"], "commitGHEventType": "referenced", "nameRev": "8ec3a29af03909b81b576e4b9a289dac2c06a505 tags/druid-0.10.1-rc1~69", "commitHash": "8ec3a29af03909b81b576e4b9a289dac2c06a505"}], "body": "Reported here: https://github.com/druid-io/druid/pull/3954#issuecomment-301583481\r\n\r\nPossible fix: https://github.com/druid-io/druid/files/1002579/queryPlus.txt\r\n\r\nSimilar to #3803\r\n\r\n@pjain1 "}, {"user": "cesure", "commits": {}, "labels": ["Bug"], "created": "2017-05-16 13:44:07", "title": "Emitted metrics constantly growing", "url": "https://github.com/apache/druid/issues/4278", "closed": "2017-05-18 23:11:03", "ttf": 2.000277777777778, "commitsDetails": [], "body": "Hi!\r\n\r\nWe started testing Druid 0.10.0 on our dev cluster and found a possible bug.\r\n\r\nWe are using the Http Emitter Module to log the metrics. Since 0.10.0 the messages sent to our HTTP API are constantly growing the longer the single nodes are running. There seems to be a problem with the \"namespace/deltaTasksStarted\" metric which is multiple times in the JSON with different timestamps. The longer the node runs (broker in this case but happens on historical nodes too) the more deltaTasksStarted entries there are per emitted message. See attached a sample JSON.\r\n\r\n[druid_broker_metrics.json.txt](https://github.com/druid-io/druid/files/1004598/druid_broker_metrics.json.txt)\r\n\r\nPlease let me know if you need further details.\r\n"}, {"user": "elloooooo", "commits": {}, "labels": ["Bug"], "created": "2017-05-16 04:56:37", "title": "The behaviour of period granularity in query is different from what is described  in doc in 0.10.0", "url": "https://github.com/apache/druid/issues/4276", "closed": "2017-06-02 21:17:03", "ttf": 17.00027777777778, "commitsDetails": [], "body": "In doc Period Granularities are described as:\r\n1. They support specifying a time zone which determines where period boundaries start\r\n2. as well as the timezone of the returned timestamps\r\n\r\nI run druid in UTC time. In my timeseries query,  \r\nThis is intervals `\"intervals\":[\"2017-05-14T00:00:00.000+08:00/2017-05-16T00:00:00.000+08:00\"]`, this is granularity `\"granularity\":{\"period\":\"P1D\",\"timeZone\":\"Asia/Shanghai\",\"type\":\"period\"}`,\r\nAnd this is what I got\r\n\r\n> [\r\n     {\r\n       \"timestamp\": \"2017-05-13T16:00:00.000Z\",\r\n       \"result\": {\r\n       \"error_count\": 6154866,\r\n       \"sum_count\": 353336733\r\n    }\r\n  },\r\n  {\r\n    \"timestamp\": \"2017-05-14T16:00:00.000Z\",\r\n    \"result\": {\r\n      \"error_count\": 6115738,\r\n      \"sum_count\": 357000114\r\n    }\r\n  }\r\n]\r\n\r\nThe result is correct according to the first description as it start the time bucket from UTC 16:00:00, but according to second description, the timestamp in result are supposed to `2017-05-14T00:00:00.000+08:00` and `2017-05-15T00:00:00.000+08:00` and they are in 0.9.2.\r\n\r\nSo, is it a bug to fix?\r\n\r\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2017-05-04 03:06:22", "title": "Overlord fails to lead and all get stuck in nasty failure loop requiring restart.", "url": "https://github.com/apache/druid/issues/4246", "closed": "2018-01-09 19:33:48", "ttf": 250.00027777777777, "commitsDetails": [], "body": "We had the following exception occur in all overlords at the same time.\r\n\r\n```\r\nFailed to lead: {class=io.druid.indexing.overlord.TaskMaster, exceptionType=class java.lang.reflect.InvocationTargetException, exceptionMessage=null}\r\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler.start(Lifecycle.java:350)\r\n\tat com.metamx.common.lifecycle.Lifecycle.start(Lifecycle.java:259)\r\n\tat io.druid.indexing.overlord.TaskMaster$1.takeLeadership(TaskMaster.java:141)\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector$WrappedListener.takeLeadership(LeaderSelector.java:534)\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:399)\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:441)\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:64)\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:245)\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:239)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@1925ca4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@6928fd46[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 151]\r\n\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)\r\n\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:326)\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533)\r\n\tat com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:159)\r\n\tat com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:135)\r\n\tat com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:121)\r\n\tat io.druid.indexing.overlord.autoscaling.AbstractWorkerResourceManagementStrategy.startManagement(AbstractWorkerResourceManagementStrategy.java:63)\r\n\tat io.druid.indexing.overlord.autoscaling.AbstractWorkerResourceManagementStrategy.startManagement(AbstractWorkerResourceManagementStrategy.java:34)\r\n\tat io.druid.indexing.overlord.RemoteTaskRunner.start(RemoteTaskRunner.java:312)\r\n\t... 19 more\r\n```\r\n\r\nand\r\n\r\n```\r\nFailed to lead: {class=io.druid.indexing.overlord.TaskMaster, exceptionType=class java.lang.reflect.InvocationTargetException, exceptionMessage=null}\r\nsun.reflect.GeneratedMethodAccessor155.invoke(Unknown Source)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler.start(Lifecycle.java:350)\r\n        at com.metamx.common.lifecycle.Lifecycle.start(Lifecycle.java:259)\r\n        at io.druid.indexing.overlord.TaskMaster$1.takeLeadership(TaskMaster.java:141)\r\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$WrappedListener.takeLeadership(LeaderSelector.java:534)\r\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:399)\r\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:441)\r\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:64)\r\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:245)\r\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:239)\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n        at java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@2928a0b1 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@6928fd46[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 151]\r\n        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)\r\n        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:326)\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533)\r\n        at com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:159)\r\n        at com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:135)\r\n        at com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:121)\r\n        at io.druid.indexing.overlord.autoscaling.AbstractWorkerResourceManagementStrategy.startManagement(AbstractWorkerResourceManagementStrategy.java:63)\r\n        at io.druid.indexing.overlord.autoscaling.AbstractWorkerResourceManagementStrategy.startManagement(AbstractWorkerResourceManagementStrategy.java:34)\r\n        at io.druid.indexing.overlord.RemoteTaskRunner.start(RemoteTaskRunner.java:312)\r\n        ... 18 more\r\n```\r\n\r\nSomehow the overlords ended up in this state and just kept repeating this failure mode. It required a restart of them all to overcome. "}, {"user": "nishantmonu51", "commits": {}, "labels": ["Bug", "Discuss"], "created": "2017-04-27 15:44:47", "title": "QueryGranularity incompatible behavior w.r.t timezone in 0.10.0 ", "url": "https://github.com/apache/druid/issues/4218", "closed": "2018-09-17 22:47:25", "ttf": 508.0002777777778, "commitsDetails": [], "body": "In 0.9.x queryGranularity when represented as string was defaulting to use DurationGranularity. Now after the changes in https://github.com/druid-io/druid/pull/3850, it is now defaulted to use PeriodGranularity instead. This change is incompatible w.r.t timezone as both implementations behave differently based on the user timezone. \r\n\r\nSample code -  \r\n```\r\n    DateTimeZone.setDefault(DateTimeZone.forOffsetHours(5));\r\n    PeriodGranularity periodGranularity = new PeriodGranularity(new Period(\"P2D\"), null, null);\r\n    DurationGranularity durationGranularity = new DurationGranularity(86400000, 0);\r\n    System.out.println(jsonMapper.writeValueAsString(periodGranularity));\r\n    System.out.println(jsonMapper.writeValueAsString(durationGranularity));\r\n```\r\n\r\noutput - \r\n```\r\n{\"type\":\"period\",\"period\":\"P2D\",\"timeZone\":\"UTC\",\"origin\":null}\r\n{\"type\":\"duration\",\"duration\":86400000,\"origin\":\"1970-01-01T05:00:00.000+05:00\"}\r\n```\r\n\r\nNotice the origin in both the versions, PeriodGranularity defaults to UTC timezone if no timezone is specified explicitly while DurationGranularity defaults to the user timezone. "}, {"user": "gianm", "commits": {"43051829f2d2272c6b1e9c6f2bf38a9252101fa3": {"commitGHEventType": "referenced", "commitUser": "leventov"}}, "changesInPackagesGIT": [], "labels": ["Area - SQL", "Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "SQL: OOME while planning semi-join with inner sort", "numCommits": 0, "created": "2017-04-25 05:24:18", "closed": "2017-10-17 20:54:01", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/4208", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 175.00027777777777, "commitsDetails": [{"commitGitStats": [{"filePath": "sql/src/test/java/io/druid/sql/calcite/CalciteQueryTest.java", "deletions": 1, "insertions": 54, "lines": 55}], "commitSpoonAstDiffStats": [{"spoonFilePath": "CalciteQueryTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.CalciteQueryTest.testUsingSubqueryAsFilterWithInnerSort()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.sql.calcite.CalciteQueryTest.newScanQueryBuilder()", "MOV": 1, "TOT": 2}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2017-10-17 13:54:00", "commitMessage": "Regression test for #4208. (#4968)\n\n", "commitUser": "leventov", "commitDateTime": "2017-10-17 15:54:00", "commitParents": ["af2bc5f814d09ebad54be64c0a461af0aae24203"], "commitGHEventType": "referenced", "nameRev": "43051829f2d2272c6b1e9c6f2bf38a9252101fa3 tags/druid-0.12.0-rc1~109", "commitHash": "43051829f2d2272c6b1e9c6f2bf38a9252101fa3"}], "body": "In CalciteQueryTest, the following will OOME the planner. Without the \"ORDER BY\" it's fine.\r\n\r\n```sql\r\nSELECT dim1, dim2 FROM druid.foo\r\nWHERE dim2 IN (\r\n  SELECT dim2\r\n  FROM druid.foo\r\n  GROUP BY dim2\r\n  ORDER BY dim2\r\n)\r\n```"}, {"user": "gianm", "commits": {"b20e3038b6a14332185f538f11fe1146f75b44d1": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Area - SQL", "Bug"], "spoonStatsSummary": {}, "title": "SQL: IN filters do not work with >= 20 elements", "numCommits": 0, "created": "2017-04-25 02:01:49", "closed": "2017-10-10 19:44:06", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "filteredCommits": [], "url": "https://github.com/apache/druid/issues/4203", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 168.00027777777777, "commitsDetails": [{"commitGitStats": [{"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/{ => builtin}/TimeFloorOperatorConversion.java", "deletions": 1, "insertions": 6, "lines": 7}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/SqlAggregator.java", "deletions": 12, "insertions": 13, "lines": 25}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/schema/InformationSchema.java", "deletions": 3, "insertions": 4, "lines": 7}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/builtin/CastOperatorConversion.java", "deletions": 0, "insertions": 191, "lines": 191}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/{ => builtin}/ApproxCountDistinctSqlAggregator.java", "deletions": 19, "insertions": 22, "lines": 41}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/builtin/SumSqlAggregator.java", "deletions": 0, "insertions": 118, "lines": 118}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/{ => builtin}/TimeParseOperatorConversion.java", "deletions": 1, "insertions": 4, "lines": 5}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/Aggregation.java", "deletions": 2, "insertions": 27, "lines": 29}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/Expressions.java", "deletions": 184, "insertions": 2, "lines": 186}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/planner/DruidOperatorTable.java", "deletions": 27, "insertions": 251, "lines": 278}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/{ => builtin}/LTrimOperatorConversion.java", "deletions": 1, "insertions": 4, "lines": 5}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/builtin/SumZeroSqlAggregator.java", "deletions": 0, "insertions": 32, "lines": 32}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/DirectOperatorConversion.java", "deletions": 0, "insertions": 58, "lines": 58}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/builtin/CountSqlAggregator.java", "deletions": 0, "insertions": 123, "lines": 123}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/{ => builtin}/LookupOperatorConversion.java", "deletions": 1, "insertions": 4, "lines": 5}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/{ => builtin}/TimestampToMillisOperatorConversion.java", "deletions": 1, "insertions": 5, "lines": 6}, {"filePath": "sql/src/test/java/io/druid/sql/calcite/util/CalciteTests.java", "deletions": 13, "insertions": 4, "lines": 17}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/{ => builtin}/FloorOperatorConversion.java", "deletions": 1, "insertions": 5, "lines": 6}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/table/DruidTable.java", "deletions": 0, "insertions": 20, "lines": 20}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/planner/Calcites.java", "deletions": 13, "insertions": 61, "lines": 74}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/{ => builtin}/TrimOperatorConversion.java", "deletions": 1, "insertions": 4, "lines": 5}, {"filePath": "sql/src/test/java/io/druid/sql/calcite/expression/ExpressionsTest.java", "deletions": 7, "insertions": 187, "lines": 194}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/UnaryPrefixOperatorConversion.java", "deletions": 0, "insertions": 66, "lines": 66}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/UnarySuffixOperatorConversion.java", "deletions": 0, "insertions": 66, "lines": 66}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/planner/PlannerFactory.java", "deletions": 4, "insertions": 38, "lines": 42}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/AliasedOperatorConversion.java", "deletions": 0, "insertions": 71, "lines": 71}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/BinaryOperatorConversion.java", "deletions": 0, "insertions": 73, "lines": 73}, {"filePath": "docs/content/misc/math-expr.md", "deletions": 0, "insertions": 1, "lines": 1}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/builtin/StrposOperatorConversion.java", "deletions": 0, "insertions": 70, "lines": 70}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/builtin/MinSqlAggregator.java", "deletions": 0, "insertions": 118, "lines": 118}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/{ => builtin}/TimeShiftOperatorConversion.java", "deletions": 1, "insertions": 4, "lines": 5}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/{ => builtin}/BTrimOperatorConversion.java", "deletions": 1, "insertions": 4, "lines": 5}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/{ => builtin}/CeilOperatorConversion.java", "deletions": 1, "insertions": 5, "lines": 6}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/planner/DruidConformance.java", "deletions": 0, "insertions": 20, "lines": 20}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/{ => builtin}/TimeFormatOperatorConversion.java", "deletions": 1, "insertions": 7, "lines": 8}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/planner/DruidConvertletTable.java", "deletions": 27, "insertions": 77, "lines": 104}, {"filePath": "processing/src/main/java/io/druid/query/expression/TimestampParseExprMacro.java", "deletions": 1, "insertions": 6, "lines": 7}, {"filePath": "extensions-core/histogram/src/main/java/io/druid/query/aggregation/histogram/sql/QuantileSqlAggregator.java", "deletions": 47, "insertions": 43, "lines": 90}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/rule/GroupByRules.java", "deletions": 250, "insertions": 65, "lines": 315}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/planner/Rules.java", "deletions": 2, "insertions": 45, "lines": 47}, {"filePath": "common/src/main/java/io/druid/math/expr/Function.java", "deletions": 0, "insertions": 22, "lines": 22}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/Aggregations.java", "deletions": 25, "insertions": 42, "lines": 67}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/{ => builtin}/TimeArithmeticOperatorConversion.java", "deletions": 1, "insertions": 4, "lines": 5}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/builtin/TruncateOperatorConversion.java", "deletions": 0, "insertions": 91, "lines": 91}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/{ => builtin}/ExtractOperatorConversion.java", "deletions": 1, "insertions": 4, "lines": 5}, {"filePath": "common/src/test/java/io/druid/math/expr/FunctionTest.java", "deletions": 0, "insertions": 8, "lines": 8}, {"filePath": "sql/src/main/java/io/druid/sql/guice/SqlModule.java", "deletions": 54, "insertions": 6, "lines": 60}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/builtin/ReinterpretOperatorConversion.java", "deletions": 0, "insertions": 32, "lines": 32}, {"filePath": "sql/src/test/java/io/druid/sql/calcite/CalciteQueryTest.java", "deletions": 63, "insertions": 274, "lines": 337}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/builtin/AvgSqlAggregator.java", "deletions": 0, "insertions": 129, "lines": 129}, {"filePath": "docs/content/querying/sql.md", "deletions": 4, "insertions": 17, "lines": 21}, {"filePath": "pom.xml", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/planner/DruidRexExecutor.java", "deletions": 13, "insertions": 36, "lines": 49}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/{ => builtin}/TimeExtractOperatorConversion.java", "deletions": 1, "insertions": 5, "lines": 6}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/aggregation/builtin/MaxSqlAggregator.java", "deletions": 0, "insertions": 118, "lines": 118}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/planner/DruidTypeSystem.java", "deletions": 0, "insertions": 31, "lines": 31}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/{ => builtin}/MillisToTimestampOperatorConversion.java", "deletions": 1, "insertions": 5, "lines": 6}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/{ => builtin}/RTrimOperatorConversion.java", "deletions": 1, "insertions": 4, "lines": 5}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/{ => builtin}/SubstringOperatorConversion.java", "deletions": 3, "insertions": 18, "lines": 21}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/builtin/DateTruncOperatorConversion.java", "deletions": 0, "insertions": 121, "lines": 121}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/OperatorConversions.java", "deletions": 0, "insertions": 6, "lines": 6}, {"filePath": "sql/src/main/java/io/druid/sql/calcite/expression/{ => builtin}/RegexpExtractOperatorConversion.java", "deletions": 1, "insertions": 4, "lines": 5}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "tooManyFiles", "authoredDateTime": "2017-10-10 12:44:05", "commitMessage": "SQL: Upgrade to Calcite 1.14.0, some refactoring of internals. (#4889)\n\n* SQL: Upgrade to Calcite 1.14.0, some refactoring of internals.\r\n\r\nThis brings benefits:\r\n- Ability to do GROUP BY and ORDER BY with ordinals.\r\n- Ability to support IN filters beyond 19 elements (fixes #4203).\r\n\r\nSome refactoring of druid-sql internals:\r\n- Builtin aggregators and operators are implemented as SqlAggregators\r\n  and SqlOperatorConversions rather being special cases. This simplifies\r\n  the Expressions and GroupByRules code, which were becoming complex.\r\n- SqlAggregator implementations are no longer responsible for filtering.\r\n\r\nAdded new functions:\r\n- Expressions: strpos.\r\n- SQL: TRUNCATE, TRUNC, LENGTH, CHAR_LENGTH, STRLEN, STRPOS, SUBSTR,\r\n  and DATE_TRUNC.\r\n\r\n* Add missing @Override annotation.\r\n\r\n* Adjustments for forbidden APIs.\r\n\r\n* Adjustments for forbidden APIs.\r\n\r\n* Disable GROUP BY alias.\r\n\r\n* Doc reword.\r\n", "commitUser": "fjy", "commitDateTime": "2017-10-10 12:44:05", "commitParents": ["4e1d0f49d89543a078903cdd30b66cb458374c38"], "commitGHEventType": "referenced", "nameRev": "b20e3038b6a14332185f538f11fe1146f75b44d1 tags/druid-0.12.0-rc1~128", "commitHash": "b20e3038b6a14332185f538f11fe1146f75b44d1"}], "body": "This is because Calcite's SqlToRelConverter converts INs of <20 elements to ORs (which then convert back into a Druid InDimFilter) and INs of >= 20 elements to a join on an inline table (which we can't plan)."}, {"user": "kaijianding", "commits": {}, "labels": ["Bug"], "created": "2017-04-24 06:28:45", "title": "BufferUnderflowException when using HLL in TopN query", "url": "https://github.com/apache/druid/issues/4199", "closed": "2017-05-23 01:33:30", "ttf": 28.00027777777778, "commitsDetails": [], "body": "I found similar issue reported at https://github.com/druid-io/druid/issues/3560  it's marked as fixed in 0.9.2\r\nBut I still met this issue in 0.9.2, I wonder is it still happening in 0.10.0?\r\nHere is the trace from historical log:\r\n\r\n> , exception=java.nio.BufferUnderflowException, query=TopNQuery{dataSource='dws_wl_hash_online', dimensionSpec=DefaultDimensionSpec{dimension='query', outputName='query'}, topNMetricSpec=NumericTopNMetricSpec{metric='PV'}, threshold=1000, querySegmentSpec=MultipleSpecificSegmentSpec{descriptors=[SegmentDescriptor{interval=2017-04-02T00:00:00.000+08:00/2017-04-02T23:59:59.999+08:00, version='2017-04-03T17:47:15.029+08:00', partitionNumber=48}, SegmentDescriptor{interval=2017-04-02T00:00:00.000+08:00/2017-04-02T23:59:59.999+08:00, version='2017-04-03T17:47:15.029+08:00', partitionNumber=157}, SegmentDescriptor{interval=2017-04-05T00:00:00.000+08:00/2017-04-05T23:59:59.999+08:00, version='2017-04-06T19:17:15.467+08:00', partitionNumber=61}, SegmentDescriptor{interval=2017-04-06T00:00:00.000+08:00/2017-04-06T23:59:59.999+08:00, version='2017-04-07T17:17:16.054+08:00', partitionNumber=163}, SegmentDescriptor{interval=2017-04-09T00:00:00.000+08:00/2017-04-09T23:59:59.999+08:00, version='2017-04-10T18:47:15.369+08:00', partitionNumber=120}, SegmentDescriptor{interval=2017-04-11T00:00:00.000+08:00/2017-04-11T23:59:59.999+08:00, version='2017-04-12T20:47:13.851+08:00', partitionNumber=98}, SegmentDescriptor{interval=2017-04-11T00:00:00.000+08:00/2017-04-11T23:59:59.999+08:00, version='2017-04-12T20:47:13.851+08:00', partitionNumber=120}, SegmentDescriptor{interval=2017-04-12T00:00:00.000+08:00/2017-04-12T23:59:59.999+08:00, version='2017-04-13T19:10:41.400+08:00', partitionNumber=142}, SegmentDescriptor{interval=2017-04-14T00:00:00.000+08:00/2017-04-14T23:59:59.999+08:00, version='2017-04-16T01:47:15.476+08:00', partitionNumber=52}, SegmentDescriptor{interval=2017-04-16T00:00:00.000+08:00/2017-04-16T23:59:59.999+08:00, version='2017-04-18T21:17:13.746+08:00', partitionNumber=224}, SegmentDescriptor{interval=2017-04-18T00:00:00.000+08:00/2017-04-18T23:59:59.999+08:00, version='2017-04-19T20:47:14.844+08:00', partitionNumber=72}]}, dimFilter=((stat_date = 20170331 || stat_date = 20170401 || stat_date = 20170402 || stat_date = 20170403 || stat_date = 20170404 || stat_date = 20170405 || stat_date = 20170406 || stat_date = 20170407 || stat_date = 20170408 || stat_date = 20170409 || stat_date = 20170410 || stat_date = 20170411 || stat_date = 20170412 || stat_date = 20170413 || stat_date = 20170414 || stat_date = 20170415 || stat_date = 20170416 || stat_date = 20170417 || stat_date = 20170418) && !query =  && cate_id = 350213), granularity='AllGranularity', aggregatorSpecs=[CardinalityAggregatorFactory{name='UV', fields='[DefaultDimensionSpec{dimension='visitor_id', outputName='visitor_id'}]'}, LongSumAggregatorFactory{fieldName='se_lpv_1d_004', name='PV'}, LongSumAggregatorFactory{fieldName='se_ipv_1d_003', name='IPV'}, DoubleSumAggregatorFactory{fieldName='pay_ord_amt_1d_007', name='\u6210\u4ea4\u91d1\u989d'}, LongSumAggregatorFactory{fieldName='pay_ord_cnt_1d_007', name='\u6210\u4ea4\u7b14\u6570'}, DoubleSumAggregatorFactory{fieldName='pay_ord_amt_1d_019', name='\u5168\u5f15\u5bfc\u6210\u4ea4\u91d1\u989d'}, LongSumAggregatorFactory{fieldName='pay_ord_cnt_1d_021', name='\u5168\u5f15\u5bfc\u6210\u4ea4\u7b14\u6570'}, LongSumAggregatorFactory{fieldName='clt_itm_cnt_1d_001', name='\u6536\u85cf\u5546\u54c1\u6570'}, LongSumAggregatorFactory{fieldName='cart_itm_cnt_1d_001', name='\u52a0\u8d2d\u5546\u54c1\u6570'}, LongSumAggregatorFactory{fieldName='pay_ord_cnt_1d_024', name='C\u5b9d\u8d1d\u6210\u4ea4\u7b14\u6570'}, DoubleSumAggregatorFactory{fieldName='pay_ord_amt_1d_025', name='C\u5b9d\u8d1d\u6210\u4ea4\u91d1\u989d'}, LongSumAggregatorFactory{fieldName='se_ipv_1d_005', name='C\u5b9d\u8d1dIPV'}, LongSumAggregatorFactory{fieldName='c_pv', name='C\u5b9d\u8d1dPV'}, LongSumAggregatorFactory{fieldName='b_pv', name='B\u5b9d\u8d1dPV'}, LongSumAggregatorFactory{fieldName='se_ipv_1d_004', name='B\u5b9d\u8d1dIPV'}], postAggregatorSpecs=[]}, peer=11.251.156.143}}\r\n> {feed=alerts, timestamp=2017-04-20T14:16:03.894+08:00, service=historical, host=hadoop0797.et2.tbsite.net:8083, severity=component-failure, description=Exception handling request, data={class=io.druid.server.QueryResource, exceptionType=class java.nio.BufferUnderflowException, exceptionMessage=null, exceptionStackTrace=java.nio.BufferUnderflowException\r\n>         at java.nio.Buffer.nextGetIndex(Buffer.java:506)\r\n>         at java.nio.HeapByteBuffer.getShort(HeapByteBuffer.java:310)\r\n>         at io.druid.query.aggregation.hyperloglog.HyperLogLogCollector.fold(HyperLogLogCollector.java:398)\r\n>         at io.druid.query.aggregation.cardinality.CardinalityAggregatorFactory.combine(CardinalityAggregatorFactory.java:198)\r\n>         at io.druid.query.topn.TopNBinaryFn.apply(TopNBinaryFn.java:106)\r\n>         at io.druid.query.topn.TopNBinaryFn.apply(TopNBinaryFn.java:39)\r\n>         at io.druid.common.guava.CombiningSequence$CombiningYieldingAccumulator.accumulate(CombiningSequence.java:212)\r\n>         at com.metamx.common.guava.BaseSequence.makeYielder(BaseSequence.java:105)\r\n>         at com.metamx.common.guava.BaseSequence.toYielder(BaseSequence.java:82)\r\n>         at io.druid.common.guava.CombiningSequence.toYielder(CombiningSequence.java:78)\r\n>         at com.metamx.common.guava.MappedSequence.toYielder(MappedSequence.java:46)\r\n>         at io.druid.query.CPUTimeMetricQueryRunner$1.toYielder(CPUTimeMetricQueryRunner.java:93)\r\n>         at com.metamx.common.guava.Sequences$1.toYielder(Sequences.java:98)\r\n>         at io.druid.server.QueryResource.doPost(QueryResource.java:231)\r\n"}, {"user": "pjain1", "commits": {}, "labels": ["Area - Streaming Ingestion", "Bug"], "created": "2017-04-17 18:50:17", "title": "Kafka Index Task should remember end offsets set by supervisor", "url": "https://github.com/apache/druid/issues/4177", "closed": "2017-11-17 23:08:00", "ttf": 214.00027777777777, "commitsDetails": [], "body": "Kafka Index Task cannot restore end offsets set by supervisor if the middleManager restarts after end offsets have been set and the task is not complete yet. Thus, it can keep on consuming records that it should not after restore."}, {"user": "jerchung", "commits": {}, "labels": ["Bug"], "created": "2017-04-13 21:56:47", "title": "S3DataSegmentPusher writes incomplete descriptor.json segment data to S3", "url": "https://github.com/apache/druid/issues/4170", "closed": "2017-04-24 16:45:34", "ttf": 10.000277777777777, "commitsDetails": [], "body": "In the code for the `S3DataSegmentPusher`, it appears that it tries to add some data to the `outSegment` (`size`, `loadSpec`, `binaryVersion`), but then it writes the old `inSegment` without the added data to deep storage, meaning that the `descriptor.json` files found in S3 deep storage are missing information.\r\n\r\nhttps://github.com/druid-io/druid/blob/druid-0.10.0-rc2/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentPusher.java#L109-L123\r\n\r\nThe metadata that gets pushed to the metadata datastore is correct as it's the `outSegment` that gets returned from the function that gets written to the datastore.\r\n\r\nhttps://github.com/druid-io/druid/blob/druid-0.10.0-rc2/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentPusher.java#L140\r\n\r\nhttps://github.com/druid-io/druid/blob/druid-0.10.0-rc2/server/src/main/java/io/druid/segment/realtime/plumber/RealtimePlumber.java#L430-L435\r\n\r\nI ran into this issue because when I was using the `insert-segment-to-db` tool to try to copy data into an entirely new cluster, it was not able to load the segments into the new historical nodes.\r\n\r\nIf this is an actual issue, and I didn't miss something when setting up my Druid cluster, I would be happy to make the changes to `S3DataSegmentPusher` and `S3DataSegmentPusherTest` to get it working."}, {"user": "gvsmirnov", "commits": {}, "labels": ["Bug"], "created": "2017-03-30 21:43:42", "title": "Kafka indexing gets stuck if there is data from outside of load period", "url": "https://github.com/apache/druid/issues/4137", "closed": "2019-01-31 23:03:52", "ttf": 672.0002777777778, "commitsDetails": [], "body": "**Steps to reproduce:**\r\n\r\n1. Set up a druid cluster with `druid-kafka-indexing-service` enabled, using the [sample spec provided in the doc](http://druid.io/docs/latest/development/extensions-core/kafka-ingestion.html#submitting-a-supervisor-spec)\r\n\r\n2. Send data to kafka:\r\n```\r\n$ cat current.data.json\r\n{ \"timestamp\": \"2017-03-30T00:00:00Z\", \"value\": 42.0 }\r\n\r\n$ cat current.data.json | kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic metrics\r\n```\r\n\r\n3. Set up a retention rule that asks to load data for the last day:\r\n<img width=\"589\" alt=\"screen shot 2017-03-31 at 00 40 49\" src=\"https://cloud.githubusercontent.com/assets/1493464/24527479/fd0731da-15aa-11e7-94db-94cabf5319e2.png\">\r\n\r\n\r\n4. Send outdated data to kafka:\r\n```\r\n$ cat old.data.json \r\n{ \"timestamp\": \"2016-04-30T00:00:00Z\", \"value\": 42.0 }\r\n\r\n$ cat old.data.json | kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic metrics\r\n```\r\n\r\n5. Wait for the task to complete (takes an hour by default, can be changed in `ioConfig.taskDuration` of the `supervisor-spec.json` file)\r\n\r\n**Expected behavior:**\r\nCurrent task completes right away, a new task is started\r\n\r\n**Actual behavior:**\r\nThe task hangs while trying to hand off the segment:\r\n\r\n```\r\n2017-03-30T21:38:29,659 INFO [task-runner-0-priority-0] io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver - Awaiting handoff of segments: [metrics-kafka_2016-04-30T00:00:00.000Z_2016-04-30T01:00:00.000Z_2017-03-30T21:36:45.791Z]\r\n2017-03-30T21:39:28,499 INFO [coordinator_handoff_scheduled_0] io.druid.segment.realtime.plumber.CoordinatorBasedSegmentHandoffNotifier - Still waiting for Handoff for Segments : [[SegmentDescriptor{interval=2016-04-30T00:00:00.000Z/2016-04-30T01:00:00.000Z, version='2017-03-30T21:36:45.791Z', partitionNumber=0}]]\r\n2017-03-30T21:40:28,479 INFO [coordinator_handoff_scheduled_0] io.druid.segment.realtime.plumber.CoordinatorBasedSegmentHandoffNotifier - Still waiting for Handoff for Segments : [[SegmentDescriptor{interval=2016-04-30T00:00:00.000Z/2016-04-30T01:00:00.000Z, version='2017-03-30T21:36:45.791Z', partitionNumber=0}]]\r\n2017-03-30T21:41:28,478 INFO [coordinator_handoff_scheduled_0] io.druid.segment.realtime.plumber.CoordinatorBasedSegmentHandoffNotifier - Still waiting for Handoff for Segments : [[SegmentDescriptor{interval=2016-04-30T00:00:00.000Z/2016-04-30T01:00:00.000Z, version='2017-03-30T21:36:45.791Z', partitionNumber=0}]]\r\n2017-03-30T21:42:28,478 INFO [coordinator_handoff_scheduled_0] io.druid.segment.realtime.plumber.CoordinatorBasedSegmentHandoffNotifier - Still waiting for Handoff for Segments : [[SegmentDescriptor{interval=2016-04-30T00:00:00.000Z/2016-04-30T01:00:00.000Z, version='2017-03-30T21:36:45.791Z', partitionNumber=0}]]\r\n2017-03-30T21:43:28,479 INFO [coordinator_handoff_scheduled_0] io.druid.segment.realtime.plumber.CoordinatorBasedSegmentHandoffNotifier - Still waiting for Handoff for Segments : [[SegmentDescriptor{interval=2016-04-30T00:00:00.000Z/2016-04-30T01:00:00.000Z, version='2017-03-30T21:36:45.791Z', partitionNumber=0}]]\r\n```"}, {"user": "gabrielpage", "commits": {}, "labels": ["Bug"], "created": "2017-03-22 10:59:44", "title": "Caching in 0.10.0-rc1 is returning wrong values", "url": "https://github.com/apache/druid/issues/4093", "closed": "2017-03-23 14:17:38", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "This issue is not present in 0.9.2\r\n\r\nThis can be replicated with the wikiticker quickstart. Once this data is loaded, execute the following 2 queries :\r\n\r\n```json\r\n{\r\n    \"queryType\": \"timeseries\",\r\n    \"dataSource\": \"wikiticker\",\r\n    \"granularity\": \"all\",\r\n    \"aggregations\": [\r\n        {\"type\": \"longSum\", \"name\": \"count\", \"fieldName\": \"count\"},\r\n        {\"type\": \"longSum\", \"name\": \"added\", \"fieldName\": \"added\"}\r\n    ],\r\n    \"intervals\": [ \"2016/2018\" ]\r\n}\r\n```\r\n\r\nThis return the correct values\r\n\r\n```json\r\n[ {\r\n  \"timestamp\" : \"2016-06-27T00:00:11.080Z\",\r\n  \"result\" : {\r\n    \"count\" : 11774265,\r\n    \"added\" : 24433\r\n  }\r\n} ]\r\n```\r\n\r\nThen execute the following query (it's identical other than the order of the aggregations)\r\n\r\n```json\r\n{\r\n    \"queryType\": \"timeseries\",\r\n    \"dataSource\": \"wikiticker\",\r\n    \"granularity\": \"all\",\r\n    \"aggregations\": [\r\n        {\"type\": \"longSum\", \"name\": \"added\", \"fieldName\": \"added\"},\r\n        {\"type\": \"longSum\", \"name\": \"count\", \"fieldName\": \"count\"}\r\n    ],\r\n    \"intervals\": [ \"2016/2018\" ]\r\n}\r\n```\r\n\r\nYields the following.\r\n\r\n```json\r\n[ {\r\n  \"timestamp\" : \"2016-06-27T00:00:11.080Z\",\r\n  \"result\" : {\r\n    \"count\" : 24433,\r\n    \"added\" : 11774265\r\n  }\r\n} ]\r\n```\r\n\r\nThe values returned have been \"switched\". With \"count\" incorrectly showing the \"added\" value in the result, and vice versa.\r\n\r\nDisabling caching on both the historical and the broker removes the problem. If caching is on either the broker or the historical then the error returns. "}, {"user": "gianm", "commits": {}, "labels": ["Bug", "Starter"], "created": "2017-03-17 04:56:20", "title": "PeriodGranularity doesn't work right in Kathmandu", "url": "https://github.com/apache/druid/issues/4073", "closed": "2018-09-17 22:50:56", "ttf": 549.0002777777778, "commitsDetails": [], "body": "Test case below. PT1H works fine but PT2H doesn't.\r\n\r\n```java\r\n@Test\r\npublic void testTruncateKathmandu() throws Exception\r\n{\r\n  final DateTimeZone tz = DateTimeZone.forID(\"Asia/Kathmandu\");\r\n  final DateTime date = new DateTime(\"2011-03-15T21:42:23.898+05:45\").withZone(DateTimeZone.UTC);\r\n  final PeriodGranularity year = new PeriodGranularity(new Period(\"P1Y\"), null, tz);\r\n  final PeriodGranularity hour = new PeriodGranularity(new Period(\"PT1H\"), null, tz);\r\n  final PeriodGranularity twoHour = new PeriodGranularity(new Period(\"PT2H\"), null, tz);\r\n\r\n  // These two pass\r\n  Assert.assertEquals(\r\n      new DateTime(\"2011-01-01T00:00:00.000+05:45\", tz),\r\n      year.toDateTime(year.bucketStart(date).getMillis())\r\n  );\r\n  Assert.assertEquals(\r\n      new DateTime(\"2011-03-15T21:00:00.000+05:45\", tz),\r\n      hour.toDateTime(hour.bucketStart(date).getMillis())\r\n  );\r\n\r\n  // This one fails\r\n  Assert.assertEquals(\r\n      new DateTime(\"2011-03-15T20:00:00.000+05:45\", tz),\r\n      twoHour.toDateTime(twoHour.bucketStart(date).getMillis())\r\n  );\r\n}\r\n```\r\n"}, {"user": "niketh", "commits": {"a5170666b61764017c3711b15ff68e3653243d9b": {"commitGHEventType": "referenced", "commitUser": "himanshug"}, "5acc448169cac03948ef22658066888fccad4662": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "Null Pointer Exception in MapBasedRow.compareTo", "numCommits": 0, "created": "2017-03-07 18:23:13", "closed": "2017-03-08 18:47:47", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/4020", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 1.0002777777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "processing/src/main/java/io/druid/query/groupby/strategy/GroupByStrategyV1.java", "deletions": 2, "insertions": 8, "lines": 10}, {"filePath": "processing/src/main/java/io/druid/query/groupby/strategy/GroupByStrategy.java", "deletions": 0, "insertions": 5, "lines": 5}, {"filePath": "processing/src/main/java/io/druid/query/groupby/strategy/GroupByStrategyV2.java", "deletions": 0, "insertions": 6, "lines": 6}, {"filePath": "processing/src/main/java/io/druid/query/groupby/GroupByQueryQueryToolChest.java", "deletions": 3, "insertions": 4, "lines": 7}], "commitSpoonAstDiffStats": [{"spoonFilePath": "GroupByStrategyV2.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.groupby.strategy.GroupByStrategyV2.doMergeResults(io.druid.query.groupby.GroupByQuery)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "GroupByStrategy.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.groupby.strategy.doMergeResults(io.druid.query.groupby.GroupByQuery)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "GroupByStrategyV1.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.groupby.strategy.GroupByStrategyV1.doMergeResults(io.druid.query.groupby.GroupByQuery)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "GroupByQueryQueryToolChest.java", "spoonMethods": [{"INS": 2, "UPD": 2, "DEL": 1, "spoonMethodName": "io.druid.query.groupby.GroupByQueryQueryToolChest.mergeResults(io.druid.query.QueryRunner).3.run(io.druid.query.Query,java.util.Map)", "MOV": 1, "TOT": 6}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2017-03-08 10:47:46", "commitMessage": "groupBy v2: Always merge queries. (#4023)\n\nThis fixes #4020 because it means the timestamp will always be included for outermost\r\nqueries. Historicals receiving queries from older brokers will think they're\r\noutermost (because CTX_KEY_OUTERMOST isn't set to \"false\"), so they'll include a\r\ntimestamp, so the older brokers will be OK.", "commitUser": "himanshug", "commitDateTime": "2017-03-08 12:47:46", "commitParents": ["c155d9a5e95e48ebb09db668f83c434fdb706c5b"], "commitGHEventType": "referenced", "nameRev": "a5170666b61764017c3711b15ff68e3653243d9b tags/druid-0.10.1-rc1~189", "commitHash": "a5170666b61764017c3711b15ff68e3653243d9b"}, {"commitGitStats": [{"filePath": "processing/src/main/java/io/druid/query/groupby/strategy/GroupByStrategyV1.java", "deletions": 2, "insertions": 8, "lines": 10}, {"filePath": "processing/src/main/java/io/druid/query/groupby/strategy/GroupByStrategy.java", "deletions": 0, "insertions": 5, "lines": 5}, {"filePath": "processing/src/main/java/io/druid/query/groupby/strategy/GroupByStrategyV2.java", "deletions": 0, "insertions": 6, "lines": 6}, {"filePath": "processing/src/main/java/io/druid/query/groupby/GroupByQueryQueryToolChest.java", "deletions": 3, "insertions": 4, "lines": 7}], "commitSpoonAstDiffStats": [{"spoonFilePath": "GroupByStrategyV2.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.groupby.strategy.GroupByStrategyV2.doMergeResults(io.druid.query.groupby.GroupByQuery)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "GroupByStrategy.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.groupby.strategy.doMergeResults(io.druid.query.groupby.GroupByQuery)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "GroupByStrategyV1.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.groupby.strategy.GroupByStrategyV1.doMergeResults(io.druid.query.groupby.GroupByQuery)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "GroupByQueryQueryToolChest.java", "spoonMethods": [{"INS": 2, "UPD": 2, "DEL": 1, "spoonMethodName": "io.druid.query.groupby.GroupByQueryQueryToolChest.mergeResults(io.druid.query.QueryRunner).3.run(io.druid.query.Query,java.util.Map)", "MOV": 1, "TOT": 6}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2017-03-13 10:52:03", "commitMessage": "groupBy v2: Always merge queries. (#4023) (#4042)\n\nThis fixes #4020 because it means the timestamp will always be included for outermost\r\nqueries. Historicals receiving queries from older brokers will think they're\r\noutermost (because CTX_KEY_OUTERMOST isn't set to \"false\"), so they'll include a\r\ntimestamp, so the older brokers will be OK.", "commitUser": "fjy", "commitDateTime": "2017-03-13 13:52:03", "commitParents": ["3fcf9f434c157f6092464a780dabae828818c8c6"], "commitGHEventType": "referenced", "nameRev": "5acc448169cac03948ef22658066888fccad4662 tags/druid-0.10.0-rc2~11", "commitHash": "5acc448169cac03948ef22658066888fccad4662"}], "body": "We get a NPE when we fire groupBy queries using Broker 0.9.1.1 and Historicals (0.10).\r\n\r\n`2017-03-07T17:52:37,896 ERROR [qtp900593187-194[groupBy_abc-groupBy2]] io.druid.server.QueryResource - Exception handling request: {class=io.druid.server.QueryRe\r\njava.lang.NullPointerException\r\n        at io.druid.data.input.MapBasedRow.compareTo(MapBasedRow.java:203) ~[druid-api-0.9.1.1.jar:0.9.1.1]\r\n        at io.druid.data.input.MapBasedRow.compareTo(MapBasedRow.java:39) ~[druid-api-0.9.1.1.jar:0.9.1.1]\r\n        at com.google.common.collect.NaturalOrdering.compare(NaturalOrdering.java:35) ~[guava-16.0.1.jar:?]\r\n        at com.google.common.collect.NaturalOrdering.compare(NaturalOrdering.java:26) ~[guava-16.0.1.jar:?]\r\n        at com.google.common.collect.ByFunctionOrdering.compare(ByFunctionOrdering.java:46) ~[guava-16.0.1.jar:?]\r\n        at java.util.PriorityQueue.siftUpUsingComparator(PriorityQueue.java:669) ~[?:1.8.0_112]\r\n        at java.util.PriorityQueue.siftUp(PriorityQueue.java:645) ~[?:1.8.0_112]\r\n        at java.util.PriorityQueue.offer(PriorityQueue.java:344) ~[?:1.8.0_112]\r\n        at java.util.PriorityQueue.add(PriorityQueue.java:321) ~[?:1.8.0_112]\r\n        at com.metamx.common.guava.MergeSequence$2.accumulate(MergeSequence.java:80) ~[java-util-0.27.9.jar:?]\r\n        at com.metamx.common.guava.MergeSequence$2.accumulate(MergeSequence.java:62) ~[java-util-0.27.9.jar:?]\r\n        at com.metamx.common.guava.YieldingAccumulators$1.accumulate(YieldingAccumulators.java:32) ~[java-util-0.27.9.jar:?]\r\n        at com.metamx.common.guava.BaseSequence.makeYielder(BaseSequence.java:104) ~[java-util-0.27.9.jar:?]\r\n        at com.metamx.common.guava.BaseSequence.toYielder(BaseSequence.java:81) ~[java-util-0.27.9.jar:?]\r\n        at com.metamx.common.guava.BaseSequence.accumulate(BaseSequence.java:67) ~[java-util-0.27.9.jar:?]\r\n        at com.metamx.common.guava.MergeSequence.toYielder(MergeSequence.java:59) ~[java-util-0.27.9.jar:?]\r\n        at com.metamx.common.guava.LazySequence.toYielder(LazySequence.java:43) ~[java-util-0.27.9.jar:?]\r\n        at io.druid.query.RetryQueryRunner$1.toYielder(RetryQueryRunner.java:105) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\r\n        at com.metamx.common.guava.YieldingSequenceBase.accumulate(YieldingSequenceBase.java:34) ~[java-util-0.27.9.jar:?]\r\n        at io.druid.query.groupby.GroupByQueryQueryToolChest.makeIncrementalIndex(GroupByQueryQueryToolChest.java:319) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\r\n        at io.druid.query.groupby.GroupByQueryQueryToolChest.mergeGroupByResults(GroupByQueryQueryToolChest.java:257) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\r\n        at io.druid.query.groupby.GroupByQueryQueryToolChest.access$000(GroupByQueryQueryToolChest.java:85) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\r\n        at io.druid.query.groupby.GroupByQueryQueryToolChest$3.run(GroupByQueryQueryToolChest.java:134) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\r\n        at io.druid.query.FinalizeResultsQueryRunner.run(FinalizeResultsQueryRunner.java:103) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\r\n        at io.druid.query.CPUTimeMetricQueryRunner.run(CPUTimeMetricQueryRunner.java:72) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\r\n        at io.druid.query.FluentQueryRunnerBuilder$FluentQueryRunner.run(FluentQueryRunnerBuilder.java:58) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\r\n        at io.druid.query.BaseQuery.run(BaseQuery.java:151) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\r\n        at io.druid.query.BaseQuery.run(BaseQuery.java:146) ~[druid-processing-0.9.1.1.jar:0.9.1.1]`"}, {"user": "akashdw", "commits": {}, "labels": ["Bug"], "created": "2017-03-03 06:26:48", "title": "HdfsDataSegmentPusher bug.", "url": "https://github.com/apache/druid/issues/3997", "closed": "2017-03-06 08:53:45", "ttf": 3.000277777777778, "commitsDetails": [], "body": "Hdfs namespace optimization changes https://github.com/druid-io/druid/pull/3877 introduced a bug in HdfsDataSegmentPusher causing exceptions during segment push (if an interval has more than one shard.)\r\n\r\nCurrent code tries to rename `version`(parent directory of index.zip) directory instead of copying index/descriptor files to the right location. Renaming `shard` directory was valid but that directory is now removed.\r\nCouldn't catch this b/c our staging environment creates only 1 shard per interval.\r\n\r\nI'm working on the fix and adding tests for multiple segments push instead of just 1.\r\n\r\n"}, {"user": "praveev", "commits": {}, "labels": ["Bug"], "created": "2017-03-02 23:18:01", "title": "Duration Granularity `toDateTime` throws UnsupportedOperationException", "url": "https://github.com/apache/druid/issues/3994", "closed": "2017-03-03 19:27:23", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "Duration Granularity throws an UnsupportedException when its `toDateTime` [method](https://github.com/druid-io/druid/blob/0.10.0/java-util/src/main/java/io/druid/java/util/common/granularity/DurationGranularity.java#L117) is called.\r\nThis is incorrect. It should let the call through to base class Granularity to retain the pre-refactor behavior.\r\n\r\nThis bug was introduced with #3850 refactor. We noticed this issue during 0.10.0 RC deployment, when we updated the historicals and nothing else. The queries to the broker where failing b/c granularity \"day\" was  being represented by DurationGranularity and its `toDateTime` invocation from `QueryableIndexStorageAdapter` threw an UnsupportedException."}, {"user": "himanshug", "commits": {}, "labels": ["Bug"], "created": "2017-02-28 20:40:25", "title": "a bug in groupBy v2?", "url": "https://github.com/apache/druid/issues/3983", "closed": "2017-03-01 21:07:13", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "One of my users (using version 0.9.2) is using time extraction function in a groupBy query.\r\n\r\nresults are always correct with groupBy-v1 while with groupBy-v2 they are sometimes correct and sometimes not \r\nhowever, without the extractionFn in the query groupBy-v2 seems to always produce correct result.\r\n\r\nhere is what the query looks like...\r\n\r\n```\r\n{\r\n  \"queryType\": \"groupBy\",\r\n  \"dataSource\": {\r\n    \"type\": \"table\",\r\n    \"name\": \"xxx\"\r\n  },\r\n  \"intervals\": {\r\n    \"type\": \"intervals\",\r\n    \"intervals\": [\"2016-03-01T00:00:00.000Z/2016-03-02T00:00:00.000Z\", \"2016-04-01T00:00:00.000Z/2016-04-02T00:00:00.000Z\", \"2016-05-01T00:00:00.000Z/2016-05-02T00:00:00.000Z\"]\r\n  },\r\n  \"filter\": {\r\n    \"type\": \"and\",\r\n    \"fields\": [{\r\n      \"type\": \"or\",\r\n      \"fields\": [{\r\n        \"type\": \"selector\",\r\n        \"dimension\": \"myDate\",\r\n        \"value\": \"20160301\"\r\n      }, {\r\n        \"type\": \"selector\",\r\n        \"dimension\": \"myDate\",\r\n        \"value\": \"20160401\"\r\n      }, {\r\n        \"type\": \"selector\",\r\n        \"dimension\": \"myDate\",\r\n        \"value\": \"20160501\"\r\n      }]\r\n    }, {\r\n      \"type\": \"selector\",\r\n      \"dimension\": \"xx\",\r\n      \"value\": \"xx\"\r\n    }, {\r\n      \"type\": \"or\",\r\n      \"fields\": [{\r\n        \"type\": \"selector\",\r\n        \"dimension\": \"xx\",\r\n        \"value\": \"yy\"\r\n      }, {\r\n        \"type\": \"selector\",\r\n        \"dimension\": \"xx\",\r\n        \"value\": \"yy\"\r\n      }, {\r\n        \"type\": \"selector\",\r\n        \"dimension\": \"xx\",\r\n        \"value\": \"yy\"\r\n      }]\r\n    }]\r\n  },\r\n  \"granularity\": {\r\n    \"type\": \"all\"\r\n  },\r\n  \"dimensions\": [{\r\n    \"type\": \"extraction\",\r\n    \"dimension\": \"myDate\",\r\n    \"outputName\": \"Month\",\r\n    \"extractionFn\": {\r\n      \"type\": \"time\",\r\n      \"timeFormat\": \"yyyyMMdd\",\r\n      \"resultFormat\": \"yyyyMM\"\r\n    }\r\n  }, {\r\n    \"type\": \"default\",\r\n    \"dimension\": \"xx\",\r\n    \"outputName\": \"xx\"\r\n  }],\r\n  \"aggregations\": [{\r\n    \"type\": \"longSum\",\r\n    \"name\": \"xx\",\r\n    \"fieldName\": \"xx\"\r\n  }],\r\n  \"postAggregations\": [],\r\n  \"limitSpec\": {\r\n    \"type\": \"default\",\r\n    \"columns\": [{\r\n      \"dimension\": \"Month\",\r\n      \"direction\": \"ascending\",\r\n      \"dimensionOrder\": {\r\n        \"type\": \"lexicographic\"\r\n      }\r\n    }],\r\n    \"limit\": 1000\r\n  },\r\n  \"context\": {\r\n    \"groupByStrategy\": \"v2\",\r\n    \"groupByIsSingleThreaded\": true\r\n  },\r\n  \"descending\": false\r\n}\r\n```\r\n\r\n"}, {"user": "jihoonson", "commits": {}, "labels": ["Bug"], "created": "2017-02-15 06:35:41", "title": "During deeply nested group-by execution, the merge buffer for the inner group-by result should be held during the outer group-by execution", "url": "https://github.com/apache/druid/issues/3938", "closed": "2017-02-16 03:47:28", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "For deeply nested group-by execution, at most two merge buffers should be acquired on the broker side for the currently accumulating sequence and the underlying input sequence which is already accumulated. With the group-by strategy v2, this intermediate result is stored on a merge buffer. \r\n\r\nObviously, the merge buffer needs to be held until the current sequence accumulation is completed. However, in the current implementation (see ```CombiningSequence```), the input sequence (```CombiningSequence.baseSequence```) is first accumulated, and then the current sequence is accumulated. The merge buffer for the input sequence is released immediately once it's accumulation is completed (```BaseSequence.IteratorMaker.cleanup()```), so it is not guaranteed to hold the intermediate result stored on that merge buffer until the current sequence is completely accumulated."}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2017-01-24 04:03:24", "title": "JSON parsing bad metrics fails ignoring `ignoreInvalidRows`", "url": "https://github.com/apache/druid/issues/3879", "closed": "2018-09-17 22:52:48", "ttf": 601.0002777777778, "commitsDetails": [], "body": "`io.druid.data.input.impl.MapInputRowParser#parse` tries to parse into a `Map<String, Object>` which will succeed for all manner of input. But the parsing is expected to fail early in order to catch parse errors.\r\n\r\nFor a JSON map which has a string in place of where a metric should be, the failure is later at the aggregation stage, bypassing the check at\r\n\r\n```json\r\nif (config.isIgnoreInvalidRows()) {\r\n```\r\n\r\n\r\nStack trace below\r\n\r\n\r\n```\r\n2017-01-24T03:45:19,041 WARN [Thread-4] org.apache.hadoop.mapred.LocalJobRunner - job_local1725905616_0001\r\njava.lang.Exception: com.metamx.common.RE: Failure on row[{\"INTERVALSTARTTIME_GMT\":\"2016-05-06T20:25:00-00:00\",\"INTERVALENDTIME_GMT\":\"2016-05-06T20:30:00-00:00\",\"OPR_DT\":\"2016-05-06\",\"OPR_HR\":\"1\r\n4\",\"NODE_ID_XML\":\"BELMONT_1_N006\",\"NODE_ID\":\"BELMONT_1_N006\",\"NODE\":\"BELMONT_1_N006\",\"MARKET_RUN_ID\":\"RTM\",\"LMP_TYPE\":\"LMP\",\"XML_DATA_ITEM\":\"LMP_PRC\",\"PNODE_RESMRID\":\"BELMONT_1_N006\",\"GRP_TYPE\":\"BELLPGE_1_GN006\",\"POS\":\"BELLPGE_1_GN006\",\"VALUE\":\"RTM\",\"OPR_INTERVAL\":\"LMP\",\"GROUP\":\"LMP_PRC\",\"lat\":37.52323862187675,\"loc\":\"CA\",\"lon\":-122.26490790641564,\"node_id\":\"BELMONT_1_N006\",\"type\":\"LOAD\"}]\r\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462) ~[hadoop-mapreduce-client-common-2.7.3.jar:?]\r\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522) [hadoop-mapreduce-client-common-2.7.3.jar:?]\r\nCaused by: com.metamx.common.RE: Failure on row[{\"INTERVALSTARTTIME_GMT\":\"2016-05-06T20:25:00-00:00\",\"INTERVALENDTIME_GMT\":\"2016-05-06T20:30:00-00:00\",\"OPR_DT\":\"2016-05-06\",\"OPR_HR\":\"14\",\"NODE_ID_XML\":\"BELMONT_1_N006\",\"NODE_ID\":\"BELMONT_1_N006\",\"NODE\":\"BELMONT_1_N006\",\"MARKET_RUN_ID\":\"RTM\",\"LMP_TYPE\":\"LMP\",\"XML_DATA_ITEM\":\"LMP_PRC\",\"PNODE_RESMRID\":\"BELMONT_1_N006\",\"GRP_TYPE\":\"BELLPGE_1_GN006\",\"POS\":\"BELLPGE_1_GN006\",\"VALUE\":\"RTM\",\"OPR_INTERVAL\":\"LMP\",\"GROUP\":\"LMP_PRC\",\"lat\":37.52323862187675,\"loc\":\"CA\",\"lon\":-122.26490790641564,\"node_id\":\"BELMONT_1_N006\",\"type\":\"LOAD\"}]\r\n        at io.druid.indexer.HadoopDruidIndexerMapper.map(HadoopDruidIndexerMapper.java:88) ~[druid-indexing-hadoop-0.9.1.1.jar:0.9.1.1]\r\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146) ~[hadoop-mapreduce-client-core-2.7.3.jar:?]\r\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787) ~[hadoop-mapreduce-client-core-2.7.3.jar:?]\r\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) ~[hadoop-mapreduce-client-core-2.7.3.jar:?]\r\n        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243) ~[hadoop-mapreduce-client-common-2.7.3.jar:?]\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_121]\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_121]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_121]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_121]\r\n        at java.lang.Thread.run(Thread.java:745) ~[?:1.8.0_121]\r\nCaused by: com.metamx.common.parsers.ParseException: Unable to parse metrics[VALUE], value[RTM]\r\n        at io.druid.data.input.MapBasedRow.getFloatMetric(MapBasedRow.java:130) ~[druid-api-0.9.1.1.jar:0.9.1.1]\r\n        at io.druid.segment.incremental.IncrementalIndex$4$3.get(IncrementalIndex.java:201) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\r\n        at io.druid.query.aggregation.DoubleSumAggregator.aggregate(DoubleSumAggregator.java:62) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\r\n        at io.druid.indexer.InputRowSerde.toBytes(InputRowSerde.java:94) ~[druid-indexing-hadoop-0.9.1.1.jar:0.9.1.1]\r\n        at io.druid.indexer.IndexGeneratorJob$IndexGeneratorMapper.innerMap(IndexGeneratorJob.java:292) ~[druid-indexing-hadoop-0.9.1.1.jar:0.9.1.1]\r\n        at io.druid.indexer.HadoopDruidIndexerMapper.map(HadoopDruidIndexerMapper.java:84) ~[druid-indexing-hadoop-0.9.1.1.jar:0.9.1.1]\r\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146) ~[hadoop-mapreduce-client-core-2.7.3.jar:?]\r\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787) ~[hadoop-mapreduce-client-core-2.7.3.jar:?]\r\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) ~[hadoop-mapreduce-client-core-2.7.3.jar:?]\r\n        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243) ~[hadoop-mapreduce-client-common-2.7.3.jar:?]\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_121]\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_121]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_121]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_121]\r\n        at java.lang.Thread.run(Thread.java:745) ~[?:1.8.0_121]\r\nCaused by: java.lang.NumberFormatException: For input string: \"RTM\"\r\n        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043) ~[?:1.8.0_121]\r\n        at sun.misc.FloatingDecimal.parseFloat(FloatingDecimal.java:122) ~[?:1.8.0_121]\r\n        at java.lang.Float.parseFloat(Float.java:451) ~[?:1.8.0_121]\r\n        at java.lang.Float.valueOf(Float.java:416) ~[?:1.8.0_121]\r\n        at io.druid.data.input.MapBasedRow.getFloatMetric(MapBasedRow.java:127) ~[druid-api-0.9.1.1.jar:0.9.1.1]\r\n        at io.druid.segment.incremental.IncrementalIndex$4$3.get(IncrementalIndex.java:201) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\r\n        at io.druid.query.aggregation.DoubleSumAggregator.aggregate(DoubleSumAggregator.java:62) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\r\n        at io.druid.indexer.InputRowSerde.toBytes(InputRowSerde.java:94) ~[druid-indexing-hadoop-0.9.1.1.jar:0.9.1.1]\r\n        at io.druid.indexer.IndexGeneratorJob$IndexGeneratorMapper.innerMap(IndexGeneratorJob.java:292) ~[druid-indexing-hadoop-0.9.1.1.jar:0.9.1.1]\r\n        at io.druid.indexer.HadoopDruidIndexerMapper.map(HadoopDruidIndexerMapper.java:84) ~[druid-indexing-hadoop-0.9.1.1.jar:0.9.1.1]\r\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146) ~[hadoop-mapreduce-client-core-2.7.3.jar:?]\r\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787) ~[hadoop-mapreduce-client-core-2.7.3.jar:?]\r\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) ~[hadoop-mapreduce-client-core-2.7.3.jar:?]\r\n        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243) ~[hadoop-mapreduce-client-common-2.7.3.jar:?]\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_121]\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_121]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_121]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_121]\r\n        at java.lang.Thread.run(Thread.java:745) ~[?:1.8.0_121]\r\n```\r\n\r\nIn this example `\"VALUE\":\"RTM\"` is expected to parse to be a `float`, not a string."}, {"user": "gianm", "commits": {}, "labels": ["Area - Web Console", "Bug"], "created": "2017-01-23 04:37:13", "title": "Coordinator rules console visually overflows with > 4 tiers", "url": "https://github.com/apache/druid/issues/3875", "closed": "2019-09-25 22:39:59", "ttf": 975.0002777777778, "commitsDetails": [], "body": "Screenshot, note the glitch after \"+ d replicant\":\r\n\r\n![image](https://cloud.githubusercontent.com/assets/1214075/22191710/74fff30a-e0e2-11e6-9693-3d41c9d1844e.png)\r\n"}, {"user": "b-slim", "commits": {}, "labels": ["Bug", "Release Notes"], "created": "2017-01-19 16:57:07", "title": "LGPL dependencies packaged with druid main tarball", "url": "https://github.com/apache/druid/issues/3866", "closed": "2017-01-21 20:28:32", "ttf": 2.000277777777778, "commitsDetails": [], "body": "We have some core dependency that brings unwanted LGPL code.\r\n```\r\nINFO] |  \\- io.airlift:airline:jar:0.7:compile\r\n[INFO] |     \\- com.google.code.findbugs:annotations:jar:2.0.3:compile\r\n```\r\nhttps://mvnrepository.com/artifact/com.google.code.findbugs/annotations/2.0.3"}, {"user": "vvararu", "commits": {}, "labels": ["Bug"], "created": "2017-01-16 12:46:36", "title": "Druid ingestion ends with SUCCESS status but exception is thrown at the end", "url": "https://github.com/apache/druid/issues/3851", "closed": "2019-01-31 23:10:34", "ttf": 745.0002777777778, "commitsDetails": [], "body": "Hi guys,\r\n\r\nI run a ingestion job druid (0.9.1.1) + hadoop 2.7.1 (custom version declared in common conf druid.extensions.hadoopDependenciesDir=/druid/hadoop-dependencies).\r\n\r\nAs deep storage - hdfs in a docker container.\r\n\r\nLoaded extensions: druid.extensions.loadList=[\"druid-histogram\", \"druid-datasketches\", \"postgresql-metadata-storage\", \"druid-kafka-indexing-service\", \"druid-hdfs-storage\"]\r\n\r\nThe job finishes successfully, data is ingested but at the very end i get the next exception:\r\n\r\n> 2017-01-13T09:44:48,064 INFO [task-runner-0-priority-0] io.druid.indexing.worker.executor.ExecutorLifecycle - Task completed with status: {\r\n>   \"id\" : \"index_hadoop_impression_segments5_2017-01-13T09:42:05.410Z\",\r\n>   \"status\" : \"SUCCESS\",\r\n>   \"duration\" : 159082\r\n> }\r\n\r\nafter fiew lines:\r\n\r\n> 2017-01-13 09:44:48,136 Thread-2 ERROR Unable to register shutdown hook because JVM is shutting down. java.lang.IllegalStateException: Not started\r\n> \tat io.druid.common.config.Log4jShutdown.addShutdownCallback(Log4jShutdown.java:45)\r\n> \tat org.apache.logging.log4j.core.impl.Log4jContextFactory.addShutdownCallback(Log4jContextFactory.java:273)\r\n> \tat org.apache.logging.log4j.core.LoggerContext.setUpShutdownHook(LoggerContext.java:256)\r\n> \tat org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:216)\r\n> \tat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:145)\r\n> \tat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41)\r\n> \tat org.apache.logging.log4j.LogManager.getContext(LogManager.java:182)\r\n> \tat org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:103)\r\n> \tat org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:43)\r\n> \tat org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:42)\r\n> \tat org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:29)\r\n> \tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:284)\r\n> \tat org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:155)\r\n> \tat org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:132)\r\n> \tat org.apache.commons.logging.LogFactory.getLog(LogFactory.java:273)\r\n> \tat org.apache.hadoop.hdfs.LeaseRenewer.<clinit>(LeaseRenewer.java:72)\r\n> \tat org.apache.hadoop.hdfs.DFSClient.getLeaseRenewer(DFSClient.java:699)\r\n> \tat org.apache.hadoop.hdfs.DFSClient.close(DFSClient.java:859)\r\n> \tat org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:853)\r\n> \tat org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2407)\r\n> \tat org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2424)\r\n> \tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)"}, {"user": "b-slim", "commits": {}, "labels": ["Bug"], "created": "2017-01-12 18:59:39", "title": "Null Handling", "url": "https://github.com/apache/druid/issues/3846", "closed": "2017-01-17 19:59:13", "ttf": 5.000277777777778, "commitsDetails": [], "body": "running manually tests on `SchemalessTestFull` and  `SchemalessTestSimple.testFullOnTimeseries` fail due to some null handling issue i guess. As @gianm mention those are not run by maven due to the name convention. \r\n```java\r\nava.lang.AssertionError: \r\nExpected :Result{timestamp=2011-01-12T00:00:00.000Z, value=TimeseriesResultValue{value=MetricValueExtractor{value={rows=11, index=900.0, addRowsIndexConstant=912.0, uniques=2.000977198748901, maxIndex=100.0, minIndex=0.0}}}}\r\nActual   :Result{timestamp=2011-01-12T00:00:00.000Z, value=TimeseriesResultValue{value=MetricValueExtractor{value={uniques=3.0021994137521975, addRowsIndexConstant=912.0, index=900.0, minIndex=0.0, maxIndex=100.0, rows=11}}}}\r\n <Click to see difference>\r\n\r\n\r\n\tat org.junit.Assert.fail(Assert.java:88)\r\n\tat org.junit.Assert.failNotEquals(Assert.java:743)\r\n\tat org.junit.Assert.assertEquals(Assert.java:118)\r\n\tat io.druid.segment.TestHelper.assertResult(TestHelper.java:222)\r\n\tat io.druid.segment.TestHelper.assertResults(TestHelper.java:140)\r\n\tat io.druid.segment.TestHelper.assertExpectedResults(TestHelper.java:93)\r\n\tat io.druid.segment.SchemalessTestSimple.testFullOnTimeseries(SchemalessTestSimple.java:170)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n\tat org.junit.runners.Suite.runChild(Suite.java:127)\r\n\tat org.junit.runners.Suite.runChild(Suite.java:26)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:160)\r\n\tat com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)\r\n\tat com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:51)\r\n\tat com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:237)\r\n\tat com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)\r\n``` "}, {"user": "drcrallen", "commits": {"2af7cf10e3998e9d9726ecde103812d7ce3f2880": {"commitGHEventType": "referenced", "commitUser": "fjy"}, "81d6b49d69d8a7676f1ba0c2eb344f1a3d4e754e": {"commitGHEventType": "referenced", "commitUser": "gianm"}, "db5bebbaaa5a60b24285f91c2157a2d97c26eae0": {"commitGHEventType": "referenced", "commitUser": "fjy"}, "9cd666282cfada95c4ccd96a1ae294311857c41e": {"commitGHEventType": "referenced", "commitUser": "gianm"}}, "changesInPackagesGIT": [], "labels": ["Area - Zookeeper/Curator", "Bug"], "spoonStatsSummary": {}, "title": "Curator leader election breaks in the overlord when zookeeper has issues", "numCommits": 0, "created": "2017-01-10 18:16:46", "closed": "2017-09-15 17:48:32", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "filteredCommits": [], "url": "https://github.com/apache/druid/issues/3837", "filteredCommitsReason": {"multipleIssueFixes": 4, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 247.00027777777777, "commitsDetails": [{"commitGitStats": [{"filePath": "pom.xml", "deletions": 1, "insertions": 1, "lines": 2}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "", "authoredDateTime": "2017-03-15 11:46:59", "commitMessage": "Update Curator to 2.12.0. (#4060) (#4061)\n\nFixes #4056, #3837.", "commitUser": "fjy", "commitDateTime": "2017-03-15 14:46:59", "commitParents": ["363718230d9dafbe7a221f0321b9d1a33f3de7ec"], "commitGHEventType": "referenced", "nameRev": "2af7cf10e3998e9d9726ecde103812d7ce3f2880 tags/druid-0.10.0-rc2~9", "commitHash": "2af7cf10e3998e9d9726ecde103812d7ce3f2880"}, {"commitGitStats": [{"filePath": "pom.xml", "deletions": 1, "insertions": 1, "lines": 2}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "", "authoredDateTime": "2017-03-23 13:44:00", "commitMessage": "Downgrade Curator. (#4103)\n\nReverts #4060, fixes #4095, unfixes #4056, #3837. Better the devil you\r\nknow than the devil you don't, I always say.\r\n\r\nSee also https://issues.apache.org/jira/browse/CURATOR-394.", "commitUser": "gianm", "commitDateTime": "2017-03-23 13:44:00", "commitParents": ["ff7f90b02d2a2bd8447370243bea4efdde248da5"], "commitGHEventType": "referenced", "nameRev": "81d6b49d69d8a7676f1ba0c2eb344f1a3d4e754e tags/druid-0.10.1-rc1~166", "commitHash": "81d6b49d69d8a7676f1ba0c2eb344f1a3d4e754e"}, {"commitGitStats": [{"filePath": "pom.xml", "deletions": 1, "insertions": 1, "lines": 2}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "", "authoredDateTime": "2017-03-23 14:20:14", "commitMessage": "Downgrade Curator. (#4103) (#4106)\n\nReverts #4060, fixes #4095, unfixes #4056, #3837. Better the devil you\r\nknow than the devil you don't, I always say.\r\n\r\nSee also https://issues.apache.org/jira/browse/CURATOR-394.", "commitUser": "fjy", "commitDateTime": "2017-03-23 14:20:14", "commitParents": ["4cd566952bea23104a43c4beeddbea9f9725a6d8"], "commitGHEventType": "referenced", "nameRev": "db5bebbaaa5a60b24285f91c2157a2d97c26eae0 tags/druid-0.10.0-rc2~3", "commitHash": "db5bebbaaa5a60b24285f91c2157a2d97c26eae0"}, {"commitGitStats": [{"filePath": "pom.xml", "deletions": 1, "insertions": 1, "lines": 2}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "", "authoredDateTime": "2017-03-15 09:38:31", "commitMessage": "Update Curator to 2.12.0. (#4060)\n\nFixes #4056, #3837.", "commitUser": "gianm", "commitDateTime": "2017-03-15 09:38:31", "commitParents": ["c4f44c0590696a335dad60ffbc2dd8c00941511d"], "commitGHEventType": "referenced", "nameRev": "9cd666282cfada95c4ccd96a1ae294311857c41e tags/druid-0.10.1-rc1~180", "commitHash": "9cd666282cfada95c4ccd96a1ae294311857c41e"}], "body": "When zookeeper has a blip, the overlord can get into a condition we call \"split brain\" where leadership election is all screwed up. This causes the state of submitted requests to be non deterministic, sometimes \"succeeding\" or sometimes registering that it succeeding but not actually returning a success.\r\n\r\nWe have found two indicators for such a scenario. One is the following error in the logs of the overlord:\r\n```\r\norg.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /DRUID_PATH/indexer/leaderLatchPath/_c_3c32cd34-4370-4e26-922d-a9b24afa4a91-lock-0000102217\r\n        at com.google.common.base.Throwables.propagate(Throwables.java:160)\r\n        at io.druid.indexing.overlord.TaskMaster.getLeader(TaskMaster.java:251)\r\n        at io.druid.indexing.overlord.http.OverlordRedirectInfo.getRedirectURL(OverlordRedirectInfo.java:52)\r\n        at io.druid.server.http.RedirectFilter.doFilter(RedirectFilter.java:73)\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)\r\n        at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:83)\r\n        at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:364)\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)\r\n        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\r\n        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:221)\r\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1125)\r\n        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\r\n        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\r\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1059)\r\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\r\n        at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)\r\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)\r\n        at org.eclipse.jetty.server.Server.handle(Server.java:497)\r\n        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:310)\r\n        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:248)\r\n        at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:540)\r\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:620)\r\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:540)\r\n        at java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /DRUID_PATH/indexer/leaderLatchPath/_c_3c32cd34-4370-4e26-922d-a9b24afa4a91-lock-0000102217\r\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)\r\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\r\n        at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1212)\r\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:304)\r\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:293)\r\n        at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:108)\r\n        at org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:290)\r\n        at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:281)\r\n        at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:42)\r\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.participantForPath(LeaderSelector.java:375)\r\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.getLeader(LeaderSelector.java:346)\r\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.getLeader(LeaderSelector.java:339)\r\n        at io.druid.indexing.overlord.TaskMaster.getLeader(TaskMaster.java:243)\r\n        ... 22 more\r\n```\r\n\r\n( This looks like https://issues.apache.org/jira/browse/CURATOR-358 )\r\n\r\n\r\nAnd the other is increased CPU on both active overlords.\r\n\r\nThe solution is to restart the overlords and let them clean up their state.\r\n\r\nIn rare scenarios, a middle manager can fail to submit its stuff to the overlord after a peon completes, and will never complete because it can get in a state where the peon already gave up its task lock, but never properly finished its segment insertion at the middle manager level, so the middle manager will retry indefinitely. The fix here is to restart the middle manager, but this looses any outstanding stuff not yet submitted to the overlord."}, {"user": "gianm", "commits": {}, "labels": ["Bug"], "created": "2017-01-05 17:31:37", "title": "groupBy v2: Results not fully merged when caching is enabled on the broker", "url": "https://github.com/apache/druid/issues/3820", "closed": "2017-02-21 17:49:50", "ttf": 47.000277777777775, "commitsDetails": [], "body": "groupBy bySegment results are not sorted by the grouping dimensions, which means that when caching is enabled on the broker, they get stored in the cache in potentially unsorted order. When they get pulled back out on the broker, CachingClusteredClient attempts to merge the resulting sequences as if they were sorted (using a MergeSequence based on query.getResultOrdering), which leads to a not fully sorted sequence.\r\n\r\ngroupBy v1's `mergeResults` does not assume the sequence from CachingClusteredClient is sorted (it re-sorts it using an IncrementalIndex) so it works fine.\r\n\r\nBut groupBy v2's `mergeResults` assumes that the sequence it's combining is already sorted, which is true if caching is disabled (because the broker is merging sequences from data nodes, and they sort their results) but is not true in this case when caching is enabled.\r\n\r\nWe could fix this by sorting by-segment results in the cache or by somehow teaching groupBy v2 that in this particular case, it shouldn't assume the sequence it's combining is already sorted. But both of those would incur a performance penalty when caching is enabled, which is counterproductive to the purpose of caching\u2026 so I'm not sure what's best.\r\n\r\nSee https://groups.google.com/d/topic/druid-user/4gmKSw33EiM/discussion for the original report."}, {"user": "gianm", "commits": {}, "labels": ["Bug"], "created": "2017-01-04 23:07:54", "title": "groupBy v2: Deadlock on deeply nested subqueries", "url": "https://github.com/apache/druid/issues/3819", "closed": "2017-02-22 20:49:37", "ttf": 48.000277777777775, "commitsDetails": [], "body": "Deeply nested groupBys can cause deadlocks, because they require a separate merge buffer on the broker for each layer beyond the first layer of the groupBy. Since the merge buffers are limited in number and are acquired one-by-one, two problems can occur:\r\n\r\n- Case 1: a nested groupBy needs X buffers but numMergeBuffers < X, so it can never get what it needs\r\n- Case 2: two nested groupBys each need X buffers, but numMergeBuffers < X * 2 - 1, so it's possible for them to deadlock (both might get X - 1 buffers and then neither of them can get the final buffer).\r\n\r\nBetter behavior would be:\r\n\r\n- Case 1: the query should fail\r\n- Case 2: buffer acquisition should be atomic, so the queries can proceed without deadlocks"}, {"user": "gianm", "commits": {"6440ddcbca1e38847b726b170b68c54221cc5e31": {"commitGHEventType": "closed", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "Druid builds do not run properly on Java 7", "numCommits": 0, "created": "2016-12-21 15:51:03", "closed": "2016-12-21 18:19:28", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/3795", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 0.0002777777777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "extensions-contrib/druid-rocketmq/src/main/java/io/druid/firehose/rocketmq/RocketMQFirehoseFactory.java", "deletions": 4, "insertions": 4, "lines": 8}, {"filePath": "extensions-core/caffeine-cache/pom.xml", "deletions": 0, "insertions": 22, "lines": 22}, {"filePath": "indexing-service/src/main/java/io/druid/indexing/overlord/supervisor/SupervisorManager.java", "deletions": 2, "insertions": 3, "lines": 5}, {"filePath": "pom.xml", "deletions": 0, "insertions": 28, "lines": 28}, {"filePath": "server/src/main/java/io/druid/server/router/CoordinatorRuleManager.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "java-util/src/main/java/io/druid/java/util/common/collect/JavaCompatUtils.java", "deletions": 0, "insertions": 35, "lines": 35}, {"filePath": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java", "deletions": 10, "insertions": 21, "lines": 31}], "commitSpoonAstDiffStats": [{"spoonFilePath": "SupervisorManager.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.overlord.supervisor.SupervisorManager.getSupervisorIds()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.overlord.supervisor.SupervisorManager.stop()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "KafkaSupervisor.java", "spoonMethods": [{"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.checkPendingCompletionTasks()", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.checkCurrentTaskState()", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.signalTasksToFinish(int)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.signalTasksToFinish(int).7.apply(java.util.List)", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.createNewTasks()", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.TaskGroup.taskIds()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.checkTaskDuration()", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "CoordinatorRuleManager.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.server.router.CoordinatorRuleManager.poll()", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "JavaCompatUtils.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.java.util.common.collect.JavaCompatUtils", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "RocketMQFirehoseFactory.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.firehose.rocketmq.RocketMQFirehoseFactory.connect(io.druid.data.input.ByteBufferInputRowParser).1.nextRow()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.firehose.rocketmq.RocketMQFirehoseFactory.connect(io.druid.data.input.ByteBufferInputRowParser).1.hasMore()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.firehose.rocketmq.RocketMQFirehoseFactory.DruidPullMessageService.doPull()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-12-21 13:19:13", "commitMessage": "Fix #3795 (Java 7 compatibility). (#3796)\n\n* Fix #3795 (Java 7 compatibility).\r\n\r\nAlso introduce Animal Sniffer checks during build, which would\r\nhave caught the original problems.\r\n\r\n* Add Animal Sniffer on caffeine-cache for JDK8.\r\n", "commitUser": "fjy", "commitDateTime": "2016-12-21 10:19:13", "commitParents": ["c0c34f82ad24098f3820e81b9feeef7f2028e0ba"], "commitGHEventType": "closed", "nameRev": "6440ddcbca1e38847b726b170b68c54221cc5e31 tags/druid-0.10.0-rc1~123", "commitHash": "6440ddcbca1e38847b726b170b68c54221cc5e31"}], "body": "Druid builds need to be done with JDK8 compilers, because one of the extensions (druid-caffeine-cache) targets JDK8. But the other modules are supposed to target JDK7 and so we pass `-source 1.7 -target 1.7` to javac. But, this is not enough to properly target JDK7, due to method signature changes the Java API. One example is that `ConcurrentHashMap.keySet`'s return type changed from `Set` to `KeySetView`, leading to errors like the one below when Druid is run on JDK7.\r\n\r\nWe use this method in 4 places:\r\n\r\n- io.druid.indexing.overlord.supervisor.SupervisorManager\r\n- io.druid.indexing.kafka.supervisor.KafkaSupervisor\r\n- io.druid.firehose.rocketmq.RocketMQFirehoseFactory\r\n- io.druid.server.router.CoordinatorRuleManager\r\n\r\nPresumably if one avoids the features corresponding to these classes (supervised ingestion, rocketmq extension, and the router; none of them are required) then things should work fine on JDK7. But still, this is not what we intended. Possible fixes:\r\n\r\n- Cast ConcurrentHashMap to Map before calling keySet\r\n- Pass the\u00a0JDK7 runtime to bootclasspath when compiling with javac\r\n\r\nIf #3746 is agreed on then this issue becomes moot. See also https://bugs.openjdk.java.net/browse/JDK-8151366 for more details.\r\n\r\n```\r\njava.lang.reflect.InvocationTargetException\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_121]\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_121]\r\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_121]\r\n    at java.lang.reflect.Method.invoke(Method.java:606) ~[?:1.7.0_121]\r\n    at com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler.stop(Lifecycle.java:362) [java-util-0.27.10.jar:?]\r\n    at com.metamx.common.lifecycle.Lifecycle.stop(Lifecycle.java:280) [java-util-0.27.10.jar:?]\r\n    at io.druid.indexing.overlord.TaskMaster.stopLeading(TaskMaster.java:227) [druid-indexing-service-0.9.2.jar:0.9.2]\r\n    at io.druid.indexing.overlord.TaskMaster.stop(TaskMaster.java:207) [druid-indexing-service-0.9.2.jar:0.9.2]\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_121]\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_121]\r\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_121]\r\n    at java.lang.reflect.Method.invoke(Method.java:606) ~[?:1.7.0_121]\r\n    at com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler.stop(Lifecycle.java:362) [java-util-0.27.10.jar:?]\r\n    at com.metamx.common.lifecycle.Lifecycle.stop(Lifecycle.java:280) [java-util-0.27.10.jar:?]\r\n    at com.metamx.common.lifecycle.Lifecycle$1.run(Lifecycle.java:306) [java-util-0.27.10.jar:?]\r\n    at java.lang.Thread.run(Thread.java:745) [?:1.7.0_121]\r\nCaused by: java.lang.NoSuchMethodError: java.util.concurrent.ConcurrentHashMap.keySet()Ljava/util/concurrent/ConcurrentHashMap$KeySetView;\r\n    at io.druid.indexing.overlord.supervisor.SupervisorManager.stop(SupervisorManager.java:116) ~[druid-indexing-service-0.9.2.jar:0.9.2]\r\n    ... 16 more\r\n```"}, {"user": "leventov", "commits": {}, "labels": ["Bug", "Contributions Welcome", "Starter"], "created": "2016-12-12 20:34:46", "title": "Race condition in KafkaLookupExtractorFactory", "url": "https://github.com/apache/druid/issues/3772", "closed": "2019-10-28 20:40:04", "ttf": 1050.0002777777777, "commitsDetails": [], "body": "https://github.com/druid-io/druid/pull/3403#discussion_r88312768\r\n\r\nFYI @gvsmirnov: just creating an issue in order not to forget about it"}, {"user": "gianm", "commits": {}, "labels": ["Bug"], "created": "2016-12-06 18:34:17", "title": "Jetty threads stuck on HttpOutput.write", "url": "https://github.com/apache/druid/issues/3745", "closed": "2017-02-14 20:52:16", "ttf": 70.00027777777778, "commitsDetails": [], "body": "See https://groups.google.com/d/msg/druid-development/rQMk8C4cUfs/atGoF8XEBQAJ for the report. I've seen this on other clusters as well with multiple Druid versions (including 0.9.1.1). It seems connected to periods of high load, especially involving long running queries. In the two times I've seen this, upgrading to Jetty 9.3.12.v20160915 helped, although that can't be done without also requiring Java 8 so I didn't submit a patch back to Druid.\r\n\r\nThis issue tracks the problem and can be resolved when we build for Java 8.\r\n\r\nThread stack of a stuck thread:\r\n\r\n```\r\n\"qtp362370312-874\" daemon prio=10 tid=0x00007f7d460ee000 nid=0x6c80 waiting on condition [0x00007f7d0f9da000]\r\n   java.lang.Thread.State: TIMED_WAITING (parking)\r\n        at sun.misc.Unsafe.park(Native Method)\r\n        - parking to wait for  <0x0000000738d2d000> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\r\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)\r\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2176)\r\n        at org.eclipse.jetty.util.SharedBlockingCallback$Blocker.block(SharedBlockingCallback.java:213)\r\n        at org.eclipse.jetty.server.HttpOutput.write(HttpOutput.java:133)\r\n        at org.eclipse.jetty.server.HttpOutput.write(HttpOutput.java:347)\r\n        at org.eclipse.jetty.servlets.gzip.DeflatedOutputStream.deflate(DeflatedOutputStream.java:74)\r\n        at org.eclipse.jetty.servlets.gzip.DeflatedOutputStream.write(DeflatedOutputStream.java:64)\r\n        at org.eclipse.jetty.servlets.gzip.GzipOutputStream.write(GzipOutputStream.java:46)\r\n```\r\n\r\nhttps://bugs.eclipse.org/bugs/show_bug.cgi?id=478923 is possibly related."}, {"user": "pjain1", "commits": {}, "labels": ["Bug"], "created": "2016-12-05 19:18:33", "title": "Regression in 3678 - allocation of pending segment fails in some cases ", "url": "https://github.com/apache/druid/issues/3741", "closed": "2017-02-02 20:57:46", "ttf": 59.000277777777775, "commitsDetails": [], "body": "After the introduction of https://github.com/druid-io/druid/pull/3678 the Kafka Indexing task can reset offset automatically without supervisor knowing about it. \r\n\r\nLets say the supervisor assigned some partition to a task to consume and the task reset the offset because the offset was not present in Kafka. It may very well happen that the fetched event falls in the next time interval and not in the same time interval as last pending allocated segment for this sequence. Since, the metadata like lastSegmentIds is restored when the `Appenderator` starts therefore when the task asks overlord to allocate a pending segment for this event, the overlord will get the last pending allocated segment which is not the correct segment and the interval check at https://github.com/druid-io/druid/blob/master/server/src/main/java/io/druid/metadata/IndexerSQLMetadataStorageCoordinator.java#L423 will fail.\r\n\r\nI was wondering how necessary is this interval check ? If the interval does not match can we just continue and allocate segment for next interval ? I think the check is present there to prevent from this kind of allocations but is it harmful ? Another option is to pass in some special flag to overlord to indicate that automatic reset has happened and thus don't be so strict about interval checks.\r\n\r\n@dclim @gianm "}, {"user": "gianm", "commits": {}, "labels": ["Bug"], "created": "2016-12-01 19:31:33", "title": "Fix mvn javadoc:jar in master", "url": "https://github.com/apache/druid/issues/3730", "closed": "2017-02-08 19:54:42", "ttf": 69.00027777777778, "commitsDetails": [], "body": "#3729 contains a fix for 0.9.2 but master still has this issue."}, {"user": "gianm", "commits": {}, "labels": ["Bug", "Incompatible"], "created": "2016-11-29 01:40:10", "title": "Improper topN caching when sorting by postaggregator  ", "url": "https://github.com/apache/druid/issues/3719", "closed": "2017-02-13 20:23:44", "ttf": 76.00027777777778, "commitsDetails": [], "body": "When a topN is sorting by a postaggregator, the postaggregator name is part of the query cache key but the definition is not. But, the postaggregator should be part of the cache key too, since it's used to determine which result rows to keep for a particular segment in the cache. If the postaggregator is redefined but keeps the same name, the query improperly uses previously cached results.\r\n\r\nI think fixing this for real will require adding a getCacheKey method to PostAggregators.\r\n\r\nIf we want to hold off on that until 0.10.0 then we could work around it in the meantime by disabling caching for any topN query that sorts on a postaggregator. But this has a performance cost that could hit more people than the bug does."}, {"user": "gianm", "commits": {"9ad34a3f03ffc1e98a74379a9929d08e7b8394af": {"commitGHEventType": "referenced", "commitUser": "fjy"}, "93b873d3e38a02feec232fe24030ee4e357379bf": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "groupBy regression on time extractions named __time", "numCommits": 0, "created": "2016-11-12 01:05:40", "closed": "2016-11-14 17:30:18", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/3683", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 2.000277777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "processing/src/test/java/io/druid/query/groupby/GroupByQueryRunnerTest.java", "deletions": 0, "insertions": 98, "lines": 98}, {"filePath": "processing/src/main/java/io/druid/query/groupby/GroupByQueryHelper.java", "deletions": 10, "insertions": 32, "lines": 42}], "commitSpoonAstDiffStats": [{"spoonFilePath": "GroupByQueryHelper.java", "spoonMethods": [{"INS": 6, "UPD": 2, "DEL": 3, "spoonMethodName": "io.druid.query.groupby.GroupByQueryHelper.createIndexAccumulatorPair(io.druid.query.groupby.GroupByQuery,io.druid.query.groupby.GroupByQueryConfig,io.druid.collections.StupidPool)", "MOV": 1, "TOT": 12}]}, {"spoonFilePath": "GroupByQueryRunnerTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.groupby.GroupByQueryRunnerTest.testGroupByWithUnderUnderTimeAsDimensionNameWithHavingAndLimit()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.groupby.GroupByQueryRunnerTest.testGroupByTimeExtractionNamedUnderUnderTime()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-11-14 09:30:18", "commitMessage": "groupBy v1: Force all dimensions to strings. (#3685)\n\nFixes #3683.", "commitUser": "fjy", "commitDateTime": "2016-11-14 09:30:18", "commitParents": ["bcd20441be05623d003d4446bf58f0463e73df37"], "commitGHEventType": "referenced", "nameRev": "9ad34a3f03ffc1e98a74379a9929d08e7b8394af tags/druid-0.10.0-rc1~191", "commitHash": "9ad34a3f03ffc1e98a74379a9929d08e7b8394af"}, {"commitGitStats": [{"filePath": "processing/src/test/java/io/druid/query/groupby/GroupByQueryRunnerTest.java", "deletions": 0, "insertions": 98, "lines": 98}, {"filePath": "processing/src/main/java/io/druid/query/groupby/GroupByQueryHelper.java", "deletions": 10, "insertions": 32, "lines": 42}], "commitSpoonAstDiffStats": [{"spoonFilePath": "GroupByQueryHelper.java", "spoonMethods": [{"INS": 6, "UPD": 2, "DEL": 3, "spoonMethodName": "io.druid.query.groupby.GroupByQueryHelper.createIndexAccumulatorPair(io.druid.query.groupby.GroupByQuery,io.druid.query.groupby.GroupByQueryConfig,io.druid.collections.StupidPool)", "MOV": 1, "TOT": 12}]}, {"spoonFilePath": "GroupByQueryRunnerTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.groupby.GroupByQueryRunnerTest.testGroupByWithUnderUnderTimeAsDimensionNameWithHavingAndLimit()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.groupby.GroupByQueryRunnerTest.testGroupByTimeExtractionNamedUnderUnderTime()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-11-14 13:30:27", "commitMessage": "groupBy v1: Force all dimensions to strings. (#3685) (#3691)\n\nFixes #3683.", "commitUser": "fjy", "commitDateTime": "2016-11-14 13:30:27", "commitParents": ["9186d5481f030a80746c37a064754fa4406a4387"], "commitGHEventType": "referenced", "nameRev": "93b873d3e38a02feec232fe24030ee4e357379bf tags/druid-0.9.2-rc3~2", "commitHash": "93b873d3e38a02feec232fe24030ee4e357379bf"}], "body": "This test passed in 0.9.1.1 but fails in 0.9.2-rc2 for groupBy engine v1. It works with the v2 engine. Nevertheless I'm marking this 0.9.2 since it's a regression in the v1 engine, which is still default.\r\n\r\n```java\r\n  @Test\r\n  public void testGroupByTimeExtractionNamedUnderUnderTime()\r\n  {\r\n    GroupByQuery query = GroupByQuery\r\n        .builder()\r\n        .setDataSource(QueryRunnerTestHelper.dataSource)\r\n        .setQuerySegmentSpec(QueryRunnerTestHelper.fullOnInterval)\r\n        .setDimensions(\r\n            Lists.newArrayList(\r\n                new DefaultDimensionSpec(\"market\", \"market\"),\r\n                new ExtractionDimensionSpec(\r\n                    Column.TIME_COLUMN_NAME,\r\n                    Column.TIME_COLUMN_NAME,\r\n                    new TimeFormatExtractionFn(\"EEEE\", null, null, null),\r\n                    null\r\n                )\r\n            )\r\n        )\r\n        .setAggregatorSpecs(\r\n            Arrays.asList(\r\n                QueryRunnerTestHelper.rowsCount,\r\n                QueryRunnerTestHelper.indexDoubleSum\r\n            )\r\n        )\r\n        .setPostAggregatorSpecs(Arrays.<PostAggregator>asList(QueryRunnerTestHelper.addRowsIndexConstant))\r\n        .setGranularity(QueryRunnerTestHelper.allGran)\r\n        .setDimFilter(\r\n            new OrDimFilter(\r\n                Arrays.<DimFilter>asList(\r\n                    new SelectorDimFilter(\"market\", \"spot\", null),\r\n                    new SelectorDimFilter(\"market\", \"upfront\", null)\r\n                )\r\n            )\r\n        )\r\n        .build();\r\n    List<Row> expectedResults = Arrays.asList(\r\n        GroupByQueryRunnerTestHelper.createExpectedRow(\r\n            \"1970-01-01\",\r\n            \"__time\",\r\n            \"Friday\",\r\n            \"market\",\r\n            \"spot\",\r\n            \"index\",\r\n            13219.574157714844,\r\n            \"rows\",\r\n            117L,\r\n            \"addRowsIndexConstant\",\r\n            13337.574157714844\r\n        ),\r\n        GroupByQueryRunnerTestHelper.createExpectedRow(\r\n            \"1970-01-01\",\r\n            \"__time\",\r\n            \"Monday\",\r\n            \"market\",\r\n            \"spot\",\r\n            \"index\",\r\n            13557.738830566406,\r\n            \"rows\",\r\n            117L,\r\n            \"addRowsIndexConstant\",\r\n            13675.738830566406\r\n        ),\r\n        GroupByQueryRunnerTestHelper.createExpectedRow(\r\n            \"1970-01-01\",\r\n            \"__time\",\r\n            \"Saturday\",\r\n            \"market\",\r\n            \"spot\",\r\n            \"index\",\r\n            13493.751281738281,\r\n            \"rows\",\r\n            117L,\r\n            \"addRowsIndexConstant\",\r\n            13611.751281738281\r\n        ),\r\n        GroupByQueryRunnerTestHelper.createExpectedRow(\r\n            \"1970-01-01\",\r\n            \"__time\",\r\n            \"Sunday\",\r\n            \"market\",\r\n            \"spot\",\r\n            \"index\",\r\n            13585.541015625,\r\n            \"rows\",\r\n            117L,\r\n            \"addRowsIndexConstant\",\r\n            13703.541015625\r\n        ),\r\n        GroupByQueryRunnerTestHelper.createExpectedRow(\r\n            \"1970-01-01\",\r\n            \"__time\",\r\n            \"Thursday\",\r\n            \"market\",\r\n            \"spot\",\r\n            \"index\",\r\n            14279.127197265625,\r\n            \"rows\",\r\n            126L,\r\n            \"addRowsIndexConstant\",\r\n            14406.127197265625\r\n        ),\r\n        GroupByQueryRunnerTestHelper.createExpectedRow(\r\n            \"1970-01-01\",\r\n            \"__time\",\r\n            \"Tuesday\",\r\n            \"market\",\r\n            \"spot\",\r\n            \"index\",\r\n            13199.471435546875,\r\n            \"rows\",\r\n            117L,\r\n            \"addRowsIndexConstant\",\r\n            13317.471435546875\r\n        ),\r\n        GroupByQueryRunnerTestHelper.createExpectedRow(\r\n            \"1970-01-01\",\r\n            \"__time\",\r\n            \"Wednesday\",\r\n            \"market\",\r\n            \"spot\",\r\n            \"index\",\r\n            14271.368591308594,\r\n            \"rows\",\r\n            126L,\r\n            \"addRowsIndexConstant\",\r\n            14398.368591308594\r\n        ),\r\n        GroupByQueryRunnerTestHelper.createExpectedRow(\r\n            \"1970-01-01\",\r\n            \"__time\",\r\n            \"Friday\",\r\n            \"market\",\r\n            \"upfront\",\r\n            \"index\",\r\n            27297.8623046875,\r\n            \"rows\",\r\n            26L,\r\n            \"addRowsIndexConstant\",\r\n            27324.8623046875\r\n        ),\r\n        GroupByQueryRunnerTestHelper.createExpectedRow(\r\n            \"1970-01-01\",\r\n            \"__time\",\r\n            \"Monday\",\r\n            \"market\",\r\n            \"upfront\",\r\n            \"index\",\r\n            27619.58447265625,\r\n            \"rows\",\r\n            26L,\r\n            \"addRowsIndexConstant\",\r\n            27646.58447265625\r\n        ),\r\n        GroupByQueryRunnerTestHelper.createExpectedRow(\r\n            \"1970-01-01\",\r\n            \"__time\",\r\n            \"Saturday\",\r\n            \"market\",\r\n            \"upfront\",\r\n            \"index\",\r\n            27820.83154296875,\r\n            \"rows\",\r\n            26L,\r\n            \"addRowsIndexConstant\",\r\n            27847.83154296875\r\n        ),\r\n        GroupByQueryRunnerTestHelper.createExpectedRow(\r\n            \"1970-01-01\",\r\n            \"__time\",\r\n            \"Sunday\",\r\n            \"market\",\r\n            \"upfront\",\r\n            \"index\",\r\n            24791.223876953125,\r\n            \"rows\",\r\n            26L,\r\n            \"addRowsIndexConstant\",\r\n            24818.223876953125\r\n        ),\r\n        GroupByQueryRunnerTestHelper.createExpectedRow(\r\n            \"1970-01-01\",\r\n            \"__time\",\r\n            \"Thursday\",\r\n            \"market\",\r\n            \"upfront\",\r\n            \"index\",\r\n            28562.748901367188,\r\n            \"rows\",\r\n            28L,\r\n            \"addRowsIndexConstant\",\r\n            28591.748901367188\r\n        ),\r\n        GroupByQueryRunnerTestHelper.createExpectedRow(\r\n            \"1970-01-01\",\r\n            \"__time\",\r\n            \"Tuesday\",\r\n            \"market\",\r\n            \"upfront\",\r\n            \"index\",\r\n            26968.280639648438,\r\n            \"rows\",\r\n            26L,\r\n            \"addRowsIndexConstant\",\r\n            26995.280639648438\r\n        ),\r\n        GroupByQueryRunnerTestHelper.createExpectedRow(\r\n            \"1970-01-01\",\r\n            \"__time\",\r\n            \"Wednesday\",\r\n            \"market\",\r\n            \"upfront\",\r\n            \"index\",\r\n            28985.5751953125,\r\n            \"rows\",\r\n            28L,\r\n            \"addRowsIndexConstant\",\r\n            29014.5751953125\r\n        )\r\n    );\r\n    Iterable<Row> results = GroupByQueryRunnerTestHelper.runQuery(factory, runner, query);\r\n    TestHelper.assertExpectedObjects(expectedResults, results, \"\");\r\n  }\r\n```"}, {"user": "leventov", "commits": {"2c5f0038fd8f063047fda629c52b319e7ecce3ce": {"commitGHEventType": "closed", "commitUser": "b-slim"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "OffHeap Lookup cache is broken (keeps everything off-heap AND on-heap)", "numCommits": 0, "created": "2016-11-04 21:42:07", "closed": "2018-05-04 17:01:16", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/3663", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 545.0002777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/NamespaceExtractionConfig.java", "deletions": 0, "insertions": 15, "lines": 15}, {"filePath": "docs/content/development/extensions-core/lookups-cached-global.md", "deletions": 2, "insertions": 5, "lines": 7}, {"filePath": "extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/OffHeapNamespaceExtractionCacheManager.java", "deletions": 1, "insertions": 1, "lines": 2}], "commitSpoonAstDiffStats": [{"spoonFilePath": "NamespaceExtractionConfig.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.lookup.namespace.NamespaceExtractionConfig.getNumBufferedEntries()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.lookup.namespace.NamespaceExtractionConfig", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.lookup.namespace.NamespaceExtractionConfig.setNumBufferedEntries(int)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "OffHeapNamespaceExtractionCacheManager.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.server.lookup.namespace.cache.OffHeapNamespaceExtractionCacheManager", "MOV": 0, "TOT": 2}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-05-04 18:00:55", "commitMessage": "Make lookup offheap buffer configurable (#5696)\n\n* Make lookup offheap buffer configurable\r\n\r\nFixes #3663\r\n\r\n* Address comments\r\n\r\n* Update docs\r\n\r\n* Update docs\r\n", "commitUser": "b-slim", "commitDateTime": "2018-05-04 10:00:55", "commitParents": ["c2b5e5ec9590e9413ddd969e74ad50bef2bff69b"], "commitGHEventType": "closed", "nameRev": "2c5f0038fd8f063047fda629c52b319e7ecce3ce tags/druid-0.13.0-incubating-rc1~304", "commitHash": "2c5f0038fd8f063047fda629c52b319e7ecce3ce"}], "body": "Here: https://github.com/druid-io/druid/blob/78159d7ca442cf8d99ea356d39932ab3adb8e10f/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/OffHeapNamespaceExtractionCacheManager.java#L79 it was supposed that MapDB caches 10 millions of bytes (see https://github.com/druid-io/druid/blob/420358029061acb09e5f40370881f88e5a36d8d5/docs/content/development/extensions-core/lookups-cached-global.md, search \"10MB\" in page), but actually MapDB caches 10 millions *entries* (check the MapDB source code).\r\n\r\nAs of now the recommendation should probably be just not to use off-heap lookup cache, until a suitable solution is found."}, {"user": "zhihuij", "commits": {}, "labels": ["Bug"], "created": "2016-11-04 08:12:32", "title": "Files left on disk when batch task with IngestSegmentFirehose is done", "url": "https://github.com/apache/druid/issues/3655", "closed": "2017-03-18 01:37:28", "ttf": 133.00027777777777, "commitsDetails": [], "body": "We use batch task(normal batch task, not hadoop), and found that after the batch task completed, there were many files left on disk, and after serval days, we have to clean the files.  Can Druid clean these files after the batch task is done?\r\n\r\nExample directory list:\r\ndrwxr-xr-x 3 user group 4096 Nov  4 00:05 2016-11-03T00:00:00.000Z_2016-11-03T01:00:00.000Z\r\ndrwxr-xr-x 3 user group 4096 Nov  4 00:05 2016-11-03T01:00:00.000Z_2016-11-03T02:00:00.000Z\r\ndrwxr-xr-x 3 user group 4096 Nov  4 00:05 2016-11-03T02:00:00.000Z_2016-11-03T03:00:00.000Z\r\ndrwxr-xr-x 3 user group 4096 Nov  4 00:05 2016-11-03T03:00:00.000Z_2016-11-03T04:00:00.000Z\r\ndrwxr-xr-x 3 user group 4096 Nov  4 00:05 2016-11-03T04:00:00.000Z_2016-11-03T05:00:00.000Z\r\ndrwxr-xr-x 3 user group 4096 Nov  4 00:05 2016-11-03T05:00:00.000Z_2016-11-03T06:00:00.000Z\r\ndrwxr-xr-x 3 user group 4096 Nov  4 06:05 2016-11-03T06:00:00.000Z_2016-11-03T07:00:00.000Z\r\ndrwxr-xr-x 3 user group 4096 Nov  4 06:05 2016-11-03T07:00:00.000Z_2016-11-03T08:00:00.000Z\r\ndrwxr-xr-x 3 user group 4096 Nov  4 06:05 2016-11-03T08:00:00.000Z_2016-11-03T09:00:00.000Z\r\ndrwxr-xr-x 3 user group 4096 Nov  4 06:05 2016-11-03T09:00:00.000Z_2016-11-03T10:00:00.000Z\r\ndrwxr-xr-x 3 user group 4096 Nov  4 06:05 2016-11-03T10:00:00.000Z_2016-11-03T11:00:00.000Z\r\ndrwxr-xr-x 3 user group 4096 Nov  4 06:05 2016-11-03T11:00:00.000Z_2016-11-03T12:00:00.000Z\r\ndrwxr-xr-x 3 user group 4096 Nov  4 12:05 2016-11-03T12:00:00.000Z_2016-11-03T13:00:00.000Z\r\ndrwxr-xr-x 3 user group 4096 Nov  4 12:05 2016-11-03T13:00:00.000Z_2016-11-03T14:00:00.000Z\r\ndrwxr-xr-x 3 user group 4096 Nov  4 12:05 2016-11-03T14:00:00.000Z_2016-11-03T15:00:00.000Z\r\ndrwxr-xr-x 3 user group 4096 Nov  4 12:05 2016-11-03T15:00:00.000Z_2016-11-03T16:00:00.000Z\r\ndrwxr-xr-x 3 user group 4096 Nov  4 12:05 2016-11-03T16:00:00.000Z_2016-11-03T17:00:00.000Z\r\ndrwxr-xr-x 3 user group 4096 Nov  4 12:05 2016-11-03T17:00:00.000Z_2016-11-03T18:00:00.000Z"}, {"user": "gianm", "commits": {"45940d6e4071739d7017718c2801e44380356432": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {}, "title": "Null key in timeseries result for hyperUnique on missing columns", "numCommits": 0, "created": "2016-10-31 19:05:46", "closed": "2016-10-31 22:51:37", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "filteredCommits": [], "url": "https://github.com/apache/druid/issues/3625", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 0.0002777777777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "processing/src/test/java/io/druid/query/SchemaEvolutionTest.java", "deletions": 0, "insertions": 372, "lines": 372}, {"filePath": "processing/src/test/java/io/druid/query/QueryRunnerTestHelper.java", "deletions": 7, "insertions": 46, "lines": 53}, {"filePath": "processing/src/main/java/io/druid/query/aggregation/LongSumAggregatorFactory.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "processing/src/main/java/io/druid/query/aggregation/LongMinAggregatorFactory.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "processing/src/main/java/io/druid/query/aggregation/LongMaxAggregatorFactory.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "processing/src/main/java/io/druid/query/aggregation/DoubleSumAggregatorFactory.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "common/src/main/java/io/druid/math/expr/Expr.java", "deletions": 14, "insertions": 45, "lines": 59}, {"filePath": "processing/src/main/java/io/druid/query/aggregation/DoubleMinAggregatorFactory.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "processing/src/main/java/io/druid/query/aggregation/AggregatorUtil.java", "deletions": 8, "insertions": 12, "lines": 20}, {"filePath": "processing/src/main/java/io/druid/query/aggregation/DoubleMaxAggregatorFactory.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "processing/src/test/java/io/druid/segment/IndexBuilder.java", "deletions": 6, "insertions": 0, "lines": 6}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "tooManyChanges", "authoredDateTime": "2016-11-01 09:40:25", "commitMessage": "Math expressions support for missing columns. (#3630)\n\nAlso add SchemaEvolutionTest to help test this kind of thing.\r\n\r\nFixes #3627 and includes test for #3625.", "commitUser": "fjy", "commitDateTime": "2016-11-01 09:40:25", "commitParents": ["0e269ce72ac75b35d27b9257a8c6774f98cfbc82"], "commitGHEventType": "referenced", "nameRev": "45940d6e4071739d7017718c2801e44380356432 tags/druid-0.10.0-rc1~222", "commitHash": "45940d6e4071739d7017718c2801e44380356432"}], "body": "If the column for a hyperUnique aggregator doesn't exist, a null key appears in the TimeseriesResultValue. If results get combined by TimeseriesBinaryFn, these null keys get filtered out. But if results don't get combined (possibly because only one segment matched the query) then the query fails with a Jackson exception when it tries to serialize the null key, because null keys aren't allowed in JSON."}, {"user": "dclim", "commits": {}, "labels": ["Bug", "duplicate"], "created": "2016-10-28 19:24:15", "title": "KafkaIndexTask never succeeds handoff if time interval for segment is marked as drop in load rules", "url": "https://github.com/apache/druid/issues/3619", "closed": "2016-10-28 23:08:19", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "If the KafkaIndexTask ingests an event with a timestamp in an interval that has been marked as DROP in the load rules, when the segment is published and the task waits for handoff, this segment will never be loaded by a historical and the task will never receive a callback and will wait around until it's killed due to timeout.\n\nProbably would be better if the task could determine that the segment will not be loaded because of the current ruleset and skip waiting for handoff.\n"}, {"user": "gianm", "commits": {}, "labels": ["Bug"], "created": "2016-10-25 18:57:21", "title": "IngestSegmentFirehoseFactory race between tasks", "url": "https://github.com/apache/druid/issues/3608", "closed": "2017-03-17 22:57:25", "ttf": 143.00027777777777, "commitsDetails": [], "body": "IngestSegmentFirehoseFactory uses a fixed task id `\"reindex\"` when creating the toolbox factory that it uses to get segments. This creates a race condition where two tasks could actually download the same segment at the same time into the same directory, and one will get clobbered and fail.\n\nOne situation where this can happen easily is if you're reindexing a datasource into _two_ datasources (maybe reducing to two different levels of grain). The tasks will proceed simultaneously, since there's no cross locking, but they'll be downloading the same segments for the input datasource.\n\nOne fix is having it actually use the real task id (which would need to get plumbed in). This has the advantage of putting the task files all together, and using the existing mechanisms for cleaning up task work directories.\n"}, {"user": "pjain1", "commits": {}, "labels": ["Bug"], "created": "2016-10-20 18:47:29", "title": "Possible race condition while updating DataSource Metadata [Kafka Indexing Service]", "url": "https://github.com/apache/druid/issues/3600", "closed": "2016-12-01 18:50:15", "ttf": 42.000277777777775, "commitsDetails": [], "body": "We have Kafka indexing service running currently with no replication and task count equivalent to number of kafka partitions. One of the tasks failed with this exception - \n\n```\ncom.metamx.common.ISE: Transaction failure publishing segments, aborting\n        at io.druid.indexing.kafka.KafkaIndexTask.run(KafkaIndexTask.java:524) ~[druid-kafka-indexing-service-0.9.3-1476736930-8d59341-1004.jar:0.9.3-1476736930-8d59341-1004]\n        at io.druid.indexing.overlord.ThreadPoolTaskRunner$ThreadPoolTaskRunnerCallable.call(ThreadPoolTaskRunner.java:436) [druid-indexing-service-0.9.3-1476736930-8d59341-1004.jar:0.9.3-1476736930-8d59341-1004]\n        at io.druid.indexing.overlord.ThreadPoolTaskRunner$ThreadPoolTaskRunnerCallable.call(ThreadPoolTaskRunner.java:408) [druid-indexing-service-0.9.3-1476736930-8d59341-1004.jar:0.9.3-1476736930-8d59341-1004]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_60]\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_60]\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_60]\n        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]\n2016-10-19T08:23:19,572 INFO [task-runner-0-priority-0] io.druid.indexing.overlord.TaskRunnerUtils - Task [index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj] status changed to [FAILED].\n2016-10-19T08:23:19,575 INFO [task-runner-0-priority-0] io.druid.indexing.worker.executor.ExecutorLifecycle - Task completed with status: {\n  \"id\" : \"index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj\",\n  \"status\" : \"FAILED\",\n  \"duration\" : 11637165\n}\n```\n\nEarlier in the task log I see this - \n\n```\n2016-10-19T08:23:19,300 INFO [task-runner-0-priority-0] io.druid.indexing.common.actions.RemoteTaskActionClient - Submitting action for task[index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj] to overlord[/druid/indexer/v1/action]: SegmentInsertAction{segments=[DataSegment{size=357264367, shardSpec=NumberedShardSpec{partitionNum=36, partitions=0}, metrics=[], dimensions=[], version='2016-10-19T05:00:01.567Z', loadSpec={type=hdfs, path=}, interval=2016-10-19T05:00:00.000Z/2016-10-19T06:00:00.000Z, dataSource='v2_metrics_cluster', binaryVersion='9'}, DataSegment{size=328688502, shardSpec=NumberedShardSpec{partitionNum=5, partitions=0}, metrics=[], dimensions=[], version='2016-10-19T07:00:00.975Z', loadSpec={type=hdfs, path=}, interval=2016-10-19T07:00:00.000Z/2016-10-19T08:00:00.000Z, dataSource='v2_metrics_cluster', binaryVersion='9'}, DataSegment{size=55419587, shardSpec=NumberedShardSpec{partitionNum=3, partitions=0}, metrics=[], dimensions=[], version='2016-10-19T08:00:00.443Z', loadSpec={type=hdfs, path=}, interval=2016-10-19T08:00:00.000Z/2016-10-19T09:00:00.000Z, dataSource='v2_metrics_cluster', binaryVersion='9'}, DataSegment{size=396175234, shardSpec=NumberedShardSpec{partitionNum=1, partitions=0}, metrics=[], dimensions=[], version='2016-10-19T06:00:00.545Z', loadSpec={type=hdfs, path=}, interval=2016-10-19T06:00:00.000Z/2016-10-19T07:00:00.000Z, dataSource='v2_metrics_cluster', binaryVersion='9'}], startMetadata=KafkaDataSourceMetadata{kafkaPartitions=KafkaPartitions{topic='mx-cluster-production-metrics', partitionOffsetMap={8=158229668}}}, endMetadata=KafkaDataSourceMetadata{kafkaPartitions=KafkaPartitions{topic='mx-cluster-production-metrics', partitionOffsetMap={8=202436580}}}}\n2016-10-19T08:23:19,318 INFO [task-runner-0-priority-0] com.metamx.http.client.pool.ChannelResourceFactory - Generating:\n2016-10-19T08:23:19,411 INFO [task-runner-0-priority-0] io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver - Transaction failure while publishing segments, checking if someone else beat us to it.\n2016-10-19T08:23:19,414 INFO [task-runner-0-priority-0] io.druid.indexing.common.actions.RemoteTaskActionClient - Performing action for task[index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj]: SegmentListUsedAction{dataSource='v2_metrics_cluster', intervals=[2016-10-19T05:00:00.000Z/2016-10-19T09:00:00.000Z]}\n2016-10-19T08:23:19,419 INFO [task-runner-0-priority-0] io.druid.indexing.common.actions.RemoteTaskActionClient - Submitting action for task[index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj] to overlord[/druid/indexer/v1/action]: SegmentListUsedAction{dataSource='v2_metrics_cluster', intervals=[2016-10-19T05:00:00.000Z/2016-10-19T09:00:00.000Z]}\n2016-10-19T08:23:19,422 INFO [task-runner-0-priority-0] com.metamx.http.client.pool.ChannelResourceFactory - Generating:\n2016-10-19T08:23:19,513 WARN [task-runner-0-priority-0] io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver - Our segments don't exist, giving up.\n2016-10-19T08:23:19,524 INFO [task-runner-0-priority-0] io.druid.segment.realtime.appenderator.AppenderatorImpl - Shutting down...\n```\n\nMeanwhile this was in the overlord log -\n\n```\n2016-10-19T08:23:19,303 INFO [qtp370356001-171] io.druid.indexing.common.actions.LocalTaskActionClient - Performing action for task[index_kafka_v2_metrics_cluster_a47237b6077ddb0_opnaifmo]: SegmentInsertAction{segments=[DataSegment{size=396764861, shardSpec=NumberedShardSpec{partitionNum=12, partitions=0}, metrics=[], dimensions=[], version='2016-10-19T06:00:00.545Z', loadSpec={type=hdfs, path=}, interval=2016-10-19T06:00:00.000Z/2016-10-19T07:00:00.000Z, dataSource='v2_metrics_cluster', binaryVersion='9'}, DataSegment{size=327245790, shardSpec=NumberedShardSpec{partitionNum=7, partitions=0}, metrics=[], dimensions=[], version='2016-10-19T07:00:00.975Z', loadSpec={type=hdfs, path=}, interval=2016-10-19T07:00:00.000Z/2016-10-19T08:00:00.000Z, dataSource='v2_metrics_cluster', binaryVersion='9'}, DataSegment{size=55834521, shardSpec=NumberedShardSpec{partitionNum=20, partitions=0}, metrics=[], dimensions=[], version='2016-10-19T08:00:00.443Z', loadSpec={type=hdfs, path=}, interval=2016-10-19T08:00:00.000Z/2016-10-19T09:00:00.000Z, dataSource='v2_metrics_cluster', binaryVersion='9'}, DataSegment{size=357963459, shardSpec=NumberedShardSpec{partitionNum=33, partitions=0}, metrics=[], dimensions=[], version='2016-10-19T05:00:01.567Z', loadSpec={type=hdfs, path=}, interval=2016-10-19T05:00:00.000Z/2016-10-19T06:00:00.000Z, dataSource='v2_metrics_cluster', binaryVersion='9'}], startMetadata=KafkaDataSourceMetadata{kafkaPartitions=KafkaPartitions{topic='mx-cluster-production-metrics', partitionOffsetMap={0=176009748}}}, endMetadata=KafkaDataSourceMetadata{kafkaPartitions=KafkaPartitions{topic='mx-cluster-production-metrics', partitionOffsetMap={0=220414028}}}}\n2016-10-19T08:23:19,321 INFO [qtp370356001-152] io.druid.indexing.common.actions.LocalTaskActionClient - Performing action for task[index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj]: SegmentInsertAction{segments=[DataSegment{size=357264367, shardSpec=NumberedShardSpec{partitionNum=36, partitions=0}, metrics=[], dimensions=[], version='2016-10-19T05:00:01.567Z', loadSpec={type=hdfs, path=}, interval=2016-10-19T05:00:00.000Z/2016-10-19T06:00:00.000Z, dataSource='v2_metrics_cluster', binaryVersion='9'}, DataSegment{size=328688502, shardSpec=NumberedShardSpec{partitionNum=5, partitions=0}, metrics=[], dimensions=[], version='2016-10-19T07:00:00.975Z', loadSpec={type=hdfs, path=}, interval=2016-10-19T07:00:00.000Z/2016-10-19T08:00:00.000Z, dataSource='v2_metrics_cluster', binaryVersion='9'}, DataSegment{size=55419587, shardSpec=NumberedShardSpec{partitionNum=3, partitions=0}, metrics=[], dimensions=[], version='2016-10-19T08:00:00.443Z', loadSpec={type=hdfs, path=}, interval=2016-10-19T08:00:00.000Z/2016-10-19T09:00:00.000Z, dataSource='v2_metrics_cluster', binaryVersion='9'}, DataSegment{size=396175234, shardSpec=NumberedShardSpec{partitionNum=1, partitions=0}, metrics=[], dimensions=[], version='2016-10-19T06:00:00.545Z', loadSpec={type=hdfs, path=}, interval=2016-10-19T06:00:00.000Z/2016-10-19T07:00:00.000Z, dataSource='v2_metrics_cluster', binaryVersion='9'}], startMetadata=KafkaDataSourceMetadata{kafkaPartitions=KafkaPartitions{topic='mx-cluster-production-metrics', partitionOffsetMap={8=158229668}}}, endMetadata=KafkaDataSourceMetadata{kafkaPartitions=KafkaPartitions{topic='mx-cluster-production-metrics', partitionOffsetMap={8=202436580}}}}\n2016-10-19T08:23:19,327 INFO [qtp370356001-171] io.druid.metadata.IndexerSQLMetadataStorageCoordinator - Updated metadata from[KafkaDataSourceMetadata{kafkaPartitions=KafkaPartitions{topic='mx-cluster-production-metrics', partitionOffsetMap={0=176009748, 1=220256750, 2=175667526, 3=202804552, 4=158505772, 5=158445369, 6=158347444, 7=202503474, 8=158229668, 9=158129698, 10=158076757, 11=157993918, 12=157748351, 13=201366328, 14=157333088, 15=157251399, 16=157198909, 17=157139279, 18=157074283, 19=156992007, 20=156273109, 21=155401237, 22=155088780, 23=154850255, 24=154639521, 25=154498798, 26=154345135, 27=154142700, 28=154004983, 29=153935016}}}] to[KafkaDataSourceMetadata{kafkaPartitions=KafkaPartitions{topic='mx-cluster-production-metrics', partitionOffsetMap={0=220414028, 1=220256750, 2=175667526, 3=202804552, 4=158505772, 5=158445369, 6=158347444, 7=202503474, 8=158229668, 9=158129698, 10=158076757, 11=157993918, 12=157748351, 13=201366328, 14=157333088, 15=157251399, 16=157198909, 17=157139279, 18=157074283, 19=156992007, 20=156273109, 21=155401237, 22=155088780, 23=154850255, 24=154639521, 25=154498798, 26=154345135, 27=154142700, 28=154004983, 29=153935016}}}].\n2016-10-19T08:23:19,342 INFO [qtp370356001-171] io.druid.metadata.IndexerSQLMetadataStorageCoordinator - Published segment [v2_metrics_cluster_2016-10-19T06:00:00.000Z_2016-10-19T07:00:00.000Z_2016-10-19T06:00:00.545Z_12] to DB\n2016-10-19T08:23:19,357 INFO [qtp370356001-171] io.druid.metadata.IndexerSQLMetadataStorageCoordinator - Published segment [v2_metrics_cluster_2016-10-19T07:00:00.000Z_2016-10-19T08:00:00.000Z_2016-10-19T07:00:00.975Z_7] to DB\n2016-10-19T08:23:19,372 INFO [qtp370356001-171] io.druid.metadata.IndexerSQLMetadataStorageCoordinator - Published segment [v2_metrics_cluster_2016-10-19T08:00:00.000Z_2016-10-19T09:00:00.000Z_2016-10-19T08:00:00.443Z_20] to DB\n2016-10-19T08:23:19,387 INFO [qtp370356001-171] io.druid.metadata.IndexerSQLMetadataStorageCoordinator - Published segment [v2_metrics_cluster_2016-10-19T05:00:00.000Z_2016-10-19T06:00:00.000Z_2016-10-19T05:00:01.567Z_33] to DB\n2016-10-19T08:23:19,392 INFO [qtp370356001-152] io.druid.metadata.IndexerSQLMetadataStorageCoordinator - Not updating metadata, compare-and-swap failure.\n2016-10-19T08:23:19,426 INFO [qtp370356001-112] io.druid.indexing.common.actions.LocalTaskActionClient - Performing action for task[index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj]: SegmentListUsedAction{dataSource='v2_metrics_cluster', intervals=[2016-10-19T05:00:00.000Z/2016-10-19T09:00:00.000Z]}\n2016-10-19T08:23:21,085 INFO [Curator-PathChildrenCache-0] io.druid.indexing.overlord.RemoteTaskRunner - Worker[] wrote FAILED status for task [index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj] on [TaskLocation{host='', port=}]\n2016-10-19T08:23:21,085 INFO [Curator-PathChildrenCache-0] io.druid.indexing.overlord.RemoteTaskRunner - Worker[] completed task[index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj] with status[FAILED]\n2016-10-19T08:23:21,085 INFO [Curator-PathChildrenCache-0] io.druid.indexing.overlord.TaskQueue - Received FAILED status for task: index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj\n```\n\nFrom the logs it seems like that two threads `qtp370356001-171` and `qtp370356001-152` wanted to update the datasource metadata. Initially, both threads read the `commit_metadata_payload` from DB and then `qtp370356001-171` updated the metadata successfully whereas `qtp370356001-152` failed as the old commit metadata it read earlier is not as same as commit metadata in DB now. All this happens in `updateDataSourceMetadataWithHandle` method of `IndexerSQLMetadataStorageCoordinator.java` class.\n\nOne way to solve this issue is to synchronize access to `updateDataSourceMetadataWithHandle` method but better way would be to retry in case of such failures. However, `retryTransaction` does not retry in case of `RuntimeException` which would be thrown in this case here - https://github.com/druid-io/druid/blob/master/server/src/main/java/io/druid/metadata/IndexerSQLMetadataStorageCoordinator.java#L342\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2016-10-19 22:14:32", "title": "[QTL] Deadlock in offheap cache", "url": "https://github.com/apache/druid/issues/3593", "closed": "2016-10-27 16:53:54", "ttf": 7.000277777777778, "commitsDetails": [], "body": "```\n=============================\n\"NamespaceExtractionCacheManager-0\":\n  waiting for ownable synchronizer 0x00000004f0550030, (a java.util.concurrent.locks.ReentrantLock$NonfairSync),\n  which is held by \"qtp1930842682-69\"\n\"qtp1930842682-69\":\n  waiting to lock monitor 0x00007fa3d4009bd8 (object 0x00000004c38b04a8, a java.util.concurrent.atomic.AtomicBoolean),\n  which is held by \"NamespaceExtractionCacheManager-0\"\n\nJava stack information for the threads listed above:\n===================================================\n\"NamespaceExtractionCacheManager-0\":\n    at sun.misc.Unsafe.park(Native Method)\n    - parking to wait for  <0x00000004f0550030> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)\n    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209)\n    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285)\n    at io.druid.server.lookup.namespace.cache.OffHeapNamespaceExtractionCacheManager.swapAndClearCache(OffHeapNamespaceExtractionCacheManager.java:112)\n    at io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager$2.run(NamespaceExtractionCacheManager.java:173)\n    - locked <0x00000004c38b04a8> (a java.util.concurrent.atomic.AtomicBoolean)\n    at io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager$4.run(NamespaceExtractionCacheManager.java:368)\n    at com.google.common.util.concurrent.MoreExecutors$ScheduledListeningDecorator$NeverSuccessfulListenableFutureTask.run(MoreExecutors.java:582)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\"qtp1930842682-69\":\n    at io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager.removeNamespaceLocalMetadata(NamespaceExtractionCacheManager.java:304)\n    - waiting to lock <0x00000004c38b04a8> (a java.util.concurrent.atomic.AtomicBoolean)\n    at io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager.delete(NamespaceExtractionCacheManager.java:447)\n    at io.druid.server.lookup.namespace.cache.OffHeapNamespaceExtractionCacheManager.delete(OffHeapNamespaceExtractionCacheManager.java:139)\n    at io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager.scheduleAndWait(NamespaceExtractionCacheManager.java:254)\n    at io.druid.query.lookup.NamespaceLookupExtractorFactory.start(NamespaceLookupExtractorFactory.java:111)\n    at io.druid.query.lookup.LookupReferencesManager.updateIfNew(LookupReferencesManager.java:235)\n    at io.druid.query.lookup.LookupListeningResource$2.post(LookupModule.java:129)\n    at io.druid.server.listener.resource.AbstractListenerHandler.handlePOSTAll(AbstractListenerHandler.java:107)\n    at io.druid.server.listener.resource.ListenerResource.serviceAnnouncementPOSTAll(ListenerResource.java:92)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n    at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\n    at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n    at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)\n    at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n    at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n    at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)\n    at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)\n    at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)\n    at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)\n    at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)\n    at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)\n    at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n    at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:286)\n    at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:276)\n    at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:181)\n    at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)\n    at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)\n    at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:120)\n    at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:135)\n    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)\n    at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:83)\n    at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:364)\n    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)\n    at org.eclipse.jetty.servlets.QoSFilter.doFilter(QoSFilter.java:200)\n    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)\n    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n    at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:221)\n    at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1125)\n    at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n    at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\n    at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1059)\n    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n    at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)\n    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)\n    at org.eclipse.jetty.server.Server.handle(Server.java:497)\n    at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:310)\n    at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:248)\n    at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:540)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:620)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:540)\n    at java.lang.Thread.run(Thread.java:745)\n\nFound 1 deadlock.\n```\n"}, {"user": "gianm", "commits": {"ca9e896163b6451f6c4a2d7ce700013bb7b38490": {"commitGHEventType": "referenced", "commitUser": "fjy"}, "0ce33bc95f557f4da5e18d16944f39bfafdda14e": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "KafkaIndexingService Load Spec not setting correctly", "numCommits": 0, "created": "2016-10-15 17:26:08", "closed": "2016-10-17 14:37:19", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/3576", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 1.0002777777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "extensions-core/hdfs-storage/src/main/java/io/druid/storage/hdfs/HdfsDataSegmentPusher.java", "deletions": 3, "insertions": 4, "lines": 7}, {"filePath": "extensions-core/hdfs-storage/src/test/java/io/druid/storage/hdfs/HdfsDataSegmentPusherTest.java", "deletions": 9, "insertions": 42, "lines": 51}], "commitSpoonAstDiffStats": [{"spoonFilePath": "HdfsDataSegmentPusher.java", "spoonMethods": [{"INS": 1, "UPD": 3, "DEL": 1, "spoonMethodName": "io.druid.storage.hdfs.HdfsDataSegmentPusher.push(java.io.File,io.druid.timeline.DataSegment)", "MOV": 1, "TOT": 6}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.storage.hdfs.HdfsDataSegmentPusher.makeLoadSpec(org.apache.hadoop.fs.Path)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "HdfsDataSegmentPusherTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.storage.hdfs.HdfsDataSegmentPusherTest", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.storage.hdfs.HdfsDataSegmentPusherTest.testUsingScheme(java.lang.String)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.storage.hdfs.HdfsDataSegmentPusherTest.testPushWithBadScheme()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.storage.hdfs.HdfsDataSegmentPusherTest.testPushWithScheme()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 7, "DEL": 2, "spoonMethodName": "io.druid.storage.hdfs.HdfsDataSegmentPusherTest.testPush()", "MOV": 5, "TOT": 14}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.storage.hdfs.HdfsDataSegmentPusherTest.testPushWithoutScheme()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-10-17 12:27:47", "commitMessage": "HdfsDataSegmentPusher: Properly include scheme, host in output path if necessary. (#3577) (#3582)\n\nFixes #3576.", "commitUser": "fjy", "commitDateTime": "2016-10-17 10:27:47", "commitParents": ["8e2b14e50494ca269e9132c7bd6d45dc0997c018"], "commitGHEventType": "referenced", "nameRev": "ca9e896163b6451f6c4a2d7ce700013bb7b38490 tags/druid-0.9.2-rc2~9", "commitHash": "ca9e896163b6451f6c4a2d7ce700013bb7b38490"}, {"commitGitStats": [{"filePath": "extensions-core/hdfs-storage/src/main/java/io/druid/storage/hdfs/HdfsDataSegmentPusher.java", "deletions": 3, "insertions": 4, "lines": 7}, {"filePath": "extensions-core/hdfs-storage/src/test/java/io/druid/storage/hdfs/HdfsDataSegmentPusherTest.java", "deletions": 9, "insertions": 42, "lines": 51}], "commitSpoonAstDiffStats": [{"spoonFilePath": "HdfsDataSegmentPusher.java", "spoonMethods": [{"INS": 1, "UPD": 3, "DEL": 1, "spoonMethodName": "io.druid.storage.hdfs.HdfsDataSegmentPusher.push(java.io.File,io.druid.timeline.DataSegment)", "MOV": 1, "TOT": 6}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.storage.hdfs.HdfsDataSegmentPusher.makeLoadSpec(org.apache.hadoop.fs.Path)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "HdfsDataSegmentPusherTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.storage.hdfs.HdfsDataSegmentPusherTest", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.storage.hdfs.HdfsDataSegmentPusherTest.testUsingScheme(java.lang.String)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.storage.hdfs.HdfsDataSegmentPusherTest.testPushWithBadScheme()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.storage.hdfs.HdfsDataSegmentPusherTest.testPushWithScheme()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 7, "DEL": 2, "spoonMethodName": "io.druid.storage.hdfs.HdfsDataSegmentPusherTest.testPush()", "MOV": 5, "TOT": 14}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.storage.hdfs.HdfsDataSegmentPusherTest.testPushWithoutScheme()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-10-17 09:37:18", "commitMessage": "HdfsDataSegmentPusher: Properly include scheme, host in output path if necessary. (#3577)\n\nFixes #3576.", "commitUser": "fjy", "commitDateTime": "2016-10-17 10:37:18", "commitParents": ["472c409b9967a5b301aea3c2233f7e3351704645"], "commitGHEventType": "referenced", "nameRev": "0ce33bc95f557f4da5e18d16944f39bfafdda14e tags/druid-0.10.0-rc1~249", "commitHash": "0ce33bc95f557f4da5e18d16944f39bfafdda14e"}], "body": "See discussion in https://groups.google.com/d/topic/druid-user/K3CM-jXNOCY/discussion.\n\nUnclear if this is a bug or a misconfiguration, so I'm opening this ticket to track figuring that out. Patches to HdfsDataSegmentPusher since 0.9.1.1 that may be related: #3196, #3494, #3547, #3555.\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2016-10-14 04:07:22", "title": "Too many open files", "url": "https://github.com/apache/druid/issues/3568", "closed": "2016-10-14 04:49:37", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "```\n2016-10-14T04:01:36,159 WARN [qtp182926464-112-acceptor-1@b0595dd-ServerConnector@5bde57ab{HTTP/1.1}{0.0.0.0:23271}] org.eclipse.jetty.server.ServerConnector - \njava.io.IOException: Too many open files\n    at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method) ~[?:1.8.0_101]\n    at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:422) ~[?:1.8.0_101]\n    at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:250) ~[?:1.8.0_101]\n    at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:377) ~[jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:500) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:620) [jetty-util-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:540) [jetty-util-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_101]\n```\n\nI'm getting too many open files on brokers with 0.9.2 a few seconds after queries start to arrive.\n"}, {"user": "gianm", "commits": {}, "labels": ["Bug"], "created": "2016-10-11 16:53:30", "title": "BufferUnderflowException in HyperLogLogCollector.fold", "url": "https://github.com/apache/druid/issues/3560", "closed": "2016-10-17 16:39:12", "ttf": 5.000277777777778, "commitsDetails": [], "body": "BufferUnderflowExceptions reported with 0.9.2-rc1: https://groups.google.com/d/topic/druid-user/bZJq0Z3U_Ms/discussion. May be related to #3314.\n\nThe stack trace was:\n\n```\n016-10-03T17:20:43,407 ERROR [processing-9] io.druid.query.GroupByMergedQueryRunner - Exception with one of the sequences!\njava.nio.BufferUnderflowException\n  at java.nio.Buffer.nextGetIndex(Buffer.java:506) ~[?:1.8.0_101]\n    at java.nio.DirectByteBuffer.getShort(DirectByteBuffer.java:590) ~[?:1.8.0_101]\n    at io.druid.query.aggregation.hyperloglog.HyperLogLogCollector.fold(HyperLogLogCollector.java:393) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at io.druid.query.aggregation.hyperloglog.HyperUniquesBufferAggregator.aggregate(HyperUniquesBufferAggregator.java:65) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at io.druid.query.groupby.GroupByQueryEngine$RowUpdater.updateValues(GroupByQueryEngine.java:237) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at io.druid.query.groupby.GroupByQueryEngine$RowUpdater.updateValues(GroupByQueryEngine.java:200) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at io.druid.query.groupby.GroupByQueryEngine$RowUpdater.updateValues(GroupByQueryEngine.java:200) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at io.druid.query.groupby.GroupByQueryEngine$RowUpdater.updateValues(GroupByQueryEngine.java:200) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at io.druid.query.groupby.GroupByQueryEngine$RowUpdater.updateValues(GroupByQueryEngine.java:200) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at io.druid.query.groupby.GroupByQueryEngine$RowUpdater.updateValues(GroupByQueryEngine.java:200) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at io.druid.query.groupby.GroupByQueryEngine$RowUpdater.updateValues(GroupByQueryEngine.java:200) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at io.druid.query.groupby.GroupByQueryEngine$RowUpdater.updateValues(GroupByQueryEngine.java:200) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at io.druid.query.groupby.GroupByQueryEngine$RowUpdater.access$100(GroupByQueryEngine.java:150) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at io.druid.query.groupby.GroupByQueryEngine$RowIterator.next(GroupByQueryEngine.java:378) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at io.druid.query.groupby.GroupByQueryEngine$RowIterator.next(GroupByQueryEngine.java:293) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at com.metamx.common.guava.BaseSequence.accumulate(BaseSequence.java:67) ~[java-util-0.27.10.jar:?]\n    at com.metamx.common.guava.ConcatSequence$1.accumulate(ConcatSequence.java:46) ~[java-util-0.27.10.jar:?]\n    at com.metamx.common.guava.ConcatSequence$1.accumulate(ConcatSequence.java:42) ~[java-util-0.27.10.jar:?]\n    at com.metamx.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:39) ~[java-util-0.27.10.jar:?]\n    at com.metamx.common.guava.FilteringAccumulator.accumulate(FilteringAccumulator.java:40) ~[java-util-0.27.10.jar:?]\n    at com.metamx.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:39) ~[java-util-0.27.10.jar:?]\n    at com.metamx.common.guava.BaseSequence.accumulate(BaseSequence.java:67) ~[java-util-0.27.10.jar:?]\n    at com.metamx.common.guava.MappedSequence.accumulate(MappedSequence.java:40) ~[java-util-0.27.10.jar:?]\n    at com.metamx.common.guava.ResourceClosingSequence.accumulate(ResourceClosingSequence.java:38) ~[java-util-0.27.10.jar:?]\n    at com.metamx.common.guava.FilteredSequence.accumulate(FilteredSequence.java:42) ~[java-util-0.27.10.jar:?]\n    at com.metamx.common.guava.MappedSequence.accumulate(MappedSequence.java:40) ~[java-util-0.27.10.jar:?]\n    at com.metamx.common.guava.ResourceClosingSequence.accumulate(ResourceClosingSequence.java:38) ~[java-util-0.27.10.jar:?]\n    at com.metamx.common.guava.ConcatSequence.accumulate(ConcatSequence.java:40) ~[java-util-0.27.10.jar:?]\n    at com.metamx.common.guava.ResourceClosingSequence.accumulate(ResourceClosingSequence.java:38) ~[java-util-0.27.10.jar:?]\n    at io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:104) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at com.metamx.common.guava.MappedSequence.accumulate(MappedSequence.java:40) ~[java-util-0.27.10.jar:?]\n    at com.metamx.common.guava.Sequences$1.accumulate(Sequences.java:90) ~[java-util-0.27.10.jar:?]\n    at com.metamx.common.guava.Sequences.toList(Sequences.java:113) ~[java-util-0.27.10.jar:?]\n    at io.druid.query.BySegmentQueryRunner.run(BySegmentQueryRunner.java:56) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:104) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at io.druid.query.spec.SpecificSegmentQueryRunner$2$1.call(SpecificSegmentQueryRunner.java:87) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at io.druid.query.spec.SpecificSegmentQueryRunner.doNamed(SpecificSegmentQueryRunner.java:171) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at io.druid.query.spec.SpecificSegmentQueryRunner.access$400(SpecificSegmentQueryRunner.java:41) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at io.druid.query.spec.SpecificSegmentQueryRunner$2.doItNamed(SpecificSegmentQueryRunner.java:162) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at io.druid.query.spec.SpecificSegmentQueryRunner$2.accumulate(SpecificSegmentQueryRunner.java:80) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at io.druid.query.CPUTimeMetricQueryRunner$1.accumulate(CPUTimeMetricQueryRunner.java:81) ~[druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at com.metamx.common.guava.Sequences$1.accumulate(Sequences.java:90) ~[java-util-0.27.10.jar:?]\n    at io.druid.query.GroupByMergedQueryRunner$1$1.call(GroupByMergedQueryRunner.java:118) [druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at io.druid.query.GroupByMergedQueryRunner$1$1.call(GroupByMergedQueryRunner.java:111) [druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_101]\n    at io.druid.query.PrioritizedListenableFutureTask.run(PrioritizedExecutorService.java:271) [druid-processing-0.9.2-rc1.jar:0.9.2-rc1]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_101]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_101]\n    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_101]\n```\n"}, {"user": "drcrallen", "commits": {"c1d3b8a30c9cd4fc8dc8f2d391214b8ac19a1da9": {"commitGHEventType": "referenced", "commitUser": "fjy"}, "eefeda25229e70fc73b75bddac4e04271b221ffc": {"commitGHEventType": "referenced", "commitUser": "b-slim"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "druid-lookups-cached-single needs stricter dependency rules in pom.xml", "numCommits": 0, "created": "2016-10-06 20:28:35", "closed": "2016-10-17 14:37:47", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/3548", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 10.000277777777777, "commitsDetails": [{"commitGitStats": [{"filePath": "extensions-core/lookups-cached-single/pom.xml", "deletions": 5, "insertions": 0, "lines": 5}, {"filePath": "extensions-core/lookups-cached-single/src/main/java/io/druid/server/lookup/jdbc/QueryKeys.java", "deletions": 4, "insertions": 34, "lines": 38}], "commitSpoonAstDiffStats": [{"spoonFilePath": "QueryKeys.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.server.lookup.jdbc", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.lookup.jdbc.QueryKeysContainerFactory", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-10-17 09:37:47", "commitMessage": "Remove dropwizard-jdbc dependency from lookups-cached-single. (#3573)\n\nFixes #3548.", "commitUser": "fjy", "commitDateTime": "2016-10-17 10:37:47", "commitParents": ["0ce33bc95f557f4da5e18d16944f39bfafdda14e"], "commitGHEventType": "referenced", "nameRev": "c1d3b8a30c9cd4fc8dc8f2d391214b8ac19a1da9 tags/druid-0.10.0-rc1~248", "commitHash": "c1d3b8a30c9cd4fc8dc8f2d391214b8ac19a1da9"}, {"commitGitStats": [{"filePath": "extensions-core/lookups-cached-single/pom.xml", "deletions": 5, "insertions": 0, "lines": 5}, {"filePath": "extensions-core/lookups-cached-single/src/main/java/io/druid/server/lookup/jdbc/QueryKeys.java", "deletions": 4, "insertions": 34, "lines": 38}], "commitSpoonAstDiffStats": [{"spoonFilePath": "QueryKeys.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.server.lookup.jdbc", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.lookup.jdbc.QueryKeysContainerFactory", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-10-17 11:58:59", "commitMessage": "Remove dropwizard-jdbc dependency from lookups-cached-single. (#3573) (#3581)\n\nFixes #3548.", "commitUser": "b-slim", "commitDateTime": "2016-10-17 09:58:59", "commitParents": ["02f04aadf8ed4efd18232a10416f8fae228716b8"], "commitGHEventType": "referenced", "nameRev": "eefeda25229e70fc73b75bddac4e04271b221ffc tags/druid-0.9.2-rc2~11", "commitHash": "eefeda25229e70fc73b75bddac4e04271b221ffc"}], "body": "```\n$ ls druid-0.9.3-SNAPSHOT/extensions/druid-lookups-cached-single/\nantlr-2.7.7.jar                 jackson-datatype-joda-2.4.6.jar         jetty-setuid-java-1.0.3.jar\naopalliance-repackaged-2.4.0-b31.jar        jackson-jaxrs-base-2.4.6.jar            jetty-util-9.2.13.v20150730.jar\nargparse4j-0.6.0.jar                jackson-jaxrs-json-provider-2.4.6.jar       jetty-webapp-9.2.13.v20150730.jar\nclassmate-1.0.0.jar             jackson-module-afterburner-2.6.3.jar        jetty-xml-9.2.13.v20150730.jar\ncommons-lang3-3.4.jar               jackson-module-jaxb-annotations-2.4.6.jar   joda-time-2.8.2.jar\ndropwizard-configuration-0.9.2.jar      javassist-3.18.1-GA.jar             jsr305-2.0.1.jar\ndropwizard-core-0.9.2.jar           javax.annotation-api-1.2.jar            jul-to-slf4j-1.7.12.jar\ndropwizard-db-0.9.2.jar             javax.el-3.0.0.jar              log4j-over-slf4j-1.7.12.jar\ndropwizard-jackson-0.9.2.jar            javax.inject-2.4.0-b31.jar          logback-classic-1.1.3.jar\ndropwizard-jdbi-0.9.2.jar           javax.servlet-api-3.1.0.jar         logback-core-1.1.3.jar\ndropwizard-jersey-0.9.2.jar         javax.ws.rs-api-2.0.1.jar           mapdb-1.0.8.jar\ndropwizard-jetty-0.9.2.jar          jboss-logging-3.1.3.GA.jar          metrics-annotation-3.1.2.jar\ndropwizard-lifecycle-0.9.2.jar          jcl-over-slf4j-1.7.12.jar           metrics-core-3.1.2.jar\ndropwizard-logging-0.9.2.jar            jdbi-2.63.1.jar                 metrics-healthchecks-3.1.2.jar\ndropwizard-metrics-0.9.2.jar            jersey-bean-validation-2.22.1.jar       metrics-jdbi-3.1.2.jar\ndropwizard-servlets-0.9.2.jar           jersey-client-2.22.1.jar            metrics-jersey2-3.1.2.jar\ndropwizard-util-0.9.2.jar           jersey-common-2.22.1.jar            metrics-jetty9-3.1.2.jar\ndropwizard-validation-0.9.2.jar         jersey-container-servlet-2.22.1.jar     metrics-json-3.1.2.jar\ndruid-lookups-cached-single-0.9.3-SNAPSHOT.jar  jersey-container-servlet-core-2.22.1.jar    metrics-jvm-3.1.2.jar\nguava-16.0.1.jar                jersey-guava-2.22.1.jar             metrics-logback-3.1.2.jar\nhibernate-validator-5.1.3.Final.jar     jersey-media-jaxb-2.22.1.jar            metrics-servlets-3.1.2.jar\nhk2-api-2.4.0-b31.jar               jersey-metainf-services-2.22.1.jar      osgi-resource-locator-1.0.1.jar\nhk2-locator-2.4.0-b31.jar           jersey-server-2.22.1.jar            slf4j-api-1.7.12.jar\nhk2-utils-2.4.0-b31.jar             jetty-continuation-9.2.13.v20150730.jar     snakeyaml-1.15.jar\njackson-annotations-2.4.6.jar           jetty-http-9.2.13.v20150730.jar         stringtemplate-3.2.jar\njackson-core-2.4.6.jar              jetty-io-9.2.5.v20141112.jar            tomcat-jdbc-8.0.28.jar\njackson-databind-2.4.6.jar          jetty-security-9.2.5.v20141112.jar      tomcat-juli-8.0.28.jar\njackson-dataformat-yaml-2.6.3.jar       jetty-server-9.2.5.v20141112.jar        validation-api-1.1.0.Final.jar\njackson-datatype-guava-2.4.6.jar        jetty-servlet-9.2.5.v20141112.jar\njackson-datatype-jdk7-2.6.3.jar         jetty-servlets-9.2.5.v20141112.jar\n```\n\nIt pulls in a hefty list.\n"}, {"user": "MdeArcayne", "commits": {"02f04aadf8ed4efd18232a10416f8fae228716b8": {"commitGHEventType": "referenced", "commitUser": "fjy"}, "3b6261c690785966779bf0f92d811b5f655bd441": {"commitGHEventType": "referenced", "commitUser": "gianm"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {}, "title": "druid-lookups-cached-single extension not present in 0.9.2-rc1", "numCommits": 0, "created": "2016-09-30 17:37:13", "closed": "2016-10-15 15:11:04", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "filteredCommits": [], "url": "https://github.com/apache/druid/issues/3527", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 14.000277777777777, "commitsDetails": [{"commitGitStats": [{"filePath": "distribution/pom.xml", "deletions": 0, "insertions": 2, "lines": 2}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-10-15 08:11:04", "commitMessage": "Add druid-lookups-cached-single to default distribution build (#3550)\n\nFixes #3527", "commitUser": "gianm", "commitDateTime": "2016-10-15 08:11:04", "commitParents": ["4554c1214b448b1b33e273fee8e9f5e142006eb9"], "commitGHEventType": "referenced", "nameRev": "3b6261c690785966779bf0f92d811b5f655bd441 tags/druid-0.10.0-rc1~251", "commitHash": "3b6261c690785966779bf0f92d811b5f655bd441"}, {"commitGitStats": [{"filePath": "distribution/pom.xml", "deletions": 0, "insertions": 2, "lines": 2}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-10-17 09:38:12", "commitMessage": "Add druid-lookups-cached-single to default distribution build (#3550) (#3575)\n\nFixes #3527", "commitUser": "fjy", "commitDateTime": "2016-10-17 10:38:12", "commitParents": ["197b7142510a257d2eb5e8037472a410c5bb1251"], "commitGHEventType": "referenced", "nameRev": "02f04aadf8ed4efd18232a10416f8fae228716b8 tags/druid-0.9.2-rc2~12", "commitHash": "02f04aadf8ed4efd18232a10416f8fae228716b8"}], "body": "Seems like the new  druid-lookups-cached-single extension is not bundled in the tarball found [here](http://static.druid.io/artifacts/releases/druid-0.9.2-rc1-bin.tar.gz).\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2016-09-27 22:31:25", "title": "Guava 14.x batch indexing fails", "url": "https://github.com/apache/druid/issues/3519", "closed": "2016-10-11 17:14:12", "ttf": 13.000277777777777, "commitsDetails": [], "body": "```\n16/09/27 22:20:37 ERROR SparkDruidIndexer$: Error in partition [1]\njava.lang.NoSuchMethodError: com.google.common.io.ByteSource.concat(Ljava/lang/Iterable;)Lcom/google/common/io/ByteSource;\n    at io.druid.segment.data.GenericIndexedWriter.combineStreams(GenericIndexedWriter.java:139)\n    at io.druid.segment.StringDimensionMergerLegacy.writeValueMetadataToFile(StringDimensionMergerLegacy.java:206)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:696)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:438)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:186)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:152)\n    at io.druid.indexer.spark.SparkDruidIndexer$$anonfun$13$$anonfun$21.apply(SparkDruidIndexer.scala:293)\n    at io.druid.indexer.spark.SparkDruidIndexer$$anonfun$13$$anonfun$21.apply(SparkDruidIndexer.scala:288)\n    at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n    at scala.collection.Iterator$class.foreach(Iterator.scala:893)\n    at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n    at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n    at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:183)\n    at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:45)\n    at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n    at scala.collection.AbstractIterator.to(Iterator.scala:1336)\n    at scala.collection.TraversableOnce$class.toList(TraversableOnce.scala:294)\n    at scala.collection.AbstractIterator.toList(Iterator.scala:1336)\n    at io.druid.indexer.spark.SparkDruidIndexer$$anonfun$13.apply(SparkDruidIndexer.scala:309)\n    at io.druid.indexer.spark.SparkDruidIndexer$$anonfun$13.apply(SparkDruidIndexer.scala:205)\n    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:801)\n    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:801)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n    at org.apache.spark.scheduler.Task.run(Task.scala:85)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n```\n\nFails from the change in https://github.com/druid-io/druid/pull/3148 which moved the prior usage of `ByteStreams.join` (in guava forever, deprecated for removal in 18) to `ByteSource concat` introduced in guava 15\n"}, {"user": "xiaoyao1991", "commits": {}, "labels": ["Bug"], "created": "2016-09-25 20:14:41", "title": "Default LRU cache does not always guarantee to limit the size under maximum", "url": "https://github.com/apache/druid/issues/3507", "closed": "2016-09-28 02:24:28", "ttf": 2.000277777777778, "commitsDetails": [], "body": "It looks to me that the default LRU MapCache does not always guarantee to limit the cache size under the preset cache size maximum limit. \n\nCurrently, the cache eviction happens when a new cache entry is inserted and the insertion cause the cache size to exceed its maximum. In this case, the cache would remove **only 1** entry(the least recently used) from the existing map. However, it doesn't check if the cache size is below maximum after removal. This can cause cache size to grow slowly to an unexpected number under some designated workload.  \n"}, {"user": "pjain1", "commits": {}, "labels": ["Bug"], "created": "2016-09-22 17:26:40", "title": "Graceful shutdown of kafka supervisor does not work properly", "url": "https://github.com/apache/druid/issues/3495", "closed": "2016-09-23 19:26:45", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "In case of graceful shutdown `workerExec` executor service is used to get the highest offset of tasks in the task group, set that as end offset for all tasks and resume them so that they finish and publish. However, since in most cases doing this will take more time than the time out for shutting down supervisor i.e. `SHUTDOWN_TIMEOUT_MILLIS`(15 seconds), `shutdownNow()` will be called on `workerExec`. Thus, when all the futures for pausing tasks resolves, further logic for getting highest offset and resuming them will not run and fail with exception like -\n\n```\nSep 21, 2016 9:42:33 PM com.google.common.util.concurrent.ExecutionList executeListener\nSEVERE: RuntimeException while executing runnable com.google.common.util.concurrent.Futures$ChainingListenableFuture@5fde229f with executor com.google.common.\nutil.concurrent.MoreExecutors$ListeningDecorator@6c5b645\njava.util.concurrent.RejectedExecutionException: Task com.google.common.util.concurrent.Futures$ChainingListenableFuture@5fde229f rejected from java.util.conc\nurrent.ThreadPoolExecutor@2fde4963[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 10]\n        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)\n        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)\n        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)\n        at com.google.common.util.concurrent.MoreExecutors$ListeningDecorator.execute(MoreExecutors.java:484)\n        at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156)\n        at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145)\n        at com.google.common.util.concurrent.AbstractFuture.set(AbstractFuture.java:185)\n        at com.google.common.util.concurrent.Futures$CombinedFuture.setOneValue(Futures.java:1626)\n        at com.google.common.util.concurrent.Futures$CombinedFuture.access$400(Futures.java:1470)\n        at com.google.common.util.concurrent.Futures$CombinedFuture$2.run(Futures.java:1548)\n        at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)\n        at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156)\n        at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145)\n        at com.google.common.util.concurrent.ListenableFutureTask.done(ListenableFutureTask.java:91)\n        at java.util.concurrent.FutureTask.finishCompletion(FutureTask.java:384)\n        at java.util.concurrent.FutureTask.setException(FutureTask.java:251)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:271)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n```\n\nThe paused tasks will do nothing and will never be finish.\n"}, {"user": "gianm", "commits": {}, "labels": ["Bug", "Starter"], "created": "2016-09-22 16:45:04", "title": "Use atomic writes for local deep storage", "url": "https://github.com/apache/druid/issues/3493", "closed": "2017-06-22 09:09:29", "ttf": 272.0002777777778, "commitsDetails": [], "body": "If Druid is configured with local deep storage, concurrent pushes (which replicated tasks do as a matter of course) can lead to data corruption. This can happen if multiple daemons run on the same machine, or if the local deep storage is actually an NFS mount used for a distributed cluster.\n\nAtomic writes in LocalDataSegmentPusher (write to X.uuid -> rename to X; rather than write directly to X) will fix it for true local fses and I _think_ it will fix it for NFS too.\n\nOriginally reported in https://groups.google.com/d/topic/druid-user/hv3BvuGdK3w/discussion.\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2016-09-21 03:30:22", "title": "Archive task writes metadata regardless of update result", "url": "https://github.com/apache/druid/issues/3475", "closed": "2016-12-01 15:49:28", "ttf": 71.00027777777778, "commitsDetails": [], "body": "The Archive task will submit a metadata update task https://github.com/druid-io/druid/blob/druid-0.9.1/indexing-service/src/main/java/io/druid/indexing/common/task/ArchiveTask.java#L99 regardless of if a segment is moved or not in the S3 mover at https://github.com/druid-io/druid/blob/druid-0.9.1/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java#L123 \n\nThis causes a large quantity of writes to the database compared to what needs to occur.\n"}, {"user": "pjain1", "commits": {}, "labels": ["Bug", "duplicate"], "created": "2016-09-19 20:19:11", "title": "Kafka Index tasks sotps consuming any data ", "url": "https://github.com/apache/druid/issues/3467", "closed": "2016-09-19 21:11:13", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "When a Kafka index task successfully publishes some segments then it persists the latest consumed offsets info in the metadata store. Now if the supervisor is stopped or stops because of some reason and restarted at a later point and the earliest msg offset in Kafka is greater than persisted offset in the metadata (probably because earlier messages were dropped by kafka brokers or something else) then Kafka Index task will get `OffsetOutOfRangeException` and keep on retrying the same offset indefinitely.  \n\nOne way to solve this issue is to reset the consumer offset if the persisted offset is less than the earliest offset at the Kafka broker. Although I think in this case the persisted offset information would need to be delete from metadata store otherwise the new tasks will fail to publish segment because of consecutive offset check. Any thoughts ?\n"}, {"user": "drcrallen", "commits": {"95e08b38ea6e77e2d6895678ea53cbdf8f5aaae0": {"commitGHEventType": "closed", "commitUser": "gianm"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {}, "title": "Deadlock in global lookup cache", "numCommits": 0, "created": "2016-09-14 14:28:28", "closed": "2016-09-16 18:54:30", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "filteredCommits": [], "url": "https://github.com/apache/druid/issues/3459", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 2.000277777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "processing/src/main/java/io/druid/query/lookup/LookupReferencesManager.java", "deletions": 21, "insertions": 119, "lines": 140}, {"filePath": "server/src/main/java/io/druid/query/lookup/LookupModule.java", "deletions": 12, "insertions": 11, "lines": 23}, {"filePath": "extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/NamespaceExtractionCacheManager.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "server/src/test/java/io/druid/query/lookup/LookupReferencesManagerTest.java", "deletions": 3, "insertions": 385, "lines": 388}, {"filePath": "extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/cache/OffHeapNamespaceExtractionCacheManagerTest.java", "deletions": 0, "insertions": 80, "lines": 80}, {"filePath": "extensions-core/lookups-cached-global/src/main/java/io/druid/query/lookup/NamespaceLookupExtractorFactory.java", "deletions": 9, "insertions": 20, "lines": 29}, {"filePath": "extensions-core/kafka-extraction-namespace/src/main/java/io/druid/query/lookup/KafkaLookupExtractorFactory.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/OffHeapNamespaceExtractionCacheManager.java", "deletions": 19, "insertions": 9, "lines": 28}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "tooManyChanges", "authoredDateTime": "2016-09-16 11:54:23", "commitMessage": "[QTL] Reduced Locking Lookups (#3071)\n\n* Lockless lookups\r\n\r\n* Fix compile problem\r\n\r\n* Make stack trace throw instead\r\n\r\n* Remove non-germane change\r\n\r\n* * Add better naming to cache keys. Makes logging nicer\r\n* Fix #3459\r\n\r\n* Move start/stop lock to non-interruptable for readability purposes\r\n", "commitUser": "gianm", "commitDateTime": "2016-09-16 11:54:23", "commitParents": ["76fcbd8fc50c6dfd1d948e7ec3553b1090cab367"], "commitGHEventType": "closed", "nameRev": "95e08b38ea6e77e2d6895678ea53cbdf8f5aaae0 tags/druid-0.9.2-rc1~16", "commitHash": "95e08b38ea6e77e2d6895678ea53cbdf8f5aaae0"}], "body": "Had an issue with a query node which ended up reporting a deadlock on `kill -3` I'm still investigating and intend to make sure https://github.com/druid-io/druid/pull/3071 avoids this case.\n\n```\nFound one Java-level deadlock:\n=============================\n\"NamespaceExtractionCacheManager-0\":\n  waiting for ownable synchronizer 0x00000004c42eaba8, (a java.util.concurrent.locks.ReentrantLock$NonfairSync),\n  which is held by \"topN_REDACTED_[2016-09-01T15:00:00.000Z/2016-09-01T16:00:00.000Z]\"\n\"topN_REDACTED_[2016-09-01T15:00:00.000Z/2016-09-01T16:00:00.000Z]\":\n  waiting for ownable synchronizer 0x00000004c42eacc8, (a java.util.concurrent.locks.ReentrantLock$NonfairSync),\n  which is held by \"topN_REDACTED_[2016-09-01T15:00:00.000Z/2016-09-01T16:00:00.000Z]\"\n\"topN_REDACTED_[2016-09-01T15:00:00.000Z/2016-09-01T16:00:00.000Z]\":\n  waiting for ownable synchronizer 0x00000004c42eaba8, (a java.util.concurrent.locks.ReentrantLock$NonfairSync),\n  which is held by \"topN_REDACTED_[2016-09-01T15:00:00.000Z/2016-09-01T16:00:00.000Z]\"\n\nJava stack information for the threads listed above:\n===================================================\n\"NamespaceExtractionCacheManager-0\":\n        at sun.misc.Unsafe.park(Native Method)\n        - parking to wait for  <0x00000004c42eaba8> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\n        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)\n        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209)\n        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285)\n        at io.druid.server.lookup.namespace.cache.OffHeapNamespaceExtractionCacheManager.swapAndClearCache(OffHeapNamespaceExtractionCacheManager.java:112)\n        at io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager$2.run(NamespaceExtractionCacheManager.java:173)\n        - locked <0x00000004c42c11a8> (a java.util.concurrent.atomic.AtomicBoolean)\n        at io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager$4.run(NamespaceExtractionCacheManager.java:368)\n        at com.google.common.util.concurrent.MoreExecutors$ScheduledListeningDecorator$NeverSuccessfulListenableFutureTask.run(MoreExecutors.java:582)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\"topN_REDACTED_[2016-09-01T15:00:00.000Z/2016-09-01T16:00:00.000Z]\":\n        at sun.misc.Unsafe.park(Native Method)\n        - parking to wait for  <0x00000004c42eacc8> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\n        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)\n        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209)\n        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285)\n        at io.druid.server.lookup.namespace.cache.OffHeapNamespaceExtractionCacheManager.getCacheMap(OffHeapNamespaceExtractionCacheManager.java:170)\n        at io.druid.query.lookup.NamespaceLookupExtractorFactory.get(NamespaceLookupExtractorFactory.java:270)\n        at io.druid.query.lookup.NamespaceLookupExtractorFactory.get(NamespaceLookupExtractorFactory.java:50)\n        at io.druid.query.lookup.RegisteredLookupExtractionFn.ensureDelegate(RegisteredLookupExtractionFn.java:142)\n        - locked <0x00000005f4d39248> (a java.lang.Object)\n        at io.druid.query.lookup.RegisteredLookupExtractionFn.getCacheKey(RegisteredLookupExtractionFn.java:97)\n        at io.druid.query.dimension.ExtractionDimensionSpec.getCacheKey(ExtractionDimensionSpec.java:96)\n        at io.druid.query.topn.TopNQueryQueryToolChest$7.computeCacheKey(TopNQueryQueryToolChest.java:309)\n        at io.druid.query.topn.TopNQueryQueryToolChest$7.computeCacheKey(TopNQueryQueryToolChest.java:298)\n        at io.druid.client.CachingQueryRunner.run(CachingQueryRunner.java:103)\n        at io.druid.query.BySegmentQueryRunner.run(BySegmentQueryRunner.java:70)\n        at io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:118)\n        at io.druid.query.spec.SpecificSegmentQueryRunner$2$1.call(SpecificSegmentQueryRunner.java:87)\n        at io.druid.query.spec.SpecificSegmentQueryRunner.doNamed(SpecificSegmentQueryRunner.java:171)\n        at io.druid.query.spec.SpecificSegmentQueryRunner.access$400(SpecificSegmentQueryRunner.java:41)\n        at io.druid.query.spec.SpecificSegmentQueryRunner$2.doItNamed(SpecificSegmentQueryRunner.java:162)\n        at io.druid.query.spec.SpecificSegmentQueryRunner$2.accumulate(SpecificSegmentQueryRunner.java:80)\n        at io.druid.query.CPUTimeMetricQueryRunner$1.accumulate(CPUTimeMetricQueryRunner.java:81)\n        at com.metamx.common.guava.Sequences$1.accumulate(Sequences.java:90)\n        at com.metamx.common.guava.Sequences.toList(Sequences.java:113)\n        at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:129)\n        at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:119)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at io.druid.query.PrioritizedListenableFutureTask.run(PrioritizedExecutorService.java:271)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\"topN_REDACTED_[2016-09-01T15:00:00.000Z/2016-09-01T16:00:00.000Z]\":\n        at sun.misc.Unsafe.park(Native Method)\n        - parking to wait for  <0x00000004c42eaba8> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\n        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)\n        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209)\n        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285)\n        at io.druid.server.lookup.namespace.cache.OffHeapNamespaceExtractionCacheManager.getCacheMap(OffHeapNamespaceExtractionCacheManager.java:170)\n        at io.druid.query.lookup.NamespaceLookupExtractorFactory.get(NamespaceLookupExtractorFactory.java:270)\n        at io.druid.query.lookup.NamespaceLookupExtractorFactory.get(NamespaceLookupExtractorFactory.java:50)\n        at io.druid.query.lookup.RegisteredLookupExtractionFn.ensureDelegate(RegisteredLookupExtractionFn.java:142)\n        - locked <0x00000005f4d35518> (a java.lang.Object)\n        at io.druid.query.lookup.RegisteredLookupExtractionFn.getCacheKey(RegisteredLookupExtractionFn.java:97)\n        at io.druid.query.dimension.ExtractionDimensionSpec.getCacheKey(ExtractionDimensionSpec.java:96)\n        at io.druid.query.topn.TopNQueryQueryToolChest$7.computeCacheKey(TopNQueryQueryToolChest.java:309)\n        at io.druid.query.topn.TopNQueryQueryToolChest$7.computeCacheKey(TopNQueryQueryToolChest.java:298)\n        at io.druid.client.CachingQueryRunner.run(CachingQueryRunner.java:103)\n        at io.druid.query.BySegmentQueryRunner.run(BySegmentQueryRunner.java:70)\n        at io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:118)\n        at io.druid.query.spec.SpecificSegmentQueryRunner$2$1.call(SpecificSegmentQueryRunner.java:87)\n        at io.druid.query.spec.SpecificSegmentQueryRunner.doNamed(SpecificSegmentQueryRunner.java:171)\n        at io.druid.query.spec.SpecificSegmentQueryRunner.access$400(SpecificSegmentQueryRunner.java:41)\n        at io.druid.query.spec.SpecificSegmentQueryRunner$2.doItNamed(SpecificSegmentQueryRunner.java:162)\n        at io.druid.query.spec.SpecificSegmentQueryRunner$2.accumulate(SpecificSegmentQueryRunner.java:80)\n        at io.druid.query.CPUTimeMetricQueryRunner$1.accumulate(CPUTimeMetricQueryRunner.java:81)\n        at com.metamx.common.guava.Sequences$1.accumulate(Sequences.java:90)\n        at com.metamx.common.guava.Sequences.toList(Sequences.java:113)\n        at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:129)\n        at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:119)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at io.druid.query.PrioritizedListenableFutureTask.run(PrioritizedExecutorService.java:271)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\nFound 1 deadlock.\n```\n"}, {"user": "pjain1", "commits": {}, "labels": ["Bug"], "created": "2016-09-13 20:42:25", "title": "Handle kafka supervisor spec metadata insertion failures", "url": "https://github.com/apache/druid/issues/3455", "closed": "2016-10-04 17:15:29", "ttf": 20.00027777777778, "commitsDetails": [], "body": "In case kafka supervisor spec metadata insertion fails then in the response database exception is returned. However since the supervisor is started before metadata insertion the indexing tasks continues to run even though the insertion failed. I think in case of metadata failures the supervisor should just stop.\n\nThis happened when I posted a supervisor spec to overlord but the supervisor table schema in the database had `version` field however I checked recently the schema has been changed to have `created_date`. Therefore, I had to delete the supervisor table and restart the overlord to have the new table schema.\n"}, {"user": "kaijianding", "commits": {}, "labels": ["Bug"], "created": "2016-08-24 07:41:24", "title": "should handle channel disconnected better in NettyHttpClient or Druid", "url": "https://github.com/apache/druid/issues/3393", "closed": "2018-02-23 17:43:24", "ttf": 548.0002777777778, "commitsDetails": [], "body": "I'm facing such a race condition:\nBroker runs a query, finally calls the httpClient.go() in DirectDruidClient, the httpClient is instance of NettyHttpClient.\nInside the NettyHttpClient.go() method, after the channel is taken from the pool and verified in good shape and ready to fire the query, then the historical side reaches the idle time and disconnect this channel.\n\nThen handler.channelDisconnected callback is called, and finally the query is failed in broker.\nThis exception occurs hundreds of times  in our druid cluster (it is a busy cluster).\n\nThis channel never has a chance to actually connect to the historical to get a single byte back, thus ,I think, the NettyHttpClient.go() should retry internally one time to get a new channel to finish the \"go\" instead of set the retVal future to fail.\n\nThe same thing can happen when channel.write(httpRequest).addListener(...) in NettyHttpClient. the channel to write can be closed by historical server after it is taken from the pool channel, thus it also should be retried.\n\nhere is the trace:\n\n```\n2016-08-24T14:03:29,029 ERROR [qtp1369854401-202[timeseries_eternal_olap_click_da76287c-4eec-4c84-a8b8-2e12ea92aa05]] io.druid.server.QueryResource - Exception handling request: {class=io.druid.server.QueryResource, exceptionType=class com.metamx.common.RE, exceptionMessage=Failure getting results from[http://hdppic0101.et2.tbsite.net:8083/druid/v2/] because of [org.jboss.netty.channel.ChannelException: Channel disconnected], exception=com.metamx.common.RE: Failure getting results from[http://hdppic0101.et2.tbsite.net:8083/druid/v2/] because of [org.jboss.netty.channel.ChannelException: Channel disconnected], query=TimeseriesQuery{dataSource='eternal_olap_click', querySegmentSpec=LegacySegmentSpec{intervals=[2016-08-24T00:00:00.000+08:00/2016-08-24T13:54:00.000+08:00]}, descending=false, dimFilter=(scene_tag = sort_type^C_coefp && bu_src = taobao_topic && item_id = tm6048738 && exper_token_str = wl_topic_sort && action_type = click), granularity='PeriodGranularity{period=PT1M, timeZone=+08:00, origin=null}', aggregatorSpecs=[DoubleSumAggregatorFactory{fieldName='ipv', name='ipv'}], postAggregatorSpecs=[], context={queryId=da76287c-4eec-4c84-a8b8-2e12ea92aa05, timeout=30000}}, peer=10.197.16.234}\ncom.metamx.common.RE: Failure getting results from[http://hdppic0101.et2.tbsite.net:8083/druid/v2/] because of [org.jboss.netty.channel.ChannelException: Channel disconnected]\n        at io.druid.client.DirectDruidClient$JsonParserIterator.init(DirectDruidClient.java:498) ~[druid-server-0.9.0.jar:0.9.0]\n        at io.druid.client.DirectDruidClient$JsonParserIterator.hasNext(DirectDruidClient.java:442) ~[druid-server-0.9.0.jar:0.9.0]\n        at com.metamx.common.guava.BaseSequence.makeYielder(BaseSequence.java:103) ~[java-util-0.27.7.jar:?]\n        at com.metamx.common.guava.BaseSequence.toYielder(BaseSequence.java:81) ~[java-util-0.27.7.jar:?]\n        at com.metamx.common.guava.MappedSequence.toYielder(MappedSequence.java:46) ~[java-util-0.27.7.jar:?]\n        at com.metamx.common.guava.MergeSequence$2.accumulate(MergeSequence.java:66) ~[java-util-0.27.7.jar:?]\n        at com.metamx.common.guava.MergeSequence$2.accumulate(MergeSequence.java:62) ~[java-util-0.27.7.jar:?]\n        at com.metamx.common.guava.YieldingAccumulators$1.accumulate(YieldingAccumulators.java:32) ~[java-util-0.27.7.jar:?]\n        at com.metamx.common.guava.BaseSequence.makeYielder(BaseSequence.java:104) ~[java-util-0.27.7.jar:?]\n        at com.metamx.common.guava.BaseSequence.toYielder(BaseSequence.java:81) ~[java-util-0.27.7.jar:?]\n        at com.metamx.common.guava.BaseSequence.accumulate(BaseSequence.java:67) ~[java-util-0.27.7.jar:?]\n        at com.metamx.common.guava.MergeSequence.toYielder(MergeSequence.java:59) ~[java-util-0.27.7.jar:?]\n        at com.metamx.common.guava.LazySequence.toYielder(LazySequence.java:43) ~[java-util-0.27.7.jar:?]\n        at io.druid.query.RetryQueryRunner$1.toYielder(RetryQueryRunner.java:105) ~[druid-processing-0.9.0.jar:0.9.0]\n        at io.druid.common.guava.CombiningSequence.toYielder(CombiningSequence.java:79) ~[druid-common-0.9.0.jar:0.9.0]\n        at com.metamx.common.guava.MappedSequence.toYielder(MappedSequence.java:46) ~[java-util-0.27.7.jar:?]\n        at io.druid.query.CPUTimeMetricQueryRunner$1.toYielder(CPUTimeMetricQueryRunner.java:93) ~[druid-processing-0.9.0.jar:0.9.0]\n        at com.metamx.common.guava.Sequences$1.toYielder(Sequences.java:98) ~[java-util-0.27.7.jar:?]\n        at io.druid.server.QueryResource.doPost(QueryResource.java:170) [druid-server-0.9.0.jar:0.9.0]\n        at sun.reflect.GeneratedMethodAccessor52.invoke(Unknown Source) ~[?:?]\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_66]\n        at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_66]\n        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60) [jersey-server-1.19.jar:1.19]\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205) [jersey-server-1.19.jar:1.19]\n        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75) [jersey-server-1.19.jar:1.19]\n        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302) [jersey-server-1.19.jar:1.19]\n        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108) [jersey-server-1.19.jar:1.19]\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205) [jersey-server-1.19.jar:1.19]\n        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75) [jersey-server-1.19.jar:1.19]\n        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302) [jersey-server-1.19.jar:1.19]\n        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108) [jersey-server-1.19.jar:1.19]\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) [jersey-server-1.19.jar:1.19]\n        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84) [jersey-server-1.19.jar:1.19]\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542) [jersey-server-1.19.jar:1.19]\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473) [jersey-server-1.19.jar:1.19]\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419) [jersey-server-1.19.jar:1.19]\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409) [jersey-server-1.19.jar:1.19]\n        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409) [jersey-servlet-1.19.jar:1.19]\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558) [jersey-servlet-1.19.jar:1.19]\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733) [jersey-servlet-1.19.jar:1.19]\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) [javax.servlet-api-3.1.0.jar:3.1.0]\n        at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:278) [guice-servlet-4.0-beta.jar:?]\n        at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:268) [guice-servlet-4.0-beta.jar:?]\n        at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:180) [guice-servlet-4.0-beta.jar:?]\n        at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:93) [guice-servlet-4.0-beta.jar:?]\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85) [guice-servlet-4.0-beta.jar:?]\n        at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:120) [guice-servlet-4.0-beta.jar:?]\n        at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:132) [guice-servlet-4.0-beta.jar:?]\n        at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:129) [guice-servlet-4.0-beta.jar:?]\n        at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:206) [guice-servlet-4.0-beta.jar:?]\n        at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:129) [guice-servlet-4.0-beta.jar:?]\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1668) [jetty-servlet-9.3.6.v20151106.jar:9.3.6.v20151106]\n        at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:45) [jetty-servlets-9.3.6.v20151106.jar:9.3.6.v20151106]\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1668) [jetty-servlet-9.3.6.v20151106.jar:9.3.6.v20151106]\n        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:581) [jetty-servlet-9.3.6.v20151106.jar:9.3.6.v20151106]\n        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:224) [jetty-server-9.3.6.v20151106.jar:9.3.6.v20151106]\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1158) [jetty-server-9.3.6.v20151106.jar:9.3.6.v20151106]\n        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:511) [jetty-servlet-9.3.6.v20151106.jar:9.3.6.v20151106]\n        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) [jetty-server-9.3.6.v20151106.jar:9.3.6.v20151106]\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1090) [jetty-server-9.3.6.v20151106.jar:9.3.6.v20151106]\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) [jetty-server-9.3.6.v20151106.jar:9.3.6.v20151106]\n        at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) [jetty-server-9.3.6.v20151106.jar:9.3.6.v20151106]\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:119) [jetty-server-9.3.6.v20151106.jar:9.3.6.v20151106]\n        at org.eclipse.jetty.server.Server.handle(Server.java:517) [jetty-server-9.3.6.v20151106.jar:9.3.6.v20151106]\n        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:308) [jetty-server-9.3.6.v20151106.jar:9.3.6.v20151106]\n        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:242) [jetty-server-9.3.6.v20151106.jar:9.3.6.v20151106]\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceAndRun(ExecuteProduceConsume.java:213) [jetty-util-9.3.6.v20151106.jar:9.3.6.v20151106]\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:147) [jetty-util-9.3.6.v20151106.jar:9.3.6.v20151106]\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:654) [jetty-util-9.3.6.v20151106.jar:9.3.6.v20151106]\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572) [jetty-util-9.3.6.v20151106.jar:9.3.6.v20151106]\n        at java.lang.Thread.run(Thread.java:756) [?:1.8.0_66]\nCaused by: java.util.concurrent.ExecutionException: org.jboss.netty.channel.ChannelException: Channel disconnected\n        at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299) ~[guava-16.0.1.jar:?]\n        at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286) ~[guava-16.0.1.jar:?]\n        at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) ~[guava-16.0.1.jar:?]\n        at io.druid.client.DirectDruidClient$JsonParserIterator.init(DirectDruidClient.java:479) ~[druid-server-0.9.0.jar:0.9.0]\n        ... 69 more\nCaused by: org.jboss.netty.channel.ChannelException: Channel disconnected\n        at com.metamx.http.client.NettyHttpClient$1.channelDisconnected(NettyHttpClient.java:311) ~[http-client-1.0.4.jar:?]\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.channelDisconnected(SimpleChannelUpstreamHandler.java:208) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.channelDisconnected(SimpleChannelUpstreamHandler.java:208) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:570) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.handler.codec.http.HttpClientCodec.handleUpstream(HttpClientCodec.java:92) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:396) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:93) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[netty-3.10.4.Final.jar:?]\n        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[netty-3.10.4.Final.jar:?]\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_66]\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_66]\n```\n"}, {"user": "gvsmirnov", "commits": {"d981a2aa02aedfdf18e0bed5848e704bd9597563": {"commitGHEventType": "referenced", "commitUser": "drcrallen"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "Kafka Lookups prevent peon processes from terminating on /stop from supervisor", "numCommits": 0, "created": "2016-08-10 12:12:51", "closed": "2016-09-19 20:44:04", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/3346", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 40.000277777777775, "commitsDetails": [{"commitGitStats": [{"filePath": "extensions-core/kafka-extraction-namespace/src/test/java/io/druid/query/lookup/KafkaLookupExtractorFactoryTest.java", "deletions": 1, "insertions": 16, "lines": 17}, {"filePath": "extensions-core/kafka-extraction-namespace/src/main/java/io/druid/query/lookup/KafkaLookupExtractorFactory.java", "deletions": 4, "insertions": 5, "lines": 9}], "commitSpoonAstDiffStats": [{"spoonFilePath": "KafkaLookupExtractorFactoryTest.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.lookup.KafkaLookupExtractorFactoryTest.testStopDeleteError()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.lookup.KafkaLookupExtractorFactoryTest.testStartStopStart()", "MOV": 11, "TOT": 12}, {"INS": 4, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.lookup.KafkaLookupExtractorFactoryTest.testStartStop()", "MOV": 2, "TOT": 6}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.lookup.KafkaLookupExtractorFactoryTest.testStartStop().2", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.lookup.KafkaLookupExtractorFactoryTest.testStartFailsFromTimeout().3", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.lookup.KafkaLookupExtractorFactoryTest.testStopDeleteError().4", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.query.lookup.KafkaLookupExtractorFactoryTest.testStartStartStop()", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "KafkaLookupExtractorFactory.java", "spoonMethods": [{"INS": 1, "UPD": 2, "DEL": 1, "spoonMethodName": "io.druid.query.lookup.KafkaLookupExtractorFactory.close()", "MOV": 2, "TOT": 6}, {"INS": 2, "UPD": 1, "DEL": 1, "spoonMethodName": "io.druid.query.lookup.KafkaLookupExtractorFactory.start()", "MOV": 2, "TOT": 6}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.lookup.KafkaLookupExtractorFactory.start().2.run()", "MOV": 1, "TOT": 2}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-09-15 03:44:27", "commitMessage": "Avoid interrupting ZookeeperConsumerConnector.shutdown() #3346 (#3403)\n\n", "commitUser": "drcrallen", "commitDateTime": "2016-09-14 17:44:27", "commitParents": ["7a2a4bc6de718945772a9d7ec7ccf866290ddaf0"], "commitGHEventType": "referenced", "nameRev": "d981a2aa02aedfdf18e0bed5848e704bd9597563 tags/druid-0.9.2-rc1~18", "commitHash": "d981a2aa02aedfdf18e0bed5848e704bd9597563"}], "body": "**Steps to reproduce:**\n1. Create a [kafka indexer](http://druid.io/docs/0.9.1.1/development/extensions-core/kafka-ingestion.html)\n2. Create a [kafka lookup](http://druid.io/docs/0.9.1.1/development/extensions-core/kafka-extraction-namespace.html)\n3. Wait for a kafka index task to start\n4. Emulate a `/stop` request sent from supervisor to worker, e.g. via\n\n```\nhttp post localhost:8100/druid/worker/v1/chat/index_kafka_Example_c2f17f3de1b02c1_fmcaakbh/stop\n```\n\n**Expected behavior:**\nWorker successfully stops\n\n**Actual behavior:**\nWorker process remains lingering until middle manager forcefully kills the process after timeout\n\n---\n\nThe reason for this is that when `KafkaLookupExtractorFactory` invokes `consumerConnector.shutdown()` upon receiving the stop command, it interrupts the worker thread, and thus the shutdown does not complete properly:\n\n```\njava.lang.InterruptedException\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2072) ~[?:1.7.0_101]\n    at java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1468) ~[?:1.7.0_101]\n    at kafka.utils.KafkaScheduler.shutdown(KafkaScheduler.scala:88) ~[kafka_2.10-0.8.2.1.jar:?]\n    at kafka.consumer.ZookeeperConsumerConnector.liftedTree1$1(ZookeeperConsumerConnector.scala:195) [kafka_2.10-0.8.2.1.jar:?]\n    at kafka.consumer.ZookeeperConsumerConnector.shutdown(ZookeeperConsumerConnector.scala:193) [kafka_2.10-0.8.2.1.jar:?]\n    at kafka.javaapi.consumer.ZookeeperConsumerConnector.shutdown(ZookeeperConsumerConnector.scala:119) [kafka_2.10-0.8.2.1.jar:?]\n    at io.druid.query.lookup.KafkaLookupExtractorFactory$2.run(KafkaLookupExtractorFactory.java:229) [druid-kafka-extraction-namespace-0.9.2-SNAPSHOT.jar:0.9.1.1]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [?:1.7.0_101]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_101]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_101]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_101]\n    at java.lang.Thread.run(Thread.java:745) [?:1.7.0_101]\n```\n\nThis leaves a lingering `ConsumerFetcherThread` behind, and so the process does not terminate.\n"}, {"user": "gianm", "commits": {}, "labels": ["Bug"], "created": "2016-08-09 15:48:57", "title": "Theta sketch column type inconsistency in segmentMetadata", "url": "https://github.com/apache/druid/issues/3339", "closed": "2016-08-10 18:03:44", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "segmentMetadata queries get the type of complex metrics from StorageAdapter's `getColumnTypeName`. On IncrementalIndex, that uses the AggregatorFactory's `getTypeName`. On QueryableIndex, it uses the ComplexColumn object's `getTypeName` (which in turn comes from the ComplexMetricSerde's `getTypeName`).\n\nThis works fine for hyperUnique and approximateHistogram, but not for theta sketches. They report \"thetaSketch\" for QueryableIndexes and \"thetaSketchBuild\" or \"thetaSketchMerge\" for IncrementalIndexes. This leads to segmentMetadata column responses looking like:\n\n```\n\"someThetaSketchColumn\": {\n  \"type\": \"STRING\",\n  \"hasMultipleValues\": false,\n  \"size\": -1,\n  \"cardinality\": null,\n  \"minValue\": null,\n  \"maxValue\": null,\n  \"errorMessage\": \"error:cannot_merge_diff_types\"\n}\n```\n\nNot sure what the best solution is, but this situation is not ideal. Probably ideally theta sketches should always be reported as type \"thetaSketch\".\n\nFixing this could potentially involve changing the implementation of StorageAdapter's `getColumnTypeName`, since it's not used for anything other than segmentMetadata queries.\n"}, {"user": "gianm", "commits": {"0299ac73b816a6ac1bfeb953d2a39a3c8277d2c2": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "groupBy v2 nested queries do not accept filtered aggregators", "numCommits": 0, "created": "2016-08-02 03:47:18", "closed": "2016-08-03 00:39:40", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/3311", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 0.0002777777777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "processing/src/main/java/io/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java", "deletions": 2, "insertions": 1, "lines": 3}, {"filePath": "processing/src/main/java/io/druid/query/aggregation/FilteredAggregatorFactory.java", "deletions": 40, "insertions": 93, "lines": 133}, {"filePath": "processing/src/main/java/io/druid/query/topn/DimExtractionTopNAlgorithm.java", "deletions": 1, "insertions": 0, "lines": 1}, {"filePath": "processing/src/main/java/io/druid/query/groupby/epinephelinae/RowBasedGrouperHelper.java", "deletions": 2, "insertions": 1, "lines": 3}, {"filePath": "processing/src/main/java/io/druid/query/topn/TopNParams.java", "deletions": 2, "insertions": 5, "lines": 7}, {"filePath": "processing/src/test/java/io/druid/segment/data/IncrementalIndexTest.java", "deletions": 0, "insertions": 46, "lines": 46}, {"filePath": "processing/src/test/java/io/druid/query/groupby/GroupByQueryRunnerTest.java", "deletions": 13, "insertions": 5, "lines": 18}, {"filePath": "processing/src/main/java/io/druid/segment/DimensionSelector.java", "deletions": 1, "insertions": 8, "lines": 9}, {"filePath": "processing/src/main/java/io/druid/query/topn/BaseTopNAlgorithm.java", "deletions": 0, "insertions": 4, "lines": 4}, {"filePath": "processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java", "deletions": 12, "insertions": 5, "lines": 17}, {"filePath": "processing/src/main/java/io/druid/query/topn/TimeExtractionTopNAlgorithm.java", "deletions": 1, "insertions": 0, "lines": 1}, {"filePath": "processing/src/test/java/io/druid/segment/incremental/IncrementalIndexStorageAdapterTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/main/java/io/druid/query/groupby/GroupByQueryEngine.java", "deletions": 2, "insertions": 4, "lines": 6}, {"filePath": "processing/src/main/java/io/druid/query/dimension/RegexFilteredDimensionSpec.java", "deletions": 1, "insertions": 6, "lines": 7}, {"filePath": "processing/src/main/java/io/druid/query/dimension/ListFilteredDimensionSpec.java", "deletions": 1, "insertions": 4, "lines": 5}, {"filePath": "processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java", "deletions": 1, "insertions": 1, "lines": 2}], "commitSpoonAstDiffStats": [{"spoonFilePath": "GroupByQueryEngine.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.groupby.GroupByQueryEngine.RowIterator.next().1.apply(java.util.Map$Entry)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.groupby.GroupByQueryEngine", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.groupby.GroupByQueryEngine.RowUpdater.updateValues(java.nio.ByteBuffer,java.util.List)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "BaseTopNAlgorithm.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.BaseTopNAlgorithm.BaseArrayProvider", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "TimeExtractionTopNAlgorithm.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.TimeExtractionTopNAlgorithm.makeInitParams(io.druid.segment.DimensionSelector,io.druid.segment.Cursor)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "IncrementalIndexStorageAdapterTest.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.incremental.IncrementalIndexStorageAdapterTest.testSanity()", "MOV": 2, "TOT": 2}]}, {"spoonFilePath": "FilteredAggregatorFactory.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.aggregation.FilteredAggregatorFactory.FilteredAggregatorValueMatcherFactory.makeStringValueMatcher(java.lang.String,com.google.common.base.Predicate)", "MOV": 3, "TOT": 4}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.aggregation.FilteredAggregatorFactory.FilteredAggregatorValueMatcherFactory.makeStringValueMatcher(java.lang.String,com.google.common.base.Predicate).2", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.aggregation.FilteredAggregatorFactory.FilteredAggregatorValueMatcherFactory.makeValueMatcher(java.lang.String,java.lang.Comparable)", "MOV": 2, "TOT": 4}]}, {"spoonFilePath": "IncrementalIndex.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.incremental.IncrementalIndex.makeColumnSelectorFactory(io.druid.query.aggregation.AggregatorFactory,com.google.common.base.Supplier,boolean).5.makeDimensionSelectorUndecorated(io.druid.query.dimension.DimensionSpec).6.getValueCardinality()", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "RegexFilteredDimensionSpec.java", "spoonMethods": [{"INS": 3, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.dimension.RegexFilteredDimensionSpec.decorate(io.druid.segment.DimensionSelector)", "MOV": 1, "TOT": 4}]}, {"spoonFilePath": "DimExtractionTopNAlgorithm.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.DimExtractionTopNAlgorithm.makeInitParams(io.druid.segment.DimensionSelector,io.druid.segment.Cursor)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "ListFilteredDimensionSpec.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.dimension.ListFilteredDimensionSpec.decorate(io.druid.segment.DimensionSelector)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "TopNParams.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.query.topn.TopNParams", "MOV": 0, "TOT": 4}]}, {"spoonFilePath": "IncrementalIndexTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.data.IncrementalIndexTest.testFilteredAggregators()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "RowBasedGrouperHelper.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.groupby.epinephelinae.RowBasedGrouperHelper.RowBasedColumnSelectorFactory.makeDimensionSelectorUndecorated(io.druid.query.dimension.DimensionSpec).1.getValueCardinality()", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "GroupByQueryRunnerTest.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.groupby.GroupByQueryRunnerTest.testSubqueryWithOuterFilterAggregator()", "MOV": 3, "TOT": 4}]}, {"spoonFilePath": "DimensionSelector.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "AggregateTopNMetricFirstAlgorithm.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.AggregateTopNMetricFirstAlgorithm.makeInitParams(io.druid.segment.DimensionSelector,io.druid.segment.Cursor)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.topn.AggregateTopNMetricFirstAlgorithm.getDimValSelectorForTopNMetric(io.druid.query.topn.TopNParams,io.druid.query.topn.TopNResultBuilder)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "PooledTopNAlgorithm.java", "spoonMethods": [{"INS": 1, "UPD": 13, "DEL": 1, "spoonMethodName": "io.druid.query.topn.PooledTopNAlgorithm.makeInitParams(io.druid.segment.DimensionSelector,io.druid.segment.Cursor)", "MOV": 1, "TOT": 16}, {"INS": 0, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.query.topn.PooledTopNAlgorithm.PooledTopNParams.Builder", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.PooledTopNAlgorithm.PooledTopNParams.Builder.withCardinality(int)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.query.topn.PooledTopNAlgorithm.PooledTopNParams", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.topn.PooledTopNAlgorithm.PooledTopNParams.Builder.build()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-08-02 17:39:40", "commitMessage": "Fix FilteredAggregators at ingestion time and in groupBy v2 nested queries. (#3312)\n\nThe common theme between the two is they both create \"fake\" DimensionSelectors\r\nthat work on top of Rows. They both do it because there isn't really any\r\ndictionary for the underlying Rows, they're just a stream of data. The fix for\r\nboth is to allow a DimensionSelector to tell callers that it has no dictionary\r\nby returning CARDINALITY_UNKNOWN from getValueCardinality. The callers, in\r\nturn, can avoid using it in ways that assume it has a dictionary.\r\n\r\nFixes #3311.", "commitUser": "fjy", "commitDateTime": "2016-08-02 17:39:40", "commitParents": ["ae3e0015b6ff293cd50d326b28b0824df6476f3f"], "commitGHEventType": "referenced", "nameRev": "0299ac73b816a6ac1bfeb953d2a39a3c8277d2c2 tags/druid-0.9.2-rc1~89", "commitHash": "0299ac73b816a6ac1bfeb953d2a39a3c8277d2c2"}], "body": "But it would be nice.\n"}, {"user": "gianm", "commits": {}, "labels": ["Bug"], "created": "2016-07-29 23:04:05", "title": "Select query fails with columns called \"timestamp\"", "url": "https://github.com/apache/druid/issues/3303", "closed": "2019-09-24 22:20:20", "ttf": 1151.0002777777777, "commitsDetails": [], "body": "If you have a dimension or metric called \"timestamp\", it will override the actual timestamp from the row, leading to an exception later on when things try to read that value as if it were a timestamp. Stack trace:\n\n```\njava.lang.IllegalArgumentException: Invalid format: \"1469433667658\" is malformed at \"7658\"\n    at org.joda.time.format.DateTimeParserBucket.doParseMillis(DateTimeParserBucket.java:187) ~[joda-time-2.8.2.jar:2.8.2]\n    at org.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:780) ~[joda-time-2.8.2.jar:2.8.2]\n    at org.joda.time.convert.StringConverter.getInstantMillis(StringConverter.java:65) ~[joda-time-2.8.2.jar:2.8.2]\n    at org.joda.time.base.BaseDateTime.<init>(BaseDateTime.java:175) ~[joda-time-2.8.2.jar:2.8.2]\n    at org.joda.time.DateTime.<init>(DateTime.java:257) ~[joda-time-2.8.2.jar:2.8.2]\n    at io.druid.query.select.EventHolder.getTimestamp(EventHolder.java:56) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\n    at io.druid.query.select.SelectResultValueBuilder$1.compare(SelectResultValueBuilder.java:45) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\n    at io.druid.query.select.SelectResultValueBuilder$1.compare(SelectResultValueBuilder.java:41) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\n    at com.google.common.collect.ComparatorOrdering.compare(ComparatorOrdering.java:38) ~[guava-16.0.1.jar:?]\n    at com.google.common.collect.ReverseOrdering.compare(ReverseOrdering.java:38) ~[guava-16.0.1.jar:?]\n    at com.google.common.collect.MinMaxPriorityQueue$Heap.crossOverUp(MinMaxPriorityQueue.java:633) ~[guava-16.0.1.jar:?]\n    at com.google.common.collect.MinMaxPriorityQueue$Heap.bubbleUp(MinMaxPriorityQueue.java:537) ~[guava-16.0.1.jar:?]\n    at com.google.common.collect.MinMaxPriorityQueue.offer(MinMaxPriorityQueue.java:280) ~[guava-16.0.1.jar:?]\n    at com.google.common.collect.MinMaxPriorityQueue.add(MinMaxPriorityQueue.java:252) ~[guava-16.0.1.jar:?]\n    at io.druid.query.select.SelectResultValueBuilder.addEntry(SelectResultValueBuilder.java:77) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\n    at io.druid.query.select.SelectBinaryFn.apply(SelectBinaryFn.java:81) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\n    at io.druid.query.select.SelectBinaryFn.apply(SelectBinaryFn.java:32) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\n    at io.druid.common.guava.CombiningSequence$CombiningYieldingAccumulator.accumulate(CombiningSequence.java:212) ~[druid-common-0.9.1.1.jar:0.9.1.1]\n    at com.metamx.common.guava.BaseSequence.makeYielder(BaseSequence.java:104) ~[java-util-0.27.9.jar:?]\n    at com.metamx.common.guava.BaseSequence.toYielder(BaseSequence.java:81) ~[java-util-0.27.9.jar:?]\n    at io.druid.common.guava.CombiningSequence.toYielder(CombiningSequence.java:78) ~[druid-common-0.9.1.1.jar:0.9.1.1]\n    at com.metamx.common.guava.MappedSequence.toYielder(MappedSequence.java:46) ~[java-util-0.27.9.jar:?]\n    at io.druid.server.QueryResource.doPost(QueryResource.java:224) [druid-server-0.9.1.1.jar:0.9.1.1]\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_10]\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_10]\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_10]\n    at java.lang.reflect.Method.invoke(Method.java:601) ~[?:1.7.0_10]\n    at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60) [jersey-server-1.19.jar:1.19]\n    at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205) [jersey-server-1.19.jar:1.19]\n    at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75) [jersey-server-1.19.jar:1.19]\n    at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302) [jersey-server-1.19.jar:1.19]\n    at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108) [jersey-server-1.19.jar:1.19]\n    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) [jersey-server-1.19.jar:1.19]\n    at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84) [jersey-server-1.19.jar:1.19]\n    at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542) [jersey-server-1.19.jar:1.19]\n    at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473) [jersey-server-1.19.jar:1.19]\n    at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419) [jersey-server-1.19.jar:1.19]\n    at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409) [jersey-server-1.19.jar:1.19]\n    at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409) [jersey-servlet-1.19.jar:1.19]\n    at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558) [jersey-servlet-1.19.jar:1.19]\n    at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733) [jersey-servlet-1.19.jar:1.19]\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) [javax.servlet-api-3.1.0.jar:3.1.0]\n    at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:278) [guice-servlet-4.0-beta.jar:?]\n    at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:268) [guice-servlet-4.0-beta.jar:?]\n    at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:180) [guice-servlet-4.0-beta.jar:?]\n    at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:93) [guice-servlet-4.0-beta.jar:?]\n    at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85) [guice-servlet-4.0-beta.jar:?]\n    at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:120) [guice-servlet-4.0-beta.jar:?]\n    at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:132) [guice-servlet-4.0-beta.jar:?]\n    at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:129) [guice-servlet-4.0-beta.jar:?]\n    at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:206) [guice-servlet-4.0-beta.jar:?]\n    at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:129) [guice-servlet-4.0-beta.jar:?]\n    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652) [jetty-servlet-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:83) [jetty-servlets-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:364) [jetty-servlets-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652) [jetty-servlet-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585) [jetty-servlet-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:221) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1125) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515) [jetty-servlet-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1059) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.Server.handle(Server.java:497) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:310) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:248) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:540) [jetty-io-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:620) [jetty-util-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:540) [jetty-util-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at java.lang.Thread.run(Thread.java:722) [?:1.7.0_10]\n```\n\nReported in: https://groups.google.com/d/topic/druid-user/Fz1drL4Xsps/discussion\n"}, {"user": "drcrallen", "commits": {"188a4bc89aa0fc19887546dd6de9c813d4ae45a2": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "DataSource segment size total reported does not match coordinator and historical", "numCommits": 0, "created": "2016-07-25 16:35:49", "closed": "2016-08-03 16:28:10", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/3283", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 8.000277777777777, "commitsDetails": [{"commitGitStats": [{"filePath": "server/src/main/java/io/druid/client/SingleServerInventoryView.java", "deletions": 8, "insertions": 0, "lines": 8}, {"filePath": "server/src/main/java/io/druid/client/ServerInventoryView.java", "deletions": 10, "insertions": 2, "lines": 12}, {"filePath": "server/src/main/java/io/druid/client/BatchServerInventoryView.java", "deletions": 18, "insertions": 0, "lines": 18}], "commitSpoonAstDiffStats": [{"spoonFilePath": "SingleServerInventoryView.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.client.SingleServerInventoryView", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.client.SingleServerInventoryView.internInventory(io.druid.timeline.DataSegment)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "ServerInventoryView.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.client.ServerInventoryView.internInventory(java.lang.Object)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.client.ServerInventoryView.2.deserializeInventory(byte[])", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "BatchServerInventoryView.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.client.BatchServerInventoryView", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.client.BatchServerInventoryView.internInventory(java.util.Set)", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-07-26 11:47:34", "commitMessage": "Revert \"Optionally intern ServerInventoryView inventory objects. (#3238)\" (#3286)\n\nThis reverts commit a931debf790eaf4454ae13f35e11ba9d39765645.\r\nFixes #3283\r\n\r\nThe core issue here is that realtime nodes announce their size as 0, so a coordinator which interns the realtime version of the data segment will not be able to see the new sized announcement when handoff occurs.\r\n\r\nThis is caused by the `eauals` method on a `DataSegment` only evaluating the identifier. the `eauals` method *should* be correct for object equivalence, and things which need to check equivalence of some sub-portion of the object should do so explicitly.", "commitUser": "fjy", "commitDateTime": "2016-07-26 11:47:34", "commitParents": ["95a58097e244fc59fe2a322f044527efac4d61c6"], "commitGHEventType": "referenced", "nameRev": "188a4bc89aa0fc19887546dd6de9c813d4ae45a2 tags/druid-0.9.2-rc1~108", "commitHash": "188a4bc89aa0fc19887546dd6de9c813d4ae45a2"}], "body": "The Size reported by a historical when looking on the coordinator at `/druid/coordinator/v1/servers` does not match the metrics reported by historicals as the sum of the historical metric `segment/used`. The number reported by the coordinator is MUCH lower. Historical servers which are >99% full are reported by the coordinator as only 91% full! I have confirmed the sum of `segment/used` as reported by the historicals is the correct on-disk size of the segment data on the historical nodes. \n\nThis really screws with capacity planning. One side effect is that the historicals will throw `Exception loading segment` ... `too large for storage` and fail to load the segment on that coordinator balancing round. This is particularly harmful when it happens during handoff, because the resources used by realtime indexing tasks cannot be freed!\n\nThe view kept by the coordinator regarding sizes on a historical node should be eventually consistent with the data emitted by the historical node itself.\n"}, {"user": "benvogan", "commits": {}, "labels": ["Bug"], "created": "2016-07-21 16:53:38", "title": "Unbounded insert into MySQL fails", "url": "https://github.com/apache/druid/issues/3274", "closed": "2018-01-09 18:43:48", "ttf": 537.0002777777778, "commitsDetails": [], "body": "My environment is totally horked from being down for so long and having a bunch of stuck/failed tasks.  I don't know precisely what is leading to having such a large number of values in the payload, but obviously there needs to be some checking of this value.\n\nCaused by: org.skife.jdbi.v2.exceptions.UnableToExecuteStatementException: com.mysql.jdbc.PacketTooBigException: Packet for query is too large (1201867 > 1048576). You can change this value on the server by setting the max_allowed_packet' variable. [statement:\"INSERT INTO druid_tasklogs (task_id, log_payload) VALUES (:entryId, :payload)\", located:\"INSERT INTO druid_tasklogs (task_id, log_payload) VALUES (:entryId, :payload)\", rewritten:\"INSERT INTO druid_tasklogs (task_id, log_payload) VALUES (?, ?)\", arguments:{ positional:{}, named:{payload:[123, 34, 116, 121, 112, 101, 34, 58, 34, 115, 101, 103, 109, 101, 110, 116, 84, 114, 97, 110, 115, 97, 99, 116, 105, 111, 110, 97, 108, 73, 110, 115, 101, 114, 116, 34, 44, 34, 115, 101, 103, 109, 101, 110, 116, 115, 34, 58, 91, 123, 34, 100, 97, 116, 97, 83, 111, 117, 114, 99, 101, 34, 58, 34, 115, 111, 114, 95, 98, 117, 115, 105, 110, 101, 115, 115, 95, 101, 118, 101, 110, 116, 95, 115, 117, 99, 99, 101, 115, 115, 95, 118, 49, 34, 44, 34, 105, 110, 116, 101, 114, 118, 97, 108, 34, 58, 34, 50, 48, 49, 54, 45, 48, 55, 45, 49, 53, 84, 50, 50, 58, 48, 48, 58, 48, 48, 46, 48, 48, 48, 90, 47, 50, 48, 49, 54, 45, 48, 55, 45, 49, 53, 84, 50, 51, 58, 48, 48, 58, 48, 48, 46, 48, 48, 48, 90, 34, 44, 34, 118, 101, 114, 115, 105, 111, 110, 34, 58, 34, 50, 48, 49, 54, 45, 48, 55, 45, 50, 49, 84, 48, 51, 58, 52, 50, 58, 48, 48, 46, 51, 52, 57, 90, 34, 44, 34, 108, 111, 97, 100, 83, 112, 101, 99, 34, 58, 123, 34, 116, 121, 112, 101, 34, 58, 34, 104, 100, 102, 115, 34, 44, 34, 112, 97, 116, 104, 34, 58, 34, 104, 100, 102, 115, 58, 47, 47, 104, 100, 102, 115, 48, 48, 49, 58, 56, 48, 50, 48, 47, 100, 114, 117, 105, 100, 50, 47, 115, 101, 103, 109, 101, 110, 116, 115, 47, 115, 111, 114, 95, 98, 117, 115, 105, 110, 101, 115, 115, 95, 101, 118, 101, 110, 116, 95, 115, 117, 99, 99, 101, 115, 115, 95, 118, 49, 47, 50, 48, 49, 54, 48, 55, 49, 53, 84, 50, 50, 48, 48, 48, 48, 46, 48, 48, 48, 90, 95, 50, 48, 49, 54, 48, 55, 49, 53, 84, 50, 51, 48, 48, 48, 48, 46, 48, 48, 48, 90, 47, 50, 48, 49, 54, 45, 48, 55, 45, 50, 49, 84, 48, 51, 95, 52, 50, 95, 48, 48, 46, 51, 52, 57, 90, 47, 57, 47, 105, 110, 100, 101, 120, 46, 122, 105, 112, 34, 125, 44, 34, 100, 105, 109, 101, 110, 115, 105, 111, 110, 115, 34, 58, 34, 117, 115, 101, 114, 95, 103, 101, 110, 100, 101, 114, 44, 117, 115, 101, 114, 95, 116, 111, 100, 97, 121, 95, 105, 115, 95, 98, 97, 100, 103, 101, 100, 44, 117, 115, 101, 114, 95, 115, 111, 99, 105, 97, 108, 95, 102, 97, 99, 101, 98, 111, 111, 107, 44, 117, 115, 101, 114, 95, 97, 103, 101, 44, 117, 115, 101, 114, 95, 97, 103, 101, 95, 98, 114, 97, 99, 107, 101, 116, 44, 117, 115, 101, 114, 95, 114, 101, 103, 105, 115, 116, 114, 97, 116, 105, 111, 110, 95, 97, 103, 101, 95, 98, 114, 97, 99, 107, 101, 116, 44, 117, 115, 101, 114, 95, 114, 101, 103, 105, 115, 116, 114, 97, 116, 105, 111, 110, 95, 97, 103, 101, 95, 100, 97, 121, 44, 117, 115, 101, 114, 95, 114, 101, 103, 105, 115, 116, 114, 97, 116, 105, 111, 110, 95, 97, 103, 101, 95, 119, 101, 101, 107, 44, 101, 118, 101, 110, 116, 95, 116, 121, 112, 101, 44, 101, 118, 101, 110, 116, 95, 115, 116, 97, 116, 117, 115, 44, 101, 118, 101, 110, 116, 95, 105, 115, 95, 111, 102, 102, 108, 105, 110, 101, 44, 101, 118, 101, 110, 116, 95, 99, 105, 116, 121, 44, 101, 118, 101, 110, 116, 95, 114, 101, 103, 105, 111, 110, 44, 101, 118, 101, 110, 116, 95, 99, 111, 117, 110, 116, 114, 121, 44, 117, 115, 101, 114, 95, 99, 111, 117, 110, 116, 114, 121, 44, 99, 104, 97, 105, 110, 95, 110, 97, 109, 101, 44, 99, 108, 105, 101, 110, 116, 95, 97, 112, 112, 95, 118, 101, 114, 115, 105, 111, 110, 44, 99, 108, 105, 101, 110, 116, 95, 105, 112, 95, 97, 100, 100, 114, 101, 115, 115, 44, 99, 108, 105, 101, 110, 116, 95, 108, 111, 99, 97, 108, 101, 44, 99, 108, 105, 101, 110, 116, 95, 111, 115, 95, 110, 97, 109, 101, 44, 99, 108, 105, 101, 110, 116, 95, 117, 115, 101, 114, 95, 97, 103, 101, 110, 116, 44, 100, 101, 118, 105, 99, 101, 95, 112, 108, 97, 116, 102, 111, 114, 109, 95, 116, 121, 112, 101, 44, 100, 101, 118, 105, 99, 101, 95, 110, 97, 109, 101, 44, 108, 111, 99, 97, 116, 105, 111, 110, 95, 110, 97, 109, 101, 44, 108, 111, 99, 97, 116, 105, 111, 110, 95, 112, 114, 111, 118, 105, 100, 101, 114, 95, 105, 100, 44, 108, 111, 99, 97, 116, 105, 111, 110, 95, 112, 114, 111, 118, 105, 100, 101, 114, 95, 110, 97, 109, 101, 44, 117, 115, 101, 114, 95, 107, 105, 99, 107, 115, 95, 98, 97, 108, 97, 110, 99, 101, 44, 97, 112, 112, 95, 105, 100, 44, 112, 97, 114, 116, 110, 101, 114, 95, 105, 100, 44, 108, 111, 99, 97, 116, 105, 111, 110, 95, 105, 100, 44, 99, 104, 97, 105, 110, 95, 105, 100, 44, 117, 115, 101, 114, 95, 105, 100, 44, 100, 101, 118, 105, 99, 101, 95, 105, 100, 44, 99, 108, 105, 101, 110, 116, 95, 101, 118, 101, 110, 116, 95, 116, 105, 109, 101, 44, 99, 108, 105, 101, 110, 116, 95, 114, 101, 113, 117, 101, 115, 116, 95, 116, 105, 109, 101, 44, 115, 101, 114, 118, 101, 114, 95, 114, 101, 113, 117, 101, 115, 116, 95, 116, 105, 109, 101, 44, 117, 115, 101, 114, 95, 114, 101, 103, 105, 115, 116, 114, 97, 116, 105, 111, 110, 95, 116, 105, 109, 101, 44, 101, 118, 101, 110, 116, 95, 99, 111, 111, 114, 100, 105, 110, 97, 116, 101, 115, 44, 108, 111, 99, 97, 116, 105, 111, 110, 95, 99, 111, 111, 114, 100, 105, 110, 97, 116, 101, 115, 34, 44, 34, 109, 101, 116, 114, 105, 99, 115, 34, 58, 34, 99, 111, 117, 110, 116, 44, 116, 114, 97, 110, 115, 97, 99, 116, 105, 111, 110, 95, 107, 105, 99, 107, 115, 44, 116, 114, 97, 110, 115, 97, 99, 116, 105, 111, 110, 95, 99, 111, 115, 116, 44, 116, 114, 97, 110, 115, 97, 99, 116, 105, 111, 110, 95, 114, 101, 118, 101, 110, 117, 101, 44, 117, 110, 105, 113, 117, 101, 95, 117, 115, 101, 114, 95, 105, 100, 44, 117, 110, 105, 113, 117, 101, 95, 117, 115, 101, 114, 95, 100, 101, 118, 105, 99, 101, 95, 105, 100, 44, 117, 110, 105, 113, 117, 101, 95, 108, 111, 99, 97, 116, 105, 111, 110, 95, 105, 100, 34, 44, 34, 115, 104, 97, 114, 100, 83, 112, 101, 99, 34, 58, 123, 34, 116, 121, 112, 101, 34, 58, 34, 110, 117, 109, 98, 101, 114, 101, 100, 34, 44, 34, 112, 97, 114, 116, 105, 116, 105, 111, 110, 78, 117, 109, 34, 58, 57, 44, 34, 112, 97, 114, 116, 105, 116, 105, 111, 110, 115, 34, 58, 48, 125, 44, 34, 98, 105, 110, 97, 114, 121, 86, 101, 114, 115, 105, 111, 110, 34, 58, 57, 44, 34, 115, 105, 122, 101, 34, 58, 52, 51, 53, 55, 56, 50, 56, 44, 34, 105, 100, 101, 110, 116, 105, 102, 105, 101, 114, 34, 58, 34, 115, 111, 114, 95, 98, 117, 115, 105, 110, 101, 115, 115, 95, 101, 118, 101, 110, 116, 95, 115, 117, 99, 99, 101, 115, 115, 95, 118, 49, 95, 50, 48, 49, 54, 45, 48, 55, 45, 49, 53, 84, 50, 50, 58, 48, 48, 58, 48, 48, 46, 48, 48, 48, 90, 95, 50, 48, 49, 54, 45, 48, 55, 45, 49, 53, 84, 50, 51, 58, 48, 48, 58, 48, 48, 46, 48, 48, 48, 90, 95, 50, 48, 49, 54, 45, 48, 55, 45, 50, 49, 84, 48, 51, 58, 52, 50, 58, 48, 48, 46, 51, 52, 57, 90, 95, 57, 34, 125, 44, 123, 34, 100, 97, 116, 97, 83, 111, 117, 114, 99, 101, 34, 58, 34, 115, 111, 114, 95, 98, 117, 115, 105, 110, 101, 115, 115, 95, 101, 118, 101, 110, 116, 95, 115, 117, 99, 99, 101, 115, 115, 95, 118, 49, 34, 44, 34, 105, 110, 116, 101, 114, 118, 97, 108, 34, 58, 34, 50, 48, 49, 54, 45, 48, 55, 45, 49, 53, 84, 50, 50, 58, 48, 48, 58, 48, 48, 46, 48, 48, 48, 90, 47, 50, 48, 49, 54, 45, 48, 55, 45, 49, 53, 84, 50, 51, 58, 48, 48, 58, 48, 48, 46, 48, 48, 48, 90, 34, 44, 34, 118, 101, 114, 115, 105, 111, 110, 34, 58, 34, 50, 48, 49, 54, 45, 48, 55, 45, 50, 49, 84, 48, 51, 58, 52, 50, 58, 48, 48, 46, 51, 52, 57, 90, 34, 44,...\n\n```\n    at org.skife.jdbi.v2.SQLStatement.internalExecute(SQLStatement.java:1334) ~[jdbi-2.63.1.jar:2.63.1]\n    at org.skife.jdbi.v2.Update.execute(Update.java:56) ~[jdbi-2.63.1.jar:2.63.1]\n    at io.druid.metadata.SQLMetadataStorageActionHandler$10.withHandle(SQLMetadataStorageActionHandler.java:359) ~[druid-server-0.9.1.jar:0.9.1]\n    at io.druid.metadata.SQLMetadataStorageActionHandler$10.withHandle(SQLMetadataStorageActionHandler.java:347) ~[druid-server-0.9.1.jar:0.9.1]\n    at org.skife.jdbi.v2.DBI.withHandle(DBI.java:281) ~[jdbi-2.63.1.jar:2.63.1]\n    ... 64 more\n```\n\nCaused by: com.mysql.jdbc.PacketTooBigException: Packet for query is too large (1201867 > 1048576). You can change this value on the server by setting the max_allowed_packet' variable.\n        at com.mysql.jdbc.MysqlIO.send(MysqlIO.java:3583) ~[?:?]\n        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2460) ~[?:?]\n        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2625) ~[?:?]\n        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2551) ~[?:?]\n        at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1861) ~[?:?]\n        at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1192) ~[?:?]\n        at org.apache.commons.dbcp2.DelegatingPreparedStatement.execute(DelegatingPreparedStatement.java:198) ~[commons-dbcp2-2.0.1.jar:2.0.1]\n        at org.apache.commons.dbcp2.DelegatingPreparedStatement.execute(DelegatingPreparedStatement.java:198) ~[commons-dbcp2-2.0.1.jar:2.0.1]\n        at org.skife.jdbi.v2.SQLStatement.internalExecute(SQLStatement.java:1328) ~[jdbi-2.63.1.jar:2.63.1]\n        at org.skife.jdbi.v2.Update.execute(Update.java:56) ~[jdbi-2.63.1.jar:2.63.1]\n        at io.druid.metadata.SQLMetadataStorageActionHandler$10.withHandle(SQLMetadataStorageActionHandler.java:359) ~[druid-server-0.9.1.jar:0.9.1]\n        at io.druid.metadata.SQLMetadataStorageActionHandler$10.withHandle(SQLMetadataStorageActionHandler.java:347) ~[druid-server-0.9.1.jar:0.9.1]\n        at org.skife.jdbi.v2.DBI.withHandle(DBI.java:281) ~[jdbi-2.63.1.jar:2.63.1]\n        ... 64 more\n"}, {"user": "sascha-coenen", "commits": {"d51097c80945e58d197e5923f5f3b40139212997": {"commitGHEventType": "referenced", "commitUser": "gianm"}, "5e6539fec6ffeaa6afcc4d2d0824c78028756bc9": {"commitGHEventType": "referenced", "commitUser": "gianm"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0.0, "UPD": 0.0, "DEL": 0.0, "MOV": 0.0, "TOT": 0.0}, "title": "Fix lz4 library incompatibility in kafka-indexing-service extension", "numCommits": 0, "created": "2016-07-20 12:33:11", "closed": "2017-05-16 18:05:33", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/3266", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 300.0002777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIndexTask.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/test/TestBroker.java", "deletions": 2, "insertions": 3, "lines": 5}, {"filePath": "docs/content/development/extensions-core/kafka-ingestion.md", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java", "deletions": 3, "insertions": 4, "lines": 7}, {"filePath": "extensions-core/kafka-indexing-service/pom.xml", "deletions": 2, "insertions": 2, "lines": 4}], "commitSpoonAstDiffStats": [{"spoonFilePath": "KafkaIndexTask.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.KafkaIndexTask.possiblyResetOffsetsOrWait(java.util.Map,org.apache.kafka.clients.consumer.KafkaConsumer,io.druid.indexing.common.TaskToolbox)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "TestBroker.java", "spoonMethods": [{"INS": 1, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.test.TestBroker.start()", "MOV": 0, "TOT": 3}]}, {"spoonFilePath": "KafkaSupervisor.java", "spoonMethods": [{"INS": 2, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.supervisor.KafkaSupervisor.getOffsetFromKafkaForPartition(int,boolean)", "MOV": 2, "TOT": 6}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2017-04-25 08:41:51", "commitMessage": "Fix lz4 library incompatibility in kafka-indexing-service extension (#4115)\n\n* Fix lz4 library incompatibility in kafka-indexing-service extension #3266\r\n\r\n* Bumped Kafka version to 0.10.2.0 for : Fix lz4 library incompatibility in kafka-indexing-service extension #3266\r\n\r\n* Replaced Lists.newArrayList() with Collections.singletonList() For Fix lz4 library incompatibility in kafka-indexing-service extension #4115\r\n", "commitUser": "gianm", "commitDateTime": "2017-04-25 12:23:51", "commitParents": ["723a855ab95bacb661084225af7a39b2994230e6"], "commitGHEventType": "referenced", "nameRev": "d51097c80945e58d197e5923f5f3b40139212997 tags/druid-0.10.1-rc1~131", "commitHash": "d51097c80945e58d197e5923f5f3b40139212997"}, {"commitGitStats": [{"filePath": "server/src/main/resources/static/old-console/js/init-0.0.2.js", "deletions": 2, "insertions": 6, "lines": 8}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "", "authoredDateTime": "2017-05-12 12:14:17", "commitMessage": "Average Server Percent Used: NaN% Error when server startup is in progress (fixes #4214) (#4240)\n\n* Fix lz4 library incompatibility in kafka-indexing-service extension #3266\r\n\r\n* Bumped Kafka version to 0.10.2.0 for : Fix lz4 library incompatibility in kafka-indexing-service extension #3266\r\n\r\n* Replaced Lists.newArrayList() with Collections.singletonList() For Fix lz4 library incompatibility in kafka-indexing-service extension #4115\r\n\r\n* Fixed: Average Server Percent Used: NaN% Error when server startup is in progress #4214\r\n", "commitUser": "gianm", "commitDateTime": "2017-05-12 15:56:17", "commitParents": ["1ebfa22955c318250ec0bcfd4b849110043f6a24"], "commitGHEventType": "referenced", "nameRev": "5e6539fec6ffeaa6afcc4d2d0824c78028756bc9 tags/druid-0.10.1-rc1~93", "commitHash": "5e6539fec6ffeaa6afcc4d2d0824c78028756bc9"}], "body": "The druid-kafka-indexing-service extension contains the following two libraries:\n- kafka-clients 0.9.0.1\n- lz4 1.3.0\n\nThe kafka-clients lib itself has a dependency on the lz4 lib version 1.2.0.\nThis doesn't lead to a version conflict as long as the Kafka topic consumed from contains message-batches that are either uncompressed or use a different compression scheme.\n\nIf the kafka topic contains messages that have been lz4 compressed by the kafka-producer, then the kafka-indexing-service fails with the following exception which is due to the version conflict:\n\n```\njava.lang.NoSuchMethodError: net.jpountz.util.Utils.checkRange([BII)V\n    at org.apache.kafka.common.record.KafkaLZ4BlockInputStream.read(KafkaLZ4BlockInputStream.java:177) \n```\n\nPossible options for a bugfix might be\n- in the kafka-indexing-service extension POM, downgrade lz4 library version \n- in the kafka-indexing-service extension POM, upgrade kafka-clients library to 0.10.0.0 which is built against lz4 version 1.3.0\n  I haven't checked if the kafka-client v 0.10 would be backwards compatible with with kafka server 0.9\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2016-07-12 22:55:38", "title": "BatchServerInventoryView is created twice", "url": "https://github.com/apache/druid/issues/3237", "closed": "2016-07-14 05:30:35", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "Brokers retain two copies of `BatchServerInventoryView`. One through `BatchServerInventoryViewProvider` and one through `FilteredBatchServerInventoryViewProvider`. This causes excessive use of heap when there are a large quantity of segments and servers.\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2016-07-11 21:19:45", "title": "`Caused by: com.metamx.common.IAE: Unknown version` on segment load", "url": "https://github.com/apache/druid/issues/3233", "closed": "2016-07-18 15:32:19", "ttf": 6.000277777777778, "commitsDetails": [], "body": "```\nio.druid.segment.loading.SegmentLoadingException: Exception loading segment[ID_REDACTED]\n        at io.druid.server.coordination.ZkCoordinator.loadSegment(ZkCoordinator.java:309) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at io.druid.server.coordination.ZkCoordinator.addSegment(ZkCoordinator.java:350) [druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at io.druid.server.coordination.SegmentChangeRequestLoad.go(SegmentChangeRequestLoad.java:44) [druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at io.druid.server.coordination.ZkCoordinator$1.childEvent(ZkCoordinator.java:152) [druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:522) [druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:516) [druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:93) [druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) [druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:85) [druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:514) [druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35) [druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.apache.curator.framework.recipes.cache.PathChildrenCache$9.run(PathChildrenCache.java:772) [druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_60]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_60]\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_60]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_60]\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_60]\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_60]\n        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]\nCaused by: com.metamx.common.IAE: Unknown version[68]\n        at io.druid.segment.data.GenericIndexed.read(GenericIndexed.java:323) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at io.druid.segment.IndexIO$V9IndexLoader.load(IndexIO.java:1028) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at io.druid.segment.IndexIO.loadIndex(IndexIO.java:216) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at io.druid.segment.loading.MMappedQueryableIndexFactory.factorize(MMappedQueryableIndexFactory.java:49) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at io.druid.segment.loading.SegmentLoaderLocalCacheManager.getSegment(SegmentLoaderLocalCacheManager.java:96) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at io.druid.server.coordination.ServerManager.loadSegment(ServerManager.java:152) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at io.druid.server.coordination.ZkCoordinator.loadSegment(ZkCoordinator.java:305) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        ... 18 more\n```\n\nUsing a regex of `.*com\\.metamx\\.common\\.IAE: Unknown version\\[(?<version>[0-9]*)\\].*` to extract the version, we can see that the errors began occurring on the 5th of July and only show a \"wrong\" version of 68 or 0.\n\n<img width=\"1418\" alt=\"screen shot 2016-07-11 at 2 18 32 pm\" src=\"https://cloud.githubusercontent.com/assets/8213081/16747123/6e2fa32c-4772-11e6-8b85-e1bcfbd3190c.png\">\n\nI have not tagged this to a particular version because I haven't been able to discern the root cause yet. This ticket is out there to see if others are encountering this as well.\n\nThis _seems_ to be coming from realtime indexing tasks, but unclear what is causing the file corruption.\n"}, {"user": "gianm", "commits": {}, "labels": ["Bug"], "created": "2016-07-06 02:06:01", "title": "Lease exceptions on push in HdfsDataSegmentPusher with replicas", "url": "https://github.com/apache/druid/issues/3219", "closed": "2016-11-02 17:38:19", "ttf": 119.00027777777778, "commitsDetails": [], "body": "Looks to be something related to the fact that the N replica tasks are all going to try to publish the same segment to the same location. See https://groups.google.com/d/msg/druid-user/kZVNO0MgCs4/mu-SHoXDBgAJ\n\nIt might work better for the hdfs pusher to create a tmp file first (index.zip.uuid?) and then rename it into place.\n"}, {"user": "gianm", "commits": {}, "labels": ["Area - Documentation", "Bug"], "created": "2016-07-01 22:06:36", "title": "JavaScript: Non-thread-safe use of shared globals", "url": "https://github.com/apache/druid/issues/3212", "closed": "2016-09-13 20:46:55", "ttf": 73.00027777777778, "commitsDetails": [], "body": "JavaScriptExtractionFn (probably some other JavaScript stuff too) has a global scope that is shared between multiple threads, leading to issues when user functions use globals instead of `var`s. The global scope should probably be thread local.\n\nMaybe related: https://groups.google.com/d/msg/druid-user/ROv2-WohcUc/QAt4aj59AwAJ\n\nAlmost surely related: an exception we saw on our cluster today with a trace like the following (on JS code that checked for definedness of a global variable before calling `.length` on it).\n\n```\norg.mozilla.javascript.EcmaError: TypeError: Cannot read property \"length\" from undefined (fn#1)\n        at org.mozilla.javascript.ScriptRuntime.constructError(ScriptRuntime.java:3689) ~[rhino-1.7R5.jar:1.7R5]\n        at org.mozilla.javascript.ScriptRuntime.constructError(ScriptRuntime.java:3667) ~[rhino-1.7R5.jar:1.7R5]\n        at org.mozilla.javascript.ScriptRuntime.typeError(ScriptRuntime.java:3695) ~[rhino-1.7R5.jar:1.7R5]\n        at org.mozilla.javascript.ScriptRuntime.typeError2(ScriptRuntime.java:3714) ~[rhino-1.7R5.jar:1.7R5]\n        at org.mozilla.javascript.ScriptRuntime.undefReadError(ScriptRuntime.java:3726) ~[rhino-1.7R5.jar:1.7R5]\n        at org.mozilla.javascript.ScriptRuntime.getObjectProp(ScriptRuntime.java:1483) ~[rhino-1.7R5.jar:1.7R5]\n        at org.mozilla.javascript.gen.fn_776._c_anonymous_0(fn:1) ~[?:?]\n        at org.mozilla.javascript.gen.fn_776.call(fn) ~[?:?]\n        at org.mozilla.javascript.ContextFactory.doTopCall(ContextFactory.java:394) ~[rhino-1.7R5.jar:1.7R5]\n        at org.mozilla.javascript.ScriptRuntime.doTopCall(ScriptRuntime.java:3090) ~[rhino-1.7R5.jar:1.7R5]\n        at org.mozilla.javascript.gen.fn_776.call(fn) ~[?:?]\n        at io.druid.query.extraction.JavaScriptExtractionFn$1.apply(JavaScriptExtractionFn.java:61) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\n        at io.druid.query.extraction.JavaScriptExtractionFn$1.apply(JavaScriptExtractionFn.java:52) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\n        at io.druid.query.extraction.JavaScriptExtractionFn.apply(JavaScriptExtractionFn.java:119) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\n        at io.druid.query.extraction.JavaScriptExtractionFn.apply(JavaScriptExtractionFn.java:125) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\n        at io.druid.segment.QueryableIndexStorageAdapter$CursorSequenceBuilder$1$1$2.lookupName(QueryableIndexStorageAdapter.java:493) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\n        at io.druid.query.topn.DimExtractionTopNAlgorithm.scanAndAggregate(DimExtractionTopNAlgorithm.java:108) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\n        at io.druid.query.topn.DimExtractionTopNAlgorithm.scanAndAggregate(DimExtractionTopNAlgorithm.java:34) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\n        at io.druid.query.topn.BaseTopNAlgorithm.run(BaseTopNAlgorithm.java:95) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\n        at io.druid.query.topn.TopNMapFn.apply(TopNMapFn.java:58) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\n        at io.druid.query.topn.TopNMapFn.apply(TopNMapFn.java:27) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\n        at io.druid.query.topn.TopNQueryEngine$1.apply(TopNQueryEngine.java:84) ~[druid-processing-0.9.1.1.jar:0.9.1.1]\n```\n"}, {"user": "Dunedan", "commits": {}, "labels": ["Bug"], "created": "2016-06-29 07:24:12", "title": "Slow segment loading from S3", "url": "https://github.com/apache/druid/issues/3202", "closed": "2017-09-21 04:34:07", "ttf": 448.0002777777778, "commitsDetails": [], "body": "We noticed that segment loading from S3 on historical nodes running Druid 0.9.0 is way slower than expected. While our hardware should be capable of loading more than 400MB/s, we only see around 42MB/s on average.\n\nWhile digging into that we noticed that even though we set `druid.segmentCache.numLoadingThreads=25` only one of the loading threads is active at a time, as shown in the following picture:\n![visualvm-sequential-segment-loading-threads](https://cloud.githubusercontent.com/assets/1735355/16443386/c1a0536a-3dd8-11e6-98e1-8e2069358166.png)\n\nThis leads to sequential loading of segments instead of parallel loading as one would expect it when using multiple threads.\n\nWe think this could be a bug in how Druid handles the segment loading from S3 (or possibly from deep storage in general).\n\nSome additional information including a log file showing the sequential loading as well is available in the following thread on the mailinglist: https://groups.google.com/forum/#!topic/druid-user/doaE61OLHEo\n"}, {"user": "drcrallen", "commits": {"bfa5c05aaa4b4b4a982d14bdba54db00a925cc25": {"commitGHEventType": "referenced", "commitUser": "nishantmonu51"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "Lookup introspection fails for global cached lookups ", "numCommits": 0, "created": "2016-06-27 17:58:43", "closed": "2016-07-01 22:50:58", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/3187", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 4.000277777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "extensions-core/kafka-extraction-namespace/src/main/java/io/druid/query/lookup/KafkaLookupExtractorIntrospectionHandler.java", "deletions": 0, "insertions": 43, "lines": 43}, {"filePath": "extensions-core/lookups-cached-global/src/main/java/io/druid/query/lookup/NamespaceLookupIntrospectHandler.java", "deletions": 0, "insertions": 105, "lines": 105}, {"filePath": "extensions-core/kafka-extraction-namespace/src/main/java/io/druid/query/lookup/KafkaLookupExtractorFactory.java", "deletions": 18, "insertions": 1, "lines": 19}, {"filePath": "extensions-core/lookups-cached-global/src/main/java/io/druid/query/lookup/NamespaceLookupExtractorFactory.java", "deletions": 68, "insertions": 3, "lines": 71}], "commitSpoonAstDiffStats": [{"spoonFilePath": "NamespaceLookupIntrospectHandler.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.lookup.NamespaceLookupIntrospectHandler", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "NamespaceLookupExtractorFactory.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.lookup.NamespaceLookupExtractorFactory.get().2", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.lookup.NamespaceLookupExtractorFactory.get().2.getCacheKey()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.lookup.NamespaceLookupExtractorFactory.1.getVersion()", "MOV": 1, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.lookup.NamespaceLookupExtractorFactory", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "KafkaLookupExtractorFactory.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.lookup.KafkaLookupExtractorFactory.KafkaLookupExtractorIntrospectionHandler", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.lookup.KafkaLookupExtractorFactory.getIntrospectHandler()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "KafkaLookupExtractorIntrospectionHandler.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.lookup.KafkaLookupExtractorIntrospectionHandler", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-07-01 15:50:57", "commitMessage": "Make global lookup cache introspector class public (#3199)\n\n* Make global lookup cache introspector class public\r\n* Fixes #3187\r\n\r\n* Make KafkaLookupExtractorIntrospectionHandler a public static class\r\n", "commitUser": "nishantmonu51", "commitDateTime": "2016-07-01 15:50:57", "commitParents": ["e1313e4b90bb434a33131a62c74abd52343f3f2e"], "commitGHEventType": "referenced", "nameRev": "bfa5c05aaa4b4b4a982d14bdba54db00a925cc25 tags/druid-0.9.2-rc1~139", "commitHash": "bfa5c05aaa4b4b4a982d14bdba54db00a925cc25"}], "body": "```\n$ curl -v http://REDACTED_HOST/druid/v1/lookups/introspect/REDACTED/version\n*   Trying REDACTED_HOST...\n* Connected to REDACTED_HOST (REDACTED_HOST) port ##### (#0)\n> GET /druid/v1/lookups/introspect/REDACTED_account_name_test/keys HTTP/1.1\n> Host: REDACTED_HOST\n> User-Agent: curl/7.43.0\n> Accept: */*\n>\n< HTTP/1.1 500 Server Error\n< Date: Mon, 27 Jun 2016 17:55:35 GMT\n< Cache-Control: must-revalidate,no-cache,no-store\n< Content-Type: text/html; charset=ISO-8859-1\n< Content-Length: 614\n< Connection: close\n< Server: Jetty(9.2.z-SNAPSHOT)\n<\n<html>\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html;charset=ISO-8859-1\"/>\n<title>Error 500 </title>\n</head>\n<body>\n<h2>HTTP ERROR: 500</h2>\n<p>Problem accessing /druid/v1/lookups/introspect/REDACTED/version. Reason:\n<pre>    javax.servlet.ServletException: com.sun.jersey.api.container.ContainerException: java.lang.IllegalAccessException: Class com.sun.jersey.spi.container.JavaMethodInvokerFactory$1 can not access a member of class io.druid.query.lookup.NamespaceLookupExtractorFactory$1 with modifiers \"public\"</pre></p>\n<hr /><i><small>Powered by Jetty://</small></i>\n</body>\n</html>\n* Closing connection 0\n```\n\nThis is a new feature for 0.9.1 so I don't think it should be a blocker. Suggesting labeling for 0.9.2\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2016-06-22 23:47:59", "title": "500 Error on SegmentMetadataQuery against router for datasource with no data", "url": "https://github.com/apache/druid/issues/3178", "closed": "2016-06-24 17:31:56", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "``` json\n{\n    \"intervals\": \"2016-06-21T19/pt1h\",\n    \"queryType\": \"segmentMetadata\",\n    \"dataSource\": \"notExist\"\n}\n```\n\nresults in a 500 error with\n\n``` json\n{\"error\":\"Can not construct instance of io.druid.query.metadata.metadata.SegmentMetadataQuery$AnalysisType, problem: No enum constant io.druid.query.metadata.metadata.SegmentMetadataQuery.AnalysisType.MINMAX\\n at [Source: HttpInputOverHTTP@59cc5787; line: 1, column: 337] (through reference chain: java.util.RegularEnumSet[3])\"}\n```\n\nWhen issued to the router.\n\nThis ALSO occurs when issued over a time range where an otherwise fine datasource has no data. This is easily checked by doing a query in the future beyond when any data should exist in a cluster.\n"}, {"user": "drcrallen", "commits": {"03cfcf002b681255a7c9e1db2aea1201ef155a67": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "Overlord assigns too many tasks to middle manager", "numCommits": 0, "created": "2016-06-22 21:00:03", "closed": "2016-08-16 05:24:48", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/3174", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 54.000277777777775, "commitsDetails": [{"commitGitStats": [{"filePath": "docs/content/configuration/indexing-service.md", "deletions": 1, "insertions": 5, "lines": 6}, {"filePath": "indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java", "deletions": 40, "insertions": 33, "lines": 73}, {"filePath": "indexing-service/src/main/java/io/druid/indexing/overlord/config/RemoteTaskRunnerConfig.java", "deletions": 1, "insertions": 1, "lines": 2}], "commitSpoonAstDiffStats": [{"spoonFilePath": "RemoteTaskRunnerConfig.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.overlord.config.RemoteTaskRunnerConfig", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "RemoteTaskRunner.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.indexing.overlord.RemoteTaskRunner.tryAssignTask(io.druid.indexing.common.task.Task,io.druid.indexing.overlord.RemoteTaskRunnerWorkItem)", "MOV": 8, "TOT": 12}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-08-10 13:29:50", "commitMessage": "fix the race described in #3174 (#3205)\n\n", "commitUser": "fjy", "commitDateTime": "2016-08-10 11:29:50", "commitParents": ["043562914d1af0286a0132e12cc02abc58578561"], "commitGHEventType": "referenced", "nameRev": "03cfcf002b681255a7c9e1db2aea1201ef155a67 tags/druid-0.9.2-rc1~68", "commitHash": "03cfcf002b681255a7c9e1db2aea1201ef155a67"}], "body": "RTR has a guard against multiple things trying to launch on the same worker via `workersWithUnacknowledgedTask.putIfAbsent(immutableZkWorker.get().getWorker().getHost(), task.getId()`. But this is enforced AFTER the check for space on the workers through `strategy.findWorkerForTask`\n\nAs such, it is possible for N tasks (where N is the number of launching threads set by `druid.indexer.runner.pendingTasksRunnerNumThreads`) to all think that worker W has capacity, and for them ALL to pass the check if the exact wrong kind of race occurs. Which means that a worker can be over-subscribed up to N-1 tasks. \n\nThe _middle manager_ logs before sending a SIGINT look like this, with the shutdown handler appearing as the last entry:\n\n```\n2016-06-22T17:45:03,425 INFO [WorkerTaskMonitor] io.druid.indexing.worker.WorkerTaskMonitor - Submitting runnable for task[index_realtime_REDACTED_2016-06-22T18:00:00.000Z_56_0]\n2016-06-22T17:45:03,428 INFO [WorkerTaskMonitor] io.druid.indexing.worker.WorkerTaskMonitor - Affirmative. Running task [index_realtime_REDACTED_2016-06-22T18:00:00.000Z_56_0]\n2016-06-22T18:30:52,522 WARN [HttpClient-Netty-Worker-2] com.metamx.http.client.NettyHttpClient - [POST REDACTED] Channel disconnected before response complete\n2016-06-22T18:30:52,523 WARN [HttpPostEmitter-1-0] com.metamx.http.client.pool.ResourcePool - Resource at key[REDACTED] was returned multiple times?\n2016-06-22T18:30:52,524 WARN [HttpPostEmitter-1-0] com.metamx.emitter.core.HttpPostEmitter - Got exception when posting events to urlString[REDACTED]. Resubmitting.\njava.util.concurrent.ExecutionException: org.jboss.netty.channel.ChannelException: Channel disconnected\n        at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at com.metamx.emitter.core.HttpPostEmitter$EmittingRunnable.run(HttpPostEmitter.java:293) [druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_91]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_91]\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_91]\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_91]\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_91]\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_91]\n        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_91]\nCaused by: org.jboss.netty.channel.ChannelException: Channel disconnected\n        at com.metamx.http.client.NettyHttpClient$1.channelDisconnected(NettyHttpClient.java:311) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.channelDisconnected(SimpleChannelUpstreamHandler.java:208) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.channelDisconnected(SimpleChannelUpstreamHandler.java:208) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:570) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.handler.codec.http.HttpClientCodec.handleUpstream(HttpClientCodec.java:92) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:493) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.handler.ssl.SslHandler.channelDisconnected(SslHandler.java:580) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:396) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.Channels$4.run(Channels.java:386) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.socket.ChannelRunnableWrapper.run(ChannelRunnableWrapper.java:40) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:391) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:315) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[druid-selfcontained-0.9.1-rc3-mmx0.jar:0.9.1-rc3-mmx0]\n        ... 3 more\n2016-06-22T18:31:52,526 INFO [HttpPostEmitter-1-0] com.metamx.http.client.pool.ChannelResourceFactory - Generating: REDACTED\n2016-06-22T19:25:45,719 INFO [Thread-68] com.metamx.common.lifecycle.Lifecycle - Running shutdown hook\n```\n\nBefore firing the shutdown handler, A thread dump was taken, and here are the threads (very booring repeated sections snipped for brevity):\n\n```\nFull thread dump Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode):\n\n\"process reaper\" #212 daemon prio=10 os_prio=0 tid=0x00007fc904004000 nid=0xb291 runnable [0x00007fc9c0e5f000]\n   java.lang.Thread.State: RUNNABLE\n    at java.lang.UNIXProcess.waitForProcessExit(Native Method)\n    at java.lang.UNIXProcess.lambda$initStreams$3(UNIXProcess.java:290)\n    at java.lang.UNIXProcess$$Lambda$9/744186075.run(Unknown Source)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"process reaper\" #211 daemon prio=10 os_prio=0 tid=0x00007fc92c00e800 nid=0xb26b runnable [0x00007fca98070000]\n   java.lang.Thread.State: RUNNABLE\n    at java.lang.UNIXProcess.waitForProcessExit(Native Method)\n    at java.lang.UNIXProcess.lambda$initStreams$3(UNIXProcess.java:290)\n    at java.lang.UNIXProcess$$Lambda$9/744186075.run(Unknown Source)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"process reaper\" #210 daemon prio=10 os_prio=0 tid=0x00007fc8ec003000 nid=0xb246 runnable [0x00007fc9c0f99000]\n   java.lang.Thread.State: RUNNABLE\n    at java.lang.UNIXProcess.waitForProcessExit(Native Method)\n    at java.lang.UNIXProcess.lambda$initStreams$3(UNIXProcess.java:290)\n    at java.lang.UNIXProcess$$Lambda$9/744186075.run(Unknown Source)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"process reaper\" #209 daemon prio=10 os_prio=0 tid=0x00007fc8f4001000 nid=0xb22e runnable [0x00007fc9c10d3000]\n   java.lang.Thread.State: RUNNABLE\n    at java.lang.UNIXProcess.waitForProcessExit(Native Method)\n    at java.lang.UNIXProcess.lambda$initStreams$3(UNIXProcess.java:290)\n    at java.lang.UNIXProcess$$Lambda$9/744186075.run(Unknown Source)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"process reaper\" #208 daemon prio=10 os_prio=0 tid=0x00007fc8fc009000 nid=0xb20f runnable [0x00007fcb6c037000]\n   java.lang.Thread.State: RUNNABLE\n    at java.lang.UNIXProcess.waitForProcessExit(Native Method)\n    at java.lang.UNIXProcess.lambda$initStreams$3(UNIXProcess.java:290)\n    at java.lang.UNIXProcess$$Lambda$9/744186075.run(Unknown Source)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"process reaper\" #207 daemon prio=10 os_prio=0 tid=0x00007fc91c001000 nid=0xb200 runnable [0x00007fcb7c044000]\n   java.lang.Thread.State: RUNNABLE\n    at java.lang.UNIXProcess.waitForProcessExit(Native Method)\n    at java.lang.UNIXProcess.lambda$initStreams$3(UNIXProcess.java:290)\n    at java.lang.UNIXProcess$$Lambda$9/744186075.run(Unknown Source)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"process reaper\" #206 daemon prio=10 os_prio=0 tid=0x00007fc914001000 nid=0xb1c5 runnable [0x00007fca98037000]\n   java.lang.Thread.State: RUNNABLE\n    at java.lang.UNIXProcess.waitForProcessExit(Native Method)\n    at java.lang.UNIXProcess.lambda$initStreams$3(UNIXProcess.java:290)\n    at java.lang.UNIXProcess$$Lambda$9/744186075.run(Unknown Source)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"process reaper\" #205 daemon prio=10 os_prio=0 tid=0x00007fc924001000 nid=0xb1b6 runnable [0x00007fcb7c0b6000]\n   java.lang.Thread.State: RUNNABLE\n    at java.lang.UNIXProcess.waitForProcessExit(Native Method)\n    at java.lang.UNIXProcess.lambda$initStreams$3(UNIXProcess.java:290)\n    at java.lang.UNIXProcess$$Lambda$9/744186075.run(Unknown Source)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"process reaper\" #204 daemon prio=10 os_prio=0 tid=0x00007fc90c003800 nid=0x9f4e runnable [0x00007fcb7c07d000]\n   java.lang.Thread.State: RUNNABLE\n    at java.lang.UNIXProcess.waitForProcessExit(Native Method)\n    at java.lang.UNIXProcess.lambda$initStreams$3(UNIXProcess.java:290)\n    at java.lang.UNIXProcess$$Lambda$9/744186075.run(Unknown Source)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"forking-task-runner-8\" #185 daemon prio=5 os_prio=0 tid=0x00007fca40027800 nid=0x32c1 runnable [0x00007fc9c0f5f000]\n   java.lang.Thread.State: RUNNABLE\n    at java.io.FileInputStream.readBytes(Native Method)\n    at java.io.FileInputStream.read(FileInputStream.java:255)\n    at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n    at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\n    at java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n    - locked <0x00000000fe9dbdd8> (a java.lang.UNIXProcess$ProcessPipeInputStream)\n    at java.io.FilterInputStream.read(FilterInputStream.java:107)\n    at com.google.common.io.ByteStreams.copy(ByteStreams.java:175)\n    at io.druid.indexing.overlord.ForkingTaskRunner$1.call(ForkingTaskRunner.java:432)\n    at io.druid.indexing.overlord.ForkingTaskRunner$1.call(ForkingTaskRunner.java:219)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"forking-task-runner-7\" #183 daemon prio=5 os_prio=0 tid=0x00007fca40024000 nid=0x3285 runnable [0x00007fc9c109a000]\n   java.lang.Thread.State: RUNNABLE\n    at java.io.FileInputStream.readBytes(Native Method)\n    at java.io.FileInputStream.read(FileInputStream.java:255)\n    at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n    at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\n    at java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n    - locked <0x00000000fec4c080> (a java.lang.UNIXProcess$ProcessPipeInputStream)\n    at java.io.FilterInputStream.read(FilterInputStream.java:107)\n    at com.google.common.io.ByteStreams.copy(ByteStreams.java:175)\n    at io.druid.indexing.overlord.ForkingTaskRunner$1.call(ForkingTaskRunner.java:432)\n    at io.druid.indexing.overlord.ForkingTaskRunner$1.call(ForkingTaskRunner.java:219)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"forking-task-runner-6\" #181 daemon prio=5 os_prio=0 tid=0x00007fca40022800 nid=0x3260 runnable [0x00007fc9c11d3000]\n   java.lang.Thread.State: RUNNABLE\n    at java.io.FileInputStream.readBytes(Native Method)\n    at java.io.FileInputStream.read(FileInputStream.java:255)\n    at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n    at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\n    at java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n    - locked <0x00000000fec10668> (a java.lang.UNIXProcess$ProcessPipeInputStream)\n    at java.io.FilterInputStream.read(FilterInputStream.java:107)\n    at com.google.common.io.ByteStreams.copy(ByteStreams.java:175)\n    at io.druid.indexing.overlord.ForkingTaskRunner$1.call(ForkingTaskRunner.java:432)\n    at io.druid.indexing.overlord.ForkingTaskRunner$1.call(ForkingTaskRunner.java:219)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"forking-task-runner-5\" #179 daemon prio=5 os_prio=0 tid=0x00007fca4001b800 nid=0x3239 runnable [0x00007fc9c12d4000]\n   java.lang.Thread.State: RUNNABLE\n    at java.io.FileInputStream.readBytes(Native Method)\n    at java.io.FileInputStream.read(FileInputStream.java:255)\n    at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n    at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\n    at java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n    - locked <0x00000000fdea6f80> (a java.lang.UNIXProcess$ProcessPipeInputStream)\n    at java.io.FilterInputStream.read(FilterInputStream.java:107)\n    at com.google.common.io.ByteStreams.copy(ByteStreams.java:175)\n    at io.druid.indexing.overlord.ForkingTaskRunner$1.call(ForkingTaskRunner.java:432)\n    at io.druid.indexing.overlord.ForkingTaskRunner$1.call(ForkingTaskRunner.java:219)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"forking-task-runner-4\" #177 daemon prio=5 os_prio=0 tid=0x00007fca40019000 nid=0x3227 runnable [0x00007fc9c13d5000]\n   java.lang.Thread.State: RUNNABLE\n    at java.io.FileInputStream.readBytes(Native Method)\n    at java.io.FileInputStream.read(FileInputStream.java:255)\n    at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n    at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\n    at java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n    - locked <0x00000000feebe2f0> (a java.lang.UNIXProcess$ProcessPipeInputStream)\n    at java.io.FilterInputStream.read(FilterInputStream.java:107)\n    at com.google.common.io.ByteStreams.copy(ByteStreams.java:175)\n    at io.druid.indexing.overlord.ForkingTaskRunner$1.call(ForkingTaskRunner.java:432)\n    at io.druid.indexing.overlord.ForkingTaskRunner$1.call(ForkingTaskRunner.java:219)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"forking-task-runner-3\" #175 daemon prio=5 os_prio=0 tid=0x00007fca40017800 nid=0x3203 runnable [0x00007fc9c14d7000]\n   java.lang.Thread.State: RUNNABLE\n    at java.io.FileInputStream.readBytes(Native Method)\n    at java.io.FileInputStream.read(FileInputStream.java:255)\n    at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n    at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\n    at java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n    - locked <0x00000000fec16040> (a java.lang.UNIXProcess$ProcessPipeInputStream)\n    at java.io.FilterInputStream.read(FilterInputStream.java:107)\n    at com.google.common.io.ByteStreams.copy(ByteStreams.java:175)\n    at io.druid.indexing.overlord.ForkingTaskRunner$1.call(ForkingTaskRunner.java:432)\n    at io.druid.indexing.overlord.ForkingTaskRunner$1.call(ForkingTaskRunner.java:219)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"forking-task-runner-2\" #173 daemon prio=5 os_prio=0 tid=0x00007fca40013000 nid=0x31f0 runnable [0x00007fc9c15d7000]\n   java.lang.Thread.State: RUNNABLE\n    at java.io.FileInputStream.readBytes(Native Method)\n    at java.io.FileInputStream.read(FileInputStream.java:255)\n    at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n    at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\n    at java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n    - locked <0x00000000feba2078> (a java.lang.UNIXProcess$ProcessPipeInputStream)\n    at java.io.FilterInputStream.read(FilterInputStream.java:107)\n    at com.google.common.io.ByteStreams.copy(ByteStreams.java:175)\n    at io.druid.indexing.overlord.ForkingTaskRunner$1.call(ForkingTaskRunner.java:432)\n    at io.druid.indexing.overlord.ForkingTaskRunner$1.call(ForkingTaskRunner.java:219)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"forking-task-runner-1\" #171 daemon prio=5 os_prio=0 tid=0x00007fca40011800 nid=0x2fd7 runnable [0x00007fc9c16d8000]\n   java.lang.Thread.State: RUNNABLE\n    at java.io.FileInputStream.readBytes(Native Method)\n    at java.io.FileInputStream.read(FileInputStream.java:255)\n    at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n    at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\n    at java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n    - locked <0x00000000fe9db928> (a java.lang.UNIXProcess$ProcessPipeInputStream)\n    at java.io.FilterInputStream.read(FilterInputStream.java:107)\n    at com.google.common.io.ByteStreams.copy(ByteStreams.java:175)\n    at io.druid.indexing.overlord.ForkingTaskRunner$1.call(ForkingTaskRunner.java:432)\n    at io.druid.indexing.overlord.ForkingTaskRunner$1.call(ForkingTaskRunner.java:219)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"forking-task-runner-0\" #169 daemon prio=5 os_prio=0 tid=0x00007fca40007800 nid=0x2fb0 runnable [0x00007fc9c17d9000]\n   java.lang.Thread.State: RUNNABLE\n    at java.io.FileInputStream.readBytes(Native Method)\n    at java.io.FileInputStream.read(FileInputStream.java:255)\n    at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n    at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\n    at java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n    - locked <0x00000000fec3de28> (a java.lang.UNIXProcess$ProcessPipeInputStream)\n    at java.io.FilterInputStream.read(FilterInputStream.java:107)\n    at com.google.common.io.ByteStreams.copy(ByteStreams.java:175)\n    at io.druid.indexing.overlord.ForkingTaskRunner$1.call(ForkingTaskRunner.java:432)\n    at io.druid.indexing.overlord.ForkingTaskRunner$1.call(ForkingTaskRunner.java:219)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"JettyScheduler\" #167 daemon prio=5 os_prio=0 tid=0x00007fcc662ca000 nid=0x2fa3 waiting on condition [0x00007fc9c1cdb000]\n   java.lang.Thread.State: TIMED_WAITING (parking)\n    at sun.misc.Unsafe.park(Native Method)\n    - parking to wait for  <0x00000000fdd6c408> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)\n    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)\n    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"qtp1041611526-166\" #166 daemon prio=5 os_prio=0 tid=0x00007fcc662bc800 nid=0x2fa2 waiting on condition [0x00007fc9c1ddc000]\n   java.lang.Thread.State: TIMED_WAITING (parking)\n    at sun.misc.Unsafe.park(Native Method)\n    - parking to wait for  <0x00000000fde17c58> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n    at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:516)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:47)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:575)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"qtp1041611526-165\" #165 daemon prio=5 os_prio=0 tid=0x00007fcc662ba800 nid=0x2fa1 waiting on condition [0x00007fc9c1edd000]\n   java.lang.Thread.State: TIMED_WAITING (parking)\n    at sun.misc.Unsafe.park(Native Method)\n    - parking to wait for  <0x00000000fde17c58> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n    at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:516)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:47)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:575)\n    at java.lang.Thread.run(Thread.java:745)\n\n........... SNIP ...........\n\n\"qtp1041611526-110\" #110 daemon prio=5 os_prio=0 tid=0x00007fcc66243800 nid=0x2f6a waiting on condition [0x00007fca99686000]\n   java.lang.Thread.State: TIMED_WAITING (parking)\n    at sun.misc.Unsafe.park(Native Method)\n    - parking to wait for  <0x00000000fde17c58> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n    at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:516)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:47)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:575)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"qtp1041611526-109\" #109 daemon prio=5 os_prio=0 tid=0x00007fcc66241800 nid=0x2f69 waiting on condition [0x00007fca99787000]\n   java.lang.Thread.State: TIMED_WAITING (parking)\n    at sun.misc.Unsafe.park(Native Method)\n    - parking to wait for  <0x00000000fde17c58> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n    at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:516)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:47)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:575)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"qtp1041611526-108-acceptor-3@45b53891-ServerConnector@4aa11206{HTTP/1.1}{0.0.0.0:8080}\" #108 daemon prio=4 os_prio=0 tid=0x00007fcc66240000 nid=0x2f68 waiting for monitor entry [0x00007fca99888000]\n   java.lang.Thread.State: BLOCKED (on object monitor)\n    at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:234)\n    - waiting to lock <0x00000000fea89390> (a java.lang.Object)\n    at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:377)\n    at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:500)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:620)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:540)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"qtp1041611526-107-acceptor-2@4cc9b5b1-ServerConnector@4aa11206{HTTP/1.1}{0.0.0.0:8080}\" #107 daemon prio=4 os_prio=0 tid=0x00007fcc6623e800 nid=0x2f67 waiting for monitor entry [0x00007fca99989000]\n   java.lang.Thread.State: BLOCKED (on object monitor)\n    at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:234)\n    - waiting to lock <0x00000000fea89390> (a java.lang.Object)\n    at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:377)\n    at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:500)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:620)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:540)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"qtp1041611526-106-acceptor-1@4b48e57b-ServerConnector@4aa11206{HTTP/1.1}{0.0.0.0:8080}\" #106 daemon prio=4 os_prio=0 tid=0x00007fcc6623c800 nid=0x2f66 runnable [0x00007fca99a8a000]\n   java.lang.Thread.State: RUNNABLE\n    at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)\n    at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:422)\n    at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:250)\n    - locked <0x00000000fea89390> (a java.lang.Object)\n    at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:377)\n    at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:500)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:620)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:540)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"qtp1041611526-105-acceptor-0@3db8592a-ServerConnector@4aa11206{HTTP/1.1}{0.0.0.0:8080}\" #105 daemon prio=4 os_prio=0 tid=0x00007fcc6623b000 nid=0x2f65 waiting for monitor entry [0x00007fca99b8b000]\n   java.lang.Thread.State: BLOCKED (on object monitor)\n    at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:234)\n    - waiting to lock <0x00000000fea89390> (a java.lang.Object)\n    at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:377)\n    at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:500)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:620)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:540)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"qtp1041611526-104-selector-ServerConnectorManager@5513435d/3\" #104 daemon prio=5 os_prio=0 tid=0x00007fcc66239000 nid=0x2f64 runnable [0x00007fca99c8c000]\n   java.lang.Thread.State: RUNNABLE\n    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\n    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\n    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)\n    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n    - locked <0x00000000fecdcd90> (a sun.nio.ch.Util$2)\n    - locked <0x00000000fecdcd78> (a java.util.Collections$UnmodifiableSet)\n    - locked <0x00000000fec855a8> (a sun.nio.ch.EPollSelectorImpl)\n    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)\n    at org.eclipse.jetty.io.SelectorManager$ManagedSelector.select(SelectorManager.java:596)\n    at org.eclipse.jetty.io.SelectorManager$ManagedSelector.run(SelectorManager.java:545)\n    at org.eclipse.jetty.util.thread.NonBlockingThread.run(NonBlockingThread.java:52)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:620)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:540)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"qtp1041611526-103-selector-ServerConnectorManager@5513435d/2\" #103 daemon prio=5 os_prio=0 tid=0x00007fcc66237800 nid=0x2f63 runnable [0x00007fca99d8d000]\n   java.lang.Thread.State: RUNNABLE\n    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\n    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\n    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)\n    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n    - locked <0x00000000fde2e648> (a sun.nio.ch.Util$2)\n    - locked <0x00000000fde2e630> (a java.util.Collections$UnmodifiableSet)\n    - locked <0x00000000fed04298> (a sun.nio.ch.EPollSelectorImpl)\n    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)\n    at org.eclipse.jetty.io.SelectorManager$ManagedSelector.select(SelectorManager.java:596)\n    at org.eclipse.jetty.io.SelectorManager$ManagedSelector.run(SelectorManager.java:545)\n    at org.eclipse.jetty.util.thread.NonBlockingThread.run(NonBlockingThread.java:52)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:620)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:540)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"qtp1041611526-102-selector-ServerConnectorManager@5513435d/1\" #102 daemon prio=5 os_prio=0 tid=0x00007fcc6628a000 nid=0x2f62 runnable [0x00007fca99e8e000]\n   java.lang.Thread.State: RUNNABLE\n    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\n    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\n    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)\n    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n    - locked <0x00000000fecfdb30> (a sun.nio.ch.Util$2)\n    - locked <0x00000000fecfdb18> (a java.util.Collections$UnmodifiableSet)\n    - locked <0x00000000fecfebb0> (a sun.nio.ch.EPollSelectorImpl)\n    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)\n    at org.eclipse.jetty.io.SelectorManager$ManagedSelector.select(SelectorManager.java:596)\n    at org.eclipse.jetty.io.SelectorManager$ManagedSelector.run(SelectorManager.java:545)\n    at org.eclipse.jetty.util.thread.NonBlockingThread.run(NonBlockingThread.java:52)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:620)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:540)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"qtp1041611526-101-selector-ServerConnectorManager@5513435d/0\" #101 daemon prio=5 os_prio=0 tid=0x00007fcc66286000 nid=0x2f61 runnable [0x00007fca99f8f000]\n   java.lang.Thread.State: RUNNABLE\n    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\n    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\n    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)\n    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n    - locked <0x00000000fecfa770> (a sun.nio.ch.Util$2)\n    - locked <0x00000000fecfa758> (a java.util.Collections$UnmodifiableSet)\n    - locked <0x00000000fecf0d10> (a sun.nio.ch.EPollSelectorImpl)\n    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)\n    at org.eclipse.jetty.io.SelectorManager$ManagedSelector.select(SelectorManager.java:596)\n    at org.eclipse.jetty.io.SelectorManager$ManagedSelector.run(SelectorManager.java:545)\n    at org.eclipse.jetty.util.thread.NonBlockingThread.run(NonBlockingThread.java:52)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:620)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:540)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"WorkerTaskMonitor\" #99 daemon prio=5 os_prio=0 tid=0x00007fcc6627a800 nid=0x2f5f waiting on condition [0x00007fca9a090000]\n   java.lang.Thread.State: WAITING (parking)\n    at sun.misc.Unsafe.park(Native Method)\n    - parking to wait for  <0x00000000fd5c4d18> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\n    at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)\n    at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)\n    at io.druid.indexing.worker.WorkerTaskMonitor.mainLoop(WorkerTaskMonitor.java:142)\n    at io.druid.indexing.worker.WorkerTaskMonitor.access$000(WorkerTaskMonitor.java:58)\n    at io.druid.indexing.worker.WorkerTaskMonitor$1.run(WorkerTaskMonitor.java:119)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"TaskMonitorCache-0\" #98 daemon prio=5 os_prio=0 tid=0x00007fcc66279000 nid=0x2f5e waiting on condition [0x00007fca9a191000]\n   java.lang.Thread.State: WAITING (parking)\n    at sun.misc.Unsafe.park(Native Method)\n    - parking to wait for  <0x00000000fdf93148> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\n    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\n    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"Curator-Framework-0\" #97 daemon prio=5 os_prio=0 tid=0x00007fcc661fe000 nid=0x2f5d waiting on condition [0x00007fca9a492000]\n   java.lang.Thread.State: WAITING (parking)\n    at sun.misc.Unsafe.park(Native Method)\n    - parking to wait for  <0x00000000fded8788> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\n    at java.util.concurrent.DelayQueue.take(DelayQueue.java:211)\n    at java.util.concurrent.DelayQueue.take(DelayQueue.java:70)\n    at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:804)\n    at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64)\n    at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"main-EventThread\" #96 daemon prio=5 os_prio=0 tid=0x00007fcc661ed000 nid=0x2f5c waiting on condition [0x00007fca9a593000]\n   java.lang.Thread.State: WAITING (parking)\n    at sun.misc.Unsafe.park(Native Method)\n    - parking to wait for  <0x00000000fdc55ec0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\n    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\n    at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:501)\n\n\"main-SendThread(ip-172-19-2-14.ec2.internal:2181)\" #95 daemon prio=5 os_prio=0 tid=0x00007fcc661ec000 nid=0x2f5b runnable [0x00007fca9a694000]\n   java.lang.Thread.State: RUNNABLE\n    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\n    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\n    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)\n    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n    - locked <0x00000000fd63b9d8> (a sun.nio.ch.Util$2)\n    - locked <0x00000000fd63b9c0> (a java.util.Collections$UnmodifiableSet)\n    - locked <0x00000000fe0b0440> (a sun.nio.ch.EPollSelectorImpl)\n    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n    at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:349)\n    at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)\n\n\"Curator-ConnectionStateManager-0\" #94 daemon prio=5 os_prio=0 tid=0x00007fcc661e6000 nid=0x2f5a waiting on condition [0x00007fca9abb6000]\n   java.lang.Thread.State: WAITING (parking)\n    at sun.misc.Unsafe.park(Native Method)\n    - parking to wait for  <0x00000000fd70df20> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\n    at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)\n    at org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:245)\n    at org.apache.curator.framework.state.ConnectionStateManager.access$000(ConnectionStateManager.java:43)\n    at org.apache.curator.framework.state.ConnectionStateManager$1.call(ConnectionStateManager.java:111)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"MonitorScheduler-0\" #93 daemon prio=5 os_prio=0 tid=0x00007fcc661e4800 nid=0x2f59 waiting on condition [0x00007fca9acb7000]\n   java.lang.Thread.State: TIMED_WAITING (parking)\n    at sun.misc.Unsafe.park(Native Method)\n    - parking to wait for  <0x00000000fdafedf0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)\n    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)\n    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"HttpPostEmitter-1-0\" #92 daemon prio=5 os_prio=0 tid=0x00007fcc661e2000 nid=0x2f58 waiting on condition [0x00007fca9adb8000]\n   java.lang.Thread.State: TIMED_WAITING (parking)\n    at sun.misc.Unsafe.park(Native Method)\n    - parking to wait for  <0x00000000fd7529c8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)\n    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)\n    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"HttpClient-Timer-0\" #26 daemon prio=5 os_prio=0 tid=0x00007fcc661ce800 nid=0x2f57 waiting on condition [0x00007fca9aeb9000]\n   java.lang.Thread.State: TIMED_WAITING (sleeping)\n    at java.lang.Thread.sleep(Native Method)\n    at org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:445)\n    at org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:364)\n    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"HttpClient-Netty-Worker-63\" #91 daemon prio=5 os_prio=0 tid=0x00007fcc660fc800 nid=0x2f56 runnable [0x00007fca9b2f1000]\n   java.lang.Thread.State: RUNNABLE\n    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\n    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\n    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)\n    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n    - locked <0x00000000fe097a80> (a sun.nio.ch.Util$2)\n    - locked <0x00000000fe097a98> (a java.util.Collections$UnmodifiableSet)\n    - locked <0x00000000fd5b50e0> (a sun.nio.ch.EPollSelectorImpl)\n    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n    at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)\n    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)\n    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)\n    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\n    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"HttpClient-Netty-Worker-62\" #90 daemon prio=5 os_prio=0 tid=0x00007fcc660d1800 nid=0x2f55 runnable [0x00007fca9b3f2000]\n   java.lang.Thread.State: RUNNABLE\n    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\n    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\n    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)\n    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n    - locked <0x00000000fd6effd8> (a sun.nio.ch.Util$2)\n    - locked <0x00000000fd6eff00> (a java.util.Collections$UnmodifiableSet)\n    - locked <0x00000000fdd75388> (a sun.nio.ch.EPollSelectorImpl)\n    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n    at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)\n    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)\n    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)\n    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\n    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\n.......... SNIP ......\n\n\"HttpClient-Netty-Worker-0\" #28 daemon prio=5 os_prio=0 tid=0x00007fcc6565f000 nid=0x2f17 runnable [0x00007fcb6f4a0000]\n   java.lang.Thread.State: RUNNABLE\n    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\n    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\n    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)\n    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n    - locked <0x00000000fe04eb08> (a sun.nio.ch.Util$2)\n    - locked <0x00000000fe04eb20> (a java.util.Collections$UnmodifiableSet)\n    - locked <0x00000000fdb2c7f8> (a sun.nio.ch.EPollSelectorImpl)\n    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n    at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)\n    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)\n    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)\n    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\n    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"HttpClient-Netty-Boss-0\" #27 daemon prio=5 os_prio=0 tid=0x00007fcc65637800 nid=0x2f16 runnable [0x00007fcb6f5a1000]\n   java.lang.Thread.State: RUNNABLE\n    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\n    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\n    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)\n    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n    - locked <0x00000000fde9f750> (a sun.nio.ch.Util$2)\n    - locked <0x00000000fde9f768> (a java.util.Collections$UnmodifiableSet)\n    - locked <0x00000000fdb2cba0> (a sun.nio.ch.EPollSelectorImpl)\n    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n    at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)\n    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)\n    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)\n    at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)\n    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"RMI TCP Accept-0\" #25 daemon prio=5 os_prio=0 tid=0x00007fcc644ba000 nid=0x2f12 runnable [0x00007fcb7cf25000]\n   java.lang.Thread.State: RUNNABLE\n    at java.net.PlainSocketImpl.socketAccept(Native Method)\n    at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n    at java.net.ServerSocket.implAccept(ServerSocket.java:545)\n    at java.net.ServerSocket.accept(ServerSocket.java:513)\n    at sun.management.jmxremote.LocalRMIServerSocketFactory$1.accept(LocalRMIServerSocketFactory.java:52)\n    at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.executeAcceptLoop(TCPTransport.java:400)\n    at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.run(TCPTransport.java:372)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"RMI TCP Accept-17071\" #24 daemon prio=5 os_prio=0 tid=0x00007fcc644b5800 nid=0x2f11 runnable [0x00007fcb7d026000]\n   java.lang.Thread.State: RUNNABLE\n    at java.net.PlainSocketImpl.socketAccept(Native Method)\n    at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n    at java.net.ServerSocket.implAccept(ServerSocket.java:545)\n    at java.net.ServerSocket.accept(ServerSocket.java:513)\n    at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.executeAcceptLoop(TCPTransport.java:400)\n    at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.run(TCPTransport.java:372)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"RMI TCP Accept-0\" #23 daemon prio=5 os_prio=0 tid=0x00007fcc644a8000 nid=0x2f10 runnable [0x00007fcbecdc3000]\n   java.lang.Thread.State: RUNNABLE\n    at java.net.PlainSocketImpl.socketAccept(Native Method)\n    at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n    at java.net.ServerSocket.implAccept(ServerSocket.java:545)\n    at java.net.ServerSocket.accept(ServerSocket.java:513)\n    at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.executeAcceptLoop(TCPTransport.java:400)\n    at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.run(TCPTransport.java:372)\n    at java.lang.Thread.run(Thread.java:745)\n\n\"Service Thread\" #21 daemon prio=9 os_prio=0 tid=0x00007fcc6425a800 nid=0x2f0e runnable [0x0000000000000000]\n   java.lang.Thread.State: RUNNABLE\n\n\"C1 CompilerThread14\" #20 daemon prio=9 os_prio=0 tid=0x00007fcc6424f000 nid=0x2f0d waiting on condition [0x0000000000000000]\n   java.lang.Thread.State: RUNNABLE\n\n\"C1 CompilerThread13\" #19 daemon prio=9 os_prio=0 tid=0x00007fcc6424d000 nid=0x2f0c waiting on condition [0x0000000000000000]\n   java.lang.Thread.State: RUNNABLE\n\n\"C1 CompilerThread12\" #18 daemon prio=9 os_prio=0 tid=0x00007fcc6424b000 nid=0x2f0b waiting on condition [0x0000000000000000]\n   java.lang.Thread.State: RUNNABLE\n\n\"C1 CompilerThread11\" #17 daemon prio=9 os_prio=0 tid=0x00007fcc64248800 nid=0x2f0a waiting on condition [0x0000000000000000]\n   java.lang.Thread.State: RUNNABLE\n\n\"C1 CompilerThread10\" #16 daemon prio=9 os_prio=0 tid=0x00007fcc64246800 nid=0x2f09 waiting on condition [0x0000000000000000]\n   java.lang.Thread.State: RUNNABLE\n\n\"C2 CompilerThread9\" #15 daemon prio=9 os_prio=0 tid=0x00007fcc64244800 nid=0x2f08 waiting on condition [0x0000000000000000]\n   java.lang.Thread.State: RUNNABLE\n\n\"C2 CompilerThread8\" #14 daemon prio=9 os_prio=0 tid=0x00007fcc64242000 nid=0x2f07 waiting on condition [0x0000000000000000]\n   java.lang.Thread.State: RUNNABLE\n\n\"C2 CompilerThread7\" #13 daemon prio=9 os_prio=0 tid=0x00007fcc6423f800 nid=0x2f06 waiting on condition [0x0000000000000000]\n   java.lang.Thread.State: RUNNABLE\n\n\"C2 CompilerThread6\" #12 daemon prio=9 os_prio=0 tid=0x00007fcc6423d800 nid=0x2f05 waiting on condition [0x0000000000000000]\n   java.lang.Thread.State: RUNNABLE\n\n\"C2 CompilerThread5\" #11 daemon prio=9 os_prio=0 tid=0x00007fcc6423b800 nid=0x2f04 waiting on condition [0x0000000000000000]\n   java.lang.Thread.State: RUNNABLE\n\n\"C2 CompilerThread4\" #10 daemon prio=9 os_prio=0 tid=0x00007fcc64239000 nid=0x2f03 waiting on condition [0x0000000000000000]\n   java.lang.Thread.State: RUNNABLE\n\n\"C2 CompilerThread3\" #9 daemon prio=9 os_prio=0 tid=0x00007fcc6422f000 nid=0x2f02 waiting on condition [0x0000000000000000]\n   java.lang.Thread.State: RUNNABLE\n\n\"C2 CompilerThread2\" #8 daemon prio=9 os_prio=0 tid=0x00007fcc6422d000 nid=0x2f01 waiting on condition [0x0000000000000000]\n   java.lang.Thread.State: RUNNABLE\n\n\"C2 CompilerThread1\" #7 daemon prio=9 os_prio=0 tid=0x00007fcc6422a800 nid=0x2f00 waiting on condition [0x0000000000000000]\n   java.lang.Thread.State: RUNNABLE\n\n\"C2 CompilerThread0\" #6 daemon prio=9 os_prio=0 tid=0x00007fcc64228800 nid=0x2eff waiting on condition [0x0000000000000000]\n   java.lang.Thread.State: RUNNABLE\n\n\"Signal Dispatcher\" #5 daemon prio=9 os_prio=0 tid=0x00007fcc64226800 nid=0x2efe waiting on condition [0x0000000000000000]\n   java.lang.Thread.State: RUNNABLE\n\n\"Surrogate Locker Thread (Concurrent GC)\" #4 daemon prio=9 os_prio=0 tid=0x00007fcc64225000 nid=0x2efd waiting on condition [0x0000000000000000]\n   java.lang.Thread.State: RUNNABLE\n\n\"Finalizer\" #3 daemon prio=8 os_prio=0 tid=0x00007fcc641f2800 nid=0x2efc in Object.wait() [0x00007fcbee312000]\n   java.lang.Thread.State: WAITING (on object monitor)\n    at java.lang.Object.wait(Native Method)\n    at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)\n    - locked <0x00000000fd5ddd58> (a java.lang.ref.ReferenceQueue$Lock)\n    at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)\n    at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)\n\n\"Reference Handler\" #2 daemon prio=10 os_prio=0 tid=0x00007fcc641ee000 nid=0x2efb in Object.wait() [0x00007fcbee413000]\n   java.lang.Thread.State: WAITING (on object monitor)\n    at java.lang.Object.wait(Native Method)\n    at java.lang.Object.wait(Object.java:502)\n    at java.lang.ref.Reference.tryHandlePending(Reference.java:191)\n    - locked <0x00000000fd5b4a38> (a java.lang.ref.Reference$Lock)\n    at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)\n\n\"main\" #1 prio=5 os_prio=0 tid=0x00007fcc6400b000 nid=0x2edb in Object.wait() [0x00007fcc6bf7c000]\n   java.lang.Thread.State: WAITING (on object monitor)\n    at java.lang.Object.wait(Native Method)\n    - waiting on <0x00000000fd782f58> (a java.lang.Thread)\n    at java.lang.Thread.join(Thread.java:1245)\n    - locked <0x00000000fd782f58> (a java.lang.Thread)\n    at java.lang.Thread.join(Thread.java:1319)\n    at com.metamx.common.lifecycle.Lifecycle.join(Lifecycle.java:317)\n    at io.druid.cli.ServerRunnable.run(ServerRunnable.java:43)\n    at io.druid.cli.Main.main(Main.java:105)\n\n\"VM Thread\" os_prio=0 tid=0x00007fcc641e6000 nid=0x2efa runnable\n\n\"Gang worker#0 (Parallel GC Threads)\" os_prio=0 tid=0x00007fcc6401c000 nid=0x2edc runnable\n\n.... SNIP ....\n\n\"Gang worker#22 (Parallel GC Threads)\" os_prio=0 tid=0x00007fcc64042800 nid=0x2ef2 runnable\n\n\"Concurrent Mark-Sweep GC Thread\" os_prio=0 tid=0x00007fcc6415b000 nid=0x2ef9 runnable\n\n\"Gang worker#0 (Parallel CMS Threads)\" os_prio=0 tid=0x00007fcc6414f800 nid=0x2ef3 runnable\n\n... SNIP ...\n\n\"Gang worker#5 (Parallel CMS Threads)\" os_prio=0 tid=0x00007fcc64158000 nid=0x2ef8 runnable\n\n\"VM Periodic Task Thread\" os_prio=0 tid=0x00007fcc644bc000 nid=0x2f13 waiting on condition\n\n```\n\n((more in comments))\n"}, {"user": "jon-wei", "commits": {}, "labels": ["Bug"], "created": "2016-06-17 21:19:01", "title": "FilteredAggregator with predicate-based ValueMatcher fails during ingestion", "url": "https://github.com/apache/druid/issues/3164", "closed": "2016-08-12 16:34:07", "ttf": 55.000277777777775, "commitsDetails": [], "body": "During ingestion, If I use a FilteredAggregator with a filter that is implemented with a predicate-based ValueMatcher (e.g., BoundFilter), the following exception is thrown:\n\n```\njava.lang.UnsupportedOperationException: value cardinality is unknown in incremental index\n    at io.druid.segment.incremental.IncrementalIndex$4$6.getValueCardinality(IncrementalIndex.java:321)\n    at io.druid.query.aggregation.FilteredAggregatorFactory$FilteredAggregatorValueMatcherFactory.makeValueMatcher(FilteredAggregatorFactory.java:273)\n    at io.druid.segment.filter.BoundFilter.makeMatcher(BoundFilter.java:143)\n    at io.druid.segment.filter.OrFilter.makeMatcher(OrFilter.java:69)\n    at io.druid.query.aggregation.FilteredAggregatorFactory.factorize(FilteredAggregatorFactory.java:64)\n    at io.druid.segment.incremental.OnheapIncrementalIndex.factorizeAggs(OnheapIncrementalIndex.java:212)\n    at io.druid.segment.incremental.OnheapIncrementalIndex.addToFacts(OnheapIncrementalIndex.java:176)\n    at io.druid.segment.incremental.IncrementalIndex.add(IncrementalIndex.java:568)\n    at io.druid.benchmark.FilteredAggregatorBenchmark.ingest(FilteredAggregatorBenchmark.java:277)\n...\n```\n\nThe ColumnSelectorFactory implementation in IncrementalIndex does not support getValueCardinality():\n\n```\n          @Override\n          public int getValueCardinality()\n          {\n            throw new UnsupportedOperationException(\"value cardinality is unknown in incremental index\");\n          }\n```\n"}, {"user": "jon-wei", "commits": {"15f833a86172bce04e8f76ecf00f99bc04fa576f": {"commitGHEventType": "referenced", "commitUser": "jon-wei"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "Only one class loader is created for multiple hadoopDependencyCoordinates", "numCommits": 0, "created": "2016-06-17 21:11:15", "closed": "2016-06-24 00:13:19", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/3163", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 6.000277777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "server/src/test/java/io/druid/initialization/InitializationTest.java", "deletions": 1, "insertions": 25, "lines": 26}, {"filePath": "server/src/main/java/io/druid/initialization/Initialization.java", "deletions": 4, "insertions": 6, "lines": 10}], "commitSpoonAstDiffStats": [{"spoonFilePath": "InitializationTest.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.initialization.InitializationTest.test04DuplicateClassLoaderExtensions()", "MOV": 0, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.initialization.InitializationTest.testExtensionsWithSameDirName()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "Initialization.java", "spoonMethods": [{"INS": 1, "UPD": 1, "DEL": 1, "spoonMethodName": "io.druid.initialization.Initialization", "MOV": 0, "TOT": 3}, {"INS": 2, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.initialization.Initialization.getClassLoaderForExtension(java.io.File)", "MOV": 4, "TOT": 8}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-06-23 17:13:19", "commitMessage": "Make extension classloader caching keyed on directory (#3165)\n\n* Make extension classloaders keyed by extension directory\r\n* Fixes #3163\r\n\r\n* Add in same-directory-name unit test\r\n", "commitUser": "jon-wei", "commitDateTime": "2016-06-23 17:13:19", "commitParents": ["66d8ad36d7aa57e240b7f595796003ba1fe5cbf2"], "commitGHEventType": "referenced", "nameRev": "15f833a86172bce04e8f76ecf00f99bc04fa576f tags/druid-0.9.2-rc1~159", "commitHash": "15f833a86172bce04e8f76ecf00f99bc04fa576f"}], "body": "If I post an indexing task with hadoop coordinates like:\n\n```\n  \"hadoopDependencyCoordinates\": [\"org.apache.hadoop:hadoop-client:2.7.1\", \"org.apache.hadoop:hadoop-aws:2.7.1\"]\n```\n\nthe function Initialization.getClassLoaderForExtension does not create a class loader for the second dependency in the list.\n\n```\n  public static URLClassLoader getClassLoaderForExtension(File extension) throws MalformedURLException\n  {\n    URLClassLoader loader = loadersMap.get(extension.getName());\n...\n```\n\nThis occurs because the two dependencies have the same \"extension name\", the hadoop version:\n\nFILE: /Users/jw/jw_repo/druid/hadoop-dependencies-tmp/hadoop-client/2.7.1\nEXTNAME: 2.7.1\n\nFILE: /Users/jw/jw_repo/druid/hadoop-dependencies-tmp/hadoop-aws/2.7.1\nEXTNAME: 2.7.1\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2016-06-08 16:58:17", "title": "Integration tests failing in 0.9.1", "url": "https://github.com/apache/druid/issues/3115", "closed": "2016-06-10 13:35:27", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "Integration tests are failing when I run them with:\n\n```\nTests run: 6, Failures: 3, Errors: 0, Skipped: 0, Time elapsed: 1,742.423 sec <<< FAILURE! - in TestSuite\ntestUnionQuery(io.druid.tests.indexer.ITUnionQueryTest)  Time elapsed: 402.581 sec  <<< FAILURE!\ncom.metamx.common.ISE: one or more queries failed\n    at io.druid.tests.indexer.ITUnionQueryTest.testUnionQuery(ITUnionQueryTest.java:119)\n\ntestTwitterQueriesFromFile(io.druid.tests.query.ITTwitterQueryTest)  Time elapsed: 119.515 sec  <<< FAILURE!\ncom.metamx.common.ISE: one or more queries failed\n    at io.druid.tests.query.ITTwitterQueryTest.testTwitterQueriesFromFile(ITTwitterQueryTest.java:62)\n\ntestWikipediaQueriesFromFile(io.druid.tests.query.ITWikipediaQueryTest)  Time elapsed: 38.084 sec  <<< FAILURE!\ncom.metamx.common.ISE: one or more queries failed\n    at io.druid.tests.query.ITWikipediaQueryTest.testWikipediaQueriesFromFile(ITWikipediaQueryTest.java:62)\n\n\nResults :\n\nFailed tests:\n  ITUnionQueryTest.testUnionQuery:119 \u00bb ISE one or more queries failed\n  ITTwitterQueryTest.testTwitterQueriesFromFile:62 \u00bb ISE one or more queries fai...\n  ITWikipediaQueryTest.testWikipediaQueriesFromFile:62 \u00bb ISE one or more queries...\n\nTests run: 6, Failures: 3, Errors: 0, Skipped: 0\n```\n\nI have a bad history of not being able to have integration tests run, so if someone could confirm/refute if the integration tests pass I would appreciate it\n"}, {"user": "pjain1", "commits": {}, "labels": ["Bug"], "created": "2016-06-02 15:30:12", "title": "killDataSourceWhitelist cannot be set using Coordinator UI", "url": "https://github.com/apache/druid/issues/3055", "closed": "2016-06-07 22:39:41", "ttf": 5.000277777777778, "commitsDetails": [], "body": "![screen shot 2016-06-02 at 9 47 18 am](https://cloud.githubusercontent.com/assets/11787983/15749171/16bc48d2-28a7-11e6-834f-213eb258007c.png)\n![screen shot 2016-06-02 at 10 29 29 am](https://cloud.githubusercontent.com/assets/11787983/15750652/edbf845c-28ac-11e6-9b8a-d61f58a34a5e.png)\n\n```\nerror:\"Can not deserialize instance of java.util.HashSet out of VALUE_STRING token\u21b5 at [Source: HttpInputOverHTTP@7522095a; line: 1, column: 220]\"\n```\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2016-06-01 18:27:15", "title": "[0.9.0] Too fast leadership changes in overlord causes multiple leaders and service failure", "url": "https://github.com/apache/druid/issues/3046", "closed": "2016-06-02 19:10:40", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "It is possible for two overlords in HA mode to get into some sort of leadership fight which causes high cpu load on both instances and general instability of the indexing service.\n\nLogs from overlord 1:\n\n```\n2016-06-01T17:55:33,794 INFO [Curator-LeaderSelector-0] io.druid.indexing.overlord.autoscaling.SimpleResourceManagementStrategy - Started Resource Management Scheduler\n2016-06-01T17:55:33,794 INFO [Curator-LeaderSelector-0] io.druid.indexing.overlord.TaskMaster - Bowing out!\n2016-06-01T17:55:33,798 ERROR [Curator-LeaderSelector-0] io.druid.indexing.overlord.TaskMaster - Failed to lead: {class=io.druid.indexing.overlord.TaskMaster, exceptionType=class java.lang.reflect.I\nnvocationTargetException, exceptionMessage=null}\njava.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_45]\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_45]\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_45]\n        at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_45]\n        at com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler.start(Lifecycle.java:350) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at com.metamx.common.lifecycle.Lifecycle.start(Lifecycle.java:259) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at io.druid.indexing.overlord.TaskMaster$1.takeLeadership(TaskMaster.java:134) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$WrappedListener.takeLeadership(LeaderSelector.java:536) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:399) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:443) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:64) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:245) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:239) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_45]\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_45]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_45]\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_45]\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_45]\n        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_45]\nCaused by: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@4614612 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@2f19712b[Shutting down, pool size = 1, active threads = 0, queued tasks = 2, completed tasks = 966]\n        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047) ~[?:1.8.0_45]\n        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047) ~[?:1.8.0_45]\n        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823) ~[?:1.8.0_45]\n        at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:326) ~[?:1.8.0_45]\n        at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533) ~[?:1.8.0_45]\n        at com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:159) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:135) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:121) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at io.druid.indexing.overlord.autoscaling.SimpleResourceManagementStrategy.startManagement(SimpleResourceManagementStrategy.java:276) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at io.druid.indexing.overlord.autoscaling.SimpleResourceManagementStrategy.startManagement(SimpleResourceManagementStrategy.java:51) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at io.druid.indexing.overlord.RemoteTaskRunner.start(RemoteTaskRunner.java:293) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        ... 19 more\n2016-06-01T17:55:33,809 ERROR [Curator-LeaderSelector-0] org.apache.curator.framework.recipes.leader.LeaderSelector - The leader threw an exception\njava.lang.RuntimeException: java.lang.reflect.InvocationTargetException\n        at com.google.common.base.Throwables.propagate(Throwables.java:160) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at io.druid.indexing.overlord.TaskMaster$1.takeLeadership(TaskMaster.java:150) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$WrappedListener.takeLeadership(LeaderSelector.java:536) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:399) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:443) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:64) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:245) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:239) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_45]\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_45]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_45]\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_45]\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_45]\n        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_45]\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_45]\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_45]\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_45]\n        at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_45]\n        at com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler.start(Lifecycle.java:350) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at com.metamx.common.lifecycle.Lifecycle.start(Lifecycle.java:259) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at io.druid.indexing.overlord.TaskMaster$1.takeLeadership(TaskMaster.java:134) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        ... 12 more\nCaused by: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@4614612 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@2f19712b[Shutting down, pool size = 1, active threads = 0, queued tasks = 2, completed tasks = 966]\n        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047) ~[?:1.8.0_45]\n        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823) ~[?:1.8.0_45]\n        at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:326) ~[?:1.8.0_45]\n        at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533) ~[?:1.8.0_45]\n        at com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:159) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:135) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:121) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at io.druid.indexing.overlord.autoscaling.SimpleResourceManagementStrategy.startManagement(SimpleResourceManagementStrategy.java:276) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at io.druid.indexing.overlord.autoscaling.SimpleResourceManagementStrategy.startManagement(SimpleResourceManagementStrategy.java:51) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at io.druid.indexing.overlord.RemoteTaskRunner.start(RemoteTaskRunner.java:293) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_45]\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_45]\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_45]\n        at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_45]\n        at com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler.start(Lifecycle.java:350) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at com.metamx.common.lifecycle.Lifecycle.start(Lifecycle.java:259) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at io.druid.indexing.overlord.TaskMaster$1.takeLeadership(TaskMaster.java:134) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        ... 12 more\n2016-06-01T17:55:44,008 INFO [Curator-LeaderSelector-0] io.druid.indexing.overlord.TaskMaster - By the power of Grayskull, I have the power!\n```\n\nLogs from overlord 2:\n\n```\n2016-06-01T17:55:25,231 INFO [Curator-LeaderSelector-0] io.druid.indexing.overlord.autoscaling.SimpleResourceManagementStrategy - Started Resource Management Scheduler\n2016-06-01T17:55:25,231 INFO [Curator-LeaderSelector-0] io.druid.indexing.overlord.TaskMaster - Bowing out!\n2016-06-01T17:55:25,231 ERROR [Curator-LeaderSelector-0] io.druid.indexing.overlord.TaskMaster - Failed to lead: {class=io.druid.indexing.overlord.TaskMaster, exceptionType=class java.lang.reflect.InvocationTargetException, exceptionMessage=null}\njava.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_45]\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_45]\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_45]\n        at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_45]\n        at com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler.start(Lifecycle.java:350) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at com.metamx.common.lifecycle.Lifecycle.start(Lifecycle.java:259) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at io.druid.indexing.overlord.TaskMaster$1.takeLeadership(TaskMaster.java:134) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$WrappedListener.takeLeadership(LeaderSelector.java:536) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:399) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:443) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:64) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:245) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:239) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_45]\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_45]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_45]\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_45]\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_45]\n        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_45]\nCaused by: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@1ade306d rejected from java.util.concurrent.ScheduledThreadPoolExecutor@20d11e35[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 17550]\n        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047) ~[?:1.8.0_45]\n        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823) ~[?:1.8.0_45]\n        at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:326) ~[?:1.8.0_45]\n        at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533) ~[?:1.8.0_45]\n        at com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:159) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:135) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:121) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at io.druid.indexing.overlord.autoscaling.SimpleResourceManagementStrategy.startManagement(SimpleResourceManagementStrategy.java:276) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at io.druid.indexing.overlord.autoscaling.SimpleResourceManagementStrategy.startManagement(SimpleResourceManagementStrategy.java:51) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at io.druid.indexing.overlord.RemoteTaskRunner.start(RemoteTaskRunner.java:293) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        ... 19 more\n2016-06-01T17:55:25,232 ERROR [Curator-LeaderSelector-0] org.apache.curator.framework.recipes.leader.LeaderSelector - The leader threw an exception\njava.lang.RuntimeException: java.lang.reflect.InvocationTargetException\n        at com.google.common.base.Throwables.propagate(Throwables.java:160) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at io.druid.indexing.overlord.TaskMaster$1.takeLeadership(TaskMaster.java:150) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$WrappedListener.takeLeadership(LeaderSelector.java:536) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:399) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:443) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:64) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:245) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:239) [druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_45]\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_45]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_45]\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_45]\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_45]\n        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_45]\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_45]\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_45]\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_45]\n        at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_45]\n        at com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler.start(Lifecycle.java:350) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at com.metamx.common.lifecycle.Lifecycle.start(Lifecycle.java:259) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at io.druid.indexing.overlord.TaskMaster$1.takeLeadership(TaskMaster.java:134) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        ... 12 more\nCaused by: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@1ade306d rejected from java.util.concurrent.ScheduledThreadPoolExecutor@20d11e35[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 17550]\n        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047) ~[?:1.8.0_45]\n        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823) ~[?:1.8.0_45]\n        at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:326) ~[?:1.8.0_45]\n        at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533) ~[?:1.8.0_45]\n        at com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:159) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:135) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:121) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at io.druid.indexing.overlord.autoscaling.SimpleResourceManagementStrategy.startManagement(SimpleResourceManagementStrategy.java:276) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at io.druid.indexing.overlord.autoscaling.SimpleResourceManagementStrategy.startManagement(SimpleResourceManagementStrategy.java:51) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at io.druid.indexing.overlord.RemoteTaskRunner.start(RemoteTaskRunner.java:293) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_45]\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_45]\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_45]\n        at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_45]\n        at com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler.start(Lifecycle.java:350) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at com.metamx.common.lifecycle.Lifecycle.start(Lifecycle.java:259) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        at io.druid.indexing.overlord.TaskMaster$1.takeLeadership(TaskMaster.java:134) ~[druid-selfcontained-0.9.0-rc3-mmx3.jar:0.9.0-rc3-mmx3]\n        ... 12 more\n2016-06-01T17:55:33,810 INFO [Curator-LeaderSelector-0] io.druid.indexing.overlord.TaskMaster - By the power of Grayskull, I have the power!\n```\n"}, {"user": "pdeva", "commits": {}, "labels": ["Bug"], "created": "2016-05-28 17:32:42", "title": "old coordinator console in 0.9.0 doesnt allow updating configuration", "url": "https://github.com/apache/druid/issues/3037", "closed": "2019-09-24 22:22:19", "ttf": 1214.0002777777777, "commitsDetails": [], "body": "Bug in druid 0.9.0.\n\nI was trying to update the 'maxSegmentsToMove' and 'replicationThrottleLimit' from the coordinator console. It errors out.\nException in coordinator logs:\n\n```\n2016-05-28 17:30:49,715 ERROR c.s.j.s.c.ContainerResponse [qtp1972772630-44] A message body writer for Java class com.google.common.collect.SingletonImmutableBiMap, and Java type class com.google.common.collect.SingletonImmutableBiMap, and MIME media type application/octet-stream was not found.\nThe registered message body writers compatible with the MIME media type are:\n*/* ->\n  com.sun.jersey.core.impl.provider.entity.FormProvider\n  com.sun.jersey.server.impl.template.ViewableMessageBodyWriter\n  com.sun.jersey.core.impl.provider.entity.StringProvider\n  com.sun.jersey.core.impl.provider.entity.ByteArrayProvider\n  com.sun.jersey.core.impl.provider.entity.FileProvider\n  com.sun.jersey.core.impl.provider.entity.InputStreamProvider\n  com.sun.jersey.core.impl.provider.entity.DataSourceProvider\n  com.sun.jersey.core.impl.provider.entity.XMLJAXBElementProvider$General\n  com.sun.jersey.core.impl.provider.entity.ReaderProvider\n  com.sun.jersey.core.impl.provider.entity.DocumentProvider\n  com.sun.jersey.core.impl.provider.entity.StreamingOutputProvider\n  com.sun.jersey.core.impl.provider.entity.SourceProvider$SourceWriter\n  com.fasterxml.jackson.jaxrs.json.JacksonJsonProvider\n  com.sun.jersey.core.impl.provider.entity.XMLRootElementProvider$General\n  com.sun.jersey.core.impl.provider.entity.XMLListElementProvider$General\napplication/octet-stream ->\n  com.sun.jersey.core.impl.provider.entity.ByteArrayProvider\n  com.sun.jersey.core.impl.provider.entity.FileProvider\n  com.sun.jersey.core.impl.provider.entity.InputStreamProvider\n  com.sun.jersey.core.impl.provider.entity.DataSourceProvider\n  com.sun.jersey.core.impl.provider.entity.StreamingOutputProvider\n\n2016-05-28 17:30:49,716 ERROR c.s.j.s.c.ContainerResponse [qtp1972772630-44] Mapped exception to response: 500 (Internal Server Error)\n2016-05-28 17:30:49,715 ERROR c.s.j.s.c.ContainerResponse [qtp1972772630-44] A message body writer for Java class com.google.common.collect.SingletonImmutableBiMap, and Java type class com.google.common.collect.SingletonImmutableBiMap, and MIME media type application/octet-stream was not found.\nThe registered message body writers compatible with the MIME media type are:\n*/* ->\n  com.sun.jersey.core.impl.provider.entity.FormProvider\n  com.sun.jersey.server.impl.template.ViewableMessageBodyWriter\n  com.sun.jersey.core.impl.provider.entity.StringProvider\n  com.sun.jersey.core.impl.provider.entity.ByteArrayProvider\n  com.sun.jersey.core.impl.provider.entity.FileProvider\n  com.sun.jersey.core.impl.provider.entity.InputStreamProvider\n  com.sun.jersey.core.impl.provider.entity.DataSourceProvider\n  com.sun.jersey.core.impl.provider.entity.XMLJAXBElementProvider$General\n  com.sun.jersey.core.impl.provider.entity.ReaderProvider\n  com.sun.jersey.core.impl.provider.entity.DocumentProvider\n  com.sun.jersey.core.impl.provider.entity.StreamingOutputProvider\n  com.sun.jersey.core.impl.provider.entity.SourceProvider$SourceWriter\n  com.fasterxml.jackson.jaxrs.json.JacksonJsonProvider\n  com.sun.jersey.core.impl.provider.entity.XMLRootElementProvider$General\n  com.sun.jersey.core.impl.provider.entity.XMLListElementProvider$General\napplication/octet-stream ->\n  com.sun.jersey.core.impl.provider.entity.ByteArrayProvider\n  com.sun.jersey.core.impl.provider.entity.FileProvider\n  com.sun.jersey.core.impl.provider.entity.InputStreamProvider\n  com.sun.jersey.core.impl.provider.entity.DataSourceProvider\n  com.sun.jersey.core.impl.provider.entity.StreamingOutputProvider\n\n2016-05-28 17:30:49,716 ERROR c.s.j.s.c.ContainerResponse [qtp1972772630-44] Mapped exception to response: 500 (Internal Server Error)\njavax.ws.rs.WebApplicationException: com.sun.jersey.api.MessageException: A message body writer for Java class com.google.common.collect.SingletonImmutableBiMap, and Java type class com.google.common.collect.SingletonImmutableBiMap, and MIME media type application/octet-stream was not found.\n\n    at com.sun.jersey.spi.container.ContainerResponse.write(ContainerResponse.java:284) [jersey-server-1.19.jar:1.19]\n    at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1510) [jersey-server-1.19.jar:1.19]\n    at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419) [jersey-server-1.19.jar:1.19]\n    at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409) [jersey-server-1.19.jar:1.19]\n    at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409) [jersey-servlet-1.19.jar:1.19]\n    at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558) [jersey-servlet-1.19.jar:1.19]\n    at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733) [jersey-servlet-1.19.jar:1.19]\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) [javax.servlet-api-3.1.0.jar:3.1.0]\n    at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:278) [guice-servlet-4.0-beta.jar:?]\n    at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:268) [guice-servlet-4.0-beta.jar:?]\n    at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:180) [guice-servlet-4.0-beta.jar:?]\n    at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:93) [guice-servlet-4.0-beta.jar:?]\n    at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85) [guice-servlet-4.0-beta.jar:?]\n    at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:120) [guice-servlet-4.0-beta.jar:?]\n    at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:132) [guice-servlet-4.0-beta.jar:?]\n    at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:129) [guice-servlet-4.0-beta.jar:?]\n    at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:206) [guice-servlet-4.0-beta.jar:?]\n    at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:129) [guice-servlet-4.0-beta.jar:?]\n    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652) [jetty-servlet-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at io.druid.server.http.RedirectFilter.doFilter(RedirectFilter.java:71) [druid-server-0.9.0.jar:0.9.0]\n    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652) [jetty-servlet-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:83) [jetty-servlets-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:364) [jetty-servlets-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652) [jetty-servlet-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585) [jetty-servlet-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:221) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1125) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515) [jetty-servlet-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1059) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.Server.handle(Server.java:497) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:310) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:248) [jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:540) [jetty-io-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:620) [jetty-util-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:540) [jetty-util-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]\nCaused by: com.sun.jersey.api.MessageException: A message body writer for Java class com.google.common.collect.SingletonImmutableBiMap, and Java type class com.google.common.collect.SingletonImmutableBiMap, and MIME media type application/octet-stream was not found.\n\n    ... 40 more\n```\n"}, {"user": "gianm", "commits": {}, "labels": ["Bug"], "created": "2016-05-27 18:34:18", "title": "Wires crossed on lookup caching", "url": "https://github.com/apache/druid/issues/3031", "closed": "2016-05-27 19:46:08", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "A couple of folks in this thread had issues with wires getting crossed when they have two lookups in the same directory. I'm not sure if this still applies in master with the recent QTL changes.\n\nhttps://groups.google.com/d/topic/imply-user-group/H-RgHyFZzVM/discussion\n"}, {"user": "redlion99", "commits": {"ba3dbf2a42300c78ff0d8718627fd25256c8a945": {"commitGHEventType": "referenced", "commitUser": "jon-wei"}}, "changesInPackagesGIT": ["processing/src/main/java/io/druid/segment/incremental", "api/src/main/java/io/druid/data/input/impl", "processing/src/main/java/io/druid/segment"], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 3, "spoonMethodsChanged": 3, "INS": 5, "UPD": 2, "DEL": 1, "MOV": 4, "TOT": 12}, "title": "NPE when dimension name is empty", "numCommits": 1, "created": "2016-05-24 07:23:39", "closed": "2018-03-06 00:27:36", "gitStatsSummary": {"deletions": 2, "insertions": 16, "lines": 18, "gitFilesChange": 3}, "statsSkippedReason": "", "changesInPackagesSPOON": ["io.druid.segment.IndexIO.V9IndexLoader.load(java.io.File,com.fasterxml.jackson.databind.ObjectMapper)", "io.druid.segment.incremental.IncrementalIndex.toTimeAndDims(io.druid.data.input.InputRow)", "io.druid.data.input.impl.DimensionSchema"], "filteredCommits": ["ba3dbf2a42300c78ff0d8718627fd25256c8a945"], "url": "https://github.com/apache/druid/issues/3007", "filteredCommitsReason": {"multipleIssueFixes": 0, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 650.0002777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "api/src/main/java/io/druid/data/input/impl/DimensionSchema.java", "deletions": 2, "insertions": 7, "lines": 9}, {"filePath": "processing/src/main/java/io/druid/segment/IndexIO.java", "deletions": 0, "insertions": 5, "lines": 5}, {"filePath": "processing/src/test/java/io/druid/segment/IndexMergerTestBase.java", "deletions": 0, "insertions": 45, "lines": 45}, {"filePath": "processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java", "deletions": 0, "insertions": 4, "lines": 4}], "commitSpoonAstDiffStats": [{"spoonFilePath": "DimensionSchema.java", "spoonMethods": [{"INS": 3, "UPD": 2, "DEL": 1, "spoonMethodName": "io.druid.data.input.impl.DimensionSchema", "MOV": 4, "TOT": 10}]}, {"spoonFilePath": "IndexIO.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.IndexIO.V9IndexLoader.load(java.io.File,com.fasterxml.jackson.databind.ObjectMapper)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "IndexMergerTestBase.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.IndexMergerTestBase.testDimensionWithEmptyName()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "IncrementalIndex.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.incremental.IncrementalIndex.toTimeAndDims(io.druid.data.input.InputRow)", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-03-06 05:45:35", "commitMessage": "Fixed NPE when dimension is null or empty. https://github.com/druid-io/druid/issues/3007 (#5299)\n\n", "commitUser": "jon-wei", "commitDateTime": "2018-03-05 16:27:35", "commitParents": ["7416d1d02d5e88b92b728c627922e6293066e03a"], "commitGHEventType": "referenced", "nameRev": "ba3dbf2a42300c78ff0d8718627fd25256c8a945 tags/druid-0.13.0-incubating-rc1~419", "commitHash": "ba3dbf2a42300c78ff0d8718627fd25256c8a945"}], "body": "Here is log of the index task.\n\n```\nMay 24, 2016 12:00:36 AM org.hibernate.validator.internal.util.Version <clinit>\nINFO: HV000001: Hibernate Validator 5.1.3.Final\n2016-05-24T00:00:39,217 WARN [main] org.apache.curator.retry.ExponentialBackoffRetry - maxRetries too large (30). Pinning to 29\nMay 24, 2016 12:00:41 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\nINFO: Registering com.fasterxml.jackson.jaxrs.json.JacksonJsonProvider as a provider class\nMay 24, 2016 12:00:41 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\nINFO: Registering io.druid.server.initialization.jetty.CustomExceptionMapper as a provider class\nMay 24, 2016 12:00:41 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\nINFO: Registering io.druid.server.StatusResource as a root resource class\nMay 24, 2016 12:00:41 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\nINFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\nMay 24, 2016 12:00:42 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\nINFO: Binding io.druid.server.initialization.jetty.CustomExceptionMapper to GuiceManagedComponentProvider with the scope \"Singleton\"\nMay 24, 2016 12:00:42 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\nINFO: Binding com.fasterxml.jackson.jaxrs.json.JacksonJsonProvider to GuiceManagedComponentProvider with the scope \"Singleton\"\nMay 24, 2016 12:00:42 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\nINFO: Binding io.druid.server.QueryResource to GuiceInstantiatedComponentProvider\nMay 24, 2016 12:00:42 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\nINFO: Binding io.druid.segment.realtime.firehose.ChatHandlerResource to GuiceInstantiatedComponentProvider\nMay 24, 2016 12:00:42 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\nINFO: Binding io.druid.server.StatusResource to GuiceManagedComponentProvider with the scope \"Undefined\"\nException in thread \"plumber_persist_0\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_1\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_2\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_3\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_4\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_5\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_6\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_7\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_8\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_9\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_10\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_11\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_12\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_13\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_14\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_15\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_16\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_17\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_18\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_19\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_20\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_21\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_22\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nException in thread \"plumber_persist_23\" java.lang.NullPointerException\n    at java.util.TreeMap.put(TreeMap.java:563)\n    at java.util.TreeSet.add(TreeSet.java:255)\n    at java.util.AbstractCollection.addAll(AbstractCollection.java:344)\n    at java.util.TreeSet.addAll(TreeSet.java:312)\n    at io.druid.segment.IndexIO$DefaultIndexIOHandler.convertV8toV9(IndexIO.java:868)\n    at io.druid.segment.IndexMerger.makeIndexFiles(IndexMerger.java:1009)\n    at io.druid.segment.IndexMerger.merge(IndexMerger.java:421)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:193)\n    at io.druid.segment.IndexMerger.persist(IndexMerger.java:159)\n    at io.druid.segment.realtime.plumber.RealtimePlumber.persistHydrant(RealtimePlumber.java:1028)\n    at io.druid.segment.realtime.plumber.RealtimePlumber$3.doRun(RealtimePlumber.java:445)\n    at io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n```\n"}, {"user": "jon-wei", "commits": {}, "labels": ["Bug"], "created": "2016-05-19 23:30:35", "title": "ImmutableBitmap.get(int value) behaves differently for Concise and Roaring", "url": "https://github.com/apache/druid/issues/2997", "closed": "2016-05-20 00:06:37", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "Suppose I have a bitmap with bits 0,4,9 set.\n\nWhen using Concise-backed ImmutableBitmap, if I call `bitmap.get(9)`, I get an IndexOutOfBoundsException. \n\nThe `get()` function seems to expect `get(2)`, if I want to read the value for bit 9, i.e., the `value` input is treated as a position within the ConciseSet's set of values [0,4,9], not as a bitmap position.\n\nAlso, given this behavior, if I have a Concise-backed ImmutableBitmap and I call get() in a loop from 0 to bitmap.size(), get(0) returns `false`, while the range from 1 to `bmp.size() - 1` returns `true` (0 should return `true` as well)\n\nIf I switch to Roaring-backed, `get()` works as expected, the input is treated as a bitmap position.\n\n---\n\nNothing within Druid currently uses `ImmutableBitmap.get(int value)`, opening this issue for future reference.\n"}, {"user": "dclim", "commits": {}, "labels": ["Bug"], "created": "2016-05-19 18:02:20", "title": "RTR may have old TaskLocation information", "url": "https://github.com/apache/druid/issues/2993", "closed": "2016-05-26 05:05:18", "ttf": 6.000277777777778, "commitsDetails": [], "body": "If a middle manager is restarted, it may restart the tasks it was previously running on different ports. This may cause systems depending on TaskLocation for communicating with the tasks to inadvertently talk to a different task than intended.\n\nThe TaskLocation data will eventually be updated through ZK announcements, but there should be a mechanism to reject requests made to the wrong recipient until the update happens.\n"}, {"user": "drcrallen", "commits": {"8e0c307e54d8fdf87c5b9447c4c53a71e6502715": {"commitGHEventType": "referenced", "commitUser": "xvrl"}, "15ccf451f9b12cbceff853dd71fac7accc258be8": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0.0, "UPD": 0.0, "DEL": 0.0, "MOV": 0.0, "TOT": 0.0}, "title": "io.druid.granularity.QueryGranularity has class initialization deadlock.", "numCommits": 0, "created": "2016-05-17 21:27:57", "closed": "2016-05-17 23:23:48", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/2979", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 0.0002777777777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "core/src/main/java/org/apache/druid/utils/JvmUtils.java", "deletions": 0, "insertions": 24, "lines": 24}, {"filePath": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/HadoopTask.java", "deletions": 7, "insertions": 16, "lines": 23}, {"filePath": "processing/src/test/java/org/apache/druid/granularity/QueryGranularityTest.java", "deletions": 13, "insertions": 0, "lines": 13}, {"filePath": "core/src/test/java/org/apache/druid/utils/JvmUtilsTest.java", "deletions": 0, "insertions": 19, "lines": 19}, {"filePath": "indexing-service/src/test/java/org/apache/druid/indexing/common/task/HadoopTaskTest.java", "deletions": 0, "insertions": 1, "lines": 1}, {"filePath": "core/pom.xml", "deletions": 0, "insertions": 2, "lines": 2}], "commitSpoonAstDiffStats": [{"spoonFilePath": "HadoopTask.java", "spoonMethods": [{"INS": 4, "UPD": 2, "DEL": 1, "spoonMethodName": "org.apache.druid.indexing.common.task.HadoopTask.buildClassLoader(java.util.List,java.util.List)", "MOV": 5, "TOT": 12}]}, {"spoonFilePath": "JvmUtils.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.utils.JvmUtils.systemClassPath()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "JvmUtilsTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.utils.JvmUtilsTest.testSystemClassPath()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "QueryGranularityTest.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "org.apache.druid.granularity.QueryGranularityTest.testDeadLock()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "HadoopTaskTest.java", "spoonMethods": []}], "spoonStatsSkippedReason": "", "authoredDateTime": "2019-08-24 20:47:54", "commitMessage": "Do not assume system classloader is URLClassLoader in Java 9+ (#8392)\n\n* Fallback to parsing classpath for hadoop task in Java 9+\r\nIn Java 9 and above we cannot assume that the system classloader is an\r\ninstance of URLClassLoader. This change adds a fallback method to parse\r\nthe system classpath in that case, and adds a unit test to validate it matches\r\nwhat JDK8 would do.\r\n\r\nNote: This has not been tested in an actual hadoop setup, so this is mostly\r\nto help us pass unit tests.\r\n\r\n* Remove granularity test of dubious value\r\nOne of our granularity tests relies on system classloader being a URLClassLoaders to\r\ncatch a bug related to class initialization and static initializers using a subclass (see\r\n#2979)\r\nThis test was added to catch a potential regression, but it assumes we would add back\r\nthe same type of static initializers to this specific class, so it seems to be of dubious value\r\nas a unit test and mostly serves to illustrate the bug.\r\n\r\nrelates to #5589", "commitUser": "xvrl", "commitDateTime": "2019-08-24 20:47:54", "commitParents": ["20f7db5d22fe746472731b5c9cea9e5a232fe1d3"], "commitGHEventType": "referenced", "nameRev": "8e0c307e54d8fdf87c5b9447c4c53a71e6502715 tags/druid-0.16.0-incubating-rc1~38", "commitHash": "8e0c307e54d8fdf87c5b9447c4c53a71e6502715"}, {"commitGitStats": [{"filePath": "processing/src/main/java/io/druid/granularity/QueryGranularity.java", "deletions": 50, "insertions": 18, "lines": 68}, {"filePath": "extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskTest.java", "deletions": 4, "insertions": 3, "lines": 7}, {"filePath": "processing/src/test/java/io/druid/segment/data/IncrementalIndexTest.java", "deletions": 7, "insertions": 7, "lines": 14}, {"filePath": "processing/src/test/java/io/druid/query/ResultGranularTimestampComparatorTest.java", "deletions": 3, "insertions": 4, "lines": 7}, {"filePath": "processing/src/test/java/io/druid/segment/TestIndex.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/main/java/io/druid/query/search/SearchQueryRunner.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "server/src/test/java/io/druid/segment/indexing/DataSchemaTest.java", "deletions": 5, "insertions": 5, "lines": 10}, {"filePath": "extensions-contrib/distinctcount/src/test/java/io/druid/query/aggregation/distinctcount/DistinctCountTimeseriesQueryTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/test/java/io/druid/query/aggregation/hyperloglog/HyperUniquesAggregationTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/test/java/io/druid/segment/IndexMergerTest.java", "deletions": 13, "insertions": 13, "lines": 26}, {"filePath": "processing/src/test/java/io/druid/query/search/SearchBinaryFnTest.java", "deletions": 10, "insertions": 10, "lines": 20}, {"filePath": "processing/src/test/java/io/druid/query/spec/SpecificSegmentQueryRunnerTest.java", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "server/src/main/java/io/druid/segment/indexing/granularity/UniformGranularitySpec.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "server/src/test/java/io/druid/segment/realtime/appenderator/AppenderatorTester.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "indexing-service/src/test/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactoryTimelineTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/main/java/io/druid/segment/incremental/IncrementalIndexSchema.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "indexing-hadoop/src/test/java/io/druid/indexer/BatchDeltaIngestionTest.java", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "indexing-hadoop/src/test/java/io/druid/indexer/DeterminePartitionsJobTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/test/java/io/druid/segment/IndexMergerV9CompatibilityTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java", "deletions": 5, "insertions": 6, "lines": 11}, {"filePath": "processing/src/test/java/io/druid/segment/SchemalessTestFull.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "processing/src/test/java/io/druid/segment/incremental/IncrementalIndexStorageAdapterTest.java", "deletions": 8, "insertions": 8, "lines": 16}, {"filePath": "processing/src/test/java/io/druid/segment/filter/SpatialFilterBonusTest.java", "deletions": 8, "insertions": 8, "lines": 16}, {"filePath": "processing/src/test/java/io/druid/segment/filter/BaseFilterTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "extensions-core/histogram/src/test/java/io/druid/query/aggregation/histogram/ApproximateHistogramAggregationTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/test/java/io/druid/segment/IndexIOTest.java", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "processing/src/test/java/io/druid/query/datasourcemetadata/DataSourceMetadataQueryTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/main/java/io/druid/query/Druids.java", "deletions": 3, "insertions": 4, "lines": 7}, {"filePath": "processing/src/test/java/io/druid/query/metadata/SegmentMetadataQueryTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/test/java/io/druid/segment/EmptyIndexTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "indexing-hadoop/src/test/java/io/druid/indexer/path/GranularityPathSpecTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "server/src/test/java/io/druid/segment/realtime/plumber/SinkTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "indexing-hadoop/src/test/java/io/druid/indexer/path/DatasourcePathSpecTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "indexing-service/src/test/java/io/druid/indexing/common/task/TaskSerdeTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/test/java/io/druid/query/select/SelectBinaryFnTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/test/java/io/druid/query/search/SearchQueryQueryToolChestTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "extensions-contrib/distinctcount/src/test/java/io/druid/query/aggregation/distinctcount/DistinctCountTopNQueryTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "server/src/test/java/io/druid/segment/realtime/firehose/IngestSegmentFirehoseTest.java", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "processing/src/main/java/io/druid/query/search/search/SearchQuery.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "processing/src/test/java/io/druid/segment/AppendTest.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "indexing-service/src/test/java/io/druid/indexing/common/actions/SegmentAllocateActionTest.java", "deletions": 26, "insertions": 27, "lines": 53}, {"filePath": "processing/src/test/java/io/druid/query/QueryRunnerTestHelper.java", "deletions": 2, "insertions": 3, "lines": 5}, {"filePath": "processing/src/test/java/io/druid/segment/incremental/IncrementalIndexTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/test/java/io/druid/segment/MetadataTest.java", "deletions": 6, "insertions": 6, "lines": 12}, {"filePath": "processing/src/test/java/io/druid/segment/IndexMergerV9WithSpatialIndexTest.java", "deletions": 8, "insertions": 8, "lines": 16}, {"filePath": "indexing-hadoop/src/test/java/io/druid/indexer/JobHelperTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "server/src/test/java/io/druid/segment/indexing/granularity/UniformGranularityTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "server/src/test/java/io/druid/segment/realtime/plumber/RealtimePlumberSchoolTest.java", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "extensions-core/datasketches/src/test/java/io/druid/query/aggregation/datasketches/theta/oldapi/OldApiSketchAggregationTest.java", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "server/src/test/java/io/druid/client/CachingQueryRunnerTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "server/src/test/java/io/druid/segment/realtime/RealtimeManagerTest.java", "deletions": 4, "insertions": 4, "lines": 8}, {"filePath": "indexing-hadoop/src/test/java/io/druid/indexer/IndexGeneratorJobTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/test/java/io/druid/segment/filter/SpatialFilterTest.java", "deletions": 8, "insertions": 8, "lines": 16}, {"filePath": "processing/src/test/java/io/druid/segment/incremental/OnheapIncrementalIndexBenchmark.java", "deletions": 3, "insertions": 4, "lines": 7}, {"filePath": "server/src/test/java/io/druid/server/coordination/ServerManagerTest.java", "deletions": 11, "insertions": 12, "lines": 23}, {"filePath": "processing/src/test/java/io/druid/query/timeseries/TimeseriesQueryQueryToolChestTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "indexing-hadoop/src/test/java/io/druid/indexer/HadoopDruidIndexerConfigTest.java", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "processing/src/test/java/io/druid/segment/SchemalessIndex.java", "deletions": 4, "insertions": 4, "lines": 8}, {"filePath": "benchmarks/src/main/java/io/druid/benchmark/IncrementalIndexAddRowsBenchmark.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "server/src/test/java/io/druid/segment/realtime/appenderator/AppenderatorTest.java", "deletions": 6, "insertions": 6, "lines": 12}, {"filePath": "processing/src/test/java/io/druid/segment/SchemalessTestSimple.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "extensions-core/datasketches/src/test/java/io/druid/query/aggregation/datasketches/theta/SketchAggregationTest.java", "deletions": 5, "insertions": 5, "lines": 10}, {"filePath": "indexing-service/src/test/java/io/druid/indexing/common/task/IndexTaskTest.java", "deletions": 4, "insertions": 4, "lines": 8}, {"filePath": "processing/src/test/java/io/druid/query/MultiValuedDimensionTest.java", "deletions": 6, "insertions": 6, "lines": 12}, {"filePath": "processing/src/main/java/io/druid/query/metadata/SegmentAnalyzer.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "indexing-hadoop/src/main/java/io/druid/indexer/hadoop/DatasourceIngestionSpec.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "indexing-service/src/test/java/io/druid/indexing/common/task/RealtimeIndexTaskTest.java", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "services/src/test/java/io/druid/cli/validate/DruidJsonValidatorTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "indexing-service/src/test/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactoryTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/main/java/io/druid/granularity/QueryGranularities.java", "deletions": 0, "insertions": 45, "lines": 45}, {"filePath": "indexing-service/src/test/java/io/druid/indexing/overlord/TaskLifecycleTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/test/java/io/druid/query/timeseries/TimeseriesBinaryFnTest.java", "deletions": 6, "insertions": 6, "lines": 12}, {"filePath": "extensions-contrib/distinctcount/src/test/java/io/druid/query/aggregation/distinctcount/DistinctCountGroupByQueryTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/test/java/io/druid/query/timeseries/TimeseriesQueryRunnerTest.java", "deletions": 4, "insertions": 5, "lines": 9}, {"filePath": "processing/src/test/java/io/druid/query/topn/TopNQueryQueryToolChestTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "server/src/test/java/io/druid/client/CachingClusteredClientTest.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "processing/src/test/java/io/druid/query/select/MultiSegmentSelectQueryTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/test/java/io/druid/segment/incremental/TimeAndDimsCompTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "server/src/test/java/io/druid/segment/realtime/FireDepartmentTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "server/src/test/java/io/druid/segment/indexing/granularity/ArbitraryGranularityTest.java", "deletions": 5, "insertions": 5, "lines": 10}, {"filePath": "indexing-hadoop/src/test/java/io/druid/indexer/hadoop/DatasourceIngestionSpecTest.java", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "processing/src/test/java/io/druid/query/groupby/GroupByQueryRunnerFactoryTest.java", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "processing/src/test/java/io/druid/query/timeseries/TimeseriesQueryRunnerBonusTest.java", "deletions": 3, "insertions": 3, "lines": 6}, {"filePath": "processing/src/test/java/io/druid/granularity/QueryGranularityTest.java", "deletions": 41, "insertions": 54, "lines": 95}, {"filePath": "processing/src/main/java/io/druid/query/topn/TopNQueryBuilder.java", "deletions": 1, "insertions": 2, "lines": 3}, {"filePath": "processing/src/test/java/io/druid/query/groupby/GroupByQueryRunnerTest.java", "deletions": 4, "insertions": 4, "lines": 8}, {"filePath": "indexing-hadoop/src/test/java/io/druid/indexer/updater/HadoopConverterJobTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/test/java/io/druid/query/topn/TopNBinaryFnBenchmark.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "indexing-hadoop/src/test/java/io/druid/indexer/DetermineHashedPartitionsJobTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "processing/src/test/java/io/druid/query/topn/TopNBinaryFnTest.java", "deletions": 8, "insertions": 8, "lines": 16}, {"filePath": "indexing-hadoop/src/test/java/io/druid/indexer/IndexGeneratorCombinerTest.java", "deletions": 2, "insertions": 2, "lines": 4}, {"filePath": "extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java", "deletions": 2, "insertions": 2, "lines": 4}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "tooManyFiles", "authoredDateTime": "2016-05-17 16:23:48", "commitMessage": "Move QueryGranularity static fields to QueryGranularities (#2980)\n\n* Move QueryGranularity static fields to QueryGranularityUtil\r\n* Fixes #2979\r\n\r\n* Add test showing #2979\r\n\r\n* change name to QueryGranularities\r\n", "commitUser": "fjy", "commitDateTime": "2016-05-17 16:23:48", "commitParents": ["eaaad01de7d89cdf5396c962e903f9f68885c50d"], "commitGHEventType": "referenced", "nameRev": "15ccf451f9b12cbceff853dd71fac7accc258be8 tags/druid-0.9.1-rc1~23", "commitHash": "15ccf451f9b12cbceff853dd71fac7accc258be8"}], "body": "`io.druid.granularity.QueryGranularity` has the following static fields\n\n``` java\n  public static final QueryGranularity ALL = new AllGranularity();\n  public static final QueryGranularity NONE = new NoneGranularity();\n```\n\nIntellij gives a warning related to https://bugs.openjdk.java.net/browse/JDK-6301579 where static initializers of a class that use a subclass can deadlock.\n\nThis happens pretty regularly in one of our libraries that uses druid jackson serde stuff but only on one pipeline. Haven't seen it anywhere else yet.\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2016-05-10 20:44:39", "title": "ObjectMapper used during result serialization is inconsistent depending on `Accept` header", "url": "https://github.com/apache/druid/issues/2949", "closed": "2016-05-10 23:03:39", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "If a http request is sent to a druid node, the `Accept` header is typically either\n1. `application/x-jackson-smile`\n2. `application/json`\n\nThe `ObjectMapper` used when specifying these are not set with the same parameters.\n\nThis can be checked by making an object which has fields annotated with `@JsonIgnore` and returning it in a `Response`. If the `@GET` method is annotated with `@Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})`, then the result will be different depending on if you have an `Accept` header of smile vs json. In the `application/json` case, the `@JsonIgnore` will be honored, in the `application/x-jackson-smile` case, getters will still be called even though the field is annotated with `@JsonIgnore`.\n\nSee https://github.com/druid-io/druid/pull/2926#issuecomment-218229829 for how I hit this case\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Area - Testing", "Bug"], "created": "2016-05-06 20:45:50", "title": "Transient test error - java.sql.SQLIntegrityConstraintViolationException ", "url": "https://github.com/apache/druid/issues/2930", "closed": "2016-05-09 16:41:32", "ttf": 2.000277777777778, "commitsDetails": [], "body": "Ran into this test failure case while building locally\n\n```\nTests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.599 sec <<< FAILURE! - in io.druid.metadata.SQLMetadataSupervisorManagerTest\ntestInsertAndGet(io.druid.metadata.SQLMetadataSupervisorManagerTest)  Time elapsed: 0.599 sec  <<< ERROR!\norg.skife.jdbi.v2.exceptions.CallbackFailedException: org.skife.jdbi.v2.exceptions.UnableToExecuteStatementException: java.sql.SQLIntegrityConstraintViolationException: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'SQL160506204201978' defined on 'DRUIDTEST7CB9E3D334F949E0B23A8BE51C7EAFE8_SUPERVISORS'. [statement:\"INSERT INTO druidTest7cb9e3d334f949e0b23a8be51c7eafe8_supervisors (id, spec_id, version, payload) VALUES (:id, :spec_id, :version, :payload)\", located:\"INSERT INTO druidTest7cb9e3d334f949e0b23a8be51c7eafe8_supervisors (id, spec_id, version, payload) VALUES (:id, :spec_id, :version, :payload)\", rewritten:\"INSERT INTO druidTest7cb9e3d334f949e0b23a8be51c7eafe8_supervisors (id, spec_id, version, payload) VALUES (?, ?, ?, ?)\", arguments:{ positional:{}, named:{id:'test-supervisor-1_2016-05-06T20:42:01.963Z',payload:[123, 34, 116, 121, 112, 101, 34, 58, 34, 84, 101, 115, 116, 83, 117, 112, 101, 114, 118, 105, 115, 111, 114, 83, 112, 101, 99, 34, 44, 34, 105, 100, 34, 58, 34, 116, 101, 115, 116, 45, 115, 117, 112, 101, 114, 118, 105, 115, 111, 114, 45, 49, 34, 44, 34, 100, 97, 116, 97, 34, 58, 123, 34, 107, 101, 121, 49, 45, 49, 34, 58, 34, 118, 97, 108, 117, 101, 49, 45, 49, 45, 51, 34, 44, 34, 107, 101, 121, 49, 45, 50, 34, 58, 34, 118, 97, 108, 117, 101, 49, 45, 50, 45, 51, 34, 125, 125],spec_id:'test-supervisor-1',version:'2016-05-06T20:42:01.963Z'}, finder:[]}]\n    at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n    at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n    at org.apache.derby.impl.sql.execute.IndexChanger.insertAndCheckDups(Unknown Source)\n    at org.apache.derby.impl.sql.execute.IndexChanger.doInsert(Unknown Source)\n    at org.apache.derby.impl.sql.execute.IndexChanger.insert(Unknown Source)\n    at org.apache.derby.impl.sql.execute.IndexSetChanger.insert(Unknown Source)\n    at org.apache.derby.impl.sql.execute.RowChangerImpl.insertRow(Unknown Source)\n    at org.apache.derby.impl.sql.execute.InsertResultSet.normalInsertCore(Unknown Source)\n    at org.apache.derby.impl.sql.execute.InsertResultSet.open(Unknown Source)\n    at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)\n    at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)\n    at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)\n    at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)\n    at org.apache.derby.impl.jdbc.EmbedPreparedStatement.execute(Unknown Source)\n    at org.skife.jdbi.v2.SQLStatement.internalExecute(SQLStatement.java:1328)\n    at org.skife.jdbi.v2.Update.execute(Update.java:56)\n    at io.druid.metadata.SQLMetadataSupervisorManager$1.withHandle(SQLMetadataSupervisorManager.java:88)\n    at io.druid.metadata.SQLMetadataSupervisorManager$1.withHandle(SQLMetadataSupervisorManager.java:83)\n    at org.skife.jdbi.v2.DBI.withHandle(DBI.java:281)\n    at io.druid.metadata.SQLMetadataSupervisorManager.insert(SQLMetadataSupervisorManager.java:81)\n    at io.druid.metadata.SQLMetadataSupervisorManagerTest.testInsertAndGet(SQLMetadataSupervisorManagerTest.java:99)\n```\n"}, {"user": "gianm", "commits": {"90b0b0a4adb9f2e7dbe7cfed2a1a420c53dad5dc": {"commitGHEventType": "referenced", "commitUser": "nishantmonu51"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "FileSystemNotFoundException when loading S3-based lookup with specific location", "numCommits": 0, "created": "2016-05-06 19:11:29", "closed": "2016-05-08 17:53:53", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/2928", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 1.0002777777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "extensions-core/namespace-lookup/src/main/java/io/druid/server/namespace/URIExtractionNamespaceFunctionFactory.java", "deletions": 28, "insertions": 24, "lines": 52}, {"filePath": "extensions-core/namespace-lookup/src/test/java/io/druid/server/namespace/URIExtractionNamespaceFunctionFactoryTest.java", "deletions": 29, "insertions": 91, "lines": 120}], "commitSpoonAstDiffStats": [{"spoonFilePath": "URIExtractionNamespaceFunctionFactoryTest.java", "spoonMethods": [{"INS": 0, "UPD": 9, "DEL": 0, "spoonMethodName": "io.druid.server.namespace.URIExtractionNamespaceFunctionFactoryTest.getParameters().3.iterator().1.next()", "MOV": 0, "TOT": 9}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.namespace.URIExtractionNamespaceFunctionFactoryTest.testWeirdSchemaOnExactURI()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.server.namespace.URIExtractionNamespaceFunctionFactoryTest.getParameters().3.iterator().1.hasNext()", "MOV": 1, "TOT": 3}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.server.namespace.URIExtractionNamespaceFunctionFactoryTest.testMissing().4", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.namespace.URIExtractionNamespaceFunctionFactoryTest.getParameters().4.iterator().1.hasNext()", "MOV": 0, "TOT": 1}, {"INS": 3, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.server.namespace.URIExtractionNamespaceFunctionFactoryTest", "MOV": 0, "TOT": 4}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.server.namespace.URIExtractionNamespaceFunctionFactoryTest.getParameters().2", "MOV": 0, "TOT": 1}, {"INS": 4, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.server.namespace.URIExtractionNamespaceFunctionFactoryTest.setUp()", "MOV": 1, "TOT": 6}, {"INS": 0, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.namespace.URIExtractionNamespaceFunctionFactoryTest.testMissing()", "MOV": 5, "TOT": 5}, {"INS": 0, "UPD": 3, "DEL": 0, "spoonMethodName": "io.druid.server.namespace.URIExtractionNamespaceFunctionFactoryTest.getParameters()", "MOV": 0, "TOT": 3}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.server.namespace.URIExtractionNamespaceFunctionFactoryTest.getParameters().1", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.server.namespace.URIExtractionNamespaceFunctionFactoryTest.getParameters().3.iterator()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.server.namespace.URIExtractionNamespaceFunctionFactoryTest.getParameters().3", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.server.namespace.URIExtractionNamespaceFunctionFactoryTest.getParameters().3.iterator().1", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.namespace.URIExtractionNamespaceFunctionFactoryTest.testMissingRegex()", "MOV": 5, "TOT": 5}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.server.namespace.URIExtractionNamespaceFunctionFactoryTest.getParameters().2.apply(java.io.File)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "URIExtractionNamespaceFunctionFactory.java", "spoonMethods": [{"INS": 1, "UPD": 3, "DEL": 4, "spoonMethodName": "io.druid.server.namespace.URIExtractionNamespaceFunctionFactory.getCachePopulator(io.druid.query.extraction.namespace.URIExtractionNamespace,java.lang.String,java.util.Map).3.call()", "MOV": 5, "TOT": 13}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-05-08 10:53:53", "commitMessage": "Make URIExtraction not require FileSystem impls for URIs it understands (#2929)\n\n* Make URIExtraction not require FileSystem impls for URIs it understands\r\n* Fixes #2928\r\n\r\n* Preserve URI information\r\n\r\n* Simply case for exact matching\r\n\r\n* Move unused variable\r\n", "commitUser": "nishantmonu51", "commitDateTime": "2016-05-08 23:11:53", "commitParents": ["8b570ab1301f8dd4ebd7839d7bd5bc47b0a92188"], "commitGHEventType": "referenced", "nameRev": "90b0b0a4adb9f2e7dbe7cfed2a1a420c53dad5dc tags/druid-0.9.1-rc1~49", "commitHash": "90b0b0a4adb9f2e7dbe7cfed2a1a420c53dad5dc"}], "body": "https://github.com/druid-io/druid/pull/2738/files#r62376615\n\nappears to be a regression since 0.9.0.\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2016-05-02 22:18:11", "title": "DataSchema.equals does not account for different parse specs meaning the same thing", "url": "https://github.com/apache/druid/issues/2914", "closed": "2016-05-10 23:09:28", "ttf": 8.000277777777777, "commitsDetails": [], "body": "`io.druid.segment.indexing.DataSchema#equals` calls `parser.equals(that.parser)` which assumes the parse spec map representation is unique per \"equal\" parse spec impl.\n\nWith legacy dimension specs which were only strings, this was the case, but with the changes in https://github.com/druid-io/druid-api/pull/75 the map portion which is the parse spec dimension specification can be represented as an array of strings (legacy) or an array of maps.\n"}, {"user": "nishantmonu51", "commits": {"f8ddfb9a4bb082fb4fc163005ff041a7a77d1f1f": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "SegmentInsertAction incompatibility introduced in #2220", "numCommits": 0, "created": "2016-05-02 17:40:25", "closed": "2016-05-04 20:54:34", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/2912", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 2.000277777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIndexTask.java", "deletions": 3, "insertions": 4, "lines": 7}, {"filePath": "indexing-service/src/test/java/io/druid/indexing/common/actions/SegmentTransactionalInsertActionTest.java", "deletions": 0, "insertions": 160, "lines": 160}, {"filePath": "indexing-service/src/test/java/io/druid/indexing/common/actions/SegmentInsertActionTest.java", "deletions": 65, "insertions": 4, "lines": 69}, {"filePath": "indexing-service/src/main/java/io/druid/indexing/common/actions/SegmentTransactionalInsertAction.java", "deletions": 0, "insertions": 143, "lines": 143}, {"filePath": "indexing-service/src/main/java/io/druid/indexing/common/actions/TaskAction.java", "deletions": 0, "insertions": 1, "lines": 1}, {"filePath": "indexing-service/src/main/java/io/druid/indexing/common/actions/SegmentInsertAction.java", "deletions": 61, "insertions": 7, "lines": 68}], "commitSpoonAstDiffStats": [{"spoonFilePath": "SegmentInsertActionTest.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.indexing.common.actions.SegmentInsertActionTest.testTransactional()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 3, "DEL": 1, "spoonMethodName": "io.druid.indexing.common.actions.SegmentInsertActionTest.testFailBadVersion()", "MOV": 1, "TOT": 5}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.indexing.common.actions.SegmentInsertActionTest.testFailTransactional()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "TaskAction.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.common.actions", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "SegmentTransactionalInsertActionTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.common.actions.SegmentTransactionalInsertActionTest", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "SegmentInsertAction.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.indexing.common.actions.SegmentInsertAction.getStartMetadata()", "MOV": 1, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 1, "spoonMethodName": "io.druid.indexing.common.actions.SegmentInsertAction.toString()", "MOV": 1, "TOT": 3}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.indexing.common.actions.SegmentInsertAction.getReturnTypeReference()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.common.actions.SegmentInsertAction.getReturnTypeReference().1", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 3, "DEL": 6, "spoonMethodName": "io.druid.indexing.common.actions.SegmentInsertAction.perform(io.druid.indexing.common.task.Task,io.druid.indexing.common.actions.TaskActionToolbox)", "MOV": 3, "TOT": 13}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.indexing.common.actions.SegmentInsertAction.getEndMetadata()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 7, "spoonMethodName": "io.druid.indexing.common.actions.SegmentInsertAction", "MOV": 0, "TOT": 7}]}, {"spoonFilePath": "KafkaIndexTask.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.KafkaIndexTask.run(io.druid.indexing.common.TaskToolbox).4.publishSegments(java.util.Set,java.lang.Object)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "SegmentTransactionalInsertAction.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.common.actions.SegmentTransactionalInsertAction", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-05-04 13:54:34", "commitMessage": "Split SegmentInsertAction and SegmentTransactionalInsertAction for backwards compat. (#2922)\n\nFixes #2912.", "commitUser": "fjy", "commitDateTime": "2016-05-04 13:54:34", "commitParents": ["e067acd443bed562fbd099f7d2ddadd6d094e9c7"], "commitGHEventType": "referenced", "nameRev": "f8ddfb9a4bb082fb4fc163005ff041a7a77d1f1f tags/druid-0.9.1-rc1~56", "commitHash": "f8ddfb9a4bb082fb4fc163005ff041a7a77d1f1f"}], "body": "In https://github.com/druid-io/druid/pull/2220 we modified the returnTypeReference in SegmentInsertAction. \n\nOn overlord update, all existing tasks start to fail on parsing the result for segmentInsertAction. \nStack trace - \n\n```\n\\ token\\n at [Source: N/A; line: -1, column: -1]\"\nexceptionStackTrace: \"java.lang.IllegalArgumentException: Can not deserialize instance\\\n\n\\ of java.util.HashSet out of START_OBJECT token\\n at [Source: N/A; line: -1,\\\n\\ column: -1]\\n\\tat com.fasterxml.jackson.databind.ObjectMapper._convert(ObjectMapper.java:2774)\\n\\\n\\tat com.fasterxml.jackson.databind.ObjectMapper.convertValue(ObjectMapper.java:2716)\\n\\\n\\tat com.fasterxml.jackson.databind.ObjectMapper.convertValue(ObjectMapper.java:2707)\\n\\\n\\tat io.druid.indexing.common.actions.RemoteTaskActionClient.submit(RemoteTaskActionClient.java:120)\\n\\\n\\tat io.druid.indexing.common.TaskToolbox.pushSegments(TaskToolbox.java:224)\\n\\\n\\tat io.druid.indexing.common.task.RealtimeIndexTask$TaskActionSegmentPublisher.publishSegment(RealtimeIndexTask.java:517)\\n\\\n\\tat io.druid.segment.realtime.plumber.RealtimePlumber$4.doRun(RealtimePlumber.java:555)\\n\\\n\\tat io.druid.common.guava.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:42)\\n\\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\\n\\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\\n\\\n\\tat java.lang.Thread.run(Thread.java:745)\\nCaused by: com.fasterxml.jackson.databind.JsonMappingException:\\\n\\ Can not deserialize instance of java.util.HashSet out of START_OBJECT token\\n\\\n\\ at [Source: N/A; line: -1, column: -1]\\n\\tat com.fasterxml.jackson.databind.JsonMappingException.from(JsonMappingException.java:148)\\n\\\n\\tat com.fasterxml.jackson.databind.DeserializationContext.mappingException(DeserializationContext.java:762)\\n\\\n\\tat com.fasterxml.jackson.databind.DeserializationContext.mappingException(DeserializationContext.java:758)\\n\\\n\\tat com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.handleNonArray(CollectionDeserializer.java:275)\\n\\\n\\tat com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:216)\\n\\\n\\tat com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:206)\\n\\\n\\tat com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:25)\\n\\\n\\tat com.fasterxml.jackson.databind.ObjectMapper._convert(ObjectMapper.java:2769)\\n\\\n\\t... 10 more\\n\"\nexceptionType: java.lang.IllegalArgumentException interval: 2016-05-02T15:00:00.000Z/2016-05-02T16:00:00.000Z\n```\n"}, {"user": "spektom", "commits": {}, "labels": ["Bug"], "created": "2016-05-02 06:08:24", "title": "Hadoop indexing time increased from 40 mins to 4 hours after upgrading to 0.9.0", "url": "https://github.com/apache/druid/issues/2906", "closed": "2016-05-18 20:16:57", "ttf": 16.00027777777778, "commitsDetails": [], "body": "All the Hadoop tasks are finished in 40 minutes, except for the last 2 reduce ones, which are stack for the rest of the time.\n\nWe've re-ordered dimensions under dimensionSpec so they come from low cardinality to high, as it was stated in the Druid 0.9.0 changelog.\n\nLong running reduce task log is attached.\n\n[stack_reduce.log.gz](https://github.com/druid-io/druid/files/244572/stack_reduce.log.gz)\n"}, {"user": "drcrallen", "commits": {"2a769a9fb7162cda56e61745357ea4393fe68569": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "S3 Lookups might be doing full GET requests to S3 instead of just looking at metadata", "numCommits": 0, "created": "2016-04-28 17:12:37", "closed": "2016-05-04 23:21:35", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/2894", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 6.000277777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3Utils.java", "deletions": 5, "insertions": 3, "lines": 8}, {"filePath": "extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentPullerTest.java", "deletions": 73, "insertions": 81, "lines": 154}, {"filePath": "extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentPuller.java", "deletions": 30, "insertions": 24, "lines": 54}], "commitSpoonAstDiffStats": [{"spoonFilePath": "S3Utils.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.storage.s3.S3Utils.isObjectInBucket(org.jets3t.service.impl.rest.httpclient.RestS3Service,java.lang.String,java.lang.String)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "S3DataSegmentPullerTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.storage.s3.S3DataSegmentPullerTest", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.storage.s3.S3DataSegmentPullerTest.testSimpleGetVersion()", "MOV": 1, "TOT": 3}, {"INS": 7, "UPD": 8, "DEL": 3, "spoonMethodName": "io.druid.storage.s3.S3DataSegmentPullerTest.testGZUncompress()", "MOV": 15, "TOT": 33}, {"INS": 6, "UPD": 6, "DEL": 3, "spoonMethodName": "io.druid.storage.s3.S3DataSegmentPullerTest.testGZUncompressRetries()", "MOV": 16, "TOT": 31}]}, {"spoonFilePath": "S3DataSegmentPuller.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.storage.s3.S3DataSegmentPuller.getSegmentFiles(io.druid.storage.s3.S3DataSegmentPuller$S3Coords,java.io.File)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 1, "spoonMethodName": "io.druid.storage.s3.S3DataSegmentPuller.getVersion(java.net.URI)", "MOV": 2, "TOT": 4}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.storage.s3.S3DataSegmentPuller.buildFileObject(java.net.URI,org.jets3t.service.impl.rest.httpclient.RestS3Service).1", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 3, "DEL": 0, "spoonMethodName": "io.druid.storage.s3.S3DataSegmentPuller.buildFileObject(java.net.URI,org.jets3t.service.impl.rest.httpclient.RestS3Service)", "MOV": 0, "TOT": 3}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.storage.s3.S3DataSegmentPuller.buildFileObject(java.net.URI,org.jets3t.service.impl.rest.httpclient.RestS3Service).1.finalize()", "MOV": 1, "TOT": 2}, {"INS": 2, "UPD": 2, "DEL": 2, "spoonMethodName": "io.druid.storage.s3.S3DataSegmentPuller.buildFileObject(java.net.URI,org.jets3t.service.impl.rest.httpclient.RestS3Service).1.openInputStream()", "MOV": 2, "TOT": 8}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-05-04 16:21:35", "commitMessage": "Make S3DataSegmentPuller do GET requests less often (#2900)\n\n* Make S3DataSegmentPuller do GET requests less often\r\n* Fixes #2894\r\n\r\n* Run intellij formatting on S3Utils\r\n\r\n* Remove forced stream fetching on getVersion\r\n\r\n* Remove unneeded finalize\r\n\r\n* Allow initial object fetching to fail and be retried\r\n", "commitUser": "fjy", "commitDateTime": "2016-05-04 16:21:35", "commitParents": ["035134d0705a037d9d09349dde3156255ec54bc4"], "commitGHEventType": "referenced", "nameRev": "2a769a9fb7162cda56e61745357ea4393fe68569 tags/druid-0.9.1-rc1~54", "commitHash": "2a769a9fb7162cda56e61745357ea4393fe68569"}], "body": "As per https://github.com/druid-io/druid/issues/2523#issuecomment-215495808 lookups regularly call `org.jets3t.service.S3Service#listObjects` when checking for new values.\n\nThis needs to be investigated to see if it can only check metadata and does not issue a full GET call.\n"}, {"user": "nishantmonu51", "commits": {"bf5e5e7b75c2e098997fcad87cb2c4af5c3991e5": {"commitGHEventType": "referenced", "commitUser": "drcrallen"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "Overlord throws Exception while gaining leadership and bails out", "numCommits": 0, "created": "2016-04-27 14:29:40", "closed": "2016-04-27 15:29:41", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/2886", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 0.0002777777777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerFactoryTest.java", "deletions": 0, "insertions": 139, "lines": 139}, {"filePath": "indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunnerFactory.java", "deletions": 6, "insertions": 4, "lines": 10}], "commitSpoonAstDiffStats": [{"spoonFilePath": "RemoteTaskRunnerFactory.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.indexing.overlord.RemoteTaskRunnerFactory.build()", "MOV": 2, "TOT": 6}, {"INS": 1, "UPD": 3, "DEL": 3, "spoonMethodName": "io.druid.indexing.overlord.RemoteTaskRunnerFactory", "MOV": 1, "TOT": 8}]}, {"spoonFilePath": "RemoteTaskRunnerFactoryTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.overlord.RemoteTaskRunnerFactoryTest", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-04-27 20:47:41", "commitMessage": "fix #2886 (#2887)\n\nFixes https://github.com/druid-io/druid/issues/2886", "commitUser": "drcrallen", "commitDateTime": "2016-04-27 08:29:41", "commitParents": ["58510d826b0728334e17a1fcdb1c0b26c115dc1f"], "commitGHEventType": "referenced", "nameRev": "bf5e5e7b75c2e098997fcad87cb2c4af5c3991e5 tags/druid-0.9.1-rc1~82", "commitHash": "bf5e5e7b75c2e098997fcad87cb2c4af5c3991e5"}], "body": "The leader threw an exception - \n\n```\njava.lang.RuntimeException: java.lang.reflect.InvocationTargetException\n        at com.google.common.base.Throwables.propagate(Throwables.java:160)\n        at io.druid.indexing.overlord.TaskMaster$1.takeLeadership(TaskMaster.java:150)\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$WrappedListener.takeLeadership(LeaderSelector.java:536)\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:399)\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:443)\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:64)\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:245)\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:239)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler.start(Lifecycle.java:350)\n        at com.metamx.common.lifecycle.Lifecycle.start(Lifecycle.java:259)\n        at io.druid.indexing.overlord.TaskMaster$1.takeLeadership(TaskMaster.java:134)\n        ... 12 more\nCaused by: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@21c56256 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@66a2d79d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2353]\n        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)\n        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)\n        at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:326)\n        at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533)\n        at com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:159)\n        at com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:135)\n        at com.metamx.common.concurrent.ScheduledExecutors.scheduleAtFixedRate(ScheduledExecutors.java:121)\n        at io.druid.indexing.overlord.autoscaling.SimpleResourceManagementStrategy.startManagement(SimpleResourceManagementStrategy.java:276)\n        at io.druid.indexing.overlord.autoscaling.SimpleResourceManagementStrategy.startManagement(SimpleResourceManagementStrategy.java:51)\n        at io.druid.indexing.overlord.RemoteTaskRunner.start(RemoteTaskRunner.java:293)\n        ... 19 more\n```\n"}, {"user": "erikdubbelboer", "commits": {}, "labels": ["Bug"], "created": "2016-04-26 07:59:55", "title": "Proper error message for druid.query.groupBy.maxResults", "url": "https://github.com/apache/druid/issues/2881", "closed": "2016-08-30 21:02:16", "ttf": 126.00027777777778, "commitsDetails": [], "body": "When a query reaches `druid.query.groupBy.maxResults` rows Druid just returns:\n\n``` json\n{\n  \"error\" : \"Unknown exception\"\n}\n```\n\nI guess it would be nicer if this was a proper error messages?\n\nThe historical nodes also don't like this condition and log a LOT (hundreds per query) of the following errors:\n\n```\ncom.metamx.common.ISE: Maximum number of rows [10000000] reached\n        at io.druid.query.groupby.GroupByQueryHelper$3.accumulate(GroupByQueryHelper.java:132) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.groupby.GroupByQueryHelper$3.accumulate(GroupByQueryHelper.java:115) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at com.metamx.common.guava.YieldingAccumulators$1.accumulate(YieldingAccumulators.java:32) ~[java-util-0.27.9.jar:?]\n        at com.metamx.common.guava.BaseSequence.makeYielder(BaseSequence.java:104) ~[java-util-0.27.9.jar:?]\n        at com.metamx.common.guava.BaseSequence.toYielder(BaseSequence.java:81) ~[java-util-0.27.9.jar:?]\n        at com.metamx.common.guava.ConcatSequence.makeYielder(ConcatSequence.java:93) ~[java-util-0.27.9.jar:?]\n        at com.metamx.common.guava.ConcatSequence.toYielder(ConcatSequence.java:72) ~[java-util-0.27.9.jar:?]\n        at com.metamx.common.guava.ResourceClosingSequence.toYielder(ResourceClosingSequence.java:41) ~[java-util-0.27.9.jar:?]\n        at com.metamx.common.guava.YieldingSequenceBase.accumulate(YieldingSequenceBase.java:34) ~[java-util-0.27.9.jar:?]\n        at io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:118) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:118) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.spec.SpecificSegmentQueryRunner$2$1.call(SpecificSegmentQueryRunner.java:87) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.spec.SpecificSegmentQueryRunner.doNamed(SpecificSegmentQueryRunner.java:171) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.spec.SpecificSegmentQueryRunner.access$400(SpecificSegmentQueryRunner.java:41) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.spec.SpecificSegmentQueryRunner$2.doItNamed(SpecificSegmentQueryRunner.java:162) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.spec.SpecificSegmentQueryRunner$2.accumulate(SpecificSegmentQueryRunner.java:80) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.CPUTimeMetricQueryRunner$1.accumulate(CPUTimeMetricQueryRunner.java:81) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at com.metamx.common.guava.Sequences$1.accumulate(Sequences.java:90) ~[java-util-0.27.9.jar:?]\n        at io.druid.query.GroupByParallelQueryRunner$1$1.call(GroupByParallelQueryRunner.java:118) [druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.GroupByParallelQueryRunner$1$1.call(GroupByParallelQueryRunner.java:109) [druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_95]\n        at io.druid.query.PrioritizedListenableFutureTask.run(PrioritizedExecutorService.java:271) [druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_95]\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_95]\n        at java.lang.Thread.run(Thread.java:745) [?:1.7.0_95]\n```\n\nand\n\n```\n2016-04-26T07:48:53,336 ERROR [processing-10] io.druid.query.GroupByParallelQueryRunner - Exception with one of the sequences!\njava.lang.NullPointerException\n        at io.druid.query.aggregation.LongSumAggregatorFactory.factorize(LongSumAggregatorFactory.java:59) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.segment.incremental.OnheapIncrementalIndex.addToFacts(OnheapIncrementalIndex.java:179) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.segment.incremental.IncrementalIndex.add(IncrementalIndex.java:566) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.groupby.GroupByQueryHelper$3.accumulate(GroupByQueryHelper.java:123) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.groupby.GroupByQueryHelper$3.accumulate(GroupByQueryHelper.java:115) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at com.metamx.common.guava.YieldingAccumulators$1.accumulate(YieldingAccumulators.java:32) ~[java-util-0.27.9.jar:?]\n        at com.metamx.common.guava.BaseSequence.makeYielder(BaseSequence.java:104) ~[java-util-0.27.9.jar:?]\n        at com.metamx.common.guava.BaseSequence.toYielder(BaseSequence.java:81) ~[java-util-0.27.9.jar:?]\n        at com.metamx.common.guava.ConcatSequence.makeYielder(ConcatSequence.java:93) ~[java-util-0.27.9.jar:?]\n        at com.metamx.common.guava.ConcatSequence.toYielder(ConcatSequence.java:72) ~[java-util-0.27.9.jar:?]\n        at com.metamx.common.guava.ResourceClosingSequence.toYielder(ResourceClosingSequence.java:41) ~[java-util-0.27.9.jar:?]\n        at com.metamx.common.guava.YieldingSequenceBase.accumulate(YieldingSequenceBase.java:34) ~[java-util-0.27.9.jar:?]\n        at io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:118) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:118) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.spec.SpecificSegmentQueryRunner$2$1.call(SpecificSegmentQueryRunner.java:87) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.spec.SpecificSegmentQueryRunner.doNamed(SpecificSegmentQueryRunner.java:171) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.spec.SpecificSegmentQueryRunner.access$400(SpecificSegmentQueryRunner.java:41) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.spec.SpecificSegmentQueryRunner$2.doItNamed(SpecificSegmentQueryRunner.java:162) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.spec.SpecificSegmentQueryRunner$2.accumulate(SpecificSegmentQueryRunner.java:80) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.CPUTimeMetricQueryRunner$1.accumulate(CPUTimeMetricQueryRunner.java:81) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at com.metamx.common.guava.Sequences$1.accumulate(Sequences.java:90) ~[java-util-0.27.9.jar:?]\n        at io.druid.query.GroupByParallelQueryRunner$1$1.call(GroupByParallelQueryRunner.java:118) [druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.GroupByParallelQueryRunner$1$1.call(GroupByParallelQueryRunner.java:109) [druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_95]\n        at io.druid.query.PrioritizedListenableFutureTask.run(PrioritizedExecutorService.java:271) [druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_95]\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_95]\n        at java.lang.Thread.run(Thread.java:745) [?:1.7.0_95]\n```\n\nand\n\n```\n2016-04-26T07:55:27,292 ERROR [processing-3] com.google.common.util.concurrent.Futures$CombinedFuture - input future failed.\njava.lang.NullPointerException\n        at io.druid.query.aggregation.LongSumAggregatorFactory.factorize(LongSumAggregatorFactory.java:59) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.segment.incremental.OnheapIncrementalIndex.addToFacts(OnheapIncrementalIndex.java:179) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.segment.incremental.IncrementalIndex.add(IncrementalIndex.java:566) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.groupby.GroupByQueryHelper$3.accumulate(GroupByQueryHelper.java:123) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.groupby.GroupByQueryHelper$3.accumulate(GroupByQueryHelper.java:115) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at com.metamx.common.guava.YieldingAccumulators$1.accumulate(YieldingAccumulators.java:32) ~[java-util-0.27.9.jar:?]\n        at com.metamx.common.guava.BaseSequence.makeYielder(BaseSequence.java:104) ~[java-util-0.27.9.jar:?]\n        at com.metamx.common.guava.BaseSequence.toYielder(BaseSequence.java:81) ~[java-util-0.27.9.jar:?]\n        at com.metamx.common.guava.ConcatSequence.makeYielder(ConcatSequence.java:93) ~[java-util-0.27.9.jar:?]\n        at com.metamx.common.guava.ConcatSequence.toYielder(ConcatSequence.java:72) ~[java-util-0.27.9.jar:?]\n        at com.metamx.common.guava.ResourceClosingSequence.toYielder(ResourceClosingSequence.java:41) ~[java-util-0.27.9.jar:?]\n        at com.metamx.common.guava.YieldingSequenceBase.accumulate(YieldingSequenceBase.java:34) ~[java-util-0.27.9.jar:?]\n        at io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:118) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:118) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.spec.SpecificSegmentQueryRunner$2$1.call(SpecificSegmentQueryRunner.java:87) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.spec.SpecificSegmentQueryRunner.doNamed(SpecificSegmentQueryRunner.java:171) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.spec.SpecificSegmentQueryRunner.access$400(SpecificSegmentQueryRunner.java:41) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.spec.SpecificSegmentQueryRunner$2.doItNamed(SpecificSegmentQueryRunner.java:162) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.spec.SpecificSegmentQueryRunner$2.accumulate(SpecificSegmentQueryRunner.java:80) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.CPUTimeMetricQueryRunner$1.accumulate(CPUTimeMetricQueryRunner.java:81) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at com.metamx.common.guava.Sequences$1.accumulate(Sequences.java:90) ~[java-util-0.27.9.jar:?]\n        at io.druid.query.GroupByParallelQueryRunner$1$1.call(GroupByParallelQueryRunner.java:118) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.query.GroupByParallelQueryRunner$1$1.call(GroupByParallelQueryRunner.java:109) ~[druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_95]\n        at io.druid.query.PrioritizedListenableFutureTask.run(PrioritizedExecutorService.java:271) [druid-processing-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_95]\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_95]\n        at java.lang.Thread.run(Thread.java:745) [?:1.7.0_95]\n```\n"}, {"user": "xvrl", "commits": {}, "labels": ["Bug"], "created": "2016-04-07 18:28:18", "title": "Performance regression introduced by #2753", "url": "https://github.com/apache/druid/issues/2804", "closed": "2016-04-15 04:39:41", "ttf": 7.000277777777778, "commitsDetails": [], "body": "#2753 introduced a performance regression. See @himanshug's comment here:\n\nhttps://github.com/druid-io/druid/pull/2753/files/1853f36e9f93376943ea14684e1de85adca174fe#r58920723\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2016-04-05 21:17:44", "title": "Duplicate primary key errors cause TaskQueue big lock to be held for way longer than it should.", "url": "https://github.com/apache/druid/issues/2793", "closed": "2016-04-05 22:21:32", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "https://github.com/druid-io/druid/pull/1896 introduced an addition of https://github.com/druid-io/druid/commit/e4e5f0375b538ac7e2a2f7c36d00bd861e584c44#diff-e677e1ba7e3cf3b5b97660cfc17749beR144 which causes errors on duplicate primary key entries to be retried way more than they should.\n\n```\n2016-04-05T20:52:57,269 WARN [qtp726762476-61] com.metamx.common.RetryUtils - Failed on try 1, retrying in 1,976ms.\norg.skife.jdbi.v2.exceptions.CallbackFailedException: org.skife.jdbi.v2.exceptions.UnableToExecuteStatementException: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'index_realtime_REDACTED_2016-04-05T20:00:0' for key 'PRIMARY' [statement:\"INSERT INTO REDACTED (id, created_date, datasource, payload, active, status_payload) VALUES (:id, :created_date, :datasource, :payload, :active, :status_payload)\", located:\"INSERT INTO REDACTED (id, created_date, datasource, payload, active, status_payload) VALUES (:id, :created_date, :datasource, :payload, :active, :status_payload)\", rewritten:\"INSERT INTO REDACTED (id, created_date, datasource, payload, active, status_payload) VALUES (?, ?, ?, ?, ?, ?)\", arguments:{ positional:{}, named:{payload:[\n...\n        at org.skife.jdbi.v2.DBI.withHandle(DBI.java:284) ~[druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at io.druid.metadata.SQLMetadataConnector$2.call(SQLMetadataConnector.java:110) ~[druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.metamx.common.RetryUtils.retry(RetryUtils.java:38) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at io.druid.metadata.SQLMetadataConnector.retryWithHandle(SQLMetadataConnector.java:115) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at io.druid.metadata.SQLMetadataStorageActionHandler.insert(SQLMetadataStorageActionHandler.java:97) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at io.druid.indexing.overlord.MetadataTaskStorage.insert(MetadataTaskStorage.java:134) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at io.druid.indexing.overlord.TaskQueue.add(TaskQueue.java:321) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at io.druid.indexing.overlord.http.OverlordResource$1.apply(OverlordResource.java:124) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at io.druid.indexing.overlord.http.OverlordResource$1.apply(OverlordResource.java:119) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at io.druid.indexing.overlord.http.OverlordResource.asLeaderWith(OverlordResource.java:518) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at io.druid.indexing.overlord.http.OverlordResource.taskPost(OverlordResource.java:116) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_60]\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_60]\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_60]\n        at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_60]\n        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:278) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:268) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:180) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:93) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:120) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:132) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:129) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:206) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:129) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n        at io.druid.server.http.RedirectFilter.doFilter(RedirectFilter.java:71) [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]\n...\n```\n\nThis retry holds onto the big lock in TaskQueue while it exhausts its attempts, causing a big backlog of things wanting to use the big lock in TaskQueue\n"}, {"user": "nishantmonu51", "commits": {"2729fea84dd553631a907f4ff883345e9f388d54": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "SelectQueryQueryToolchest.filterSegments break if dataSource name contains DataSegment.delimiter", "numCommits": 0, "created": "2016-04-04 15:41:22", "closed": "2016-05-03 17:14:28", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/2786", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 29.00027777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "common/src/main/java/io/druid/common/utils/StringUtils.java", "deletions": 0, "insertions": 2, "lines": 2}, {"filePath": "api/pom.xml", "deletions": 0, "insertions": 25, "lines": 25}, {"filePath": "api/src/main/java/io/druid/timeline/DataSegmentUtils.java", "deletions": 0, "insertions": 190, "lines": 190}, {"filePath": "processing/src/main/java/io/druid/query/select/SelectQueryQueryToolChest.java", "deletions": 2, "insertions": 5, "lines": 7}, {"filePath": "api/src/test/java/io/druid/timeline/DataSegmentUtilsTest.java", "deletions": 0, "insertions": 123, "lines": 123}, {"filePath": "processing/src/main/java/io/druid/segment/SegmentDesc.java", "deletions": 108, "insertions": 0, "lines": 108}, {"filePath": "api/src/test/resources/log4j2.xml", "deletions": 0, "insertions": 35, "lines": 35}, {"filePath": "processing/src/main/java/io/druid/query/select/SelectQueryEngine.java", "deletions": 2, "insertions": 6, "lines": 8}], "commitSpoonAstDiffStats": [{"spoonFilePath": "StringUtils.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.common.utils.StringUtils", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "SelectQueryEngine.java", "spoonMethods": [{"INS": 2, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.select.SelectQueryEngine.process(io.druid.query.select.SelectQuery,io.druid.segment.Segment)", "MOV": 0, "TOT": 3}]}, {"spoonFilePath": "DataSegmentUtilsTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.timeline.DataSegmentUtilsTest", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "SelectQueryQueryToolChest.java", "spoonMethods": [{"INS": 2, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.select.SelectQueryQueryToolChest.filterSegments(io.druid.query.select.SelectQuery,java.util.List)", "MOV": 1, "TOT": 4}]}, {"spoonFilePath": "DataSegmentUtils.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.timeline.DataSegmentUtils", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "SegmentDesc.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.SegmentDesc", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-05-03 14:37:28", "commitMessage": "Fix parsing fail of segment id with datasource containing underscore (#2797)\n\n* Fix parsing fail of segment id with underscored datasource (Fix for #2786)\r\n\r\n* addressed comment\r\n\r\n* renamed and moved code into api. added log4 dependency for tests\r\n\r\n* addressed comments\r\n\r\n* fixed test fails\r\n", "commitUser": "fjy", "commitDateTime": "2016-05-02 22:37:28", "commitParents": ["6c5bf91f9a189fe1cec4511c2beb6d617aa6024a"], "commitGHEventType": "referenced", "nameRev": "2729fea84dd553631a907f4ff883345e9f388d54 tags/druid-0.9.1-rc1~65", "commitHash": "2729fea84dd553631a907f4ff883345e9f388d54"}], "body": "SelectQuery with pagingSpec breaks when datasource name contains DataSegment.delimiter i.e '_' e.g test_ds. \nRefer to SegmentDesc.valueOf() method which splits the segmentIdentifier and fails with dataSource name containing an underscore. \n\nStack trace for failure - \njava.lang.IllegalArgumentException: Invalid format: \"ds\"\n    at org.joda.time.format.DateTimeParserBucket.doParseMillis(DateTimeParserBucket.java:187)\n    at org.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:780)\n    at org.joda.time.convert.StringConverter.getInstantMillis(StringConverter.java:65)\n    at org.joda.time.base.BaseDateTime.<init>(BaseDateTime.java:175)\n    at org.joda.time.DateTime.<init>(DateTime.java:257)\n    at io.druid.segment.SegmentDesc.valueOf(SegmentDesc.java:52)\n"}, {"user": "jon-wei", "commits": {}, "labels": ["Bug"], "created": "2016-04-01 23:46:40", "title": "Inconsistent null/empty str handling for multi-value/single-value dimensions in QueryableIndexStorageAdapter", "url": "https://github.com/apache/druid/issues/2776", "closed": "2016-04-11 22:25:47", "ttf": 9.000277777777777, "commitsDetails": [], "body": "In the Cursor returned by QueryableIndexStorageAdapter, the DimensionSelectors created from the Cursor have different null handling behavior for multi-value and single-value dimensions.\n\nIn the implementation of DimensionSelector.lookupName(id):\n- For multi-value dimensions, Strings.nullToEmpty() is called on the value before it is returned or passed to the extractionFn.\n- For single-value dimensions, there is no null->empty str conversion.\n\nDimensionSelectors originating from an IncrementalIndex do not perform the null->empty str conversion either.\n"}, {"user": "jon-wei", "commits": {}, "labels": ["Bug"], "created": "2016-04-01 23:40:32", "title": "SelectorDimFilter optimize() does not work correctly with LookupExtractionFn missing value handling", "url": "https://github.com/apache/druid/issues/2775", "closed": "2016-04-11 22:24:52", "ttf": 9.000277777777777, "commitsDetails": [], "body": "If optimize() is called on a ExtractionDimFilter/SelectorDimFilter with a LookupExtractionFn, an incorrect filter will be returned in some cases.\n\nSuppose we have a single dimension, `dimA`, with rows:\n\n```\n{dimA = \"a\"}\n{dimA = \"b\"}\n{dimA = \"c\"}\n{dimA = \"d\"}.\n```\n\nSuppose we define a LookupExtractionFn with the following underlying map, with `retainMissingValues` set to `true`:\n\n```\n{\"a\" -> \"d\"}.\n```\n\nIf we define a selector/extraction filter that matches on \"d\" using the LookupExtractionFn above and call optimize() on the filter, the unapply() reverse-lookup will only pick up value \"a\". The optimize() step has no knowledge of the untransformed value \"d\", and the resulting InFilter will not match all the rows it needs to.\n\n---\n\nSimilarly, if `retainMissingValues` is false, and `replaceMissingValuesWith` has the same value as the selector value, optimize() will not be aware of all the row values that it needs.\n\nUsing the same example rows, suppose we have the following selector filter and lookup extraction:\n\n```\nselector: \n- value: \"b\"\nlookup:\n- map: \"a\" -> \"b\"\n- replaceMissingValuesWith = \"b\"\n```\n\nThis should match all rows, since \"a\" will be mapped to \"b\", and \"b\",\"c\",\"d\" are not in the lookup map and will be transformed to \"b\".\n\nIf we call optimize() on this filter, the resulting filter built from reverse-lookup will only select for value \"a\". It is not aware of all the other values that will be transformed to \"b\" via the `replaceMissingValuesWith` property.\n"}, {"user": "jon-wei", "commits": {}, "labels": ["Bug"], "created": "2016-04-01 23:20:24", "title": "DimensionPredicateFilter and JavaScriptFilter don't handle null columns properly", "url": "https://github.com/apache/druid/issues/2772", "closed": "2016-04-05 21:40:15", "ttf": 3.000277777777778, "commitsDetails": [], "body": "If a DimensionPredicateFilter or JavaScriptFilter is applied to a null column, these filters will currently return an empty bitmap, even if the filter matches on `null`.\n\nWhen the filter matches on `null`, these filters should return the complement of the empty bitmap.\n"}, {"user": "guobingkun", "commits": {}, "labels": ["Area - Web Console", "Bug", "Starter"], "created": "2016-03-28 15:57:17", "title": "Coordinator console doesn't work for dynamic config.", "url": "https://github.com/apache/druid/issues/2746", "closed": "2019-09-25 22:44:22", "ttf": 1276.0002777777777, "commitsDetails": [], "body": "Coordinator dynamic config doesn't work when you post a JSON object. \n\nFor example,\n\n![image](https://cloud.githubusercontent.com/assets/11789051/14082054/7e20dbf8-f4d3-11e5-99d8-45418858b0c0.png)\n\nPosting `[\"wikipedia\"]` to killDataSourceWhitelist will not work as expected. I think the reason is that Coordinator console automatically added quotes to `[\"wikipedia\"]`, which makes it \n\n![image](https://cloud.githubusercontent.com/assets/11789051/14082096/b700a21e-f4d3-11e5-9684-f2e244727c3b.png)\n"}, {"user": "navis", "commits": {}, "labels": ["Bug"], "created": "2016-03-24 06:46:35", "title": "WikipediaIrcDecoder is not working in master branch", "url": "https://github.com/apache/druid/issues/2717", "closed": "2016-03-26 06:08:38", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "looks like regression from #2646. seeing https://github.com/maxmind/GeoIP2-java/issues/57, we need upgrade version of jackson-binding to >= 2.7.0\n\n```\n2016-03-24T06:25:56,227 ERROR [task-runner-0-priority-0] io.druid.indexing.common.task.RealtimeIndexTask - Exception aborted realtime processing[wikipedia]: {class=io.druid.indexing.common.task.RealtimeIndexTask, exceptionType=class java.lang.IllegalArgumentException, exceptionMessage=Instantiation of [simple type, class io.druid.segment.realtime.firehose.WikipediaIrcDecoder] value failed: com.fasterxml.jackson.databind.node.ArrayNode.<init>(Lcom/fasterxml/jackson/databind/node/JsonNodeFactory;Ljava/util/List;)V}\njava.lang.IllegalArgumentException: Instantiation of [simple type, class io.druid.segment.realtime.firehose.WikipediaIrcDecoder] value failed: com.fasterxml.jackson.databind.node.ArrayNode.<init>(Lcom/fasterxml/jackson/databind/node/JsonNodeFactory;Ljava/util/List;)V\n        at com.fasterxml.jackson.databind.ObjectMapper._convert(ObjectMapper.java:2774) ~[jackson-databind-2.4.6.jar:2.4.6]\n        at com.fasterxml.jackson.databind.ObjectMapper.convertValue(ObjectMapper.java:2700) ~[jackson-databind-2.4.6.jar:2.4.6]\n        at io.druid.segment.indexing.DataSchema.getParser(DataSchema.java:101) ~[druid-server-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.indexing.common.task.RealtimeIndexTask.run(RealtimeIndexTask.java:336) [druid-indexing-service-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.indexing.overlord.ThreadPoolTaskRunner$ThreadPoolTaskRunnerCallable.call(ThreadPoolTaskRunner.java:411) [druid-indexing-service-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at io.druid.indexing.overlord.ThreadPoolTaskRunner$ThreadPoolTaskRunnerCallable.call(ThreadPoolTaskRunner.java:384) [druid-indexing-service-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_75]\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_75]\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_75]\n        at java.lang.Thread.run(Thread.java:745) [?:1.7.0_75]\nCaused by: com.fasterxml.jackson.databind.JsonMappingException: Instantiation of [simple type, class io.druid.segment.realtime.firehose.WikipediaIrcDecoder] value failed: com.fasterxml.jackson.databind.node.ArrayNode.<init>(Lcom/fasterxml/jackson/databind/node/JsonNodeFactory;Ljava/util/List;)V\n        at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.wrapException(StdValueInstantiator.java:405) ~[jackson-databind-2.4.6.jar:2.4.6]\n        at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromObjectWith(StdValueInstantiator.java:234) ~[jackson-databind-2.4.6.jar:2.4.6]\n        at com.fasterxml.jackson.databind.deser.impl.PropertyBasedCreator.build(PropertyBasedCreator.java:167) ~[jackson-databind-2.4.6.jar:2.4.6]\n        at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:398) ~[jackson-databind-2.4.6.jar:2.4.6]\n        at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1064) ~[jackson-databind-2.4.6.jar:2.4.6]\n        at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:264) ~[jackson-databind-2.4.6.jar:2.4.6]\n        at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:156) ~[jackson-databind-2.4.6.jar:2.4.6]\n        at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:126) ~[jackson-databind-2.4.6.jar:2.4.6]\n        at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:113) ~[jackson-databind-2.4.6.jar:2.4.6]\n        at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:84) ~[jackson-databind-2.4.6.jar:2.4.6]\n        at com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:132) ~[jackson-databind-2.4.6.jar:2.4.6]\n        at com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:536) ~[jackson-databind-2.4.6.jar:2.4.6]\n        at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:344) ~[jackson-databind-2.4.6.jar:2.4.6]\n        at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1064) ~[jackson-databind-2.4.6.jar:2.4.6]\n        at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:264) ~[jackson-databind-2.4.6.jar:2.4.6]\n        at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:156) ~[jackson-databind-2.4.6.jar:2.4.6]...\n```\n"}, {"user": "gianm", "commits": {"08c784fbf6425e9470d47baef2c2a86f96416e54": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "KafkaIndexTask non-deterministic maxRowsPerSegment handling", "numCommits": 0, "created": "2016-03-23 00:19:15", "closed": "2016-04-19 05:29:52", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/2703", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 27.00027777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIndexTask.java", "deletions": 7, "insertions": 13, "lines": 20}, {"filePath": "server/src/test/java/io/druid/segment/realtime/appenderator/FiniteAppenderatorDriverTest.java", "deletions": 2, "insertions": 5, "lines": 7}, {"filePath": "extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/test/TestBroker.java", "deletions": 0, "insertions": 2, "lines": 2}, {"filePath": "server/src/main/java/io/druid/segment/realtime/appenderator/FiniteAppenderatorDriverMetadata.java", "deletions": 9, "insertions": 10, "lines": 19}, {"filePath": "extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskTest.java", "deletions": 8, "insertions": 51, "lines": 59}, {"filePath": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIOConfig.java", "deletions": 6, "insertions": 6, "lines": 12}, {"filePath": "server/src/main/java/io/druid/segment/realtime/appenderator/FiniteAppenderatorDriver.java", "deletions": 61, "insertions": 102, "lines": 163}, {"filePath": "server/src/main/java/io/druid/segment/realtime/appenderator/SegmentAllocator.java", "deletions": 1, "insertions": 3, "lines": 4}, {"filePath": "indexing-service/src/main/java/io/druid/indexing/appenderator/ActionBasedSegmentAllocator.java", "deletions": 4, "insertions": 2, "lines": 6}], "commitSpoonAstDiffStats": [{"spoonFilePath": "TestBroker.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.test.TestBroker.start()", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "FiniteAppenderatorDriverMetadata.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriverMetadata.getActiveSegments()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 3, "DEL": 2, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriverMetadata.toString()", "MOV": 2, "TOT": 8}, {"INS": 0, "UPD": 3, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriverMetadata.getPreviousSegmentId()", "MOV": 0, "TOT": 3}, {"INS": 0, "UPD": 9, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriverMetadata", "MOV": 0, "TOT": 9}]}, {"spoonFilePath": "FiniteAppenderatorDriverTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriverTest.testSimple()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriverTest.TestSegmentAllocator.allocate(org.joda.time.DateTime,java.lang.String,java.lang.String)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriverTest", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriverTest.setUp()", "MOV": 1, "TOT": 3}]}, {"spoonFilePath": "FiniteAppenderatorDriver.java", "spoonMethods": [{"INS": 0, "UPD": 5, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver", "MOV": 0, "TOT": 5}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver.getActiveSegment(org.joda.time.DateTime,java.lang.String)", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 1, "DEL": 2, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver.startJob()", "MOV": 6, "TOT": 10}, {"INS": 2, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver.finish(io.druid.segment.realtime.appenderator.TransactionalSegmentPublisher,io.druid.data.input.Committer)", "MOV": 0, "TOT": 3}, {"INS": 2, "UPD": 1, "DEL": 2, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver.wrapCommitter(io.druid.data.input.Committer)", "MOV": 3, "TOT": 8}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver.wrapCommitter(io.druid.data.input.Committer).3", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver.getActiveSegment(org.joda.time.DateTime)", "MOV": 6, "TOT": 7}, {"INS": 0, "UPD": 4, "DEL": 3, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver.getSegment(org.joda.time.DateTime)", "MOV": 8, "TOT": 15}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver.identifiersToStrings(java.lang.Iterable).5", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver.publishAll(io.druid.segment.realtime.appenderator.TransactionalSegmentPublisher,io.druid.data.input.Committer)", "MOV": 3, "TOT": 7}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver.getActiveSegments()", "MOV": 2, "TOT": 3}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver.segmentsToIdentifiers(java.lang.Iterable)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 1, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver.getActiveSegmentCount()", "MOV": 1, "TOT": 3}, {"INS": 10, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver.getSegment(org.joda.time.DateTime,java.lang.String)", "MOV": 0, "TOT": 10}, {"INS": 5, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver.add(io.druid.data.input.InputRow,java.lang.String,com.google.common.base.Supplier)", "MOV": 0, "TOT": 5}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver.segmentsToIdentifiers(java.lang.Iterable).4", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver.identifiersToStrings(java.lang.Iterable)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "ActionBasedSegmentAllocator.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.indexing.appenderator.ActionBasedSegmentAllocator.allocate(org.joda.time.DateTime,java.lang.String)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 3, "spoonMethodName": "io.druid.indexing.appenderator.ActionBasedSegmentAllocator", "MOV": 0, "TOT": 3}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.appenderator.ActionBasedSegmentAllocator.allocate(org.joda.time.DateTime,java.lang.String,java.lang.String)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "SegmentAllocator.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.appenderator.allocate(org.joda.time.DateTime,java.lang.String,java.lang.String)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "KafkaIndexTaskTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.KafkaIndexTaskTest", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.KafkaIndexTaskTest.testRunOnNothing()", "MOV": 0, "TOT": 1}, {"INS": 5, "UPD": 4, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.KafkaIndexTaskTest.testRunOneTaskTwoPartitions()", "MOV": 4, "TOT": 13}]}, {"spoonFilePath": "KafkaIndexTask.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.indexing.kafka.KafkaIndexTask.newDriver(io.druid.segment.realtime.appenderator.Appenderator,io.druid.indexing.common.TaskToolbox)", "MOV": 1, "TOT": 2}, {"INS": 4, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.indexing.kafka.KafkaIndexTask.run(io.druid.indexing.common.TaskToolbox)", "MOV": 0, "TOT": 5}]}, {"spoonFilePath": "KafkaIOConfig.java", "spoonMethods": [{"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.KafkaIOConfig.getSequenceName()", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 6, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.KafkaIOConfig", "MOV": 0, "TOT": 6}, {"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.indexing.kafka.KafkaIOConfig.toString()", "MOV": 0, "TOT": 2}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-04-18 22:29:52", "commitMessage": "KafkaIndexTask: Use a separate sequence per Kafka partition in order to make (#2844)\n\nsegment creation deterministic.\r\n\r\nThis means that each segment will contain data from just one Kafka\r\npartition. So, users will probably not want to have a super high number\r\nof Kafka partitions...\r\n\r\nFixes #2703.", "commitUser": "fjy", "commitDateTime": "2016-04-18 22:29:52", "commitParents": ["7b65ca78893576f7d4f50ab9c87aad922cc739f5"], "commitGHEventType": "referenced", "nameRev": "08c784fbf6425e9470d47baef2c2a86f96416e54 tags/druid-0.9.1-rc1~99", "commitHash": "08c784fbf6425e9470d47baef2c2a86f96416e54"}], "body": "KafkaIndexTask's ability to do replicated ingestion depends on the replicas behaving deterministically -- the same messages should go into the same segments. This may not always happen, because while individual Kafka partitions are strongly ordered, we are not guaranteed that messages are interleaved in any particular order. So it's possible for two tasks to see:\n- task1: (p0, o1), (p0, o2), (p1, o1), (p1, o2)\n- task2: (p0, o1), (p1, o1), (p0, o2), (p1, o2)\n\nIf maxRowsPerSegment is 2 then different segments will be generated.\n\nAs far as I can tell- the only safe way to combine messages from different kafka partitions in the same realtime segment is to have some deterministic way of handling the above case. I think we can punt on that for now, and do the simple fix of isolating messages from different kafka partitions into different segments. So if you have two partitions in a task, you'll get at least two segments. We would have to tell users not to use an excessive number of kafka partitions as this will also lead to an excessive number of druid segments.\n"}, {"user": "rasahner", "commits": {"527b728f3eada8b468fa83a2b7b96147e0e8bc57": {"commitGHEventType": "referenced", "commitUser": "drcrallen"}, "25967d0ed89446c42bb52f90b3019ce56eeb00a0": {"commitGHEventType": "closed", "commitUser": "drcrallen"}}, "changesInPackagesGIT": ["server/src/main/java/io/druid/server"], "labels": ["Area - Querying", "Bug"], "spoonStatsSummary": {"spoonFilesChanged": 1, "spoonMethodsChanged": 2, "INS": 1, "UPD": 0, "DEL": 0, "MOV": 1, "TOT": 2}, "title": "integration test using router failed after #2674", "numCommits": 1, "created": "2016-03-18 13:37:52", "closed": "2016-03-19 16:38:39", "gitStatsSummary": {"deletions": 1, "insertions": 13, "lines": 14, "gitFilesChange": 1}, "statsSkippedReason": "", "changesInPackagesSPOON": ["io.druid.server.AsyncQueryForwardingServlet.destroy()", "io.druid.server.AsyncQueryForwardingServlet.init()"], "filteredCommits": ["25967d0ed89446c42bb52f90b3019ce56eeb00a0"], "url": "https://github.com/apache/druid/issues/2681", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 1.0002777777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "server/src/main/java/io/druid/server/AsyncQueryForwardingServlet.java", "deletions": 1, "insertions": 13, "lines": 14}], "commitSpoonAstDiffStats": [{"spoonFilePath": "AsyncQueryForwardingServlet.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServlet.destroy()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServlet.init()", "MOV": 1, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-03-19 09:38:31", "commitMessage": "Merge pull request #2689 from metamx/fix-broadcast-lockup\n\nfix servlet startup sequence, fixes #2681", "commitUser": "drcrallen", "commitDateTime": "2016-03-19 09:38:31", "commitParents": ["11b8d1ed70d7e79e831c82e68544636887fae34a", "25967d0ed89446c42bb52f90b3019ce56eeb00a0"], "commitGHEventType": "referenced", "nameRev": "527b728f3eada8b468fa83a2b7b96147e0e8bc57 tags/druid-0.9.1-rc1~188", "commitHash": "527b728f3eada8b468fa83a2b7b96147e0e8bc57"}, {"commitGitStats": [{"filePath": "server/src/main/java/io/druid/server/AsyncQueryForwardingServlet.java", "deletions": 1, "insertions": 13, "lines": 14}], "commitSpoonAstDiffStats": [{"spoonFilePath": "AsyncQueryForwardingServlet.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServlet.destroy()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServlet.init()", "MOV": 1, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2016-03-18 15:05:57", "commitMessage": "fix servlet startup sequence, fixes #2681\n", "commitUser": "drcrallen", "commitDateTime": "2016-03-18 15:06:15", "commitParents": ["52522eb0e00d606dbfdecef7c8ab9df759e493d6"], "commitGHEventType": "closed", "nameRev": "25967d0ed89446c42bb52f90b3019ce56eeb00a0 tags/druid-0.9.1-rc1~188^2", "commitHash": "25967d0ed89446c42bb52f90b3019ce56eeb00a0"}], "body": "After PR https://github.com/druid-io/druid/pull/2674, the integration test ITRealtimeIndexTaskTest failed because the first query it tried timed out.  The rest of the tests passed.  The failed test is the only one that uses the router.  The router log says this at the time the query was done:\n\n```\n2016-03-18T13:15:46,350 WARN [HttpClient@162774556-169-selector-ClientSelectorManager@1e835d66/2] org.eclipse.jetty.io.SelectorManager - Could not process key for channel java.nio.channels.SocketChannel[closed]\njava.util.concurrent.RejectedExecutionException: org.eclipse.jetty.client.PoolingHttpDestination$1@3165714f\n   at org.eclipse.jetty.util.thread.QueuedThreadPool.execute(QueuedThreadPool.java:360) ~[jetty-util-9.2.5.v20141112.jar:9.2.5.v20141112]\n   at org.eclipse.jetty.client.PoolingHttpDestination.failed(PoolingHttpDestination.java:59) ~[jetty-client-9.2.5.v20141112.jar:9.2.5.v20141112]\n   at org.eclipse.jetty.client.ConnectionPool$1.failed(ConnectionPool.java:112) ~[jetty-client-9.2.5.v20141112.jar:9.2.5.v20141112]        \n   at org.eclipse.jetty.client.AbstractHttpClientTransport.connectFailed(AbstractHttpClientTransport.java:126) ~[jetty-client-9.2.5.v20141112.jar:9.2.5.v20141112]\n   at org.eclipse.jetty.client.AbstractHttpClientTransport$ClientSelectorManager.connectionFailed(AbstractHttpClientTransport.java:169) ~[jetty-client-9.2.5.v20141112.jar:9.2.5.v20141112]        \n   at org.eclipse.jetty.io.SelectorManager$ManagedSelector$Connect.failed(SelectorManager.java:933) ~[jetty-io-9.2.5.v20141112.jar:9.2.5.v20141112]\n```\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2016-02-25 22:46:29", "title": "Chat Handler QoS parsing is not correct", "url": "https://github.com/apache/druid/issues/2551", "closed": "2016-02-25 23:49:24", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "https://github.com/druid-io/druid/pull/2503/files#r54178326\n\nThe parsing for the chat handler QoS introduced in #2503 is parsing the integer OF the property name, not of the value of the property.\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2016-02-25 16:15:46", "title": "Auto-detection of ports goes way beyond 65535", "url": "https://github.com/apache/druid/issues/2546", "closed": "2016-02-26 01:12:53", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "PortFinder currently goes up to `Integer.MAX_VALUE`. This is incorrect. Ports can only be 16 bits, so the maximum number is 0xFFFF\n"}, {"user": "boneill42", "commits": {}, "labels": ["Bug"], "created": "2016-02-25 16:14:51", "title": "Peon port assignment/re-use", "url": "https://github.com/apache/druid/issues/2545", "closed": "2016-02-25 17:02:31", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "Peon port assignment appears to increase infinitely.    The `PortFinder` discovers new ports, which it returns to the `ForkingTaskRunner`, but those ports are never put back in the pool and re-used.\n\nThis becomes a big issue if you have security groups that restrict the connectivity between machines.  Any small port range is quickly exhausted and you are forced to open up connectivity to the entire machine.\n\nWe will attempt to patch, and submit from our fork.\n"}, {"user": "gianm", "commits": {}, "labels": ["Bug"], "created": "2016-02-19 20:21:54", "title": "Half-ingested events are possible when exceptions are caught on IncrementalIndex.add", "url": "https://github.com/apache/druid/issues/2510", "closed": "2016-02-23 23:16:30", "ttf": 4.000277777777778, "commitsDetails": [], "body": "Pre-0.8.2, and post #2505, RealtimeManager catches ParseExceptions on `plumber.add`, increments the unparseable counter, and continues. Post #2505 RealtimeIndexTask does too. This can cause half-ingested events, because the ParseExceptions get thrown while the index is updating aggregators in its facts map. In this case:\n- other aggregators are not updated,\n- but a row does end up in the facts table (with potentially some aggregators updated and some not),\n- and the event is counted \"unparseable\" rather than \"processed\" which is strange as now the \"processed\" count does not match the number of events actually added to the index.\n\nOne example of when a ParseException can get thrown is if you have a longSum aggregator reading from a field in the InputRow that cannot be parsed as a number. (& other things like this)\n\nPossible better behaviors,\n1. abort realtime processing, like happened between 0.8.2 and #2505. Folks might have a hard time with this one, though, if they aren't validating their inputs before passing them to Druid.\n2. don't include the event in the index at all, and count it \"unparseable\". But this might be tough to implement given that when this exception is thrown, the facts table has already been partially updated.\n3. keep the same indexing behavior, but count the event as \"processed\" instead of \"unparseable\"\n4. catch ParseException on `agg.aggregate()` in OnheapIncrementalIndex, and move on to the next aggregator, and count the event as \"processed\".\n\npotential risk with options 3 & 4: inconsistent aggregator state, if any aggregator out there is implemented such that its state cannot be relied on after it encounters an exception during `aggregate`.\n"}, {"user": "nishantmonu51", "commits": {}, "labels": ["Bug"], "created": "2016-02-12 07:53:36", "title": "IncrementalIndexTest fails during testConcurrentAddRead for OffheapIncrementalIndex ", "url": "https://github.com/apache/druid/issues/2456", "closed": "2016-02-12 22:15:39", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "faced this on one of our build machines - \n\n```\nTests run: 10, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.274 sec <<< FAILURE! - in io.druid.segment.data.IncrementalIndexTest\ntestConcurrentAddRead[1](io.druid.segment.data.IncrementalIndexTest)  Time elapsed: 0.011 sec  <<< ERROR!\njava.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 3, Size: 4\n    at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)\n    at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)\n    at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\n    at io.druid.segment.data.IncrementalIndexTest.testConcurrentAddRead(IncrementalIndexTest.java:528)\nCaused by: java.lang.IndexOutOfBoundsException: Index: 3, Size: 4\n    at java.util.ArrayList.rangeCheck(ArrayList.java:635)\n    at java.util.ArrayList.get(ArrayList.java:411)\n    at io.druid.segment.incremental.OffheapIncrementalIndex.getMetricLongValue(OffheapIncrementalIndex.java:304)\n    at io.druid.segment.incremental.IncrementalIndexStorageAdapter$1$1$6.get(IncrementalIndexStorageAdapter.java:479)\n    at io.druid.query.aggregation.LongSumAggregator.aggregate(LongSumAggregator.java:60)\n    at io.druid.query.timeseries.TimeseriesQueryEngine$1.apply(TimeseriesQueryEngine.java:70)\n    at io.druid.query.timeseries.TimeseriesQueryEngine$1.apply(TimeseriesQueryEngine.java:54)\n    at io.druid.query.QueryRunnerHelper$1.apply(QueryRunnerHelper.java:80)\n    at io.druid.query.QueryRunnerHelper$1.apply(QueryRunnerHelper.java:75)\n    at com.metamx.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:39)\n    at com.metamx.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:39)\n    at com.metamx.common.guava.YieldingAccumulators$1.accumulate(YieldingAccumulators.java:32)\n    at com.metamx.common.guava.BaseSequence.makeYielder(BaseSequence.java:104)\n    at com.metamx.common.guava.BaseSequence.toYielder(BaseSequence.java:81)\n    at com.metamx.common.guava.BaseSequence.accumulate(BaseSequence.java:67)\n    at com.metamx.common.guava.MappedSequence.accumulate(MappedSequence.java:40)\n    at com.metamx.common.guava.MappedSequence.accumulate(MappedSequence.java:40)\n    at com.metamx.common.guava.FilteredSequence.accumulate(FilteredSequence.java:42)\n    at com.metamx.common.guava.MappedSequence.accumulate(MappedSequence.java:40)\n    at io.druid.segment.data.IncrementalIndexTest$4.run(IncrementalIndexTest.java:489)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\n```\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2016-02-11 02:28:22", "title": "Selfcontained jar gets put at end of dependency tree causing errors in some jobs", "url": "https://github.com/apache/druid/issues/2443", "closed": "2016-02-12 06:02:10", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "PullDeps won't consider excluding dependencies outside the stock exclude list. So if, for example, you have extra jars in the aws credentials extension who have different versions which are pulled in by... say... spark. Then when the hadoop isolating classloader tries to load up the hadoop dependencies and then the druid and then the extension ones, you'll end up with not the classes you want because they are either the wrong version or loaded in the wrong classloaders. This will lead to a nasty stacktrace like this:\n\n```\njava.lang.RuntimeException: java.lang.reflect.InvocationTargetException\n    at com.google.common.base.Throwables.propagate(Throwables.java:160) ~[druid-selfcontained-0.9.0-rc1-mmx2.jar:0.9.0-rc1-mmx2]\n    at io.druid.indexing.common.task.HadoopTask.invokeForeignLoader(HadoopTask.java:170) ~[druid-selfcontained-0.9.0-rc1-mmx2.jar:0.9.0-rc1-mmx2]\n    at io.druid.indexer.spark.SparkBatchIndexTask.run(SparkBatchIndexTask.scala:155) [druid-spark-batch_2.10-0.0.27.jar:0.0.27]\n    at io.druid.indexing.overlord.ThreadPoolTaskRunner$ThreadPoolTaskRunnerCallable.call(ThreadPoolTaskRunner.java:338) [druid-selfcontained-0.9.0-rc1-mmx2.jar:0.9.0-rc1-mmx2]\n    at io.druid.indexing.overlord.ThreadPoolTaskRunner$ThreadPoolTaskRunnerCallable.call(ThreadPoolTaskRunner.java:318) [druid-selfcontained-0.9.0-rc1-mmx2.jar:0.9.0-rc1-mmx2]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_72]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_72]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_72]\n    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_72]\nCaused by: java.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_72]\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_72]\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_72]\n    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_72]\n    at io.druid.indexing.common.task.HadoopTask.invokeForeignLoader(HadoopTask.java:167) ~[druid-selfcontained-0.9.0-rc1-mmx2.jar:0.9.0-rc1-mmx2]\n    ... 7 more\nCaused by: java.util.ServiceConfigurationError: io.druid.initialization.DruidModule: Provider io.druid.storage.s3.S3StorageDruidModule could not be instantiated\n    at java.util.ServiceLoader.fail(ServiceLoader.java:232) ~[?:1.8.0_72]\n    at java.util.ServiceLoader.access$100(ServiceLoader.java:185) ~[?:1.8.0_72]\n    at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384) ~[?:1.8.0_72]\n    at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) ~[?:1.8.0_72]\n    at java.util.ServiceLoader$1.next(ServiceLoader.java:480) ~[?:1.8.0_72]\n    at io.druid.initialization.Initialization.getFromExtensions(Initialization.java:132) ~[druid-selfcontained-0.9.0-rc1-mmx2.jar:0.9.0-rc1-mmx2]\n    at io.druid.initialization.Initialization.makeInjectorWithModules(Initialization.java:318) ~[druid-selfcontained-0.9.0-rc1-mmx2.jar:0.9.0-rc1-mmx2]\n    at io.druid.indexer.spark.SerializedJsonStatic$.liftedTree1$1(SparkDruidIndexer.scala:421) ~[?:?]\n    at io.druid.indexer.spark.SerializedJsonStatic$.injector$lzycompute(SparkDruidIndexer.scala:420) ~[?:?]\n    at io.druid.indexer.spark.SerializedJsonStatic$.injector(SparkDruidIndexer.scala:419) ~[?:?]\n    at io.druid.indexer.spark.SerializedJsonStatic$.liftedTree2$1(SparkDruidIndexer.scala:449) ~[?:?]\n    at io.druid.indexer.spark.SerializedJsonStatic$.mapper$lzycompute(SparkDruidIndexer.scala:448) ~[?:?]\n    at io.druid.indexer.spark.SerializedJsonStatic$.mapper(SparkDruidIndexer.scala:447) ~[?:?]\n    at io.druid.indexer.spark.SparkBatchIndexTask$.runTask(SparkBatchIndexTask.scala:277) ~[?:?]\n    at io.druid.indexer.spark.SparkBatchIndexTask.runTask(SparkBatchIndexTask.scala) ~[?:?]\n    at io.druid.indexer.spark.Runner.runTask(Runner.java:29) ~[?:?]\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_72]\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_72]\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_72]\n    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_72]\n    at io.druid.indexing.common.task.HadoopTask.invokeForeignLoader(HadoopTask.java:167) ~[druid-selfcontained-0.9.0-rc1-mmx2.jar:0.9.0-rc1-mmx2]\n    ... 7 more\nCaused by: java.lang.VerifyError: Bad type on operand stack\nException Details:\n  Location:\n    io/druid/storage/s3/S3StorageDruidModule.getRestS3Service(Lcom/amazonaws/auth/AWSCredentialsProvider;)Lorg/jets3t/service/impl/rest/httpclient/RestS3Service; @24: invokespecial\n  Reason:\n    Type 'io/druid/storage/s3/AWSSessionCredentialsAdapter' (current frame, stack[2]) is not assignable to 'org/jets3t/service/security/ProviderCredentials'\n  Current Frame:\n    bci: @24\n    flags: { }\n    locals: { 'io/druid/storage/s3/S3StorageDruidModule', 'com/amazonaws/auth/AWSCredentialsProvider' }\n    stack: { uninitialized 12, uninitialized 12, 'io/druid/storage/s3/AWSSessionCredentialsAdapter' }\n  Bytecode:\n    0x0000000: 2bb9 0027 0100 c100 2899 0013 bb00 2959\n    0x0000010: bb00 2a59 2bb7 002b b700 2cb0 bb00 2959\n    0x0000020: bb00 2d59 2bb9 0027 0100 b900 2e01 002b\n    0x0000030: b900 2701 00b9 002f 0100 b700 30b7 002c\n    0x0000040: b0                                     \n  Stackmap Table:\n    same_frame(@28)\n\n    at java.lang.Class.getDeclaredConstructors0(Native Method) ~[?:1.8.0_72]\n    at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671) ~[?:1.8.0_72]\n    at java.lang.Class.getConstructor0(Class.java:3075) ~[?:1.8.0_72]\n    at java.lang.Class.newInstance(Class.java:412) ~[?:1.8.0_72]\n    at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_72]\n    at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) ~[?:1.8.0_72]\n    at java.util.ServiceLoader$1.next(ServiceLoader.java:480) ~[?:1.8.0_72]\n    at io.druid.initialization.Initialization.getFromExtensions(Initialization.java:132) ~[druid-selfcontained-0.9.0-rc1-mmx2.jar:0.9.0-rc1-mmx2]\n    at io.druid.initialization.Initialization.makeInjectorWithModules(Initialization.java:318) ~[druid-selfcontained-0.9.0-rc1-mmx2.jar:0.9.0-rc1-mmx2]\n    at io.druid.indexer.spark.SerializedJsonStatic$.liftedTree1$1(SparkDruidIndexer.scala:421) ~[?:?]\n    at io.druid.indexer.spark.SerializedJsonStatic$.injector$lzycompute(SparkDruidIndexer.scala:420) ~[?:?]\n    at io.druid.indexer.spark.SerializedJsonStatic$.injector(SparkDruidIndexer.scala:419) ~[?:?]\n    at io.druid.indexer.spark.SerializedJsonStatic$.liftedTree2$1(SparkDruidIndexer.scala:449) ~[?:?]\n    at io.druid.indexer.spark.SerializedJsonStatic$.mapper$lzycompute(SparkDruidIndexer.scala:448) ~[?:?]\n    at io.druid.indexer.spark.SerializedJsonStatic$.mapper(SparkDruidIndexer.scala:447) ~[?:?]\n    at io.druid.indexer.spark.SparkBatchIndexTask$.runTask(SparkBatchIndexTask.scala:277) ~[?:?]\n    at io.druid.indexer.spark.SparkBatchIndexTask.runTask(SparkBatchIndexTask.scala) ~[?:?]\n    at io.druid.indexer.spark.Runner.runTask(Runner.java:29) ~[?:?]\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_72]\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_72]\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_72]\n    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_72]\n    at io.druid.indexing.common.task.HadoopTask.invokeForeignLoader(HadoopTask.java:167) ~[druid-selfcontained-0.9.0-rc1-mmx2.jar:0.9.0-rc1-mmx2]\n    ... 7 more\n```\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug"], "created": "2016-02-10 00:36:11", "title": "HadoopTask does not load extension jars into isolated classpath", "url": "https://github.com/apache/druid/issues/2428", "closed": "2016-02-10 21:16:52", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "HadoopTask's creation of the isolated classloader creation finds the extension jar URLs but does not actually use them in the classloader. It instead simply includes them as dependent jars.\n\nAs such any extension classes used in runTask will fail since they are not in the isolated classloader.\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug", "Starter"], "created": "2016-02-03 23:26:18", "title": "ExtractionDimFilter value NPE", "url": "https://github.com/apache/druid/issues/2391", "closed": "2016-02-04 01:47:14", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "`io.druid.query.filter.ExtractionDimFilter` accepts a null `value` but the cache key does not check for this. As such the query will fail during generating cache information with NPE at `byte[] valueBytes = StringUtils.toUtf8(value);`\n\n```\njava.lang.NullPointerException\\n\\tat com.metamx.common.StringUtils.toUtf8(StringUtils.java:62)\\n\\\n    \\tat io.druid.query.filter.ExtractionDimFilter.getCacheKey(ExtractionDimFilter.java:75)\\n\\\n    \\tat io.druid.query.timeseries.TimeseriesQueryQueryToolChest$4.computeCacheKey(TimeseriesQueryQueryToolChest.java:148)\\n\\\n    \\tat io.druid.query.timeseries.TimeseriesQueryQueryToolChest$4.computeCacheKey(TimeseriesQueryQueryToolChest.java:141)\\n\\\n    \\tat io.druid.client.CachingClusteredClient.run(CachingClusteredClient.java:189)\\n\\\n    \\tat io.druid.query.RetryQueryRunner.run(RetryQueryRunner.java:62)\\n\\tat io.druid.query.IntervalChunkingQueryRunner.run(IntervalChunkingQueryRunner.java:67)\\n\\\n    \\tat io.druid.query.UnionQueryRunner.run(UnionQueryRunner.java:65)\\n\\tat io.druid.query.ResultMergeQueryRunner.doRun(ResultMergeQueryRunner.java:41)\\n\\\n    \\tat io.druid.query.BySegmentSkippingQueryRunner.run(BySegmentSkippingQueryRunner.java:44)\\n\\\n    \\tat io.druid.query.FinalizeResultsQueryRunner.run(FinalizeResultsQueryRunner.java:100)\\n\\\n    \\tat io.druid.query.CPUTimeMetricQueryRunner.run(CPUTimeMetricQueryRunner.java:74)\\n\\\n    \\tat io.druid.query.BaseQuery.run(BaseQuery.java:78)\\n\\tat io.druid.query.BaseQuery.run(BaseQuery.java:73)\\n\\\n```\n"}, {"user": "fjy", "commits": {}, "labels": ["Bug"], "created": "2016-02-03 01:58:21", "title": "Overlord console generates NPE for scaling stats", "url": "https://github.com/apache/druid/issues/2378", "closed": "2016-02-03 02:45:03", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "In master right now when you visit the overlord console after just starting up an overlord:\n\n2016-02-02T19:33:46,283 WARN [qtp2041263297-48] org.eclipse.jetty.servlet.ServletHandler - /druid/indexer/v1/scaling\njava.lang.NullPointerException\n    at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:213) ~[guava-16.0.1.jar:?]\n    at com.google.common.base.Optional.of(Optional.java:85) ~[guava-16.0.1.jar:?]\n    at io.druid.indexing.overlord.RemoteTaskRunner.getScalingStats(RemoteTaskRunner.java:354) ~[druid-indexing-service-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]\n    at io.druid.indexing.overlord.TaskMaster.getScalingStats(TaskMaster.java:278) ~[druid-indexing-service-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]\n    at io.druid.indexing.overlord.http.OverlordResource.getScalingState(OverlordResource.java:436) ~[druid-indexing-service-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_65]\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_65]\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_65]\n    at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_65]\n"}, {"user": "himanshug", "commits": {}, "labels": ["Bug"], "created": "2016-01-25 20:45:11", "title": "introduce mechanism to limit response context size", "url": "https://github.com/apache/druid/issues/2331", "closed": "2016-02-02 18:06:59", "ttf": 7.000277777777778, "commitsDetails": [], "body": "druid query response can contain header \"X-Druid-Response-Context\" which is unbounded in size and can lead to exception like below.\n\n```\n2015-12-16T17:33:21,646 WARN [qtp296974277-44] org.eclipse.jetty.server.HttpChannel - /druid/v2/?pretty\njava.io.IOException: Response header too large\n    at org.eclipse.jetty.http.HttpGenerator.generateResponse(HttpGenerator.java:400) ~[jetty-http-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.HttpConnection$SendCallback.process(HttpConnection.java:599) ~[jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.util.IteratingCallback.processing(IteratingCallback.java:246) ~[jetty-util-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.util.IteratingCallback.iterate(IteratingCallback.java:208) ~[jetty-util-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.HttpConnection.send(HttpConnection.java:448) ~[jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.HttpChannel.sendResponse(HttpChannel.java:762) ~[jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.HttpChannel.write(HttpChannel.java:800) ~[jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.HttpOutput.write(HttpOutput.java:139) ~[jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.HttpOutput.write(HttpOutput.java:132) ~[jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.HttpOutput.write(HttpOutput.java:347) ~[jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n...\nCaused by: java.nio.BufferOverflowException\n    at java.nio.Buffer.nextPutIndex(Buffer.java:521) ~[?:1.8.0_51]\n    at java.nio.HeapByteBuffer.put(HeapByteBuffer.java:169) ~[?:1.8.0_51]\n    at org.eclipse.jetty.http.HttpGenerator.putSanitisedValue(HttpGenerator.java:1057) ~[jetty-http-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.http.HttpGenerator.putTo(HttpGenerator.java:1079) ~[jetty-http-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.http.HttpGenerator.generateHeaders(HttpGenerator.java:703) ~[jetty-http-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.http.HttpGenerator.generateResponse(HttpGenerator.java:385) ~[jetty-http-9.2.5.v20141112.jar:9.2.5.v20141112]\n    ... 74 more\n```\n\nthere should be a mechanism to configure the header size in jetty and also a way to enforce that limit w/o getting jetty exceptions and query failure.\n\nit was exposed in https://github.com/druid-io/druid/issues/2108\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2016-01-20 22:15:09", "title": "Fix overlord injection problems [master]", "url": "https://github.com/apache/druid/issues/2308", "closed": "2016-01-21 00:04:57", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "https://github.com/druid-io/druid/pull/1953 introduces regression where overlord startup fails with\n\n```\nException in thread \"main\" com.google.inject.CreationException: Guice creation errors:\n\n1) No implementation for java.util.concurrent.ScheduledExecutorService was bound.\n  while locating java.util.concurrent.ScheduledExecutorService\n    for parameter 9 at io.druid.indexing.overlord.RemoteTaskRunnerFactory.<init>(RemoteTaskRunnerFactory.java:72)\n  at io.druid.cli.CliOverlord$1.configureRunners(CliOverlord.java:201)\n\n2) No implementation for java.util.concurrent.ScheduledExecutorService was bound.\n  while locating java.util.concurrent.ScheduledExecutorService\n    for parameter 3 at io.druid.indexing.overlord.autoscaling.SimpleResourceManagementStrategy.<init>(SimpleResourceManagementStrategy.java:78)\n  at io.druid.cli.CliOverlord$1.configureAutoscale(CliOverlord.java:209)\n\n2 errors\n    at com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:448)\n    at com.google.inject.internal.InternalInjectorCreator.initializeStatically(InternalInjectorCreator.java:155)\n    at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:107)\n    at com.google.inject.Guice.createInjector(Guice.java:96)\n    at com.google.inject.Guice.createInjector(Guice.java:73)\n    at com.google.inject.Guice.createInjector(Guice.java:62)\n    at io.druid.initialization.Initialization.makeInjectorWithModules(Initialization.java:322)\n    at io.druid.cli.GuiceRunnable.makeInjector(GuiceRunnable.java:57)\n    at io.druid.cli.ServerRunnable.run(ServerRunnable.java:39)\n    at io.druid.cli.Main.main(Main.java:107)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)\n\n```\n"}, {"user": "himanshug", "commits": {}, "labels": ["Bug"], "created": "2016-01-13 16:18:42", "title": "chunkPeriod parameter in context with TopN query produces incorrect results", "url": "https://github.com/apache/druid/issues/2262", "closed": "2017-03-15 20:52:48", "ttf": 427.0002777777778, "commitsDetails": [], "body": "resultset seem to contain data only for last chunk instead of all.\n\nsee https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/druid-user/RmLszymJ3Ro/2SbW9tksBgAJ\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug", "Priority - High", "Starter"], "created": "2016-01-04 18:05:11", "title": "Azure Deep Storage incorrectly accounting segment size", "url": "https://github.com/apache/druid/issues/2195", "closed": "2016-01-08 01:49:19", "ttf": 3.000277777777778, "commitsDetails": [], "body": "As per the discussion at https://groups.google.com/d/msg/druid-user/Bt2wiMn282Q/C3tmZumvCwAJ , The Azure Deep Storage extension incorrectly uses the compressed data size instead of the uncompressed data size when publishing segment metadata.\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2015-12-18 19:34:02", "title": "Log4jShutdown.stop not idempotent", "url": "https://github.com/apache/druid/issues/2124", "closed": "2016-01-05 18:13:34", "ttf": 17.00027777777778, "commitsDetails": [], "body": "```\n2015-12-18T12:19:51,984 ERROR [Thread-77] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Exception when stopping method[public void io.druid.initialization.Log4jShutterDownerModule$Log4jShutterDowner.stop()] on object[io.druid.initialization.Log4jShutterDownerModule$Log4jShutterDowner@7675c171]\njava.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_66]\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_66]\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_66]\n    at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_66]\n    at com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler.stop(Lifecycle.java:337) [druid-selfcontained-0.8.3-rc2-mmx1.jar:0.8.3-rc2-mmx1]\n    at com.metamx.common.lifecycle.Lifecycle.stop(Lifecycle.java:261) [druid-selfcontained-0.8.3-rc2-mmx1.jar:0.8.3-rc2-mmx1]\n    at io.druid.cli.CliPeon$2.run(CliPeon.java:241) [druid-selfcontained-0.8.3-rc2-mmx1.jar:0.8.3-rc2-mmx1]\n    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_66]\nCaused by: java.lang.IllegalStateException: Expected state [STARTED] found [STOPPED]\n    at io.druid.common.config.Log4jShutdown.stop(Log4jShutdown.java:102) ~[druid-selfcontained-0.8.3-rc2-mmx1.jar:0.8.3-rc2-mmx1]\n    at io.druid.initialization.Log4jShutterDownerModule$Log4jShutterDowner.stop(Log4jShutterDownerModule.java:109) ~[druid-selfcontained-0.8.3-rc2-mmx1.jar:0.8.3-rc2-mmx1]\n    ... 8 more\n```\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2015-12-18 19:32:45", "title": "ServerDiscoverySelector.stop() not idempotent", "url": "https://github.com/apache/druid/issues/2123", "closed": "2016-01-05 18:13:46", "ttf": 17.00027777777778, "commitsDetails": [], "body": "```\n2015-12-18T12:19:51,982 ERROR [Thread-77] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Exception when stopping method[public void io.druid.curator.discovery.ServerDiscoverySelector.stop() throws java.io.IOException] on object[io.druid.curator.discovery.ServerDiscoverySelector@26e0d39c]\njava.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_66]\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_66]\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_66]\n    at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_66]\n    at com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler.stop(Lifecycle.java:337) [druid-selfcontained-0.8.3-rc2-mmx1.jar:0.8.3-rc2-mmx1]\n    at com.metamx.common.lifecycle.Lifecycle.stop(Lifecycle.java:261) [druid-selfcontained-0.8.3-rc2-mmx1.jar:0.8.3-rc2-mmx1]\n    at io.druid.cli.CliPeon$2.run(CliPeon.java:241) [druid-selfcontained-0.8.3-rc2-mmx1.jar:0.8.3-rc2-mmx1]\n    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_66]\nCaused by: java.lang.IllegalStateException: Already closed or has not been started\n    at com.google.common.base.Preconditions.checkState(Preconditions.java:176) ~[druid-selfcontained-0.8.3-rc2-mmx1.jar:0.8.3-rc2-mmx1]\n    at org.apache.curator.x.discovery.details.ServiceCacheImpl.close(ServiceCacheImpl.java:104) ~[druid-selfcontained-0.8.3-rc2-mmx1.jar:0.8.3-rc2-mmx1]\n    at org.apache.curator.x.discovery.details.ServiceProviderImpl.close(ServiceProviderImpl.java:78) ~[druid-selfcontained-0.8.3-rc2-mmx1.jar:0.8.3-rc2-mmx1]\n    at io.druid.curator.discovery.ServerDiscoverySelector.stop(ServerDiscoverySelector.java:122) ~[druid-selfcontained-0.8.3-rc2-mmx1.jar:0.8.3-rc2-mmx1]\n    ... 8 more\n```\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2015-12-18 19:31:20", "title": "ExecutorLifecycle stop not idempotent", "url": "https://github.com/apache/druid/issues/2122", "closed": "2016-01-05 18:14:02", "ttf": 17.00027777777778, "commitsDetails": [], "body": "The following error appears during shutdown now\n\n```\n2015-12-18T12:19:51,978 ERROR [Thread-77] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Exception when stopping method[public void io.druid.indexing.worker.executor.ExecutorLifecycle.stop() throws java.lang.Exception] on object[io.druid.indexing.worker.executor.ExecutorLifecycle@61911947]\njava.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_66]\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_66]\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_66]\n    at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_66]\n    at com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler.stop(Lifecycle.java:337) [druid-selfcontained-0.8.3-rc2-mmx1.jar:0.8.3-rc2-mmx1]\n    at com.metamx.common.lifecycle.Lifecycle.stop(Lifecycle.java:261) [druid-selfcontained-0.8.3-rc2-mmx1.jar:0.8.3-rc2-mmx1]\n    at io.druid.cli.CliPeon$2.run(CliPeon.java:241) [druid-selfcontained-0.8.3-rc2-mmx1.jar:0.8.3-rc2-mmx1]\n    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_66]\nCaused by: java.nio.channels.ClosedChannelException\n    at sun.nio.ch.FileLockImpl.release(FileLockImpl.java:58) ~[?:1.8.0_66]\n    at io.druid.indexing.worker.executor.ExecutorLifecycle.stop(ExecutorLifecycle.java:220) ~[druid-selfcontained-0.8.3-rc2-mmx1.jar:0.8.3-rc2-mmx1]\n    ... 8 more\n```\n"}, {"user": "jon-wei", "commits": {}, "labels": ["Bug"], "created": "2015-12-17 01:55:56", "title": "uncoveredIntervals can overflow query response header", "url": "https://github.com/apache/druid/issues/2108", "closed": "2016-01-28 05:04:35", "ttf": 42.000277777777775, "commitsDetails": [], "body": "CachingClusteredClient builds up a list of \"uncovered intervals\", intervals within the query interval that had no data in the underlying segments.\n\nThis list is returned as part of the response header in the \"X-Druid-Response-Context\" field.\n\nIf there are a large number of uncovered intervals, the response header may not be large enough to hold this list. On my local system I saw the header buffer had an 8KB size limit.\n\nI noticed this while running a SegmentMetadataQuery on a set of minute-granularity segments that only contained data for even minutes.\n\ne.g.\n\n```\n2015-12-16T17:33:21,646 WARN [qtp296974277-44] org.eclipse.jetty.server.HttpChannel - /druid/v2/?pretty\njava.io.IOException: Response header too large\n    at org.eclipse.jetty.http.HttpGenerator.generateResponse(HttpGenerator.java:400) ~[jetty-http-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.HttpConnection$SendCallback.process(HttpConnection.java:599) ~[jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.util.IteratingCallback.processing(IteratingCallback.java:246) ~[jetty-util-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.util.IteratingCallback.iterate(IteratingCallback.java:208) ~[jetty-util-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.HttpConnection.send(HttpConnection.java:448) ~[jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.HttpChannel.sendResponse(HttpChannel.java:762) ~[jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.HttpChannel.write(HttpChannel.java:800) ~[jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.HttpOutput.write(HttpOutput.java:139) ~[jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.HttpOutput.write(HttpOutput.java:132) ~[jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.server.HttpOutput.write(HttpOutput.java:347) ~[jetty-server-9.2.5.v20141112.jar:9.2.5.v20141112]\n...\nCaused by: java.nio.BufferOverflowException\n    at java.nio.Buffer.nextPutIndex(Buffer.java:521) ~[?:1.8.0_51]\n    at java.nio.HeapByteBuffer.put(HeapByteBuffer.java:169) ~[?:1.8.0_51]\n    at org.eclipse.jetty.http.HttpGenerator.putSanitisedValue(HttpGenerator.java:1057) ~[jetty-http-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.http.HttpGenerator.putTo(HttpGenerator.java:1079) ~[jetty-http-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.http.HttpGenerator.generateHeaders(HttpGenerator.java:703) ~[jetty-http-9.2.5.v20141112.jar:9.2.5.v20141112]\n    at org.eclipse.jetty.http.HttpGenerator.generateResponse(HttpGenerator.java:385) ~[jetty-http-9.2.5.v20141112.jar:9.2.5.v20141112]\n    ... 74 more\n```\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2015-10-20 15:56:49", "title": "Peon javaOpts are not quoted or escaped when passing to the Peon", "url": "https://github.com/apache/druid/issues/1841", "closed": "2015-10-21 18:15:36", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "Related to https://github.com/druid-io/druid/issues/1827\nAnd https://github.com/druid-io/druid/pull/1748\nBut the logic in `ForkingTaskRunner` as per\n\n```\nfor (String propName : props.stringPropertyNames()) {\n  for (String allowedPrefix : config.getAllowedPrefixes()) {\n    if (propName.startsWith(allowedPrefix)) {\n      command.add(String.format( \"-D%s=%s\", propName, props.getProperty(propName)));\n    }\n  }\n}\n```\n\nDoes not escape or quote the java options properly. This causes properties like `druid.indexer.runner.javaOpts=-server -Xmx256m -XX:OnOutOfMemoryError=\\\\\\\"kill -9 %p\\\\\\\"` to be placed on the java command line as `druid.indexer.runner.javaOpts=-server -Xmx256m -XX:OnOutOfMemoryError=\"kill -9 %p\"` which fails because\n\n```\n#\n# java.lang.OutOfMemoryError: Java heap space\n# -XX:OnOutOfMemoryError=\"\"kill -9 %p\"\"\n#   Executing /bin/sh -c \"\"kill -9 55925\"\"...\n```\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2015-10-14 16:44:25", "title": "JsonConfigurator does not escape quoted options", "url": "https://github.com/apache/druid/issues/1827", "closed": "2018-08-03 22:06:16", "ttf": 1024.0002777777777, "commitsDetails": [], "body": "`io/druid/druid-api/0.3.13/druid-api-0.3.13-sources.jar!/io/druid/guice/JsonConfigurator.java:85` (or there-abouts) puts property values in quotes, this makes https://github.com/druid-io/druid/pull/1748 have to have a lot of extra escape characters. So the runtime.properties file has to have something like this: `druid.indexer.runner.javaOpts=-server -Xmx256m -XX:OnOutOfMemoryError=\\\\\\\"kill -9 %p\\\\\\\"` in order to actually pick up the options properly whenever `value = jsonMapper.readValue(modifiedPropValue, Object.class);` is called.\n\nThe proper behavior would be to escape quotes in the modifiedPropValue when it encapsulates the property value in `\"%s\"`\n"}, {"user": "drcrallen", "commits": {"149333d77dac415c950b8c6c7c6546ecdb96f22d": {"commitGHEventType": "closed", "commitUser": "xvrl"}, "7d52a2a86e2e2089183aed10e1d869eb2cc9bc6c": {"commitGHEventType": "referenced", "commitUser": "drcrallen"}}, "changesInPackagesGIT": ["server/src/main/java/io/druid/server/router", "server/src/main/java/io/druid/server", "server/src/main/java/io/druid/curator/discovery"], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 4, "spoonMethodsChanged": 13, "INS": 16, "UPD": 14, "DEL": 5, "MOV": 28, "TOT": 63}, "title": "Query cancellation is not properly forwarded by Router", "numCommits": 1, "created": "2015-10-02 20:20:22", "closed": "2015-10-30 22:00:47", "gitStatsSummary": {"deletions": 51, "insertions": 153, "lines": 204, "gitFilesChange": 4}, "statsSkippedReason": "", "changesInPackagesSPOON": ["io.druid.server.AsyncQueryForwardingServlet.service(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)", "io.druid.server.AsyncQueryForwardingServlet.rewriteURI(javax.servlet.http.HttpServletRequest)", "io.druid.server.AsyncQueryForwardingServlet.makeURI(java.lang.String,java.lang.String,java.lang.String)", "io.druid.server.AsyncQueryForwardingServlet.rewriteURI(javax.servlet.http.HttpServletRequest,java.lang.String)", "io.druid.server.router.QueryHostFinder.getAllHosts()", "io.druid.server.router.TieredBrokerHostSelector.getAllBrokers()", "io.druid.curator.discovery.ServerDiscoverySelector.pick().1.getHost()", "io.druid.server.router.QueryHostFinder.findServer(io.druid.query.Query)", "io.druid.curator.discovery.ServerDiscoverySelector.getAll()", "io.druid.server.router.QueryHostFinder", "io.druid.curator.discovery.ServerDiscoverySelector.pick()", "io.druid.curator.discovery.ServerDiscoverySelector", "io.druid.server.router.QueryHostFinder.getHost(io.druid.query.Query)"], "filteredCommits": ["149333d77dac415c950b8c6c7c6546ecdb96f22d"], "url": "https://github.com/apache/druid/issues/1802", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 28.00027777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "server/src/main/java/io/druid/server/AsyncQueryForwardingServlet.java", "deletions": 20, "insertions": 65, "lines": 85}, {"filePath": "server/src/main/java/io/druid/server/router/QueryHostFinder.java", "deletions": 6, "insertions": 33, "lines": 39}, {"filePath": "server/src/test/java/io/druid/server/initialization/BaseJettyTest.java", "deletions": 0, "insertions": 9, "lines": 9}, {"filePath": "server/src/main/java/io/druid/server/router/TieredBrokerHostSelector.java", "deletions": 0, "insertions": 6, "lines": 6}, {"filePath": "server/src/main/java/io/druid/curator/discovery/ServerDiscoverySelector.java", "deletions": 25, "insertions": 49, "lines": 74}, {"filePath": "server/src/test/java/io/druid/server/AsyncQueryForwardingServletTest.java", "deletions": 3, "insertions": 86, "lines": 89}], "commitSpoonAstDiffStats": [{"spoonFilePath": "BaseJettyTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.initialization.BaseJettyTest.DefaultResource.delete()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "AsyncQueryForwardingServlet.java", "spoonMethods": [{"INS": 3, "UPD": 4, "DEL": 2, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServlet.service(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)", "MOV": 4, "TOT": 13}, {"INS": 1, "UPD": 7, "DEL": 1, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServlet.rewriteURI(javax.servlet.http.HttpServletRequest)", "MOV": 18, "TOT": 27}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServlet.rewriteURI(javax.servlet.http.HttpServletRequest,java.lang.String)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServlet.makeURI(java.lang.String,java.lang.String,java.lang.String)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "QueryHostFinder.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.router.QueryHostFinder.findServer(io.druid.query.Query)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.router.QueryHostFinder.getAllHosts()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.server.router.QueryHostFinder", "MOV": 0, "TOT": 1}, {"INS": 4, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.server.router.QueryHostFinder.getHost(io.druid.query.Query)", "MOV": 1, "TOT": 6}]}, {"spoonFilePath": "TieredBrokerHostSelector.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.router.TieredBrokerHostSelector.getAllBrokers()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "AsyncQueryForwardingServletTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServletTest.testDeleteBroadcast()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServletTest.makeTestDeleteServer(int,java.util.concurrent.CountDownLatch)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServletTest.testRewriteURI()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServletTest.ProxyJettyServerInit.initialize(org.eclipse.jetty.server.Server,com.google.inject.Injector).1.getAllHosts()", "MOV": 0, "TOT": 1}, {"INS": 3, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServletTest.ProxyJettyServerInit.initialize(org.eclipse.jetty.server.Server,com.google.inject.Injector).2.rewriteURI(javax.servlet.http.HttpServletRequest,java.lang.String)", "MOV": 0, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServletTest.ProxyJettyServerInit.initialize(org.eclipse.jetty.server.Server,com.google.inject.Injector)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 2, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServletTest.ProxyJettyServerInit.initialize(org.eclipse.jetty.server.Server,com.google.inject.Injector).2.rewriteURI(javax.servlet.http.HttpServletRequest)", "MOV": 3, "TOT": 6}]}, {"spoonFilePath": "ServerDiscoverySelector.java", "spoonMethods": [{"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.curator.discovery.ServerDiscoverySelector.pick().1.getHost()", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.curator.discovery.ServerDiscoverySelector.pick()", "MOV": 5, "TOT": 7}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.curator.discovery.ServerDiscoverySelector.getAll()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.curator.discovery.ServerDiscoverySelector", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2015-10-08 21:50:32", "commitMessage": "forward cancellation request to all brokers, fixes #1802\n", "commitUser": "xvrl", "commitDateTime": "2015-10-29 16:58:38", "commitParents": ["56e0709da9d2a80a7be4de74c22950141fc5d439"], "commitGHEventType": "closed", "nameRev": "149333d77dac415c950b8c6c7c6546ecdb96f22d tags/druid-0.9.0-rc1~295^2", "commitHash": "149333d77dac415c950b8c6c7c6546ecdb96f22d"}, {"commitGitStats": [{"filePath": "server/src/main/java/io/druid/server/AsyncQueryForwardingServlet.java", "deletions": 20, "insertions": 65, "lines": 85}, {"filePath": "server/src/main/java/io/druid/server/router/QueryHostFinder.java", "deletions": 6, "insertions": 33, "lines": 39}, {"filePath": "server/src/test/java/io/druid/server/initialization/BaseJettyTest.java", "deletions": 0, "insertions": 9, "lines": 9}, {"filePath": "server/src/main/java/io/druid/server/router/TieredBrokerHostSelector.java", "deletions": 0, "insertions": 6, "lines": 6}, {"filePath": "server/src/main/java/io/druid/curator/discovery/ServerDiscoverySelector.java", "deletions": 25, "insertions": 49, "lines": 74}, {"filePath": "server/src/test/java/io/druid/server/AsyncQueryForwardingServletTest.java", "deletions": 3, "insertions": 86, "lines": 89}], "commitSpoonAstDiffStats": [{"spoonFilePath": "BaseJettyTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.initialization.BaseJettyTest.DefaultResource.delete()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "AsyncQueryForwardingServlet.java", "spoonMethods": [{"INS": 3, "UPD": 4, "DEL": 2, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServlet.service(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)", "MOV": 4, "TOT": 13}, {"INS": 1, "UPD": 7, "DEL": 1, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServlet.rewriteURI(javax.servlet.http.HttpServletRequest)", "MOV": 18, "TOT": 27}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServlet.rewriteURI(javax.servlet.http.HttpServletRequest,java.lang.String)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServlet.makeURI(java.lang.String,java.lang.String,java.lang.String)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "QueryHostFinder.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.router.QueryHostFinder.findServer(io.druid.query.Query)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.router.QueryHostFinder.getAllHosts()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.server.router.QueryHostFinder", "MOV": 0, "TOT": 1}, {"INS": 4, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.server.router.QueryHostFinder.getHost(io.druid.query.Query)", "MOV": 1, "TOT": 6}]}, {"spoonFilePath": "TieredBrokerHostSelector.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.router.TieredBrokerHostSelector.getAllBrokers()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "AsyncQueryForwardingServletTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServletTest.testDeleteBroadcast()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServletTest.makeTestDeleteServer(int,java.util.concurrent.CountDownLatch)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServletTest.testRewriteURI()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServletTest.ProxyJettyServerInit.initialize(org.eclipse.jetty.server.Server,com.google.inject.Injector).1.getAllHosts()", "MOV": 0, "TOT": 1}, {"INS": 3, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServletTest.ProxyJettyServerInit.initialize(org.eclipse.jetty.server.Server,com.google.inject.Injector).2.rewriteURI(javax.servlet.http.HttpServletRequest,java.lang.String)", "MOV": 0, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServletTest.ProxyJettyServerInit.initialize(org.eclipse.jetty.server.Server,com.google.inject.Injector)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 2, "spoonMethodName": "io.druid.server.AsyncQueryForwardingServletTest.ProxyJettyServerInit.initialize(org.eclipse.jetty.server.Server,com.google.inject.Injector).2.rewriteURI(javax.servlet.http.HttpServletRequest)", "MOV": 3, "TOT": 6}]}, {"spoonFilePath": "ServerDiscoverySelector.java", "spoonMethods": [{"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.curator.discovery.ServerDiscoverySelector.pick().1.getHost()", "MOV": 0, "TOT": 2}, {"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.curator.discovery.ServerDiscoverySelector.pick()", "MOV": 5, "TOT": 7}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.curator.discovery.ServerDiscoverySelector.getAll()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.curator.discovery.ServerDiscoverySelector", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2015-10-30 15:00:45", "commitMessage": "Merge pull request #1888 from metamx/router-cancellation-forward\n\nforward cancellation request to all brokers, fixes #1802", "commitUser": "drcrallen", "commitDateTime": "2015-10-30 15:00:45", "commitParents": ["25a0eb7ed5b60a42c4a6a7d8f1459e5cc10c4f71", "149333d77dac415c950b8c6c7c6546ecdb96f22d"], "commitGHEventType": "referenced", "nameRev": "7d52a2a86e2e2089183aed10e1d869eb2cc9bc6c tags/druid-0.9.0-rc1~295", "commitHash": "7d52a2a86e2e2089183aed10e1d869eb2cc9bc6c"}], "body": "`io.druid.server.AsyncQueryForwardingServlet` does not attempt to make sure a query cancellation is received by proper broker. Instead of forwarding the cancellation DELETE to the proper broker, it will choose the default broker and send the delete to that one.\n"}, {"user": "jtmhom88", "commits": {}, "labels": ["Bug"], "created": "2015-09-29 20:36:32", "title": "IngestSegmentFirehose fails to read all rows from segment", "url": "https://github.com/apache/druid/issues/1785", "closed": "2015-10-02 16:27:22", "ttf": 2.000277777777778, "commitsDetails": [], "body": "The situation is that I tried delta ingestion with 0.8.1 stable release.\nI used the following spec to do the \"multi\" ingestion.\n\n``` json\n\"ioConfig\" : {\n    \"type\" : \"hadoop\",\n    \"inputSpec\" : {\n      \"type\" : \"multi\",\n      \"children\": [\n        {\n            \"type\" : \"dataSource\",\n            \"ingestionSpec\" : {\n                \"dataSource\": \"events\",\n                \"interval\": \"2015-09-13T03:00:00/PT3H\"\n            }\n        },\n        {\n            \"type\" : \"static\",\n            \"paths\": \"hdfs://ipaddress/file1.gz,hdfs://ipaddress/file2.gz,hdfs://ipaddress/file3.gz, ...\"\n        }\n      ]\n    },\n```\n\nHere is a basic description of the problem.\n1. I have 10 data files\n2. I static batch ingest 10 of them\n3. Issue a count query or a longsum query. \n10files, count:16311627, longsum:17826751\n4. Use 7 of the 10 files, batch ingest them.\n5. Issue a count/longsum query.\n7files, count: 12428400, longsum: 13580744\n6. Then take the 3 remaining files and delta ingest them.\n7. Issue count/longsum query.\n7+3 multi count:4559291, longsum:5062801\nNotice that not only do the counts not match up, but they drop by 40%.\nIf you then look at the console, you will notice a concurrent drop in the segment size by the same amount.\n\nI have data files to reproduce the bug, but I would like to send them via email and not post them publicly.\nPlease send me a request to jhom@kochava.com and I will send them to you with instructions on how to reproduce the issue.\n"}, {"user": "gianm", "commits": {}, "labels": ["Bug"], "created": "2015-09-26 21:20:34", "title": "SegmentMetadataQuery cache key must include analysisTypes", "url": "https://github.com/apache/druid/issues/1779", "closed": "2015-09-30 15:03:32", "ttf": 3.000277777777778, "commitsDetails": [], "body": "Currently it does not include the analysisTypes, so cached results may erroneously be returned.\n"}, {"user": "gianm", "commits": {"2d847ad6548d85fbacd5f177dea08c69186f3201": {"commitGHEventType": "referenced", "commitUser": "drcrallen"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "Union bySegment queries can get confused about which descriptors to use", "numCommits": 0, "created": "2015-09-13 19:16:34", "closed": "2015-09-29 19:23:25", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/1727", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 16.00027777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "common/src/test/java/io/druid/timeline/VersionedIntervalTimelineTest.java", "deletions": 75, "insertions": 0, "lines": 75}, {"filePath": "docs/content/querying/datasource.md", "deletions": 0, "insertions": 1, "lines": 1}, {"filePath": "processing/src/test/java/io/druid/query/QueryRunnerTestHelper.java", "deletions": 21, "insertions": 9, "lines": 30}, {"filePath": "processing/src/test/java/io/druid/query/UnionQueryRunnerTest.java", "deletions": 0, "insertions": 86, "lines": 86}, {"filePath": "server/src/main/java/io/druid/client/BrokerServerView.java", "deletions": 20, "insertions": 3, "lines": 23}, {"filePath": "common/src/main/java/io/druid/timeline/UnionTimeLineLookup.java", "deletions": 78, "insertions": 0, "lines": 78}, {"filePath": "processing/src/test/java/io/druid/query/timeseries/TimeSeriesUnionQueryRunnerTest.java", "deletions": 85, "insertions": 70, "lines": 155}, {"filePath": "server/src/main/java/io/druid/server/ClientQuerySegmentWalker.java", "deletions": 4, "insertions": 8, "lines": 12}, {"filePath": "server/src/main/java/io/druid/server/coordination/ServerManager.java", "deletions": 23, "insertions": 11, "lines": 34}, {"filePath": "processing/src/main/java/io/druid/query/UnionQueryRunner.java", "deletions": 13, "insertions": 14, "lines": 27}, {"filePath": "indexing-service/src/main/java/io/druid/indexing/overlord/ThreadPoolTaskRunner.java", "deletions": 33, "insertions": 19, "lines": 52}, {"filePath": "server/src/main/java/io/druid/segment/realtime/RealtimeManager.java", "deletions": 28, "insertions": 17, "lines": 45}], "commitSpoonAstDiffStats": [{"spoonFilePath": "RealtimeManager.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 1, "spoonMethodName": "io.druid.segment.realtime.RealtimeManager.getQueryRunnerForSegments(io.druid.query.Query,java.lang.Iterable).1.apply(java.lang.String)", "MOV": 2, "TOT": 4}, {"INS": 1, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.segment.realtime.RealtimeManager.getQueryRunnerForSegments(io.druid.query.Query,java.lang.Iterable)", "MOV": 1, "TOT": 4}]}, {"spoonFilePath": "ServerManager.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.server.coordination.ServerManager.buildAndDecorateQueryRunner(io.druid.query.QueryRunnerFactory,io.druid.query.QueryToolChest,io.druid.segment.ReferenceCountingSegment,io.druid.query.SegmentDescriptor,com.google.common.base.Function,java.util.concurrent.atomic.AtomicLong)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 4, "DEL": 0, "spoonMethodName": "io.druid.server.coordination.ServerManager.getQueryRunnerForSegments(io.druid.query.Query,java.lang.Iterable)", "MOV": 1, "TOT": 6}, {"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.server.coordination.ServerManager.getTimelineLookup(io.druid.query.DataSource)", "MOV": 3, "TOT": 5}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.server.coordination.ServerManager.buildAndDecorateQueryRunner(io.druid.query.QueryRunnerFactory,io.druid.query.QueryToolChest,io.druid.segment.ReferenceCountingSegment,io.druid.query.SegmentDescriptor,com.google.common.base.Function,java.util.concurrent.atomic.AtomicLong).5", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.server.coordination.ServerManager.getBuilderFn(io.druid.query.QueryToolChest).6", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.server.coordination.ServerManager.getBuilderFn(io.druid.query.QueryToolChest)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.coordination.ServerManager.getTimelineLookup(io.druid.query.DataSource).3.apply(java.lang.String)", "MOV": 1, "TOT": 1}, {"INS": 2, "UPD": 4, "DEL": 1, "spoonMethodName": "io.druid.server.coordination.ServerManager.getQueryRunnerForIntervals(io.druid.query.Query,java.lang.Iterable)", "MOV": 3, "TOT": 10}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.server.coordination.ServerManager.getQueryRunnerForSegments(io.druid.query.Query,java.lang.Iterable).4", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.server.coordination.ServerManager.getQueryRunnerForSegments(io.druid.query.Query,java.lang.Iterable).4.apply(io.druid.query.SegmentDescriptor)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "UnionQueryRunnerTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.UnionQueryRunnerTest", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "ClientQuerySegmentWalker.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.ClientQuerySegmentWalker.makeRunner(io.druid.query.Query)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "TimeSeriesUnionQueryRunnerTest.java", "spoonMethods": [{"INS": 0, "UPD": 3, "DEL": 0, "spoonMethodName": "io.druid.query.timeseries.TimeSeriesUnionQueryRunnerTest.testUnionResultMerging().2.run(io.druid.query.Query,java.util.Map)", "MOV": 3, "TOT": 6}, {"INS": 0, "UPD": 1, "DEL": 1, "spoonMethodName": "io.druid.query.timeseries.TimeSeriesUnionQueryRunnerTest.testUnionResultMerging()", "MOV": 1, "TOT": 3}, {"INS": 1, "UPD": 3, "DEL": 2, "spoonMethodName": "io.druid.query.timeseries.TimeSeriesUnionQueryRunnerTest.testUnionResultMerging().1.run(io.druid.query.Query,java.util.Map)", "MOV": 1, "TOT": 7}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.timeseries.TimeSeriesUnionQueryRunnerTest.testUnionResultMerging().1", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "BrokerServerView.java", "spoonMethods": [{"INS": 1, "UPD": 4, "DEL": 1, "spoonMethodName": "io.druid.client.BrokerServerView.getTimeline(io.druid.query.DataSource)", "MOV": 3, "TOT": 9}]}, {"spoonFilePath": "UnionQueryRunner.java", "spoonMethods": [{"INS": 4, "UPD": 2, "DEL": 3, "spoonMethodName": "io.druid.query.UnionQueryRunner.run(io.druid.query.Query,java.util.Map)", "MOV": 1, "TOT": 10}, {"INS": 0, "UPD": 6, "DEL": 0, "spoonMethodName": "io.druid.query.UnionQueryRunner", "MOV": 0, "TOT": 6}, {"INS": 0, "UPD": 2, "DEL": 1, "spoonMethodName": "io.druid.query.UnionQueryRunner.run(io.druid.query.Query,java.util.Map).1.apply(io.druid.query.QueryRunner)", "MOV": 1, "TOT": 4}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.UnionQueryRunner.run(io.druid.query.Query,java.util.Map).1.apply(io.druid.query.DataSource)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "QueryRunnerTestHelper.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.QueryRunnerTestHelper.NoopIntervalChunkingQueryRunnerDecorator().6", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.QueryRunnerTestHelper.makeUnionQueryRunner(io.druid.query.QueryRunnerFactory,io.druid.segment.Segment,io.druid.query.DataSource).5.apply(java.lang.String)", "MOV": 1, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 4, "spoonMethodName": "io.druid.query.QueryRunnerTestHelper.makeUnionQueryRunner(io.druid.query.QueryRunnerFactory,io.druid.segment.Segment,io.druid.query.DataSource)", "MOV": 0, "TOT": 4}, {"INS": 0, "UPD": 0, "DEL": 4, "spoonMethodName": "io.druid.query.QueryRunnerTestHelper.makeUnionQueryRunners(io.druid.query.QueryRunnerFactory,io.druid.query.DataSource)", "MOV": 0, "TOT": 4}]}, {"spoonFilePath": "UnionTimeLineLookup.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.timeline.UnionTimeLineLookup", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "ThreadPoolTaskRunner.java", "spoonMethods": [{"INS": 3, "UPD": 0, "DEL": 2, "spoonMethodName": "io.druid.indexing.overlord.ThreadPoolTaskRunner.getQueryRunnerImpl(io.druid.query.Query)", "MOV": 3, "TOT": 8}, {"INS": 0, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.overlord.ThreadPoolTaskRunner.getQueryRunnerImpl(io.druid.query.Query).2.apply(java.lang.String)", "MOV": 4, "TOT": 4}]}, {"spoonFilePath": "VersionedIntervalTimelineTest.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.timeline.VersionedIntervalTimelineTest.testUnionTimeLineLookup()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.timeline.VersionedIntervalTimelineTest.testUnionTimeLineLookupNonExistentDelegates()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.timeline.VersionedIntervalTimelineTest.testUnionTimeLineLookupReturnsSortedValues()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2015-09-29 12:23:25", "commitMessage": "Merge pull request #1730 from metamx/union-queries-fix\n\nfix #1727 - Union bySegment queries fix", "commitUser": "drcrallen", "commitDateTime": "2015-09-29 12:23:25", "commitParents": ["eb48a9eb819f66fb29fe6643e1792ede366576ef", "573aa96bd6d84838025792fde22e85b270c24e67"], "commitGHEventType": "referenced", "nameRev": "2d847ad6548d85fbacd5f177dea08c69186f3201 tags/druid-0.8.2-rc1~23", "commitHash": "2d847ad6548d85fbacd5f177dea08c69186f3201"}], "body": "When historicals get a union query with multiple datasources, they can get confused and scan segments other than the ones the broker intended. This is because they use `UnionTimeLineLookup.findEntry(interval, version)` to resolve the descriptors into Segments, but it's possible for segments in different datasources to have the same interval and version. This is actually always going to happen if the datasources were ingested with standalone realtime, since the version is fixed to the start of the hour.\n\nThis shows a couple of union queries where one segment is scanned twice and one is not scanned at all (which one is scanned depends on the order of the datasources; see union.json vs union2.json): https://gist.github.com/gianm/fc3a1eba0dbafbf87720\n"}, {"user": "drcrallen", "commits": {"d2e400f0636f816b00000e16ca23f5491459248f": {"commitGHEventType": "referenced", "commitUser": "drcrallen"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "Zombie tasks able to acquire locks after failure", "numCommits": 0, "created": "2015-09-09 23:11:59", "closed": "2015-09-29 16:38:42", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/1715", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 19.00027777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "indexing-service/src/test/java/io/druid/indexing/overlord/TaskLockboxTest.java", "deletions": 0, "insertions": 100, "lines": 100}, {"filePath": "indexing-service/src/test/java/io/druid/indexing/overlord/http/OverlordResourceTest.java", "deletions": 1, "insertions": 11, "lines": 12}, {"filePath": "indexing-service/src/main/java/io/druid/indexing/overlord/TaskLockbox.java", "deletions": 24, "insertions": 47, "lines": 71}, {"filePath": "indexing-service/src/main/java/io/druid/indexing/overlord/TaskQueue.java", "deletions": 10, "insertions": 54, "lines": 64}], "commitSpoonAstDiffStats": [{"spoonFilePath": "OverlordResourceTest.java", "spoonMethods": [{"INS": 6, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.overlord.http.OverlordResourceTest.setUp()", "MOV": 0, "TOT": 7}]}, {"spoonFilePath": "TaskLockboxTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.overlord.TaskLockboxTest", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "TaskLockbox.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.overlord.TaskLockbox.add(io.druid.indexing.common.task.Task)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexing.overlord.TaskLockbox.unlock(io.druid.indexing.common.task.Task)", "MOV": 1, "TOT": 2}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.overlord.TaskLockbox", "MOV": 0, "TOT": 1}, {"INS": 3, "UPD": 1, "DEL": 3, "spoonMethodName": "io.druid.indexing.overlord.TaskLockbox.syncFromStorage()", "MOV": 1, "TOT": 8}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.overlord.TaskLockbox.remove(io.druid.indexing.common.task.Task)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.overlord.TaskLockbox.tryLock(io.druid.indexing.common.task.Task,org.joda.time.Interval,com.google.common.base.Optional)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "TaskQueue.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.overlord.TaskQueue.toTaskIDMap(java.util.List)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.overlord.TaskQueue.addTaskInternal(io.druid.indexing.common.task.Task)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 1, "spoonMethodName": "io.druid.indexing.overlord.TaskQueue.notifyStatus(io.druid.indexing.common.task.Task,io.druid.indexing.common.TaskStatus)", "MOV": 3, "TOT": 5}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.overlord.TaskQueue.removeTaskInternal(io.druid.indexing.common.task.Task)", "MOV": 0, "TOT": 1}, {"INS": 10, "UPD": 6, "DEL": 1, "spoonMethodName": "io.druid.indexing.overlord.TaskQueue.syncFromStorage()", "MOV": 8, "TOT": 25}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.overlord.TaskQueue.add(io.druid.indexing.common.task.Task)", "MOV": 1, "TOT": 2}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2015-09-29 09:38:42", "commitMessage": "Merge pull request #1740 from metamx/validate-locks\n\nfix #1715", "commitUser": "drcrallen", "commitDateTime": "2015-09-29 09:38:42", "commitParents": ["25bbc0b92303200991148ed5f9d68f808d695f7b", "b638400acb726d98bfd8462ee69425d087906089"], "commitGHEventType": "referenced", "nameRev": "d2e400f0636f816b00000e16ca23f5491459248f tags/druid-0.8.2-rc1~26", "commitHash": "d2e400f0636f816b00000e16ca23f5491459248f"}], "body": "We had an issue where disconnect somewhere occurred long enough for the overlord to log a task as failed, but the task was still running. After 45 mins or so the task submitted a lock acquire request and it was granted, but since the task had already been failed, it never was properly cleaned up.\n\nThese are from the overlord\n\n```\n2015-09-09T07:33:58,102 INFO [RemoteTaskRunner-Scheduled-Cleanup--0] io.druid.indexing.overlord.RemoteTaskRunner - Running scheduled cleanup for Worker[REDACTED:8080]\n2015-09-09T07:33:58,106 INFO [RemoteTaskRunner-Scheduled-Cleanup--0] io.druid.indexing.overlord.RemoteTaskRunner - Failing task[index_realtime_REDACTED]\n2015-09-09T07:33:58,106 INFO [RemoteTaskRunner-Scheduled-Cleanup--0] io.druid.indexing.overlord.TaskQueue - Received FAILED status for task: index_realtime_REDACTED\n2015-09-09T07:33:58,109 INFO [RemoteTaskRunner-Scheduled-Cleanup--0] io.druid.indexing.overlord.RemoteTaskRunner - Can't shutdown! No worker running task index_realtime_REDACTED\n2015-09-09T07:33:58,110 INFO [RemoteTaskRunner-Scheduled-Cleanup--0] io.druid.indexing.overlord.MetadataTaskStorage - Updating task index_realtime_REDACTED to status: TaskStatus{id=index_realtime_REDACTED, status=FAILED, duration=-1}\n2015-09-09T07:33:58,132 INFO [RemoteTaskRunner-Scheduled-Cleanup--0] io.druid.indexing.overlord.TaskLockbox - Removing task[index_realtime_REDACTED] from TaskLock[index_realtime_REDACTED]\n```\n\n```\n2015-09-09T08:15:15,428 INFO [qtpREDACTED] io.druid.indexing.common.actions.LocalTaskActionClient - Performing action for task[index_realtime_REDACTED]: LockAcquireAction{interval=2015-09-09T08:00:00.000Z/2015-09-09T09:00:00.000Z}\n2015-09-09T08:15:15,428 INFO [qtpREDACTED] io.druid.indexing.overlord.TaskLockbox - Created new TaskLockPosse: TaskLockPosse{taskLock=TaskLock{groupId=index_realtime_REDACTED, dataSource=REDACTED, interval=2015-09-09T08:00:00.000Z/2015-09-09T09:00:00.000Z, version=2015-09-09T08:15:15.428Z}, taskIds=[]}\n2015-09-09T08:15:15,428 INFO [qtpREDACTED] io.druid.indexing.overlord.TaskLockbox - Added task[index_realtime_REDACTED] to TaskLock[index_realtime_REDACTED]\n```\n\nSuffice to say, this is a long-running task that acquires multiple 1-hr locks during the course of its execution.\n"}, {"user": "fjy", "commits": {"7545ab718841455650bc080bb053d9b5c7ca6ff5": {"commitGHEventType": "referenced", "commitUser": "drcrallen"}, "ba41f37ce1a9131a34dcb6cee8a0e1e7098f763a": {"commitGHEventType": "closed", "commitUser": "xvrl"}}, "changesInPackagesGIT": ["extensions/mysql-metadata-storage/src/main/java/io/druid/metadata/storage/mysql"], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 1, "spoonMethodsChanged": 1, "INS": 1, "UPD": 2, "DEL": 2, "MOV": 2, "TOT": 7}, "title": "MySQL 5.7.6 breaks Druid mysql metadata storage adapter", "numCommits": 1, "created": "2015-09-04 01:06:13", "closed": "2015-11-18 01:36:44", "gitStatsSummary": {"deletions": 5, "insertions": 4, "lines": 9, "gitFilesChange": 1}, "statsSkippedReason": "", "changesInPackagesSPOON": ["io.druid.metadata.storage.mysql.MySQLConnector.tableExists(org.skife.jdbi.v2.Handle,java.lang.String)"], "filteredCommits": ["ba41f37ce1a9131a34dcb6cee8a0e1e7098f763a"], "url": "https://github.com/apache/druid/issues/1701", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 75.00027777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "extensions/mysql-metadata-storage/src/main/java/io/druid/metadata/storage/mysql/MySQLConnector.java", "deletions": 5, "insertions": 4, "lines": 9}], "commitSpoonAstDiffStats": [{"spoonFilePath": "MySQLConnector.java", "spoonMethods": [{"INS": 1, "UPD": 2, "DEL": 2, "spoonMethodName": "io.druid.metadata.storage.mysql.MySQLConnector.tableExists(org.skife.jdbi.v2.Handle,java.lang.String)", "MOV": 2, "TOT": 7}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2015-11-17 17:36:41", "commitMessage": "Merge pull request #1983 from metamx/fix-mysql-57\n\nfix #1701 - MySQL 5.7 defaults break database character set check", "commitUser": "drcrallen", "commitDateTime": "2015-11-17 17:36:41", "commitParents": ["8fcf2403e35382b26d326dacb2a0fbf5e3d6202c", "ba41f37ce1a9131a34dcb6cee8a0e1e7098f763a"], "commitGHEventType": "referenced", "nameRev": "7545ab718841455650bc080bb053d9b5c7ca6ff5 tags/druid-0.9.0-rc1~246", "commitHash": "7545ab718841455650bc080bb053d9b5c7ca6ff5"}, {"commitGitStats": [{"filePath": "extensions/mysql-metadata-storage/src/main/java/io/druid/metadata/storage/mysql/MySQLConnector.java", "deletions": 5, "insertions": 4, "lines": 9}], "commitSpoonAstDiffStats": [{"spoonFilePath": "MySQLConnector.java", "spoonMethods": [{"INS": 1, "UPD": 2, "DEL": 2, "spoonMethodName": "io.druid.metadata.storage.mysql.MySQLConnector.tableExists(org.skife.jdbi.v2.Handle,java.lang.String)", "MOV": 2, "TOT": 7}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2015-11-17 15:50:30", "commitMessage": "fix #1701 - MySQL 5.7 defaults break database character set check\n", "commitUser": "xvrl", "commitDateTime": "2015-11-17 15:51:58", "commitParents": ["8fa34ee671950275a4ba7b0edcf09c9f2928083d"], "commitGHEventType": "closed", "nameRev": "ba41f37ce1a9131a34dcb6cee8a0e1e7098f763a tags/druid-0.9.0-rc1~246^2", "commitHash": "ba41f37ce1a9131a34dcb6cee8a0e1e7098f763a"}], "body": "MySQL version 5.7.6 changed permissions required for `SHOW VARIABLES` (see https://dev.mysql.com/doc/refman/5.7/en/show-variables.html). Druid uses this to check the default database character set. Default MySQL permissions are not enough anymore, causing Druid to fail this check and preventing startup.\n\n**Workaround:** use MySQL <= 5.7.5 or to set `show_compatibility_56 = on` in the MySQL configuration.\n"}, {"user": "Hailei", "commits": {"3f7ba58227c81d42006f1d5f74d2fbe6b63c2440": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Bug", "Starter"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "If RealtimeTuningConfig set ingestOffheap to true,it will  throw IllegalArgumentException exception  in the OffheapIncrementalIndex initialisation process", "numCommits": 0, "created": "2015-06-18 10:14:48", "closed": "2015-07-14 15:50:09", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/1447", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 26.00027777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "indexing-service/src/main/java/io/druid/indexing/common/task/IndexTask.java", "deletions": 0, "insertions": 1, "lines": 1}, {"filePath": "server/src/main/java/io/druid/segment/indexing/RealtimeTuningConfig.java", "deletions": 6, "insertions": 17, "lines": 23}, {"filePath": "indexing-service/src/test/java/io/druid/indexing/overlord/TaskLifecycleTest.java", "deletions": 0, "insertions": 1, "lines": 1}, {"filePath": "server/src/test/java/io/druid/segment/realtime/FireDepartmentTest.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "indexing-service/src/test/java/io/druid/indexing/common/task/TaskSerdeTest.java", "deletions": 1, "insertions": 4, "lines": 5}, {"filePath": "server/src/test/java/io/druid/segment/realtime/plumber/SinkTest.java", "deletions": 0, "insertions": 1, "lines": 1}, {"filePath": "server/src/test/java/io/druid/segment/realtime/plumber/RealtimePlumberSchoolTest.java", "deletions": 0, "insertions": 1, "lines": 1}, {"filePath": "server/src/main/java/io/druid/segment/realtime/plumber/Sink.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "server/src/test/java/io/druid/segment/realtime/RealtimeManagerTest.java", "deletions": 0, "insertions": 1, "lines": 1}], "commitSpoonAstDiffStats": [{"spoonFilePath": "TaskLifecycleTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.overlord.TaskLifecycleTest.testRealtimeIndexTask()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "RealtimeTuningConfig.java", "spoonMethods": [{"INS": 4, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.indexing.RealtimeTuningConfig", "MOV": 0, "TOT": 4}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.indexing.RealtimeTuningConfig.getAggregationBufferRatio()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.indexing.RealtimeTuningConfig.withVersioningPolicy(io.druid.segment.realtime.plumber.VersioningPolicy)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.indexing.RealtimeTuningConfig.withBasePersistDirectory(java.io.File)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.indexing.RealtimeTuningConfig.makeDefaultTuningConfig()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "RealtimePlumberSchoolTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.plumber.RealtimePlumberSchoolTest.setUp()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "Sink.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.plumber.Sink.makeNewCurrIndex(long,io.druid.segment.indexing.DataSchema)", "MOV": 1, "TOT": 2}]}, {"spoonFilePath": "SinkTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.plumber.SinkTest.testSwap()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "IndexTask.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.common.task.IndexTask.convertTuningConfig(io.druid.timeline.partition.ShardSpec,io.druid.indexing.common.task.IndexTask$IndexTuningConfig)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "FireDepartmentTest.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.FireDepartmentTest.testSerde()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "TaskSerdeTest.java", "spoonMethods": [{"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexing.common.task.TaskSerdeTest.testRealtimeIndexTaskSerde()", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "RealtimeManagerTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.realtime.RealtimeManagerTest.setUp()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2015-07-14 08:50:08", "commitMessage": "Merge pull request #1504 from metamx/fix-1447\n\nfix for #1447", "commitUser": "fjy", "commitDateTime": "2015-07-14 08:50:08", "commitParents": ["14a07077a2f6956228449494fcff515258f6b4af", "5fe27fe4ad6ee391aef229046621b397447eae52"], "commitGHEventType": "referenced", "nameRev": "3f7ba58227c81d42006f1d5f74d2fbe6b63c2440 tags/druid-0.8.1-rc1~73", "commitHash": "3f7ba58227c81d42006f1d5f74d2fbe6b63c2440"}], "body": "Druid Version: 0.7.3\nException  as following:\n\n> 2015-06-18T15:44:46,475 ERROR [chief-dsp_report] io.druid.segment.realtime.RealtimeManager - RuntimeException aborted realtime processing[dsp_report]: {class=io.druid.segment.realtime.RealtimeManager, exceptionType=class java.lang.IllegalArgumentException, exceptionMessage=Maximum total buffer size must be greater than aggregation buffer size}\n> java.lang.IllegalArgumentException: Maximum total buffer size must be greater than aggregation buffer size\n>         at com.google.common.base.Preconditions.checkArgument(Preconditions.java:125) ~[guava-16.0.1.jar:?]\n>         at io.druid.segment.incremental.OffheapIncrementalIndex.<init>(OffheapIncrementalIndex.java:91) ~[druid-processing-0.7.3.jar:0.7.3]\n\nDeep into the code,in the Sink  initialisation process,\n\n``` java\n final IncrementalIndex newIndex;\n    if (config.isIngestOffheap()) {\n      newIndex = new OffheapIncrementalIndex(\n          indexSchema,\n          // Assuming half space for aggregates\n          new OffheapBufferPool(config.getBufferSize()),\n          true,\n          config.getBufferSize()\n      );\n```\n\nbufferPool's size is the same as maxTotalBufferSize.So will throw above exception.\nAs is said on Comment,Assuming half space for aggregates,Like HadoopTuningConfig,we need add a field named aggregationBufferRatio,default value is 0.5.\n"}, {"user": "fjy", "commits": {}, "labels": ["Area - Querying", "Bug", "Priority - Showstopper", "Starter"], "created": "2015-06-13 00:57:48", "title": "GroupBys cannot order by a post aggregation", "url": "https://github.com/apache/druid/issues/1436", "closed": "2015-07-10 15:29:41", "ttf": 27.00027777777778, "commitsDetails": [], "body": "A user reported this problem in the following thread:\nhttps://groups.google.com/forum/#!topic/druid-development/7pFpogEf-1M\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug"], "created": "2015-06-12 21:08:07", "title": "Hadoop tasks fail in master when using s3_zip", "url": "https://github.com/apache/druid/issues/1433", "closed": "2015-06-17 04:42:40", "ttf": 4.000277777777778, "commitsDetails": [], "body": "https://github.com/druid-io/druid/pull/1428 uses FileContext, which is a little more modern way of handling files. But https://issues.apache.org/jira/browse/HADOOP-10643 means it won't work with s3n.\n\nParts of #1428 need to be reverted to the prior method.\n"}, {"user": "himanshug", "commits": {}, "labels": ["Area - Querying", "Bug", "Priority - High", "Starter"], "created": "2015-06-03 13:53:32", "title": "Nested groupBy query fails when outer query has multiple aggregators having same \"fieldName\"", "url": "https://github.com/apache/druid/issues/1419", "closed": "2015-10-01 02:28:34", "ttf": 119.00027777777778, "commitsDetails": [], "body": "Group thread:\nhttps://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/druid-user/L4A1iMhzOco/w_5C030krzEJ\n\nFailing Query:\n\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\":{\n    \"type\": \"query\",\n    \"query\": {\n      \"queryType\": \"groupBy\",\n      \"dataSource\": \"platform\",\n      \"granularity\": \"all\",\n      \"dimensions\": [\"treatment_name\", \"subject_id\", \"dim_app_family\"],\n      \"aggregations\": [\n        { \"type\": \"longSum\", \"name\": \"value\", \"fieldName\": \"count\" }\n      ],\n      \"intervals\": [ \"2012-01-01T00:00:00.000/2020-01-03T00:00:00.000\" ]\n    }\n  },\n  \"granularity\": \"all\",\n  \"dimensions\": [\"treatment_name\", \"dim_app_family\"],\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"first_moment\", \"fieldName\": \"value\" },\n    { \"type\": \"javascript\",\n      \"name\": \"second_moment\",\n      \"fieldNames\": [\"value\"],\n      \"fnAggregate\": \"function(current, v) { return current + v * v; } \",\n      \"fnCombine\": \"function(partialA, partialB) { return partialA + partialB; }\",\n      \"fnReset\": \"function() { return 0; }\"\n    }\n  ],\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2020-01-03T00:00:00.000\" ]\n}\n\n```\n\nException Stacktrace:\n\n```\njava.lang.IllegalArgumentException: [value] already defined\n    at com.google.common.base.Preconditions.checkArgument(Preconditions.java:120)\n    at io.druid.query.Queries.verifyAggregations(Queries.java:42)\n    at io.druid.query.groupby.GroupByQuery.<init>(GroupByQuery.java:103)\n    at io.druid.query.groupby.GroupByQuery$Builder.build(GroupByQuery.java:531)\n    at io.druid.query.groupby.GroupByQueryQueryToolChest.mergeGroupByResults(GroupByQueryQueryToolChest.java:160)\n```\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug", "Priority - High"], "created": "2015-05-27 13:14:57", "title": "AnnouncerTest sporadically fails in master in TravisCI", "url": "https://github.com/apache/druid/issues/1393", "closed": "2016-01-06 15:43:46", "ttf": 224.00027777777777, "commitsDetails": [], "body": "AnnouncerTest seems to be failing pretty regularly in TravisCI these days.\n\nSpecifically `AnnouncerTest.testSessionKilled` will often fail when checking the data in ZK\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug", "Priority - Medium"], "created": "2015-05-14 02:05:45", "title": "Convert Segment Task does not close properly on error", "url": "https://github.com/apache/druid/issues/1363", "closed": "2015-06-10 18:07:06", "ttf": 27.00027777777778, "commitsDetails": [], "body": "When a failure occurs in a convert segment subtask the failure is returned with the sub-task ID rather than the master taskID, thus the failure is never registered properly, and the overlord is stuck thinking the parent task should still be running.\n"}, {"user": "xvrl", "commits": {}, "labels": ["Bug"], "created": "2015-05-13 07:58:19", "title": "Race condition in autoscaling terminates nodes that were just assigned tasks", "url": "https://github.com/apache/druid/issues/1360", "closed": "2015-06-23 07:35:06", "ttf": 40.000277777777775, "commitsDetails": [], "body": "It is possible for the indexing service auto-scaling to terminate nodes that have just been assigned a task, but that have not updated the task status to running yet.\n\nAutoscaling currently only relies on task status provided by the workers, but does not take into account tasks that are known to be running or about to run on a given worker. If you are unlucky and autoscaling checks right after the task has been assigned, but right before the worker announces the task status, and the worker is not running any other tasks, your tasks will get killed.\n\nRunning task replicas may not help in this situations. If the indexing service was recently updated and spawned new middle-managers that aren't running anything yet, it is possible for multiple nodes to be killed that were about to run tasks.\n"}, {"user": "vvsatya", "commits": {"f6a8e030cc491aa6199c907d37cda253da4e4d41": {"commitGHEventType": "referenced", "commitUser": "jon-wei"}, "86040e2104fa1a719e0ae585ea54e1e149eb7013": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Area - Querying", "Bug", "Priority - High", "Starter"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "Select query failing if milliseconds used as time for indexing", "numCommits": 0, "created": "2015-05-01 22:32:06", "closed": "2019-06-19 20:33:20", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/1332", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 1509.0002777777777, "commitsDetails": [{"commitGitStats": [{"filePath": "core/src/main/java/org/apache/druid/java/util/common/DateTimes.java", "deletions": 1, "insertions": 11, "lines": 12}, {"filePath": "core/src/test/java/org/apache/druid/java/util/common/DateTimesTest.java", "deletions": 0, "insertions": 19, "lines": 19}], "commitSpoonAstDiffStats": [{"spoonFilePath": "DateTimesTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.java.util.common.DateTimesTest.testStringToDateTimeConverstion_RethrowInitialException()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.java.util.common.DateTimesTest.testStringToDateTimeConversion()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "DateTimes.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.java.util.common.DateTimes.of(java.lang.String)", "MOV": 1, "TOT": 2}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2019-02-25 17:36:01", "commitMessage": "Select query failing if miliseconds used as time for indexing (#6937)\n\n* [#1332] Fix - select failing if milis used for idx.\r\n\r\n* Formating correction.\r\n\r\n* Address comment: throw original exception.\r\n\r\n* Using constant values in tests\r\n\r\n- Try converting to Integer and then multiply by 1000L to achieve milis.\r\n- If not successful try converting to Long or rethrow original\r\nexception.\r\n\r\n* DateTime#of has to support \"2011-01-01T00:00:00\"\r\n\r\n- in addition to seconds and milisecs, this method currently supports\r\neven a date string.\r\n\r\n* Handle only milisec timestamps and ISO8601 strings\r\n", "commitUser": "jon-wei", "commitDateTime": "2019-02-25 14:36:01", "commitParents": ["9a066558a447bf18f9b10699984fd1621f0ef26e"], "commitGHEventType": "referenced", "nameRev": "f6a8e030cc491aa6199c907d37cda253da4e4d41 tags/druid-0.15.0-incubating-rc1~264", "commitHash": "f6a8e030cc491aa6199c907d37cda253da4e4d41"}, {"commitGitStats": [{"filePath": "core/src/main/java/org/apache/druid/java/util/common/DateTimes.java", "deletions": 1, "insertions": 11, "lines": 12}, {"filePath": "core/src/test/java/org/apache/druid/java/util/common/DateTimesTest.java", "deletions": 0, "insertions": 19, "lines": 19}], "commitSpoonAstDiffStats": [{"spoonFilePath": "DateTimesTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.java.util.common.DateTimesTest.testStringToDateTimeConverstion_RethrowInitialException()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.java.util.common.DateTimesTest.testStringToDateTimeConversion()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "DateTimes.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "org.apache.druid.java.util.common.DateTimes.of(java.lang.String)", "MOV": 1, "TOT": 2}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2019-02-26 09:12:32", "commitMessage": "Select query failing if miliseconds used as time for indexing (#6937) (#7140)\n\n* [#1332] Fix - select failing if milis used for idx.\r\n\r\n* Formating correction.\r\n\r\n* Address comment: throw original exception.\r\n\r\n* Using constant values in tests\r\n\r\n- Try converting to Integer and then multiply by 1000L to achieve milis.\r\n- If not successful try converting to Long or rethrow original\r\nexception.\r\n\r\n* DateTime#of has to support \"2011-01-01T00:00:00\"\r\n\r\n- in addition to seconds and milisecs, this method currently supports\r\neven a date string.\r\n\r\n* Handle only milisec timestamps and ISO8601 strings", "commitUser": "fjy", "commitDateTime": "2019-02-26 09:12:32", "commitParents": ["a8ae7156b1751fdc2ce2a978f9728cbf79b0ed2c"], "commitGHEventType": "referenced", "nameRev": "86040e2104fa1a719e0ae585ea54e1e149eb7013 tags/druid-0.14.0-incubating~44", "commitHash": "86040e2104fa1a719e0ae585ea54e1e149eb7013"}], "body": "Steps:\nFor indexing, use time stamp in milliseconds. Fire a select query without listing any dimensions so it is expected to get all dimensions.\n\nRootCause:\nIf we look at io.druid.query.select.SelectQueryEngine, the way we are constructing the variable \"theEvent\", first we are adding DateTime object then following loop overwriting the TimeStamp value with String value\n\nLater this String value is attempted to convert as date in io.druid.query.select.EventHolder.getTimestamp() method.\n\nWorkaround: Provide an explicit list of dimensions excluding timestamp. Timestamp any way included for every row.\n\nException:\n2015-04-22T20:52:47,321 ERROR [select_integration1_[2015-04-22T00:00:00.000Z/2015-04-22T20:51:25.746Z]] io.druid.query.ChainedExecutionQueryRunner - Exception with one of the sequences!\njava.lang.IllegalArgumentException: Invalid format: \"1429735285805\" is malformed at \"5805\"\n    at org.joda.time.format.DateTimeParserBucket.doParseMillis(DateTimeParserBucket.java:187) ~[joda-time-2.6.jar:2.6]\n    at org.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:780) ~[joda-time-2.6.jar:2.6]\n    at org.joda.time.convert.StringConverter.getInstantMillis(StringConverter.java:65) ~[joda-time-2.6.jar:2.6]\n    at org.joda.time.base.BaseDateTime.<init>(BaseDateTime.java:175) ~[joda-time-2.6.jar:2.6]\n    at org.joda.time.DateTime.<init>(DateTime.java:257) ~[joda-time-2.6.jar:2.6]\n    at io.druid.query.select.EventHolder.getTimestamp(EventHolder.java:54) ~[druid-processing-0.7.0.jar:0.7.0]\n    at io.druid.query.select.SelectResultValueBuilder$1.compare(SelectResultValueBuilder.java:40) ~[druid-processing-0.7.0.jar:0.7.0]\n    at io.druid.query.select.SelectResultValueBuilder$1.compare(SelectResultValueBuilder.java:36) ~[druid-processing-0.7.0.jar:0.7.0]\n    at com.google.common.collect.ComparatorOrdering.compare(ComparatorOrdering.java:38) ~[guava-16.0.1.jar:?]\n    at com.google.common.collect.ReverseOrdering.compare(ReverseOrdering.java:38) ~[guava-16.0.1.jar:?]\n    at com.google.common.collect.MinMaxPriorityQueue$Heap.crossOverUp(MinMaxPriorityQueue.java:633) ~[guava-16.0.1.jar:?]\n    at com.google.common.collect.MinMaxPriorityQueue$Heap.bubbleUp(MinMaxPriorityQueue.java:537) ~[guava-16.0.1.jar:?]\n    at com.google.common.collect.MinMaxPriorityQueue.offer(MinMaxPriorityQueue.java:280) ~[guava-16.0.1.jar:?]\n    at com.google.common.collect.MinMaxPriorityQueue.add(MinMaxPriorityQueue.java:252) ~[guava-16.0.1.jar:?]\n    at io.druid.query.select.SelectResultValueBuilder.addEntry(SelectResultValueBuilder.java:72) ~[druid-processing-0.7.0.jar:0.7.0]\n    at io.druid.query.select.SelectQueryEngine$1.apply(SelectQueryEngine.java:147) ~[druid-processing-0.7.0.jar:0.7.0]\n    at io.druid.query.select.SelectQueryEngine$1.apply(SelectQueryEngine.java:75) ~[druid-processing-0.7.0.jar:0.7.0]\n    at io.druid.query.QueryRunnerHelper$1.apply(QueryRunnerHelper.java:74) ~[druid-processing-0.7.0.jar:0.7.0]\n    at io.druid.query.QueryRunnerHelper$1.apply(QueryRunnerHelper.java:69) ~[druid-processing-0.7.0.jar:0.7.0]\n    at com.metamx.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:39) ~[java-util-0.26.14.jar:?]\n    at com.metamx.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:39) ~[java-util-0.26.14.jar:?]\n    at com.metamx.common.guava.YieldingAccumulators$1.accumulate(YieldingAccumulators.java:32) ~[java-util-0.26.14.jar:?]\n    at com.metamx.common.guava.BaseSequence.makeYielder(BaseSequence.java:104) ~[java-util-0.26.14.jar:?]\n    at com.metamx.common.guava.BaseSequence.toYielder(BaseSequence.java:81) ~[java-util-0.26.14.jar:?]\n    at com.metamx.common.guava.BaseSequence.accumulate(BaseSequence.java:67) ~[java-util-0.26.14.jar:?]\n    at com.metamx.common.guava.MappedSequence.accumulate(MappedSequence.java:40) ~[java-util-0.26.14.jar:?]\n    at com.metamx.common.guava.MappedSequence.accumulate(MappedSequence.java:40) ~[java-util-0.26.14.jar:?]\n    at com.metamx.common.guava.FilteredSequence.accumulate(FilteredSequence.java:42) ~[java-util-0.26.14.jar:?]\n    at com.metamx.common.guava.Sequences.toList(Sequences.java:113) ~[java-util-0.26.14.jar:?]\n    at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:130) [druid-processing-0.7.0.jar:0.7.0]\n    at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:120) [druid-processing-0.7.0.jar:0.7.0]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_75]\n    at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) [guava-16.0.1.jar:?]\n    at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:132) [?:1.7.0_75]\n    at com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:58) [guava-16.0.1.jar:?]\n    at io.druid.query.ChainedExecutionQueryRunner$1$1.apply(ChainedExecutionQueryRunner.java:118) [druid-processing-0.7.0.jar:0.7.0]\n    at io.druid.query.ChainedExecutionQueryRunner$1$1.apply(ChainedExecutionQueryRunner.java:110) [druid-processing-0.7.0.jar:0.7.0]\n    at com.google.common.collect.Iterators$8.transform(Iterators.java:794) [guava-16.0.1.jar:?]\n    at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48) [guava-16.0.1.jar:?]\n    at com.google.common.collect.Iterators.addAll(Iterators.java:357) [guava-16.0.1.jar:?]\n    at com.google.common.collect.Lists.newArrayList(Lists.java:147) [guava-16.0.1.jar:?]\n    at com.google.common.collect.Lists.newArrayList(Lists.java:129) [guava-16.0.1.jar:?]\n    at io.druid.query.ChainedExecutionQueryRunner$1.make(ChainedExecutionQueryRunner.java:105) [druid-processing-0.7.0.jar:0.7.0]\n    at com.metamx.common.guava.BaseSequence.toYielder(BaseSequence.java:78) [java-util-0.26.14.jar:?]\n    at com.metamx.common.guava.BaseSequence.accumulate(BaseSequence.java:67) [java-util-0.26.14.jar:?]\n    at io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:102) [druid-processing-0.7.0.jar:0.7.0]\n    at io.druid.query.spec.SpecificSegmentQueryRunner$2$1.call(SpecificSegmentQueryRunner.java:85) [druid-processing-0.7.0.jar:0.7.0]\n    at io.druid.query.spec.SpecificSegmentQueryRunner.doNamed(SpecificSegmentQueryRunner.java:169) [druid-processing-0.7.0.jar:0.7.0]\n    at io.druid.query.spec.SpecificSegmentQueryRunner.access$400(SpecificSegmentQueryRunner.java:39) [druid-processing-0.7.0.jar:0.7.0]\n    at io.druid.query.spec.SpecificSegmentQueryRunner$2.doItNamed(SpecificSegmentQueryRunner.java:160) [druid-processing-0.7.0.jar:0.7.0]\n    at io.druid.query.spec.SpecificSegmentQueryRunner$2.accumulate(SpecificSegmentQueryRunner.java:78) [druid-processing-0.7.0.jar:0.7.0]\n    at com.metamx.common.guava.Sequences.toList(Sequences.java:113) [java-util-0.26.14.jar:?]\n    at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:130) [druid-processing-0.7.0.jar:0.7.0]\n    at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:120) [druid-processing-0.7.0.jar:0.7.0]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_75]\n    at io.druid.query.PrioritizedExecutorService$PrioritizedListenableFutureTask.run(PrioritizedExecutorService.java:202) [druid-processing-0.7.0.jar:0.7.0]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_75]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_75]\n    at java.lang.Thread.run(Thread.java:745) [?:1.7.0_75]\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2015-03-23 21:52:21", "title": "`max` aggregator on `__time` column returns double", "url": "https://github.com/apache/druid/issues/1237", "closed": "2015-06-07 18:56:48", "ttf": 75.00027777777778, "commitsDetails": [], "body": "Doing a `max` aggregator on the `__time` column returns a double value, not a long value.\n\nA simple test of this is something like:\n\n``` json\n{\n  \"queryType\"  : \"timeseries\",\n  \"dataSource\" : \"tpch_year\",\n  \"granularity\": \"all\",\n  \"intervals\": [ \"1970-01-01T00:00:00.000/2019-01-03T00:00:00.000\" ],\n  \"aggregations\": [\n    { \"type\": \"max\", \"name\": \"lastItem\", \"fieldName\": \"__time\" }\n  ]\n}\n```\n\nresults in\n\n``` json\n[ {\n  \"timestamp\" : \"1992-01-02T00:00:00.000Z\",\n  \"result\" : {\n    \"lastItem\" : 9.12470376448E11\n  }\n} ]\n```\n\nAn example use case for this would be to get a \"last seen\" timestamp when the query is either filtered or groupBy.\n"}, {"user": "haadcode", "commits": {}, "labels": ["Bug"], "created": "2015-03-19 15:23:38", "title": "Cardinality aggregator throws ClassCastException in GroupBy query when queried from realtime node", "url": "https://github.com/apache/druid/issues/1227", "closed": "2015-04-30 06:37:06", "ttf": 41.000277777777775, "commitsDetails": [], "body": "When doing the following query **on realtime node**:\n\n``` javascript\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"events\",\n  \"granularity\": \"all\",\n  \"dimensions\": [\"site\"],\n  \"aggregations\": [\n    {\"type\": \"count\", \"fieldName\": \"userId\", \"name\": \"Events\"},\n    {\"type\": \"cardinality\", \"fieldNames\": [\"userId\"], \"name\": \"Unique Users\", \"byRow\": true}\n  ],\n  \"intervals\": [\"2015-03-01T00:00/2015-04-01T00\"]\n}\n```\n\nDruid returns:\n\n``` javascript\n{ \"error\" : \"java.lang.Double cannot be cast to io.druid.query.aggregation.hyperloglog.HyperLogLogCollector\"}\n```\n\nFollowing exception can be found from realtime node's log:\n\n```\njava.lang.ClassCastException: java.lang.Double cannot be cast to io.druid.query.aggregation.hyperloglog.HyperLogLogCollector\n    at io.druid.query.aggregation.hyperloglog.HyperUniquesAggregator.aggregate(HyperUniquesAggregator.java:46) ~[druid-processing-0.7.0.jar:0.7.0]\n    at io.druid.segment.incremental.OnheapIncrementalIndex.addToFacts(OnheapIncrementalIndex.java:169) ~[druid-processing-0.7.0.jar:0.7.0]\n    at io.druid.segment.incremental.IncrementalIndex.add(IncrementalIndex.java:431) ~[druid-processing-0.7.0.jar:0.7.0]\n    at io.druid.query.groupby.GroupByQueryHelper$3.accumulate(GroupByQueryHelper.java:112) ~[druid-processing-0.7.0.jar:0.7.0]\n    at io.druid.query.groupby.GroupByQueryHelper$3.accumulate(GroupByQueryHelper.java:104) ~[druid-processing-0.7.0.jar:0.7.0]\n    at com.metamx.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:39) ~[java-util-0.26.14.jar:?]\n    at com.metamx.common.guava.YieldingAccumulators$1.accumulate(YieldingAccumulators.java:32) ~[java-util-0.26.14.jar:?]\n    at com.metamx.common.guava.BaseSequence.makeYielder(BaseSequence.java:104) ~[java-util-0.26.14.jar:?]\n    at com.metamx.common.guava.BaseSequence.toYielder(BaseSequence.java:81) ~[java-util-0.26.14.jar:?]\n    at com.metamx.common.guava.ResourceClosingSequence.toYielder(ResourceClosingSequence.java:41) ~[java-util-0.26.14.jar:?]\n    at com.metamx.common.guava.YieldingSequenceBase.accumulate(YieldingSequenceBase.java:34) ~[java-util-0.26.14.jar:?]\n    at com.metamx.common.guava.MappedSequence.accumulate(MappedSequence.java:40) ~[java-util-0.26.14.jar:?]\n    at io.druid.query.GroupByParallelQueryRunner$1$1.call(GroupByParallelQueryRunner.java:115) ~[druid-processing-0.7.0.jar:0.7.0]\n    at io.druid.query.GroupByParallelQueryRunner$1$1.call(GroupByParallelQueryRunner.java:106) ~[druid-processing-0.7.0.jar:0.7.0]\n    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) ~[?:1.7.0_25]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:166) ~[?:1.7.0_25]\n    at io.druid.query.PrioritizedExecutorService$PrioritizedListenableFutureTask.run(PrioritizedExecutorService.java:202) ~[druid-processing-0.7.0.jar:0.7.0]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[?:1.7.0_25]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[?:1.7.0_25]\n    at java.lang.Thread.run(Thread.java:724) [?:1.7.0_25]\n```\n\nThe userId dimension is in format: 123456789012345678.\nIf the queryType is changed from _groupBy_ to _timeseries_, the query is successful. If the query is made via broker node, the query is successful. Using Druid 0.7.0.\n"}, {"user": "xvrl", "commits": {}, "labels": ["Bug"], "created": "2015-03-17 17:01:02", "title": "injection of TaskStorageConfig broken for tutorials", "url": "https://github.com/apache/druid/issues/1212", "closed": "2015-03-17 20:08:43", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "https://github.com/druid-io/druid/pull/1195 seems to have broken examples and tutorials.\n\n```\n2015-03-16T21:21:25,612 INFO [main] io.druid.initialization.Initialization - Loading extension[io.druid.extensions:druid-examples] for class[io.druid.initialization.DruidModule]\n2015-03-16T21:21:25,614 INFO [main] io.druid.initialization.Initialization - Adding extension module[class io.druid.examples.ExamplesDruidModule] for class[io.druid.initialization.DruidModule]\n2015-03-16T21:21:25,614 INFO [main] io.druid.initialization.Initialization - Loading extension[io.druid.extensions:mysql-metadata-storage] for class[io.druid.initialization.DruidModule]\n2015-03-16T21:21:25,615 INFO [main] io.druid.initialization.Initialization - Adding extension module[class io.druid.metadata.storage.mysql.MySQLMetadataStorageModule] for class[io.druid.initialization.DruidModule]\nException in thread \"main\" com.google.inject.CreationException: Guice creation errors:\n\n1) Could not find a suitable constructor in io.druid.indexing.common.config.TaskStorageConfig. Classes must have either one (and only one) constructor annotated with @Inject or a zero-argument constructor that is not private.\n  at io.druid.indexing.common.config.TaskStorageConfig.class(TaskStorageConfig.java:29)\n  while locating io.druid.indexing.common.config.TaskStorageConfig\n    for parameter 0 at io.druid.indexing.overlord.HeapMemoryTaskStorage.<init>(HeapMemoryTaskStorage.java:60)\n  at io.druid.cli.CliPeon$1.configureTaskActionClient(CliPeon.java:189)\n\n1 error\n    at com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:448)\n    at com.google.inject.internal.InternalInjectorCreator.initializeStatically(InternalInjectorCreator.java:155)\n    at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:107)\n    at com.google.inject.Guice.createInjector(Guice.java:96)\n    at com.google.inject.Guice.createInjector(Guice.java:73)\n    at com.google.inject.Guice.createInjector(Guice.java:62)\n    at io.druid.initialization.Initialization.makeInjectorWithModules(Initialization.java:369)\n    at io.druid.cli.GuiceRunnable.makeInjector(GuiceRunnable.java:55)\n    at io.druid.cli.CliPeon.run(CliPeon.java:205)\n    at io.druid.cli.Main.main(Main.java:88)\n```\n\nreported in https://groups.google.com/forum/#!topic/druid-user/zxEwe-kiq-4\n"}, {"user": "ethanocentricity", "commits": {}, "labels": ["Bug", "Ease of Use", "Priority - High"], "created": "2015-03-12 22:20:13", "title": "new datasource doesn't show up in 0.7.0 coordinator console until handed off", "url": "https://github.com/apache/druid/issues/1198", "closed": "2019-06-19 20:33:58", "ttf": 1559.0002777777777, "commitsDetails": [], "body": "When we launch a new realtime datasource (hourly segment size), it doesn't show up in the new coordinator console until after the first segment is handed off.  .\n\nI believe the problem is that:\n<COORDINATOR_URL>/druid/coordinator/v1/metadata/datasources\ndoesn't seem to include the new datasource until after the handoff occurs.\n\nThe other services do return metadata about that datasource: \n- <COORDINATOR_URL>/druid/coordinator/v1/datasources/<DATASOURCE>/intervals?simple\n- <COORDINATOR_URL>druid/coordinator/v1/datasources/<DATASOURCE>/intervals/<INTERVAL_RANGE>?full\n"}, {"user": "fjy", "commits": {"f0f453940f2d52368d1f87c382a6abaa34bc9373": {"commitGHEventType": "referenced", "commitUser": "drcrallen"}}, "changesInPackagesGIT": [], "labels": ["Bug", "Ease of Use", "Priority - Showstopper"], "spoonStatsSummary": {}, "title": "Handling nulls and empty strings is still inconsistent", "numCommits": 0, "created": "2015-02-12 23:39:31", "closed": "2015-06-23 05:44:52", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "filteredCommits": [], "url": "https://github.com/apache/druid/issues/1120", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 130.00027777777777, "commitsDetails": [{"commitGitStats": [{"filePath": "processing/src/test/resources/druid.sample.json.bottom", "deletions": 1, "insertions": 2, "lines": 3}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "", "authoredDateTime": "2015-06-22 10:54:24", "commitMessage": "Merge pull request #1456 from metamx/test-1120\n\ntest for #1120", "commitUser": "drcrallen", "commitDateTime": "2015-06-22 10:54:24", "commitParents": ["0a5bb909a2a87a9a23e0b16ff8119f3c701372b3", "f9cdb0ad61526c65750ab988304457f5ed5c2023"], "commitGHEventType": "referenced", "nameRev": "f0f453940f2d52368d1f87c382a6abaa34bc9373 tags/druid-0.8.0-rc2~10^2~2", "commitHash": "f0f453940f2d52368d1f87c382a6abaa34bc9373"}], "body": "The easiest way to verify is this:\n\nOn line 47 of druid.sample.json.bottom, add the following\n\"quality\":\"\"\n\nUnit tests will fail in testDifferentMetrics as the test expects 1 unique for quality and this change generates 2 unique quality values.\n\nchange that line to:\n\"quality\":null\n\nUnit tests will now pass.\n\nSad face.\n"}, {"user": "flowbehappy", "commits": {"708f35151d29a57a469c6077963557da8a6b5ac7": {"commitGHEventType": "referenced", "commitUser": "fjy"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "index task failed with storage type as \"hdfs\"", "numCommits": 0, "created": "2015-02-12 10:43:30", "closed": "2015-03-10 08:01:37", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/1116", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 25.00027777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "indexing-hadoop/src/main/java/io/druid/indexer/JobHelper.java", "deletions": 4, "insertions": 4, "lines": 8}, {"filePath": "indexing-hadoop/src/main/java/io/druid/indexer/IndexGeneratorJob.java", "deletions": 7, "insertions": 8, "lines": 15}], "commitSpoonAstDiffStats": [{"spoonFilePath": "IndexGeneratorJob.java", "spoonMethods": [{"INS": 2, "UPD": 7, "DEL": 1, "spoonMethodName": "io.druid.indexer.IndexGeneratorJob.IndexGeneratorReducer.serializeOutIndex(io.druid.indexer.Context,io.druid.indexer.Bucket,java.io.File,java.util.List)", "MOV": 5, "TOT": 15}]}, {"spoonFilePath": "JobHelper.java", "spoonMethods": [{"INS": 2, "UPD": 3, "DEL": 1, "spoonMethodName": "io.druid.indexer.JobHelper.setupClasspath(io.druid.indexer.HadoopDruidIndexerConfig,org.apache.hadoop.mapreduce.Job)", "MOV": 2, "TOT": 8}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2015-02-25 13:03:59", "commitMessage": "Merge pull request #1121 from gianm/issue-1116\n\nUse the proper FileSystems for writing segments and caching jars. (for issue #1116)", "commitUser": "fjy", "commitDateTime": "2015-02-25 13:03:59", "commitParents": ["005f4da2c0ca1cf3a140d6ff09f3bee1d35f75ec", "fd5a7d1f084a8d05a9fff600cd76f567eedad1a9"], "commitGHEventType": "referenced", "nameRev": "708f35151d29a57a469c6077963557da8a6b5ac7 tags/druid-0.7.1-rc1~59", "commitHash": "708f35151d29a57a469c6077963557da8a6b5ac7"}], "body": "```\njava.lang.Exception: java.lang.IllegalArgumentException: Pathname /druid/segment_data/datasource1/datasource1/2013-08-20T03:00:00.000Z_2013-08-20T04:\n00:00.000Z/2015-02-12T10:21:15.522Z/0 from hdfs://master.hadoop:9000/druid/segment_data/datasource1/datasource1/2013-08-20T03:00:00.000Z_2013-08-20T0\n4:00:00.000Z/2015-02-12T10:21:15.522Z/0 is not a valid DFS filename.\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)\nCaused by: java.lang.IllegalArgumentException: Pathname /druid/segment_data/datasource1/datasource1/2013-08-20T03:00:00.000Z_2013-08-20T04:00:00.000Z\n/2015-02-12T10:21:15.522Z/0 from hdfs://master.hadoop:9000/druid/segment_data/datasource1/datasource1/2013-08-20T03:00:00.000Z_2013-08-20T04:00:00.00\n0Z/2015-02-12T10:21:15.522Z/0 is not a valid DFS filename.\n        at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:187)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:101)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$16.doCall(DistributedFileSystem.java:820)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$16.doCall(DistributedFileSystem.java:816)\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:816)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:809)\n        at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1815)\n        at io.druid.indexer.IndexGeneratorJob$IndexGeneratorReducer.serializeOutIndex(IndexGeneratorJob.java:408)\n        at io.druid.indexer.IndexGeneratorJob$IndexGeneratorReducer.reduce(IndexGeneratorJob.java:388)\n        at io.druid.indexer.IndexGeneratorJob$IndexGeneratorReducer.reduce(IndexGeneratorJob.java:253)\n        at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)                                                                 \n        at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)                                                    \n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)                                                              \n        at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)                               \n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)                                                   \n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)                                                                  \n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)                                           \n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)                                           \n        at java.lang.Thread.run(Thread.java:745)   \n```\n\nThe `fileSystem` here https://github.com/druid-io/druid/blob/master/indexing-hadoop/src/main/java/io/druid/indexer/IndexGeneratorJob.java#L435 supposed to be a `DistributedFileSystem` but it is not.\n\nNote that it only go wrong in indexing service's remote mode. In local mode it is fine.\n"}, {"user": "fjy", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug", "Priority - High"], "created": "2015-02-03 18:47:28", "title": "Realtime cleanup may not work if there are exceptions during cleanup", "url": "https://github.com/apache/druid/issues/1082", "closed": "2019-11-08 22:54:47", "ttf": 1739.0002777777777, "commitsDetails": [], "body": "T0aD in IRC has reported a strange issue with segments not being handed off. I believe the following may have occurred:\n\n1) A segment was successfully handed off but something went wrong during the cleanup process (we don't have the logs, but it appears the directory was not cleaned up)\n2) The realtime node was restarted. The node found the directory of the segment exists, and announced it was serving that segment. Subsequently, the segment was not removed.\n"}, {"user": "fjy", "commits": {}, "labels": ["Bug"], "created": "2015-01-27 22:51:56", "title": "RejectionPolicy Serde is broken", "url": "https://github.com/apache/druid/issues/1069", "closed": "2015-01-27 22:52:18", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "Rejection policy is being read as \"rejectionPolicy\" and being written as \"rejectionPolicyFactory\". This is a bug with the real-time tuning config.\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2015-01-17 01:19:15", "title": "GroupBy with complex metrics fails if there is a post-aggregator with the same name", "url": "https://github.com/apache/druid/issues/1045", "closed": "2015-03-11 03:25:28", "ttf": 53.000277777777775, "commitsDetails": [], "body": "See #1044 for unit tests catching this bug.\n"}, {"user": "drcrallen", "commits": {"9cf8662aeb04a170e5a54939867028c79bf54965": {"commitGHEventType": "referenced", "commitUser": "xvrl"}, "f81a0d92107548cdd3166a45b4f418a2c989b0f3": {"commitGHEventType": "closed", "commitUser": "himanshug"}}, "changesInPackagesGIT": [], "labels": ["Bug", "Ease of Use", "Priority - Medium"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "Loadable interface in classpath with any extension coordinate dynamically loaded causes problems", "numCommits": 0, "created": "2015-01-07 18:38:12", "closed": "2015-08-10 17:19:13", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/1016", "filteredCommitsReason": {"multipleIssueFixes": 2, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 214.00027777777777, "commitsDetails": [{"commitGitStats": [{"filePath": "docs/content/configuration/index.md", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "server/src/test/java/io/druid/initialization/InitializationTest.java", "deletions": 9, "insertions": 42, "lines": 51}, {"filePath": "server/src/main/java/io/druid/initialization/Initialization.java", "deletions": 14, "insertions": 49, "lines": 63}], "commitSpoonAstDiffStats": [{"spoonFilePath": "InitializationTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.initialization.InitializationTest.test04DuplicateClassLoaderExtensions()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.initialization.InitializationTest.test04MakeInjectorWithModules()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.initialization.InitializationTest.test04MakeInjectorWithModules().2", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "Initialization.java", "spoonMethods": [{"INS": 6, "UPD": 3, "DEL": 3, "spoonMethodName": "io.druid.initialization.Initialization.getFromExtensions(io.druid.guice.ExtensionsConfig,java.lang.Class)", "MOV": 21, "TOT": 33}, {"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.initialization.Initialization.clearLoadedModules()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.initialization.Initialization.getLoadersMap()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2015-06-10 14:42:24", "commitMessage": "Merge pull request #1427 from guobingkun/fix_issue_1016\n\nFix duplicate extension loading issue described in #1016", "commitUser": "xvrl", "commitDateTime": "2015-06-10 14:42:24", "commitParents": ["78d468700b28f5a38f422edb4bb2d2503e610f75", "bdf4d541e315856bfa368d09c630bb80f4c42ede"], "commitGHEventType": "referenced", "nameRev": "9cf8662aeb04a170e5a54939867028c79bf54965 tags/druid-0.8.0-rc1~6", "commitHash": "9cf8662aeb04a170e5a54939867028c79bf54965"}, {"commitGitStats": [{"filePath": "server/src/main/java/io/druid/initialization/Initialization.java", "deletions": 7, "insertions": 15, "lines": 22}], "commitSpoonAstDiffStats": [{"spoonFilePath": "Initialization.java", "spoonMethods": [{"INS": 3, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.initialization.Initialization.makeInjectorWithModules(com.google.inject.Injector,java.lang.Iterable)", "MOV": 1, "TOT": 5}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2015-06-04 14:43:26", "commitMessage": "Merge pull request #1425 from guobingkun/guice_fix\n\nAnyways, @guobingkun , if possible,  can you please test proposed change and send another PR to fix #1016 .  but that doesn't necessarily hold this PR, so I'm merging this.", "commitUser": "himanshug", "commitDateTime": "2015-06-04 14:43:26", "commitParents": ["f44f7f07de5d75a6596f0d60f5f42de500d308f4", "dc26350480188441b0fe492a3b158774d59caaff"], "commitGHEventType": "closed", "nameRev": "f81a0d92107548cdd3166a45b4f418a2c989b0f3 tags/druid-0.8.0-rc1~19", "commitHash": "f81a0d92107548cdd3166a45b4f418a2c989b0f3"}], "body": "Currently the initialization routine checks for implementations in the extension coordinates, and optionally (via `druid.extensions.searchCurrentClassloader`) in the classpath.\n\nThis can cause unexpected binding behavior where items are attempted to be bound twice if the extension happens to also be in the classpath.\n\nSteps to reproduce:\n1.Unzip a fresh copy of the services self contained tar.gz\n2.Copy an extension to the lib directory which happens to also be in the list of coordinates under config/_common `cp ~/.m2/repository/io/druid/extensions/mysql-metadata-storage/0.7.0-SNAPSHOT/mysql-metadata-storage-0.7.0-SNAPSHOT.jar lib/`\n3.Run it `java -cp config/_common/:config/coordinator/:lib/* io.druid.cli.Main server coordinator`\n\nresult:\n\n```\n2015-01-07 11:26:58,945 INFO [main] io.druid.initialization.Initialization - Adding extension module[class io.druid.metadata.storage.mysql.MySQLMetadataStorageModule] for class[io.druid.initialization.DruidModule]\nException in thread \"main\" com.google.inject.CreationException: Guice creation errors:\n\n1) Map injection failed due to duplicated key \"mysql\"\n  at com.google.inject.multibindings.MapBinder$RealMapBinder$1.initialize(MapBinder.java:357)\n  at io.druid.guice.PolyBind.optionBinder(PolyBind.java:94)\n\n1 error\n    at com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:448)\n    at com.google.inject.internal.InternalInjectorCreator.injectDynamically(InternalInjectorCreator.java:176)\n    at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:110)\n    at com.google.inject.Guice.createInjector(Guice.java:96)\n    at com.google.inject.Guice.createInjector(Guice.java:73)\n    at com.google.inject.Guice.createInjector(Guice.java:62)\n    at io.druid.initialization.Initialization.makeInjectorWithModules(Initialization.java:371)\n    at io.druid.cli.GuiceRunnable.makeInjector(GuiceRunnable.java:57)\n    at io.druid.cli.ServerRunnable.run(ServerRunnable.java:39)\n    at io.druid.cli.Main.main(Main.java:90)\n```\n\nAlternatively, put the extension's jar in the classpath, then run with a COMPLETELY DIFFERENT extension (like `druid.extensions.coordinates=[\"io.druid.extensions:druid-kafka-seven\"]`)\n\nsame effect.\n"}, {"user": "pdeva", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug", "Priority - Showstopper"], "created": "2014-11-19 04:48:56", "title": "changing segment granularity on realtime nodes causes ingestion to stop", "url": "https://github.com/apache/druid/issues/888", "closed": "2019-07-03 01:12:18", "ttf": 1686.0002777777777, "commitsDetails": [], "body": "discussed and described in detail on \nhttps://groups.google.com/forum/#!msg/druid-development/P3xjuX9xp30/HWGFubDjf0sJ\n\nThe solution given, to delete the directory of the realtime node is incorrect since it will result in loss of data\n"}, {"user": "xvrl", "commits": {}, "labels": ["Bug"], "created": "2014-11-13 17:48:54", "title": "Update `tools convertProps` to migrate configuration from 0.6 to 0.7", "url": "https://github.com/apache/druid/issues/864", "closed": "2015-01-20 18:21:11", "ttf": 68.00027777777778, "commitsDetails": [], "body": ""}, {"user": "nishantmonu51", "commits": {}, "labels": ["Bug"], "created": "2014-11-13 17:13:56", "title": "duplicate results with topN when origin is specified in queryGranularity", "url": "https://github.com/apache/druid/issues/862", "closed": "2014-11-14 20:11:42", "ttf": 1.0002777777777778, "commitsDetails": [], "body": "Query - \n\n```\n{\n \"dataSource\": \"druid_ingest\",\n \"queryType\": \"topN\",\n \"dimension\": \"DEVICE_TYPE\",\n \"intervals\": [\n   \"2014-11-02T18:30:00.000Z/2014-11-13T18:29:59.999Z\"\n ],\n \"granularity\": {\n   \"type\": \"period\",\n   \"period\": \"PT24H\",\n   \"origin\": \"2014-11-02T18:30:00.000Z\"\n },\n \"aggregations\": [\n   {\n     \"type\": \"doubleSum\",\n     \"name\": \"pageviews\",\n     \"fieldName\": \"pageviews\"\n   }\n ],\n \"threshold\": 5,\n \"metric\": \"pageviews\",\n \"filter\": {\n   \"type\": \"or\",\n   \"fields\": [\n     {\n       \"type\": \"selector\",\n       \"dimension\": \"DEVICE_TYPE\",\n       \"value\": \"COMPUTER\"\n     },\n     {\n       \"type\": \"selector\",\n       \"dimension\": \"DEVICE_TYPE\",\n       \"value\": \"MOBILE\"\n     },\n     {\n       \"type\": \"selector\",\n       \"dimension\": \"DEVICE_TYPE\",\n       \"value\": \"TABLET\"\n     },\n     {\n       \"type\": \"selector\",\n       \"dimension\": \"DEVICE_TYPE\",\n       \"value\": \"iPhone Simulator\"\n     },\n     {\n       \"type\": \"selector\",\n       \"dimension\": \"DEVICE_TYPE\",\n       \"value\": \"iPhone\"\n     }\n   ]\n },\n \"context\": {\n   \"useCache\": false,\n   \"populateCache\": false\n }\n}\n```\n\nquery results - \nhttps://dl.dropboxusercontent.com/u/70900291/dupe-results.txt\n\nbySegment results - \nhttps://dl.dropboxusercontent.com/u/70900291/bySegmentDupes\n"}, {"user": "drcrallen", "commits": {}, "labels": ["Bug"], "created": "2014-11-13 00:28:25", "title": "Coordinator and Broker don't pick up segment announcements from Historical compute nodes", "url": "https://github.com/apache/druid/issues/853", "closed": "2014-11-13 16:19:02", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "The recent 0.7.x master has some changes in how server views are persisted. There is currently a bug where segments announced by the historical node don't get picked up by the coordinator or broker if only defaults are used. The work around is to set `-Ddruid.announcer.type=batch` to force all of the services to use the same segment announcer type.\n"}, {"user": "nishantmonu51", "commits": {"8c78b97ae120934115da8e5053c5d9f7645ed334": {"commitGHEventType": "referenced", "commitUser": "drcrallen"}, "699844857992bc961c612f38781d9050be9d4e5a": {"commitGHEventType": "referenced", "commitUser": "xvrl"}}, "changesInPackagesGIT": ["processing/src/main/java/io/druid/query/search", "processing/src/main/java/io/druid/query", "processing/src/main/java/io/druid/query/topn"], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 7, "spoonMethodsChanged": 16, "INS": 11, "UPD": 18, "DEL": 4, "MOV": 6, "TOT": 39}, "title": "ClassCastException for TopN with BySegment=true", "numCommits": 1, "created": "2014-11-12 05:29:09", "closed": "2014-11-14 21:10:54", "gitStatsSummary": {"deletions": 20, "insertions": 52, "lines": 72, "gitFilesChange": 7}, "statsSkippedReason": "", "changesInPackagesSPOON": ["io.druid.query.topn.BySegmentTopNResultValue.getIntervalString()", "io.druid.query.search.BySegmentSearchResultValue.getIntervalString()", "io.druid.query.search.BySegmentSearchResultValue.toString()", "io.druid.query.topn.TopNQueryQueryToolChest.ThresholdAdjustingQueryRunner.run(io.druid.query.Query).1.apply(io.druid.query.Result)", "io.druid.query.BySegmentResultValueClass.getIntervalString()", "io.druid.query.BySegmentResultValueClass.getInterval()", "io.druid.query.search.BySegmentSearchResultValue", "io.druid.query.getInterval()", "io.druid.query.search.BySegmentSearchResultValue.getInterval()", "io.druid.query.topn.BySegmentTopNResultValue.getInterval()", "io.druid.query.FinalizeResultsQueryRunner.run(io.druid.query.Query).1.apply(java.lang.Object)", "io.druid.query.BySegmentResultValueClass.getResults()", "io.druid.query.search.SearchQueryQueryToolChest.SearchThresholdAdjustingQueryRunner.run(io.druid.query.Query).1.apply(io.druid.query.Result)", "io.druid.query.BySegmentResultValueClass.getSegmentId()", "io.druid.query.topn.BySegmentTopNResultValue", "io.druid.query.topn.BySegmentTopNResultValue.toString()"], "filteredCommits": ["8c78b97ae120934115da8e5053c5d9f7645ed334"], "url": "https://github.com/apache/druid/issues/845", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 2.000277777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "processing/src/main/java/io/druid/query/BySegmentResultValue.java", "deletions": 1, "insertions": 5, "lines": 6}, {"filePath": "processing/src/main/java/io/druid/query/topn/TopNQueryQueryToolChest.java", "deletions": 2, "insertions": 5, "lines": 7}, {"filePath": "processing/src/main/java/io/druid/query/search/SearchQueryQueryToolChest.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "processing/src/main/java/io/druid/query/topn/BySegmentTopNResultValue.java", "deletions": 7, "insertions": 15, "lines": 22}, {"filePath": "processing/src/main/java/io/druid/query/FinalizeResultsQueryRunner.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "processing/src/main/java/io/druid/query/BySegmentResultValueClass.java", "deletions": 1, "insertions": 10, "lines": 11}, {"filePath": "processing/src/main/java/io/druid/query/search/BySegmentSearchResultValue.java", "deletions": 7, "insertions": 15, "lines": 22}], "commitSpoonAstDiffStats": [{"spoonFilePath": "FinalizeResultsQueryRunner.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.FinalizeResultsQueryRunner.run(io.druid.query.Query).1.apply(java.lang.Object)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "SearchQueryQueryToolChest.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.search.SearchQueryQueryToolChest.SearchThresholdAdjustingQueryRunner.run(io.druid.query.Query).1.apply(io.druid.query.Result)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "TopNQueryQueryToolChest.java", "spoonMethods": [{"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNQueryQueryToolChest.ThresholdAdjustingQueryRunner.run(io.druid.query.Query).1.apply(io.druid.query.Result)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "BySegmentResultValueClass.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.BySegmentResultValueClass.getResults()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.BySegmentResultValueClass.getInterval()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.BySegmentResultValueClass.getSegmentId()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.BySegmentResultValueClass.getIntervalString()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "BySegmentTopNResultValue.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.topn.BySegmentTopNResultValue.getIntervalString()", "MOV": 2, "TOT": 4}, {"INS": 0, "UPD": 6, "DEL": 0, "spoonMethodName": "io.druid.query.topn.BySegmentTopNResultValue", "MOV": 0, "TOT": 6}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.topn.BySegmentTopNResultValue.getInterval()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 1, "DEL": 1, "spoonMethodName": "io.druid.query.topn.BySegmentTopNResultValue.toString()", "MOV": 1, "TOT": 4}]}, {"spoonFilePath": "BySegmentResultValue.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.getInterval()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "BySegmentSearchResultValue.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.query.search.BySegmentSearchResultValue.getIntervalString()", "MOV": 2, "TOT": 4}, {"INS": 0, "UPD": 6, "DEL": 0, "spoonMethodName": "io.druid.query.search.BySegmentSearchResultValue", "MOV": 0, "TOT": 6}, {"INS": 1, "UPD": 1, "DEL": 1, "spoonMethodName": "io.druid.query.search.BySegmentSearchResultValue.toString()", "MOV": 1, "TOT": 4}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.search.BySegmentSearchResultValue.getInterval()", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2014-11-13 09:02:07", "commitMessage": "Backport of Fix TopN with BySegment=true (#845) for 0.6.x\nSee https://github.com/metamx/druid/pull/856 for more details\nDoes not include unit tests\n", "commitUser": "drcrallen", "commitDateTime": "2014-11-13 09:10:22", "commitParents": ["e3507866e91b57a6181f5da94dfe2c05e452323a"], "commitGHEventType": "referenced", "nameRev": "8c78b97ae120934115da8e5053c5d9f7645ed334 tags/druid-0.6.161~9^2~1", "commitHash": "8c78b97ae120934115da8e5053c5d9f7645ed334"}, {"commitGitStats": [{"filePath": "processing/src/main/java/io/druid/query/BySegmentResultValue.java", "deletions": 2, "insertions": 4, "lines": 6}, {"filePath": "processing/src/main/java/io/druid/query/topn/TopNQueryQueryToolChest.java", "deletions": 2, "insertions": 5, "lines": 7}, {"filePath": "processing/src/main/java/io/druid/query/search/SearchQueryQueryToolChest.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "processing/src/main/java/io/druid/query/topn/BySegmentTopNResultValue.java", "deletions": 7, "insertions": 9, "lines": 16}, {"filePath": "processing/src/main/java/io/druid/query/FinalizeResultsQueryRunner.java", "deletions": 1, "insertions": 1, "lines": 2}, {"filePath": "processing/src/main/java/io/druid/query/BySegmentResultValueClass.java", "deletions": 1, "insertions": 4, "lines": 5}, {"filePath": "processing/src/main/java/io/druid/query/search/BySegmentSearchResultValue.java", "deletions": 7, "insertions": 9, "lines": 16}], "commitSpoonAstDiffStats": [{"spoonFilePath": "FinalizeResultsQueryRunner.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.FinalizeResultsQueryRunner.run(io.druid.query.Query).1.apply(java.lang.Object)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "SearchQueryQueryToolChest.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.query.search.SearchQueryQueryToolChest.SearchThresholdAdjustingQueryRunner.run(io.druid.query.Query).1.apply(io.druid.query.Result)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "TopNQueryQueryToolChest.java", "spoonMethods": [{"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.query.topn.TopNQueryQueryToolChest.ThresholdAdjustingQueryRunner.run(io.druid.query.Query).1.apply(io.druid.query.Result)", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "BySegmentResultValueClass.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.BySegmentResultValueClass.getResults()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.BySegmentResultValueClass.getInterval()", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.query.BySegmentResultValueClass.getSegmentId()", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "BySegmentTopNResultValue.java", "spoonMethods": [{"INS": 0, "UPD": 3, "DEL": 0, "spoonMethodName": "io.druid.query.topn.BySegmentTopNResultValue.getIntervalString()", "MOV": 0, "TOT": 3}, {"INS": 0, "UPD": 6, "DEL": 0, "spoonMethodName": "io.druid.query.topn.BySegmentTopNResultValue", "MOV": 0, "TOT": 6}, {"INS": 1, "UPD": 1, "DEL": 1, "spoonMethodName": "io.druid.query.topn.BySegmentTopNResultValue.toString()", "MOV": 1, "TOT": 4}]}, {"spoonFilePath": "BySegmentResultValue.java", "spoonMethods": [{"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.query.getIntervalString()", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "BySegmentSearchResultValue.java", "spoonMethods": [{"INS": 0, "UPD": 3, "DEL": 0, "spoonMethodName": "io.druid.query.search.BySegmentSearchResultValue.getIntervalString()", "MOV": 0, "TOT": 3}, {"INS": 0, "UPD": 6, "DEL": 0, "spoonMethodName": "io.druid.query.search.BySegmentSearchResultValue", "MOV": 0, "TOT": 6}, {"INS": 1, "UPD": 1, "DEL": 1, "spoonMethodName": "io.druid.query.search.BySegmentSearchResultValue.toString()", "MOV": 1, "TOT": 4}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2014-11-14 13:12:33", "commitMessage": "Merge pull request #863 from metamx/0.6.x-fixFor-845-rebase\n\nBackport of Fix TopN with BySegment=true (#845) for 0.6.x", "commitUser": "xvrl", "commitDateTime": "2014-11-14 13:12:33", "commitParents": ["81430ca3aa2c9eab8855c1d44b38f5c792fe78ff", "a8718956df81c8dff804579f4a2c55eb6acb363f"], "commitGHEventType": "referenced", "nameRev": "699844857992bc961c612f38781d9050be9d4e5a tags/druid-0.6.161~9", "commitHash": "699844857992bc961c612f38781d9050be9d4e5a"}], "body": "java.lang.ClassCastException: io.druid.query.topn.TopNResultValue cannot be cast to io.druid.query.topn.BySegmentTopNResultValue\n        at io.druid.query.topn.TopNQueryQueryToolChest$ThresholdAdjustingQueryRunner$1.apply(TopNQueryQueryToolChest.java:433)\n        at io.druid.query.topn.TopNQueryQueryToolChest$ThresholdAdjustingQueryRunner$1.apply(TopNQueryQueryToolChest.java:428)\n        at com.metamx.common.guava.MappingYieldingAccumulator.accumulate(MappingYieldingAccumulator.java:57)\n        at io.druid.collections.OrderedMergeSequence.makeYielder(OrderedMergeSequence.java:165)\n        at io.druid.collections.OrderedMergeSequence.toYielder(OrderedMergeSequence.java:132)\n        at com.metamx.common.guava.LazySequence.toYielder(LazySequence.java:43)\n        at io.druid.query.MetricsEmittingQueryRunner$1.toYielder(MetricsEmittingQueryRunner.java:133)\n        at com.metamx.common.guava.MappedSequence.toYielder(MappedSequence.java:46)\n        at com.metamx.common.guava.MappedSequence.toYielder(MappedSequence.java:46)\n        at io.druid.server.QueryResource.doPost(QueryResource.java:160)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n\nworkaround - Setting the threshold to more than minThreshold (1000) bypasses thresholdAdjustingQueryRunner and makes it work\n"}, {"user": "xvrl", "commits": {"adb4fec264e5fd0d4b0dbfef5d2d83a613ec9a98": {"commitGHEventType": "closed", "commitUser": "xvrl"}}, "changesInPackagesGIT": ["server/src/main/java/io/druid/metadata"], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 1, "spoonMethodsChanged": 1, "INS": 1, "UPD": 0, "DEL": 0, "MOV": 1, "TOT": 2}, "title": "Indexing service does not show failed tasks in console", "numCommits": 1, "created": "2014-11-04 04:51:54", "closed": "2014-11-12 22:31:06", "gitStatsSummary": {"deletions": 1, "insertions": 1, "lines": 2, "gitFilesChange": 1}, "statsSkippedReason": "", "changesInPackagesSPOON": ["io.druid.metadata.SQLMetadataStorageActionHandler.insert(java.lang.String,org.joda.time.DateTime,java.lang.String,java.lang.Object,boolean,java.lang.Object).1.withHandle(org.skife.jdbi.v2.Handle)"], "filteredCommits": ["adb4fec264e5fd0d4b0dbfef5d2d83a613ec9a98"], "url": "https://github.com/apache/druid/issues/819", "filteredCommitsReason": {"multipleIssueFixes": 0, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 8.000277777777777, "commitsDetails": [{"commitGitStats": [{"filePath": "server/src/main/java/io/druid/metadata/SQLMetadataStorageActionHandler.java", "deletions": 1, "insertions": 1, "lines": 2}], "commitSpoonAstDiffStats": [{"spoonFilePath": "SQLMetadataStorageActionHandler.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.metadata.SQLMetadataStorageActionHandler.insert(java.lang.String,org.joda.time.DateTime,java.lang.String,java.lang.Object,boolean,java.lang.Object).1.withHandle(org.skife.jdbi.v2.Handle)", "MOV": 1, "TOT": 2}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2014-11-04 11:04:23", "commitMessage": "properly serialize task timestamps, fixes #819\n", "commitUser": "xvrl", "commitDateTime": "2014-11-04 11:06:34", "commitParents": ["5041aaa108c184cec3efbb0a83bdc67ed6d0be6b"], "commitGHEventType": "closed", "nameRev": "adb4fec264e5fd0d4b0dbfef5d2d83a613ec9a98 tags/druid-0.7.0-rc1~105^2~11^2~10", "commitHash": "adb4fec264e5fd0d4b0dbfef5d2d83a613ec9a98"}], "body": "If a forked peon fails to start, the resulting task will not be visible on the completed tasks after it shuts down.\n"}, {"user": "textractor", "commits": {}, "labels": ["Bug"], "created": "2014-10-24 23:42:03", "title": "Typo in OrderByColumnSpec.java", "url": "https://github.com/apache/druid/issues/810", "closed": "2015-03-02 23:14:04", "ttf": 128.00027777777777, "commitsDetails": [], "body": "In the line:\n\n```\n      final String lowerDimension = directionString.toLowerCase();\n```\n\nlowerDimension should be lowerDirection.\n"}, {"user": "gianm", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug", "Priority - High", "Starter"], "created": "2014-10-21 20:40:41", "title": "indexer/time/run/millis metric doesn't work", "url": "https://github.com/apache/druid/issues/802", "closed": "2019-09-14 18:56:55", "ttf": 1788.0002777777777, "commitsDetails": [], "body": "It is always zero. This is interesting because the default duration (in TaskStatus) is -1.\n"}, {"user": "gianm", "commits": {}, "labels": ["Bug", "Priority - High"], "created": "2014-10-10 18:30:36", "title": "RTR can assign tasks to terminated workers", "url": "https://github.com/apache/druid/issues/784", "closed": "2015-06-23 07:36:28", "ttf": 255.00027777777777, "commitsDetails": [], "body": "We've observed the RTR assign a task to a worker after a terminate has been issued. This will then cause the task to fail. The (sanitized) logs are below. All the references to tasks were to the same task, and all references to IPs and instance IDs were to the same IP and ID (and those were the same instance).\n\n```\n2014-10-10 18:00:01,264 INFO [qtp1816291116-44] io.druid.indexing.overlord.DbTaskStorage - Inserting task index_realtime_xxx with status: TaskStatus{id=index_realtime_xxx, status=RUNNING, duration=-1}\n2014-10-10 18:00:01,289 INFO [TaskQueue-Manager] io.druid.indexing.overlord.TaskQueue - Asking taskRunner to run: index_realtime_xxx\n2014-10-10 18:00:01,289 INFO [TaskQueue-Manager] io.druid.indexing.overlord.RemoteTaskRunner - Added pending task index_realtime_xxx\n2014-10-10 18:00:01,992 INFO [ScalingExec--0] io.druid.indexing.overlord.scaling.EC2AutoScalingStrategy - Terminating instances[[i-xxxxxxxx]]\n2014-10-10 18:00:02,558 INFO [pool-21-thread-1] io.druid.indexing.overlord.RemoteTaskRunner - Coordinator asking Worker[10.2.3.4:8080] to add task[index_realtime_xxx]\n2014-10-10 18:00:02,563 INFO [pool-21-thread-1] io.druid.indexing.overlord.RemoteTaskRunner - Task index_realtime_xxx switched from pending to running (on [10.2.3.4:8080])\n2014-10-10 18:00:02,579 INFO [PathChildrenCache-1] io.druid.indexing.overlord.RemoteTaskRunner - Worker[10.2.3.4:8080] wrote RUNNING status for task: index_realtime_xxx\n2014-10-10 18:00:03,104 INFO [PathChildrenCache-1] io.druid.indexing.overlord.RemoteTaskRunner - [10.2.3.4:8080]: Found [index_realtime_xxx] running\n2014-10-10 18:00:04,366 INFO [PathChildrenCache-1] io.druid.indexing.overlord.RemoteTaskRunner - Failing task[index_realtime_xxx]\n2014-10-10 18:00:04,366 INFO [PathChildrenCache-1] io.druid.indexing.overlord.TaskQueue - Received FAILED status for task: index_realtime_xxx\n2014-10-10 18:00:04,368 INFO [PathChildrenCache-1] io.druid.indexing.overlord.RemoteTaskRunner - Can't shutdown! No worker running task index_realtime_xxx\n2014-10-10 18:00:04,370 INFO [PathChildrenCache-1] io.druid.indexing.overlord.DbTaskStorage - Updating task index_realtime_xxx to status: TaskStatus{id=index_realtime_xxx, status=FAILED, duration=-1}\n```\n"}, {"user": "fjy", "commits": {}, "labels": ["Bug"], "created": "2014-10-01 22:05:11", "title": "Approximate histograms do not work with groupBys", "url": "https://github.com/apache/druid/issues/775", "closed": "2014-10-07 17:53:45", "ttf": 5.000277777777778, "commitsDetails": [], "body": "https://groups.google.com/forum/#!topic/druid-development/SXXqF4dDyao\n"}, {"user": "pdeva", "commits": {}, "labels": ["Bug"], "created": "2014-09-20 19:15:03", "title": "Historical node cannot reconnect to zookeeper", "url": "https://github.com/apache/druid/issues/751", "closed": "2018-08-04 04:50:15", "ttf": 1413.0002777777777, "commitsDetails": [], "body": "Try this:\n1. take down your zookeeper cluster completely (that means clear all data from the disk too).\n2. bring it back up so its brand new (but with some ips)\n3. all other nodes will properly connect to the zookeeper and get discovered, except historical nodes.  \n   this can be verified by going to the coordinator web console, where u can see the realtime node present but not historical\n"}, {"user": "pdeva", "commits": {}, "labels": ["Bug"], "created": "2014-09-16 02:08:01", "title": "last 8kb log link in indexer console returns full log", "url": "https://github.com/apache/druid/issues/739", "closed": "2014-12-16 16:04:54", "ttf": 91.00027777777778, "commitsDetails": [], "body": "to reproduce:\n1. produce a task log > 8kb.\n2. click the 'last 8kb log' link in indexer console\n3. the whole log is returned.\n"}, {"user": "pdeva", "commits": {}, "labels": ["Bug"], "created": "2014-09-15 22:10:33", "title": "realtime nodes cannot specify timestamp column", "url": "https://github.com/apache/druid/issues/737", "closed": "2019-07-03 01:12:18", "ttf": 1751.0002777777777, "commitsDetails": [], "body": "the following config which is valid will result in timestamp column being treated as dimension:\nhttps://gist.github.com/pdeva/614465c0765cc3eb319b/e7c754ac59e44ed99d73ab62dc4b5b3e6f25d810\n\nthis was modified based on chat conversation to have a dimension exclusion:\nhttps://gist.github.com/pdeva/614465c0765cc3eb319b/3175630c37ed3024d83c6cabc827050c81919035\n\nthis results in the realtime node throwing an exception during handoff.\nMaybe the exception is because the realtime node was switched from prev schema to new one in the middle of an hour. Please clarify.\n\n```\n2014-09-15 22:03:31,066 WARN [qtp2126245039-35] org.eclipse.jetty.servlet.ServletHandler - /druid/v2/\ncom.metamx.common.ISE: Merging when a merge isn't supposed to happen[SegmentAnalysis{id='dripstat_2014-09-15T21:00:00.000Z_2014-09-15T22:00:00.000Z_2014-09-15T21:00:00.000Z', interval=[2014-09-15T21:00:00.000Z/2014-09-15T21:07:00.001Z], size=2672341}], [SegmentAnalysis{id='dripstat_2014-09-15T21:00:00.000Z_2014-09-15T22:00:00.000Z_2014-09-15T21:00:00.000Z', interval=[2014-09-15T21:00:00.000Z/2014-09-15T21:17:00.001Z], size=3442228}]\n    at io.druid.query.metadata.SegmentMetadataQueryQueryToolChest$2$2.apply(SegmentMetadataQueryQueryToolChest.java:117)\n    at io.druid.query.metadata.SegmentMetadataQueryQueryToolChest$2$2.apply(SegmentMetadataQueryQueryToolChest.java:102)\n    at io.druid.common.guava.CombiningSequence$CombiningYieldingAccumulator.accumulate(CombiningSequence.java:207)\n    at com.metamx.common.guava.BaseSequence.makeYielder(BaseSequence.java:104)\n    at com.metamx.common.guava.BaseSequence.toYielder(BaseSequence.java:81)\n    at com.metamx.common.guava.ConcatSequence.makeYielder(ConcatSequence.java:93)\n    at com.metamx.common.guava.ConcatSequence.toYielder(ConcatSequence.java:72)\n    at io.druid.query.MetricsEmittingQueryRunner$1.toYielder(MetricsEmittingQueryRunner.java:133)\n    at io.druid.query.spec.SpecificSegmentQueryRunner$2$2.call(SpecificSegmentQueryRunner.java:93)\n    at io.druid.query.spec.SpecificSegmentQueryRunner$2$2.call(SpecificSegmentQueryRunner.java:89)\n    at io.druid.query.spec.SpecificSegmentQueryRunner.doNamed(SpecificSegmentQueryRunner.java:149)\n    at io.druid.query.spec.SpecificSegmentQueryRunner.access$300(SpecificSegmentQueryRunner.java:35)\n    at io.druid.query.spec.SpecificSegmentQueryRunner$2.doItNamed(SpecificSegmentQueryRunner.java:140)\n    at io.druid.query.spec.SpecificSegmentQueryRunner$2.toYielder(SpecificSegmentQueryRunner.java:87)\n    at com.metamx.common.guava.ConcatSequence.makeYielder(ConcatSequence.java:93)\n    at com.metamx.common.guava.ConcatSequence.toYielder(ConcatSequence.java:72)\n    at io.druid.common.guava.CombiningSequence.toYielder(CombiningSequence.java:78)\n    at com.metamx.common.guava.MappedSequence.toYielder(MappedSequence.java:46)\n    at io.druid.server.QueryResource.doPost(QueryResource.java:163)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:483)\n    at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n    at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\n    at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n    at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)\n    at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n    at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n    at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1511)\n    at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1442)\n    at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1391)\n    at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1381)\n    at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n    at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:538)\n    at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:716)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n    at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:278)\n    at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:268)\n    at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:180)\n    at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:93)\n    at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:120)\n    at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:132)\n    at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:129)\n    at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:206)\n    at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:129)\n    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1650)\n    at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:83)\n    at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:300)\n    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1650)\n    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:583)\n    at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:221)\n    at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1125)\n    at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n    at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\n    at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1059)\n    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n    at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)\n    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)\n    at org.eclipse.jetty.server.Server.handle(Server.java:485)\n    at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:290)\n    at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:248)\n    at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:540)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:606)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:535)\n    at java.lang.Thread.run(Thread.java:745)\n```\n"}, {"user": "pdeva", "commits": {"888be42cbfac41ae1988091e9ed553a231dec3df": {"commitGHEventType": "referenced", "commitUser": "fjy"}, "91eecfe3e76c100b350498cb457853546ce3303c": {"commitGHEventType": "closed", "commitUser": "nishantmonu51"}}, "changesInPackagesGIT": ["server/src/main/java/io/druid/guice", "common/src/main/java/io/druid/common/utils"], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 2, "spoonMethodsChanged": 2, "INS": 0, "UPD": 2, "DEL": 1, "MOV": 0, "TOT": 3}, "title": "Indexing task with IngestSegmentFirehose shows 'success' status even if task threw Exception", "numCommits": 1, "created": "2014-09-14 05:19:03", "closed": "2014-09-30 18:58:32", "gitStatsSummary": {"deletions": 5, "insertions": 2, "lines": 7, "gitFilesChange": 2}, "statsSkippedReason": "", "changesInPackagesSPOON": ["io.druid.common.utils.VMUtils.getMaxDirectMemory()", "io.druid.guice.DruidProcessingModule.getIntermediateResultsPool(io.druid.query.DruidProcessingConfig)"], "filteredCommits": ["91eecfe3e76c100b350498cb457853546ce3303c"], "url": "https://github.com/apache/druid/issues/730", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 16.00027777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "server/src/test/java/io/druid/guice/DruidProcessingModuleTest.java", "deletions": 0, "insertions": 48, "lines": 48}, {"filePath": "server/src/main/java/io/druid/guice/DruidProcessingModule.java", "deletions": 3, "insertions": 0, "lines": 3}, {"filePath": "common/src/main/java/io/druid/common/utils/VMUtils.java", "deletions": 2, "insertions": 2, "lines": 4}], "commitSpoonAstDiffStats": [{"spoonFilePath": "VMUtils.java", "spoonMethods": [{"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.common.utils.VMUtils.getMaxDirectMemory()", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "DruidProcessingModuleTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.guice.DruidProcessingModuleTest", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "DruidProcessingModule.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.guice.DruidProcessingModule.getIntermediateResultsPool(io.druid.query.DruidProcessingConfig)", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2014-09-30 12:58:31", "commitMessage": "Merge pull request #771 from metamx/fix-730\n\nfix #730 - memory check throws exception if it fails", "commitUser": "fjy", "commitDateTime": "2014-09-30 12:58:31", "commitParents": ["16c9a029a6790d3a4dc0e912e094f9f6a61e410e", "91eecfe3e76c100b350498cb457853546ce3303c"], "commitGHEventType": "referenced", "nameRev": "888be42cbfac41ae1988091e9ed553a231dec3df tags/druid-0.6.157~14", "commitHash": "888be42cbfac41ae1988091e9ed553a231dec3df"}, {"commitGitStats": [{"filePath": "server/src/test/java/io/druid/guice/DruidProcessingModuleTest.java", "deletions": 0, "insertions": 48, "lines": 48}, {"filePath": "server/src/main/java/io/druid/guice/DruidProcessingModule.java", "deletions": 3, "insertions": 0, "lines": 3}, {"filePath": "common/src/main/java/io/druid/common/utils/VMUtils.java", "deletions": 2, "insertions": 2, "lines": 4}], "commitSpoonAstDiffStats": [{"spoonFilePath": "VMUtils.java", "spoonMethods": [{"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "io.druid.common.utils.VMUtils.getMaxDirectMemory()", "MOV": 0, "TOT": 2}]}, {"spoonFilePath": "DruidProcessingModuleTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.guice.DruidProcessingModuleTest", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "DruidProcessingModule.java", "spoonMethods": [{"INS": 0, "UPD": 0, "DEL": 1, "spoonMethodName": "io.druid.guice.DruidProcessingModule.getIntermediateResultsPool(io.druid.query.DruidProcessingConfig)", "MOV": 0, "TOT": 1}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2014-09-30 22:51:40", "commitMessage": "fix #730 - memory check throws exception if fails\n\nfix exception handling, do not catch ProvisionException\n", "commitUser": "nishantmonu51", "commitDateTime": "2014-09-30 22:51:40", "commitParents": ["16c9a029a6790d3a4dc0e912e094f9f6a61e410e"], "commitGHEventType": "closed", "nameRev": "91eecfe3e76c100b350498cb457853546ce3303c tags/druid-0.6.157~14^2", "commitHash": "91eecfe3e76c100b350498cb457853546ce3303c"}], "body": "Here is the log of the task. Result is success even though an exception was clearly thrown.\nWhat is unclear is if the task actually ran and succeeded despite the exception.\nUsing druid 0.6.146.\n\n```\n2014-09-14 05:03:31,367 INFO [main] io.druid.guice.PropertiesModule - Loading properties from runtime.properties\n2014-09-14 05:03:31,409 INFO [main] org.hibernate.validator.internal.util.Version - HV000001: Hibernate Validator 5.0.1.Final\n2014-09-14 05:03:32,023 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.guice.ExtensionsConfig] from props[druid.extensions.] as [ExtensionsConfig{searchCurrentClassloader=true, coordinates=[io.druid.extensions:druid-s3-extensions:0.6.146], localRepository='/root/.m2/repository', remoteRepositories=[http://repo1.maven.org/maven2/, https://metamx.artifactoryonline.com/metamx/pub-libs-releases-local]}]\n2014-09-14 05:03:32,166 INFO [main] io.druid.initialization.Initialization - Loading extension[io.druid.extensions:druid-s3-extensions:0.6.146] for class[io.druid.cli.CliCommandCreator]\n2014-09-14 05:03:32,530 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/io/druid/extensions/druid-s3-extensions/0.6.146/druid-s3-extensions-0.6.146.jar]\n2014-09-14 05:03:32,530 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/net/java/dev/jets3t/jets3t/0.9.1/jets3t-0.9.1.jar]\n2014-09-14 05:03:32,530 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/commons-codec/commons-codec/1.7/commons-codec-1.7.jar]\n2014-09-14 05:03:32,530 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar]\n2014-09-14 05:03:32,530 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar]\n2014-09-14 05:03:32,530 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar]\n2014-09-14 05:03:32,531 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/mx4j/mx4j/3.0.2/mx4j-3.0.2.jar]\n2014-09-14 05:03:32,531 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/javax/mail/mail/1.4.7/mail-1.4.7.jar]\n2014-09-14 05:03:32,531 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/commons-logging/commons-logging/1.1.1/commons-logging-1.1.1.jar]\n2014-09-14 05:03:32,531 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/org/bouncycastle/bcprov-jdk15/1.46/bcprov-jdk15-1.46.jar]\n2014-09-14 05:03:32,531 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar]\n2014-09-14 05:03:32,531 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/com/amazonaws/aws-java-sdk/1.6.0.1/aws-java-sdk-1.6.0.1.jar]\n2014-09-14 05:03:32,531 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.2.3/jackson-core-2.2.3.jar]\n2014-09-14 05:03:32,531 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.2.3/jackson-annotations-2.2.3.jar]\n2014-09-14 05:03:32,532 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/org/apache/httpcomponents/httpclient/4.2/httpclient-4.2.jar]\n2014-09-14 05:03:32,532 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/org/apache/httpcomponents/httpcore/4.2/httpcore-4.2.jar]\n2014-09-14 05:03:32,532 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/com/metamx/emitter/0.2.11/emitter-0.2.11.jar]\n2014-09-14 05:03:32,532 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/com/metamx/http-client/0.9.6/http-client-0.9.6.jar]\n2014-09-14 05:03:32,541 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/io/netty/netty/3.9.0.Final/netty-3.9.0.Final.jar]\n2014-09-14 05:03:32,541 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/log4j/log4j/1.2.16/log4j-1.2.16.jar]\n2014-09-14 05:03:32,541 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/com/google/guava/guava/16.0.1/guava-16.0.1.jar]\n2014-09-14 05:03:32,541 INFO [main] io.druid.initialization.Initialization - Added URL[file:/root/.m2/repository/commons-io/commons-io/2.0.1/commons-io-2.0.1.jar]\n2014-09-14 05:03:32,730 INFO [main] io.druid.initialization.Initialization - Loading extension[io.druid.extensions:druid-s3-extensions:0.6.146] for class[io.druid.initialization.DruidModule]\n2014-09-14 05:03:32,813 INFO [main] io.druid.initialization.Initialization - Adding extension module[class io.druid.storage.s3.S3StorageDruidModule] for class[io.druid.initialization.DruidModule]\n2014-09-14 05:03:32,814 INFO [main] io.druid.initialization.Initialization - Adding extension module[class io.druid.firehose.s3.S3FirehoseDruidModule] for class[io.druid.initialization.DruidModule]\n2014-09-14 05:03:33,203 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class com.metamx.emitter.core.LoggingEmitterConfig] from props[druid.emitter.logging.] as [LoggingEmitterConfig{loggerClass='com.metamx.emitter.core.LoggingEmitter', logLevel='info'}]\n2014-09-14 05:03:33,363 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.server.metrics.DruidMonitorSchedulerConfig] from props[druid.monitoring.] as [io.druid.server.metrics.DruidMonitorSchedulerConfig@1556f2dd]\n2014-09-14 05:03:33,379 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.server.metrics.MonitorsConfig] from props[druid.monitoring.] as [MonitorsConfig{monitors=[]}]\n2014-09-14 05:03:33,399 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.server.DruidNode] from props[druid.] as [DruidNode{serviceName='overlord', host='184.172.97.75:8088', port=8088}]\n2014-09-14 05:03:33,485 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.server.initialization.ServerConfig] from props[druid.server.http.] as [io.druid.server.initialization.ServerConfig@5aa6202e]\n2014-09-14 05:03:33,494 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.indexing.common.config.TaskConfig] from props[druid.indexer.task.] as [io.druid.indexing.common.config.TaskConfig@4aa3d36]\n2014-09-14 05:03:33,498 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.guice.http.DruidHttpClientConfig] from props[druid.global.http.] as [io.druid.guice.http.DruidHttpClientConfig@49d98dc5]\n2014-09-14 05:03:33,598 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.client.indexing.IndexingServiceSelectorConfig] from props[druid.selectors.indexing.] as [io.druid.client.indexing.IndexingServiceSelectorConfig@4f449e8f]\n2014-09-14 05:03:33,601 INFO [main] org.skife.config.ConfigurationObjectFactory - Assigning value [duzk.dripstat.com] for [druid.zk.service.host] on [io.druid.curator.CuratorConfig#getZkHosts()]\n2014-09-14 05:03:33,603 INFO [main] org.skife.config.ConfigurationObjectFactory - Assigning default value [30000] for [druid.zk.service.sessionTimeoutMs] on [io.druid.curator.CuratorConfig#getZkSessionTimeoutMs()]\n2014-09-14 05:03:33,604 INFO [main] org.skife.config.ConfigurationObjectFactory - Assigning value [true] for [druid.curator.compress] on [io.druid.curator.CuratorConfig#enableCompression()]\n2014-09-14 05:03:33,698 WARN [main] org.apache.curator.retry.ExponentialBackoffRetry - maxRetries too large (30). Pinning to 29\n2014-09-14 05:03:33,732 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.server.initialization.CuratorDiscoveryConfig] from props[druid.discovery.curator.] as [io.druid.server.initialization.CuratorDiscoveryConfig@4bdc8b5d]\n2014-09-14 05:03:33,907 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.indexing.common.RetryPolicyConfig] from props[druid.peon.taskActionClient.retry.] as [io.druid.indexing.common.RetryPolicyConfig@7f829c76]\n2014-09-14 05:03:33,910 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.storage.s3.AWSCredentialsConfig] from props[druid.s3.] as [io.druid.storage.s3.AWSCredentialsConfig@4e904fd5]\n2014-09-14 05:03:34,003 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.storage.s3.S3DataSegmentPusherConfig] from props[druid.storage.] as [io.druid.storage.s3.S3DataSegmentPusherConfig@15ac59c2]\n2014-09-14 05:03:34,007 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.storage.s3.S3DataSegmentArchiverConfig] from props[druid.storage.] as [io.druid.storage.s3.S3DataSegmentArchiverConfig@3cec79d3]\n2014-09-14 05:03:34,024 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.client.DruidServerConfig] from props[druid.server.] as [io.druid.client.DruidServerConfig@1fcf9739]\n2014-09-14 05:03:34,028 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.server.initialization.BatchDataSegmentAnnouncerConfig] from props[druid.announcer.] as [io.druid.server.initialization.BatchDataSegmentAnnouncerConfig@13f9ad9]\n2014-09-14 05:03:34,028 INFO [main] org.skife.config.ConfigurationObjectFactory - Using method itself for [druid.zk.paths.base] on [io.druid.server.initialization.ZkPathsConfig#getZkBasePath()]\n2014-09-14 05:03:34,029 INFO [main] org.skife.config.ConfigurationObjectFactory - Using method itself for [druid.zk.paths.indexer.tasksPath] on [io.druid.server.initialization.ZkPathsConfig#getIndexerTaskPath()]\n2014-09-14 05:03:34,029 INFO [main] org.skife.config.ConfigurationObjectFactory - Using method itself for [druid.zk.paths.indexer.statusPath] on [io.druid.server.initialization.ZkPathsConfig#getIndexerStatusPath()]\n2014-09-14 05:03:34,029 INFO [main] org.skife.config.ConfigurationObjectFactory - Using method itself for [druid.zk.paths.indexer.leaderLatchPath] on [io.druid.server.initialization.ZkPathsConfig#getIndexerLeaderLatchPath()]\n2014-09-14 05:03:34,029 INFO [main] org.skife.config.ConfigurationObjectFactory - Using method itself for [druid.zk.paths.liveSegmentsPath] on [io.druid.server.initialization.ZkPathsConfig#getLiveSegmentsPath()]\n2014-09-14 05:03:34,029 INFO [main] org.skife.config.ConfigurationObjectFactory - Using method itself for [druid.zk.paths.loadQueuePath] on [io.druid.server.initialization.ZkPathsConfig#getLoadQueuePath()]\n2014-09-14 05:03:34,030 INFO [main] org.skife.config.ConfigurationObjectFactory - Using method itself for [druid.zk.paths.announcementsPath] on [io.druid.server.initialization.ZkPathsConfig#getAnnouncementsPath()]\n2014-09-14 05:03:34,030 INFO [main] org.skife.config.ConfigurationObjectFactory - Using method itself for [druid.zk.paths.connectorPath] on [io.druid.server.initialization.ZkPathsConfig#getConnectorPath()]\n2014-09-14 05:03:34,030 INFO [main] org.skife.config.ConfigurationObjectFactory - Using method itself for [druid.zk.paths.coordinatorPath] on [io.druid.server.initialization.ZkPathsConfig#getCoordinatorPath()]\n2014-09-14 05:03:34,030 INFO [main] org.skife.config.ConfigurationObjectFactory - Using method itself for [druid.zk.paths.indexer.announcementsPath] on [io.druid.server.initialization.ZkPathsConfig#getIndexerAnnouncementPath()]\n2014-09-14 05:03:34,031 INFO [main] org.skife.config.ConfigurationObjectFactory - Using method itself for [druid.zk.paths.propertiesPath] on [io.druid.server.initialization.ZkPathsConfig#getPropertiesPath()]\n2014-09-14 05:03:34,031 INFO [main] org.skife.config.ConfigurationObjectFactory - Using method itself for [druid.zk.paths.servedSegmentsPath] on [io.druid.server.initialization.ZkPathsConfig#getServedSegmentsPath()]\n2014-09-14 05:03:34,058 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[interface io.druid.server.coordination.DataSegmentAnnouncerProvider] from props[druid.announcer.] as [io.druid.server.coordination.BatchDataSegmentAnnouncerProvider@3ac8cf9b]\n2014-09-14 05:03:34,062 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[interface io.druid.client.FilteredServerViewProvider] from props[druid.announcer.] as [io.druid.client.FilteredBatchServerViewProvider@c1fa7d4]\n2014-09-14 05:03:34,072 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.query.QueryConfig] from props[druid.query.] as [io.druid.query.QueryConfig@537c8c7e]\n2014-09-14 05:03:34,084 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.query.search.search.SearchQueryConfig] from props[druid.query.search.] as [io.druid.query.search.search.SearchQueryConfig@15e0fe05]\n2014-09-14 05:03:34,088 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.query.groupby.GroupByQueryConfig] from props[druid.query.groupBy.] as [io.druid.query.groupby.GroupByQueryConfig@18ac53e8]\n2014-09-14 05:03:34,089 INFO [main] org.skife.config.ConfigurationObjectFactory - Assigning value [1003960960] for [druid.computation.buffer.size] on [io.druid.query.DruidProcessingConfig#intermediateComputeSizeBytes()]\n2014-09-14 05:03:34,089 INFO [main] org.skife.config.ConfigurationObjectFactory - Assigning value [2] for [druid.processing.numThreads] on [io.druid.query.DruidProcessingConfig#getNumThreads()]\n2014-09-14 05:03:34,089 INFO [main] org.skife.config.ConfigurationObjectFactory - Using method itself for [${base_path}.columnCache.sizeBytes] on [io.druid.query.DruidProcessingConfig#columnCacheSizeBytes()]\n2014-09-14 05:03:34,089 INFO [main] org.skife.config.ConfigurationObjectFactory - Assigning default value [processing-%s] for [${base_path}.formatString] on [com.metamx.common.concurrent.ExecutorServiceConfig#getFormatString()]\n2014-09-14 05:03:34,095 WARN [main] io.druid.guice.DruidProcessingModule - Guice provision errors:\n\n1) Not enough direct memory.  Please adjust -XX:MaxDirectMemorySize, druid.processing.buffer.sizeBytes, or druid.processing.numThreads: maxDirectMemory[1,401,946,112], memoryNeeded[3,011,882,880] = druid.processing.buffer.sizeBytes[1,003,960,960] * ( druid.processing.numThreads[2] + 1 )\n\n1 error\ncom.google.inject.ProvisionException: Guice provision errors:\n\n1) Not enough direct memory.  Please adjust -XX:MaxDirectMemorySize, druid.processing.buffer.sizeBytes, or druid.processing.numThreads: maxDirectMemory[1,401,946,112], memoryNeeded[3,011,882,880] = druid.processing.buffer.sizeBytes[1,003,960,960] * ( druid.processing.numThreads[2] + 1 )\n\n1 error\n    at io.druid.guice.DruidProcessingModule.getIntermediateResultsPool(DruidProcessingModule.java:87)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:483)\n    at com.google.inject.internal.ProviderMethod.get(ProviderMethod.java:105)\n    at com.google.inject.internal.ProviderInternalFactory.provision(ProviderInternalFactory.java:86)\n    at com.google.inject.internal.InternalFactoryToInitializableAdapter.provision(InternalFactoryToInitializableAdapter.java:55)\n    at com.google.inject.internal.ProviderInternalFactory.circularGet(ProviderInternalFactory.java:66)\n    at com.google.inject.internal.InternalFactoryToInitializableAdapter.get(InternalFactoryToInitializableAdapter.java:47)\n    at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46)\n    at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1058)\n    at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)\n    at com.google.inject.Scopes$1$1.get(Scopes.java:65)\n    at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:41)\n    at com.google.inject.internal.SingleParameterInjector.inject(SingleParameterInjector.java:38)\n    at com.google.inject.internal.SingleParameterInjector.getAll(SingleParameterInjector.java:62)\n    at com.google.inject.internal.ConstructorInjector.provision(ConstructorInjector.java:107)\n    at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:88)\n    at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:269)\n    at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46)\n    at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1058)\n    at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)\n    at com.google.inject.Scopes$1$1.get(Scopes.java:65)\n    at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:41)\n    at com.google.inject.internal.SingleParameterInjector.inject(SingleParameterInjector.java:38)\n    at com.google.inject.internal.SingleParameterInjector.getAll(SingleParameterInjector.java:62)\n    at com.google.inject.internal.ConstructorInjector.provision(ConstructorInjector.java:107)\n    at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:88)\n    at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:269)\n    at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46)\n    at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1058)\n    at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)\n    at com.google.inject.Scopes$1$1.get(Scopes.java:65)\n    at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:41)\n    at com.google.inject.internal.FactoryProxy.get(FactoryProxy.java:56)\n    at com.google.inject.internal.InjectorImpl$3$1.call(InjectorImpl.java:1005)\n    at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1058)\n    at com.google.inject.internal.InjectorImpl$3.get(InjectorImpl.java:1001)\n    at com.google.inject.spi.ProviderLookup$1.get(ProviderLookup.java:90)\n    at com.google.inject.spi.ProviderLookup$1.get(ProviderLookup.java:90)\n    at com.google.inject.multibindings.MapBinder$RealMapBinder$2.get(MapBinder.java:389)\n    at com.google.inject.multibindings.MapBinder$RealMapBinder$2.get(MapBinder.java:385)\n    at com.google.inject.internal.ProviderInternalFactory.provision(ProviderInternalFactory.java:86)\n    at com.google.inject.internal.InternalFactoryToInitializableAdapter.provision(InternalFactoryToInitializableAdapter.java:55)\n    at com.google.inject.internal.ProviderInternalFactory.circularGet(ProviderInternalFactory.java:66)\n    at com.google.inject.internal.InternalFactoryToInitializableAdapter.get(InternalFactoryToInitializableAdapter.java:47)\n    at com.google.inject.internal.SingleParameterInjector.inject(SingleParameterInjector.java:38)\n    at com.google.inject.internal.SingleParameterInjector.getAll(SingleParameterInjector.java:62)\n    at com.google.inject.internal.ConstructorInjector.provision(ConstructorInjector.java:107)\n    at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:88)\n    at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:269)\n    at com.google.inject.internal.FactoryProxy.get(FactoryProxy.java:56)\n    at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46)\n    at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1058)\n    at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)\n    at com.google.inject.Scopes$1$1.get(Scopes.java:65)\n    at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:41)\n    at com.google.inject.internal.SingleParameterInjector.inject(SingleParameterInjector.java:38)\n    at com.google.inject.internal.SingleParameterInjector.getAll(SingleParameterInjector.java:62)\n    at com.google.inject.internal.ConstructorInjector.provision(ConstructorInjector.java:107)\n    at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:88)\n    at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:269)\n    at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46)\n    at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1058)\n    at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)\n    at com.google.inject.Scopes$1$1.get(Scopes.java:65)\n    at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:41)\n    at com.google.inject.internal.SingleParameterInjector.inject(SingleParameterInjector.java:38)\n    at com.google.inject.internal.SingleParameterInjector.getAll(SingleParameterInjector.java:62)\n    at com.google.inject.internal.ConstructorInjector.provision(ConstructorInjector.java:107)\n    at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:88)\n    at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:269)\n    at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46)\n    at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1058)\n    at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)\n    at io.druid.guice.LifecycleScope$1.get(LifecycleScope.java:49)\n    at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:41)\n    at com.google.inject.internal.FactoryProxy.get(FactoryProxy.java:56)\n    at com.google.inject.internal.SingleParameterInjector.inject(SingleParameterInjector.java:38)\n    at com.google.inject.internal.SingleParameterInjector.getAll(SingleParameterInjector.java:62)\n    at com.google.inject.internal.ConstructorInjector.provision(ConstructorInjector.java:107)\n    at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:88)\n    at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:269)\n    at com.google.inject.internal.InjectorImpl$3$1.call(InjectorImpl.java:1005)\n    at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1051)\n    at com.google.inject.internal.InjectorImpl$3.get(InjectorImpl.java:1001)\n    at com.google.inject.internal.InjectorImpl.getInstance(InjectorImpl.java:1036)\n    at io.druid.guice.LifecycleModule$2.start(LifecycleModule.java:134)\n    at io.druid.cli.GuiceRunnable.initLifecycle(GuiceRunnable.java:72)\n    at io.druid.cli.CliPeon.run(CliPeon.java:202)\n    at io.druid.cli.Main.main(Main.java:90)\n2014-09-14 05:03:34,104 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.query.topn.TopNQueryConfig] from props[druid.query.topN.] as [io.druid.query.topn.TopNQueryConfig@36b310aa]\n2014-09-14 05:03:34,131 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[interface io.druid.server.log.RequestLoggerProvider] from props[druid.request.logging.] as [io.druid.server.log.NoopRequestLoggerProvider@2a32fb6]\n2014-09-14 05:03:34,141 INFO [main] org.eclipse.jetty.util.log - Logging initialized @3378ms\n2014-09-14 05:03:34,245 INFO [main] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Invoking start method[public void com.metamx.emitter.core.LoggingEmitter.start()] on object[com.metamx.emitter.core.LoggingEmitter@4f3e7344].\n2014-09-14 05:03:34,246 INFO [main] com.metamx.emitter.core.LoggingEmitter - Start: started [true]\n2014-09-14 05:03:34,246 INFO [main] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Invoking start method[public void com.metamx.emitter.service.ServiceEmitter.start()] on object[com.metamx.emitter.service.ServiceEmitter@1e141e42].\n2014-09-14 05:03:34,247 INFO [main] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Invoking start method[public void com.metamx.metrics.MonitorScheduler.start()] on object[com.metamx.metrics.MonitorScheduler@228cea97].\n2014-09-14 05:03:34,249 INFO [main] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Invoking start method[public void com.metamx.http.client.HttpClient.start()] on object[com.metamx.http.client.HttpClient@5524b72f].\n2014-09-14 05:03:34,249 INFO [main] io.druid.curator.CuratorModule - Starting Curator\n2014-09-14 05:03:34,249 INFO [main] org.apache.curator.framework.imps.CuratorFrameworkImpl - Starting\n2014-09-14 05:03:34,260 INFO [main] org.apache.zookeeper.ZooKeeper - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT\n2014-09-14 05:03:34,260 INFO [main] org.apache.zookeeper.ZooKeeper - Client environment:host.name=du-indexer.dripstat.com\n2014-09-14 05:03:34,260 INFO [main] org.apache.zookeeper.ZooKeeper - Client environment:java.version=1.8.0_20\n2014-09-14 05:03:34,260 INFO [main] org.apache.zookeeper.ZooKeeper - Client environment:java.vendor=Oracle Corporation\n2014-09-14 05:03:34,260 INFO [main] org.apache.zookeeper.ZooKeeper - Client environment:java.home=/usr/lib/jvm/java-8-oracle/jre\n2014-09-14 05:03:34,260 INFO [main] org.apache.zookeeper.ZooKeeper - Client environment:java.class.path=lib/org.abego.treelayout.core-1.0.1.jar:lib/rhino-1.7R4.jar:lib/wagon-provider-api-2.4.jar:lib/mysql-connector-java-5.1.18.jar:lib/javax.inject-1.jar:lib/jsr305-2.0.1.jar:lib/jackson-databind-2.2.3.jar:lib/lz4-1.1.2.jar:lib/commons-logging-1.1.1.jar:lib/mx4j-3.0.2.jar:lib/guava-16.0.1.jar:lib/maxminddb-0.2.0.jar:lib/aether-util-0.9.0.M2.jar:lib/jersey-server-1.17.1.jar:lib/commons-pool-1.6.jar:lib/maven-aether-provider-3.1.1.jar:lib/java-xmlbuilder-0.4.jar:lib/maven-model-builder-3.1.1.jar:lib/jackson-mapper-asl-1.9.13.jar:lib/jersey-core-1.17.1.jar:lib/okhttp-1.0.2.jar:lib/jetty-servlets-9.2.2.v20140723.jar:lib/asm-3.1.jar:lib/hibernate-validator-5.0.1.Final.jar:lib/commons-cli-1.2.jar:lib/curator-client-2.6.0.jar:lib/maven-repository-metadata-3.1.1.jar:lib/jline-0.9.94.jar:lib/google-http-client-jackson2-1.15.0-rc.jar:lib/log4j-1.2.16.jar:lib/curator-recipes-2.6.0.jar:lib/zookeeper-3.4.6.jar:lib/aether-connector-file-0.9.0.M2.jar:lib/irc-api-1.0-0011.jar:lib/jackson-jaxrs-json-provider-2.2.3.jar:lib/emitter-0.2.11.jar:lib/druid-indexing-service-0.6.146.jar:lib/http-client-0.9.6.jar:lib/druid-indexing-hadoop-0.6.146.jar:lib/druid-common-0.6.146.jar:lib/jackson-datatype-guava-2.2.3.jar:lib/commons-lang-2.6.jar:lib/activation-1.1.1.jar:lib/joda-time-2.1.jar:lib/jackson-core-2.2.3.jar:lib/sigar-1.6.5.132.jar:lib/jetty-http-9.2.2.v20140723.jar:lib/icu4j-4.8.1.jar:lib/slf4j-api-1.6.4.jar:lib/slf4j-log4j12-1.6.2.jar:lib/google-http-client-1.15.0-rc.jar:lib/curator-framework-2.6.0.jar:lib/druid-server-0.6.146.jar:lib/commons-codec-1.7.jar:lib/compress-lzf-0.8.4.jar:lib/aether-impl-0.9.0.M2.jar:lib/guice-4.0-beta.jar:lib/aopalliance-1.0.jar:lib/maven-settings-builder-3.1.1.jar:lib/druid-api-0.2.7.jar:lib/jackson-core-asl-1.9.13.jar:lib/jetty-server-9.2.2.v20140723.jar:lib/xpp3-1.1.4c.jar:lib/geoip2-0.4.0.jar:lib/netty-3.9.0.Final.jar:lib/guice-servlet-4.0-beta.jar:lib/jetty-io-9.2.2.v20140723.jar:lib/jackson-jaxrs-base-2.2.3.jar:lib/config-magic-0.9.jar:lib/maven-settings-3.1.1.jar:lib/bytebuffer-collections-0.0.2.jar:lib/aether-api-0.9.0.M2.jar:lib/protobuf-java-2.5.0.jar:lib/druid-processing-0.6.146.jar:lib/javax.el-3.0.0.jar:lib/jboss-logging-3.1.1.GA.jar:lib/jersey-guice-1.17.1.jar:lib/aether-connector-okhttp-0.0.9.jar:lib/tesla-aether-0.0.5.jar:lib/jetty-servlet-9.2.2.v20140723.jar:lib/plexus-interpolation-1.19.jar:lib/bcprov-jdk15-1.46.jar:lib/commons-dbcp-1.4.jar:lib/plexus-utils-3.0.15.jar:lib/spymemcached-2.8.4.jar:lib/httpclient-4.2.jar:lib/jackson-dataformat-smile-2.2.3.jar:lib/airline-0.6.jar:lib/curator-x-discovery-2.6.0.jar:lib/httpcore-4.2.jar:lib/jetty-proxy-9.2.2.v20140723.jar:lib/antlr4-runtime-4.0.jar:lib/classmate-0.8.0.jar:lib/server-metrics-0.0.9.jar:lib/javax.servlet-api-3.1.0.jar:lib/maven-model-3.1.1.jar:lib/jdbi-2.32.jar:lib/jetty-security-9.2.2.v20140723.jar:lib/alphanum-1.0.3.jar:lib/extendedset-1.3.4.jar:lib/druid-services-0.6.146.jar:lib/jetty-continuation-9.2.2.v20140723.jar:lib/jackson-module-jaxb-annotations-2.2.3.jar:lib/java-util-0.26.6.jar:lib/validation-api-1.1.0.Final.jar:lib/opencsv-2.3.jar:lib/guice-multibindings-4.0-beta.jar:lib/jersey-servlet-1.17.1.jar:lib/jackson-datatype-joda-2.2.3.jar:lib/jetty-util-9.2.2.v20140723.jar:lib/jets3t-0.9.1.jar:lib/mail-1.4.7.jar:lib/commons-io-2.0.1.jar:lib/jackson-annotations-2.2.3.jar:lib/aws-java-sdk-1.6.0.1.jar:lib/jetty-client-9.2.2.v20140723.jar:lib/aether-spi-0.9.0.M2.jar:config/overlord:/opt/dripstat/dripstat.jar\n2014-09-14 05:03:34,260 INFO [main] org.apache.zookeeper.ZooKeeper - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib\n2014-09-14 05:03:34,260 INFO [main] org.apache.zookeeper.ZooKeeper - Client environment:java.io.tmpdir=/tmp\n2014-09-14 05:03:34,261 INFO [main] org.apache.zookeeper.ZooKeeper - Client environment:java.compiler=<NA>\n2014-09-14 05:03:34,261 INFO [main] org.apache.zookeeper.ZooKeeper - Client environment:os.name=Linux\n2014-09-14 05:03:34,261 INFO [main] org.apache.zookeeper.ZooKeeper - Client environment:os.arch=amd64\n2014-09-14 05:03:34,261 INFO [main] org.apache.zookeeper.ZooKeeper - Client environment:os.version=3.13.0-32-generic\n2014-09-14 05:03:34,261 INFO [main] org.apache.zookeeper.ZooKeeper - Client environment:user.name=root\n2014-09-14 05:03:34,261 INFO [main] org.apache.zookeeper.ZooKeeper - Client environment:user.home=/root\n2014-09-14 05:03:34,261 INFO [main] org.apache.zookeeper.ZooKeeper - Client environment:user.dir=/opt/druid-services-0.6.146\n2014-09-14 05:03:34,262 INFO [main] org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=duzk.dripstat.com sessionTimeout=30000 watcher=org.apache.curator.ConnectionState@4893b344\n2014-09-14 05:03:34,284 INFO [main] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Invoking start method[public void io.druid.curator.discovery.ServerDiscoverySelector.start() throws java.lang.Exception] on object[io.druid.curator.discovery.ServerDiscoverySelector@107f4980].\n2014-09-14 05:03:34,292 INFO [main-SendThread(104.130.2.143:2181)] org.apache.zookeeper.ClientCnxn - Opening socket connection to server 104.130.2.143/104.130.2.143:2181. Will not attempt to authenticate using SASL (unknown error)\n2014-09-14 05:03:34,383 INFO [main-SendThread(104.130.2.143:2181)] org.apache.zookeeper.ClientCnxn - Socket connection established to 104.130.2.143/104.130.2.143:2181, initiating session\n2014-09-14 05:03:34,424 INFO [main-SendThread(104.130.2.143:2181)] org.apache.zookeeper.ClientCnxn - Session establishment complete on server 104.130.2.143/104.130.2.143:2181, sessionid = 0x14815a952eb0060, negotiated timeout = 30000\n2014-09-14 05:03:34,428 INFO [main-EventThread] org.apache.curator.framework.state.ConnectionStateManager - State change: CONNECTED\n2014-09-14 05:03:35,705 INFO [main] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Invoking start method[public void io.druid.curator.announcement.Announcer.start()] on object[io.druid.curator.announcement.Announcer@c35af2a].\n2014-09-14 05:03:35,706 INFO [main] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Invoking start method[public void io.druid.client.ServerInventoryView.start() throws java.lang.Exception] on object[io.druid.client.BatchServerInventoryView@3cb8c8ce].\n2014-09-14 05:03:35,711 INFO [main] org.eclipse.jetty.server.Server - jetty-9.2.2.v20140723\nSep 14, 2014 5:03:35 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\nINFO: Registering com.fasterxml.jackson.jaxrs.json.JacksonJsonProvider as a provider class\nSep 14, 2014 5:03:35 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\nINFO: Registering io.druid.server.StatusResource as a root resource class\nSep 14, 2014 5:03:35 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\nINFO: Initiating Jersey application, version 'Jersey: 1.17.1 02/28/2013 12:47 PM'\n2014-09-14 05:03:35,929 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - Created new InventoryCacheListener for /druid/segments/104.130.16.70:8080\n2014-09-14 05:03:35,929 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - Starting inventory cache for 104.130.16.70:8080, inventoryPath /druid/segments/104.130.16.70:8080\n2014-09-14 05:03:35,930 INFO [ServerInventoryView-0] io.druid.client.BatchServerInventoryView - New Server[DruidServerMetadata{name='104.130.16.70:8080', host='104.130.16.70:8080', maxSize=3000000000, tier='_default_tier', type='historical', priority='0'}]\n2014-09-14 05:03:35,930 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - Created new InventoryCacheListener for /druid/segments/23.253.158.204:8080\n2014-09-14 05:03:35,931 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - Starting inventory cache for 23.253.158.204:8080, inventoryPath /druid/segments/23.253.158.204:8080\n2014-09-14 05:03:35,931 INFO [ServerInventoryView-0] io.druid.client.BatchServerInventoryView - New Server[DruidServerMetadata{name='23.253.158.204:8080', host='23.253.158.204:8080', maxSize=0, tier='_default_tier', type='realtime', priority='0'}]\n2014-09-14 05:03:35,931 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - Created new InventoryCacheListener for /druid/segments/23.253.158.210:8080\n2014-09-14 05:03:35,931 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - Starting inventory cache for 23.253.158.210:8080, inventoryPath /druid/segments/23.253.158.210:8080\n2014-09-14 05:03:35,931 INFO [ServerInventoryView-0] io.druid.client.BatchServerInventoryView - New Server[DruidServerMetadata{name='23.253.158.210:8080', host='23.253.158.210:8080', maxSize=0, tier='_default_tier', type='realtime', priority='0'}]\nSep 14, 2014 5:03:35 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\nINFO: Binding com.fasterxml.jackson.jaxrs.json.JacksonJsonProvider to GuiceManagedComponentProvider with the scope \"Singleton\"\nSep 14, 2014 5:03:36 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\nINFO: Binding io.druid.server.QueryResource to GuiceInstantiatedComponentProvider\nSep 14, 2014 5:03:36 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\nINFO: Binding io.druid.segment.realtime.firehose.ChatHandlerResource to GuiceInstantiatedComponentProvider\nSep 14, 2014 5:03:36 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\nINFO: Binding io.druid.server.StatusResource to GuiceManagedComponentProvider with the scope \"Undefined\"\n2014-09-14 05:03:36,452 INFO [main] org.eclipse.jetty.server.handler.ContextHandler - Started o.e.j.s.ServletContextHandler@485e13d7{/,null,AVAILABLE}\n2014-09-14 05:03:36,461 INFO [main] org.eclipse.jetty.server.ServerConnector - Started ServerConnector@5984feef{HTTP/1.1}{0.0.0.0:8088}\n2014-09-14 05:03:36,461 INFO [main] org.eclipse.jetty.server.Server - Started @5700ms\n2014-09-14 05:03:36,462 INFO [main] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Invoking start method[public void io.druid.server.coordination.AbstractDataSegmentAnnouncer.start()] on object[io.druid.server.coordination.BatchDataSegmentAnnouncer@6c0905f6].\n2014-09-14 05:03:36,462 INFO [main] io.druid.server.coordination.AbstractDataSegmentAnnouncer - Announcing self[DruidServerMetadata{name='184.172.97.75:8088', host='184.172.97.75:8088', maxSize=0, tier='_default_tier', type='indexer-executor', priority='0'}] at [/druid/announcements/184.172.97.75:8088]\n2014-09-14 05:03:36,612 INFO [main] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Invoking start method[public void io.druid.indexing.worker.executor.ExecutorLifecycle.start()] on object[io.druid.indexing.worker.executor.ExecutorLifecycle@1d61a348].\n2014-09-14 05:03:36,715 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - Created new InventoryCacheListener for /druid/segments/184.172.97.75:8088\n2014-09-14 05:03:36,715 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - Starting inventory cache for 184.172.97.75:8088, inventoryPath /druid/segments/184.172.97.75:8088\n2014-09-14 05:03:36,715 INFO [ServerInventoryView-0] io.druid.client.BatchServerInventoryView - New Server[DruidServerMetadata{name='184.172.97.75:8088', host='184.172.97.75:8088', maxSize=0, tier='_default_tier', type='indexer-executor', priority='0'}]\n2014-09-14 05:03:36,736 INFO [main] io.druid.indexing.worker.executor.ExecutorLifecycle - Running with task: {\n  \"type\" : \"index\",\n  \"id\" : \"index_dripstat_2014-09-14T05:03:30.411Z\",\n  \"schema\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"dripstat\",\n      \"parser\" : null,\n      \"metricsSpec\" : [ {\n        \"type\" : \"count\",\n        \"name\" : \"count\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"min\",\n        \"fieldName\" : \"min\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"max\",\n        \"fieldName\" : \"max\"\n      }, {\n        \"type\" : \"longSum\",\n        \"name\" : \"callCount\",\n        \"fieldName\" : \"callCount\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"totalTime\",\n        \"fieldName\" : \"totalTime\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"value\",\n        \"fieldName\" : \"value\"\n      } ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"HOUR\",\n        \"queryGranularity\" : {\n          \"type\" : \"duration\",\n          \"duration\" : 60000,\n          \"origin\" : \"1970-01-01T00:00:00.000Z\"\n        },\n        \"intervals\" : [ \"2014-09-07T01:00:00.000Z/2014-09-07T02:00:00.000Z\" ]\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"ingestSegment\",\n        \"dataSource\" : \"dripstat\",\n        \"interval\" : \"2014-09-07T01:00:00.000Z/2014-09-07T02:00:00.000Z\",\n        \"filter\" : null,\n        \"dimensions\" : [ ],\n        \"metrics\" : [ ]\n      }\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"targetPartitionSize\" : 5000000,\n      \"rowFlushBoundary\" : 500000,\n      \"numShards\" : -1\n    }\n  },\n  \"dataSource\" : \"dripstat\",\n  \"groupId\" : \"index_dripstat_2014-09-14T05:03:30.411Z\",\n  \"interval\" : \"2014-09-07T01:00:00.000Z/2014-09-07T02:00:00.000Z\",\n  \"resource\" : {\n    \"availabilityGroup\" : \"index_dripstat_2014-09-14T05:03:30.411Z\",\n    \"requiredCapacity\" : 1\n  }\n}\n2014-09-14 05:03:36,740 INFO [main] io.druid.indexing.common.actions.RemoteTaskActionClient - Performing action for task[index_dripstat_2014-09-14T05:03:30.411Z]: LockTryAcquireAction{interval=2014-09-07T01:00:00.000Z/2014-09-07T02:00:00.000Z}\n2014-09-14 05:03:36,747 INFO [main] io.druid.indexing.common.actions.RemoteTaskActionClient - Submitting action for task[index_dripstat_2014-09-14T05:03:30.411Z] to overlord[http://184.172.97.75:8080/druid/indexer/v1/action]: LockTryAcquireAction{interval=2014-09-07T01:00:00.000Z/2014-09-07T02:00:00.000Z}\n2014-09-14 05:03:36,762 INFO [main] com.metamx.http.client.pool.ChannelResourceFactory - Generating: http://184.172.97.75:8080\n2014-09-14 05:03:36,799 INFO [main] com.metamx.http.client.pool.ChannelResourceFactory - Generating: http://184.172.97.75:8080\n2014-09-14 05:03:36,800 INFO [main] com.metamx.http.client.pool.ChannelResourceFactory - Generating: http://184.172.97.75:8080\n2014-09-14 05:03:36,801 INFO [main] com.metamx.http.client.pool.ChannelResourceFactory - Generating: http://184.172.97.75:8080\n2014-09-14 05:03:36,801 INFO [main] com.metamx.http.client.pool.ChannelResourceFactory - Generating: http://184.172.97.75:8080\n2014-09-14 05:03:36,890 INFO [task-runner-0] io.druid.indexing.overlord.ThreadPoolTaskRunner - Running task: index_dripstat_2014-09-14T05:03:30.411Z\n2014-09-14 05:03:36,891 INFO [task-runner-0] io.druid.indexing.common.actions.RemoteTaskActionClient - Performing action for task[index_dripstat_2014-09-14T05:03:30.411Z]: LockListAction{}\n2014-09-14 05:03:36,895 INFO [task-runner-0] io.druid.indexing.common.actions.RemoteTaskActionClient - Submitting action for task[index_dripstat_2014-09-14T05:03:30.411Z] to overlord[http://184.172.97.75:8080/druid/indexer/v1/action]: LockListAction{}\n2014-09-14 05:03:36,930 INFO [task-runner-0] io.druid.indexing.firehose.IngestSegmentFirehoseFactory - Connecting firehose: dataSource[dripstat], interval[2014-09-07T01:00:00.000Z/2014-09-07T02:00:00.000Z]\n2014-09-14 05:03:36,937 INFO [task-runner-0] io.druid.indexing.common.actions.RemoteTaskActionClient - Performing action for task[reingest]: SegmentListUsedAction{dataSource='dripstat', interval=2014-09-07T01:00:00.000Z/2014-09-07T02:00:00.000Z}\n2014-09-14 05:03:36,941 INFO [task-runner-0] io.druid.indexing.common.actions.RemoteTaskActionClient - Submitting action for task[reingest] to overlord[http://184.172.97.75:8080/druid/indexer/v1/action]: SegmentListUsedAction{dataSource='dripstat', interval=2014-09-07T01:00:00.000Z/2014-09-07T02:00:00.000Z}\n2014-09-14 05:03:43,006 INFO [task-runner-0] io.druid.storage.s3.S3DataSegmentPuller - Pulling index at path[s3://dripstat/dripstat/2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z/2014-09-07T01:00:00.000Z/0/index.zip] to outDir[/tmp/persistent/task/reingest/work/dripstat/2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z/2014-09-07T01:00:00.000Z/0]\n2014-09-14 05:03:44,120 INFO [task-runner-0] io.druid.storage.s3.S3DataSegmentPuller - Pull of file[dripstat/dripstat/2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z/2014-09-07T01:00:00.000Z/0/index.zip] completed in 417 millis\n2014-09-14 05:03:44,138 INFO [task-runner-0] io.druid.guice.PropertiesModule - Loading properties from runtime.properties\n2014-09-14 05:03:44,145 INFO [task-runner-0] org.skife.config.ConfigurationObjectFactory - Assigning value [1003960960] for [druid.computation.buffer.size] on [io.druid.query.DruidProcessingConfig#intermediateComputeSizeBytes()]\n2014-09-14 05:03:44,145 INFO [task-runner-0] org.skife.config.ConfigurationObjectFactory - Assigning value [2] for [druid.processing.numThreads] on [io.druid.query.DruidProcessingConfig#getNumThreads()]\n2014-09-14 05:03:44,145 INFO [task-runner-0] org.skife.config.ConfigurationObjectFactory - Using method itself for [${base_path}.columnCache.sizeBytes] on [io.druid.query.DruidProcessingConfig#columnCacheSizeBytes()]\n2014-09-14 05:03:44,145 INFO [task-runner-0] org.skife.config.ConfigurationObjectFactory - Assigning default value [processing-%s] for [${base_path}.formatString] on [com.metamx.common.concurrent.ExecutorServiceConfig#getFormatString()]\n2014-09-14 05:03:44,205 INFO [task-runner-0] io.druid.segment.CompressedPools - Allocating new littleEndByteBuf[1]\n2014-09-14 05:03:44,205 INFO [task-runner-0] io.druid.segment.CompressedPools - Allocating new outputBytesPool[1]\n2014-09-14 05:03:44,455 INFO [task-runner-0] io.druid.indexing.common.task.IndexTask - Determining partitions for interval[2014-09-07T01:00:00.000Z/2014-09-07T02:00:00.000Z] with targetPartitionSize[5000000]\n2014-09-14 05:03:44,455 INFO [task-runner-0] io.druid.indexing.firehose.IngestSegmentFirehoseFactory - Connecting firehose: dataSource[dripstat], interval[2014-09-07T01:00:00.000Z/2014-09-07T02:00:00.000Z]\n2014-09-14 05:03:44,455 INFO [task-runner-0] io.druid.indexing.common.actions.RemoteTaskActionClient - Performing action for task[reingest]: SegmentListUsedAction{dataSource='dripstat', interval=2014-09-07T01:00:00.000Z/2014-09-07T02:00:00.000Z}\n2014-09-14 05:03:44,456 INFO [task-runner-0] io.druid.indexing.common.actions.RemoteTaskActionClient - Submitting action for task[reingest] to overlord[http://184.172.97.75:8080/druid/indexer/v1/action]: SegmentListUsedAction{dataSource='dripstat', interval=2014-09-07T01:00:00.000Z/2014-09-07T02:00:00.000Z}\n2014-09-14 05:03:44,799 INFO [task-runner-0] io.druid.indexing.common.task.IndexTask - No suitable partition dimension found\n2014-09-14 05:03:44,800 INFO [task-runner-0] io.druid.indexing.firehose.IngestSegmentFirehoseFactory - Connecting firehose: dataSource[dripstat], interval[2014-09-07T01:00:00.000Z/2014-09-07T02:00:00.000Z]\n2014-09-14 05:03:44,801 INFO [task-runner-0] io.druid.indexing.common.actions.RemoteTaskActionClient - Performing action for task[reingest]: SegmentListUsedAction{dataSource='dripstat', interval=2014-09-07T01:00:00.000Z/2014-09-07T02:00:00.000Z}\n2014-09-14 05:03:44,801 INFO [task-runner-0] io.druid.indexing.common.actions.RemoteTaskActionClient - Submitting action for task[reingest] to overlord[http://184.172.97.75:8080/druid/indexer/v1/action]: SegmentListUsedAction{dataSource='dripstat', interval=2014-09-07T01:00:00.000Z/2014-09-07T02:00:00.000Z}\n2014-09-14 05:03:45,151 INFO [task-runner-0] io.druid.indexing.common.index.YeOldePlumberSchool - Spilling index[0] with rows[60] to: /tmp/persistent/task/index_dripstat_2014-09-14T05:03:30.411Z/work/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z_0/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z/spill0\n2014-09-14 05:03:45,155 INFO [task-runner-0] io.druid.segment.IndexMerger - Starting persist for interval[2014-09-07T01:00:00.000Z/2014-09-07T02:00:00.000Z], rows[60]\n2014-09-14 05:03:45,170 INFO [task-runner-0] io.druid.segment.IndexMerger - outDir[/tmp/persistent/task/index_dripstat_2014-09-14T05:03:30.411Z/work/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z_0/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z/spill0/v8-tmp] completed index.drd in 1 millis.\n2014-09-14 05:03:45,171 INFO [task-runner-0] io.druid.segment.IndexMerger - outDir[/tmp/persistent/task/index_dripstat_2014-09-14T05:03:30.411Z/work/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z_0/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z/spill0/v8-tmp] completed dim conversions in 1 millis.\n2014-09-14 05:03:45,200 INFO [task-runner-0] io.druid.segment.CompressedPools - Allocating new chunkEncoder[1]\n2014-09-14 05:03:45,224 INFO [task-runner-0] io.druid.segment.IndexMerger - outDir[/tmp/persistent/task/index_dripstat_2014-09-14T05:03:30.411Z/work/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z_0/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z/spill0/v8-tmp] completed walk through of 60 rows in 53 millis.\n2014-09-14 05:03:45,224 INFO [task-runner-0] io.druid.segment.IndexMerger - outDir[/tmp/persistent/task/index_dripstat_2014-09-14T05:03:30.411Z/work/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z_0/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z/spill0/v8-tmp] completed inverted.drd in 0 millis.\n2014-09-14 05:03:45,230 INFO [task-runner-0] io.druid.segment.IndexIO$DefaultIndexIOHandler - Converting v8[/tmp/persistent/task/index_dripstat_2014-09-14T05:03:30.411Z/work/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z_0/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z/spill0/v8-tmp] to v9[/tmp/persistent/task/index_dripstat_2014-09-14T05:03:30.411Z/work/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z_0/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z/spill0]\n2014-09-14 05:03:45,230 INFO [task-runner-0] io.druid.segment.IndexIO$DefaultIndexIOHandler - Processing file[index.drd]\n2014-09-14 05:03:45,231 INFO [task-runner-0] io.druid.segment.IndexIO$DefaultIndexIOHandler - Processing file[inverted.drd]\n2014-09-14 05:03:45,231 INFO [task-runner-0] io.druid.segment.IndexIO$DefaultIndexIOHandler - Processing file[met_callcount_LITTLE_ENDIAN.drd]\n2014-09-14 05:03:45,235 INFO [task-runner-0] io.druid.segment.IndexIO$DefaultIndexIOHandler - Processing file[met_count_LITTLE_ENDIAN.drd]\n2014-09-14 05:03:45,235 INFO [task-runner-0] io.druid.segment.IndexIO$DefaultIndexIOHandler - Processing file[met_max_LITTLE_ENDIAN.drd]\n2014-09-14 05:03:45,236 INFO [task-runner-0] io.druid.segment.IndexIO$DefaultIndexIOHandler - Processing file[met_min_LITTLE_ENDIAN.drd]\n2014-09-14 05:03:45,236 INFO [task-runner-0] io.druid.segment.IndexIO$DefaultIndexIOHandler - Processing file[met_totaltime_LITTLE_ENDIAN.drd]\n2014-09-14 05:03:45,236 INFO [task-runner-0] io.druid.segment.IndexIO$DefaultIndexIOHandler - Processing file[met_value_LITTLE_ENDIAN.drd]\n2014-09-14 05:03:45,237 INFO [task-runner-0] io.druid.segment.IndexIO$DefaultIndexIOHandler - Processing file[spatial.drd]\n2014-09-14 05:03:45,237 INFO [task-runner-0] io.druid.segment.IndexIO$DefaultIndexIOHandler - Processing file[time_LITTLE_ENDIAN.drd]\n2014-09-14 05:03:45,238 INFO [task-runner-0] io.druid.segment.IndexIO$DefaultIndexIOHandler - Skipped files[[index.drd, inverted.drd, spatial.drd]]\n2014-09-14 05:03:45,244 INFO [task-runner-0] io.druid.storage.s3.S3DataSegmentPusher - Uploading [/tmp/persistent/task/index_dripstat_2014-09-14T05:03:30.411Z/work/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z_0/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z/spill0] to S3\n2014-09-14 05:03:45,245 INFO [task-runner-0] io.druid.utils.CompressionUtils - Adding file[/tmp/persistent/task/index_dripstat_2014-09-14T05:03:30.411Z/work/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z_0/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z/spill0/00000.smoosh] with size[1,686].  Total size so far[0]\n2014-09-14 05:03:45,246 INFO [task-runner-0] io.druid.utils.CompressionUtils - Adding file[/tmp/persistent/task/index_dripstat_2014-09-14T05:03:30.411Z/work/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z_0/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z/spill0/meta.smoosh] with size[156].  Total size so far[1,686]\n2014-09-14 05:03:45,247 INFO [task-runner-0] io.druid.utils.CompressionUtils - Adding file[/tmp/persistent/task/index_dripstat_2014-09-14T05:03:30.411Z/work/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z_0/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z/spill0/version.bin] with size[4].  Total size so far[1,842]\n2014-09-14 05:03:45,255 INFO [task-runner-0] io.druid.storage.s3.S3DataSegmentPusher - Pushing S3Object [key=dripstat/2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z/2014-09-14T05:03:30.655Z/0/index.zip, bucket=dripstat, lastModified=null, dataInputStream=null, Metadata={x-amz-acl=bucket-owner-full-control, md5-hash=6f98a6303f7d61e171db3c80b5f02302, Content-Length=1154, Content-Type=application/zip, Content-MD5=b5imMD99YeFx2zyAtfAjAg==}].\n2014-09-14 05:03:45,381 INFO [task-runner-0] io.druid.storage.s3.S3DataSegmentPusher - Pushing S3Object [key=dripstat/2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z/2014-09-14T05:03:30.655Z/0/descriptor.json, bucket=dripstat, lastModified=null, dataInputStream=null, Metadata={x-amz-acl=bucket-owner-full-control, md5-hash=3b801e12813624088c7701222336c761, Content-Length=361, Content-Type=application/octet-stream, Content-MD5=O4AeEoE2JAiMdwEiIzbHYQ==}]\n2014-09-14 05:03:45,489 INFO [task-runner-0] io.druid.storage.s3.S3DataSegmentPusher - Deleting zipped index File[/tmp/druid7533792041048289796index.zip]\n2014-09-14 05:03:45,489 INFO [task-runner-0] io.druid.storage.s3.S3DataSegmentPusher - Deleting descriptor file[/tmp/druid7471639425194965755descriptor.json]\n2014-09-14 05:03:45,489 INFO [task-runner-0] io.druid.indexing.common.index.YeOldePlumberSchool - Uploaded segment[dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z]\n2014-09-14 05:03:45,489 INFO [task-runner-0] io.druid.indexing.common.index.YeOldePlumberSchool - Deleting Index File[/tmp/persistent/task/index_dripstat_2014-09-14T05:03:30.411Z/work/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z_0/dripstat_2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z_2014-09-14T05:03:30.655Z/spill0]\n2014-09-14 05:03:45,490 INFO [task-runner-0] io.druid.indexing.common.task.IndexTask - Task[index_dripstat_2014-09-14T05:03:30.411Z] interval[2014-09-07T01:00:00.000Z/2014-09-07T02:00:00.000Z] partition[0] took in 105,838 rows (105,838 processed, 0 unparseable, 0 thrown away) and output 60 rows\n2014-09-14 05:03:45,499 INFO [task-runner-0] io.druid.indexing.common.actions.RemoteTaskActionClient - Performing action for task[index_dripstat_2014-09-14T05:03:30.411Z]: SegmentInsertAction{segments=[DataSegment{size=1846, shardSpec=NoneShardSpec, metrics=[count, min, max, callCount, totalTime, value], dimensions=[], version='2014-09-14T05:03:30.655Z', loadSpec={type=s3_zip, bucket=dripstat, key=dripstat/2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z/2014-09-14T05:03:30.655Z/0/index.zip}, interval=2014-09-07T01:00:00.000Z/2014-09-07T02:00:00.000Z, dataSource='dripstat', binaryVersion='9'}]}\n2014-09-14 05:03:45,504 INFO [task-runner-0] io.druid.indexing.common.actions.RemoteTaskActionClient - Submitting action for task[index_dripstat_2014-09-14T05:03:30.411Z] to overlord[http://184.172.97.75:8080/druid/indexer/v1/action]: SegmentInsertAction{segments=[DataSegment{size=1846, shardSpec=NoneShardSpec, metrics=[count, min, max, callCount, totalTime, value], dimensions=[], version='2014-09-14T05:03:30.655Z', loadSpec={type=s3_zip, bucket=dripstat, key=dripstat/2014-09-07T01:00:00.000Z_2014-09-07T02:00:00.000Z/2014-09-14T05:03:30.655Z/0/index.zip}, interval=2014-09-07T01:00:00.000Z/2014-09-07T02:00:00.000Z, dataSource='dripstat', binaryVersion='9'}]}\n2014-09-14 05:03:45,848 INFO [task-runner-0] io.druid.indexing.overlord.ThreadPoolTaskRunner - Removing task directory: /tmp/persistent/task/index_dripstat_2014-09-14T05:03:30.411Z/work\n2014-09-14 05:03:45,851 INFO [task-runner-0] io.druid.indexing.worker.executor.ExecutorLifecycle - Task completed with status: {\n  \"id\" : \"index_dripstat_2014-09-14T05:03:30.411Z\",\n  \"status\" : \"SUCCESS\",\n  \"duration\" : 8959\n}\n2014-09-14 05:03:45,855 INFO [main] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Invoking stop method[public void io.druid.server.coordination.AbstractDataSegmentAnnouncer.stop()] on object[io.druid.server.coordination.BatchDataSegmentAnnouncer@6c0905f6].\n2014-09-14 05:03:45,855 INFO [main] io.druid.server.coordination.AbstractDataSegmentAnnouncer - Stopping class io.druid.server.coordination.BatchDataSegmentAnnouncer with config[io.druid.server.initialization.ZkPathsConfig$$EnhancerByCGLIB$$84a9c10c@57ad1178]\n2014-09-14 05:03:45,855 INFO [main] io.druid.curator.announcement.Announcer - unannouncing [/druid/announcements/184.172.97.75:8088]\n2014-09-14 05:03:45,893 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - Closing inventory cache for 184.172.97.75:8088. Also removing listeners.\n2014-09-14 05:03:45,894 INFO [main] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Invoking stop method[public void io.druid.indexing.worker.executor.ExecutorLifecycle.stop()] on object[io.druid.indexing.worker.executor.ExecutorLifecycle@1d61a348].\n2014-09-14 05:03:45,896 INFO [ServerInventoryView-0] io.druid.client.BatchServerInventoryView - Server Disappeared[DruidServerMetadata{name='184.172.97.75:8088', host='184.172.97.75:8088', maxSize=0, tier='_default_tier', type='indexer-executor', priority='0'}]\n2014-09-14 05:03:45,901 INFO [main] org.eclipse.jetty.server.ServerConnector - Stopped ServerConnector@5984feef{HTTP/1.1}{0.0.0.0:8088}\n2014-09-14 05:03:45,903 INFO [main] org.eclipse.jetty.server.handler.ContextHandler - Stopped o.e.j.s.ServletContextHandler@485e13d7{/,null,UNAVAILABLE}\n2014-09-14 05:03:45,906 INFO [main] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Invoking stop method[public void io.druid.indexing.overlord.ThreadPoolTaskRunner.stop()] on object[io.druid.indexing.overlord.ThreadPoolTaskRunner@7b3cde6f].\n2014-09-14 05:03:45,907 INFO [main] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Invoking stop method[public void io.druid.client.ServerInventoryView.stop() throws java.io.IOException] on object[io.druid.client.BatchServerInventoryView@3cb8c8ce].\n2014-09-14 05:03:45,908 INFO [main] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Invoking stop method[public void io.druid.curator.announcement.Announcer.stop()] on object[io.druid.curator.announcement.Announcer@c35af2a].\n2014-09-14 05:03:45,908 INFO [main] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Invoking stop method[public void io.druid.curator.discovery.ServerDiscoverySelector.stop() throws java.io.IOException] on object[io.druid.curator.discovery.ServerDiscoverySelector@107f4980].\n2014-09-14 05:03:45,909 INFO [main] io.druid.curator.CuratorModule - Stopping Curator\n2014-09-14 05:03:45,945 INFO [main] org.apache.zookeeper.ZooKeeper - Session: 0x14815a952eb0060 closed\n2014-09-14 05:03:45,945 INFO [main-EventThread] org.apache.zookeeper.ClientCnxn - EventThread shut down\n2014-09-14 05:03:45,945 INFO [main] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Invoking stop method[public void com.metamx.http.client.HttpClient.stop()] on object[com.metamx.http.client.HttpClient@5524b72f].\n2014-09-14 05:03:45,957 INFO [main] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Invoking stop method[public void com.metamx.metrics.MonitorScheduler.stop()] on object[com.metamx.metrics.MonitorScheduler@228cea97].\n2014-09-14 05:03:45,957 INFO [main] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Invoking stop method[public void com.metamx.emitter.service.ServiceEmitter.close() throws java.io.IOException] on object[com.metamx.emitter.service.ServiceEmitter@1e141e42].\n2014-09-14 05:03:45,958 INFO [main] com.metamx.emitter.core.LoggingEmitter - Close: started [false]\n2014-09-14 05:03:45,958 INFO [main] com.metamx.common.lifecycle.Lifecycle$AnnotationBasedHandler - Invoking stop method[public void com.metamx.emitter.core.LoggingEmitter.close() throws java.io.IOException] on object[com.metamx.emitter.core.LoggingEmitter@4f3e7344].\n```\n"}, {"user": "gianm", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug", "Ease of Use", "Priority - Showstopper"], "created": "2014-08-29 02:43:18", "title": "Reported classloader issues with hdfs-storage-module", "url": "https://github.com/apache/druid/issues/713", "closed": "2019-10-15 04:43:18", "ttf": 1873.0002777777777, "commitsDetails": [], "body": "The loading and usage of the hdfs-storage-module is supposed to be able to work without actually having hadoop jars on Druid's main classpath. (Since extensions are supposed to be able to use their own ClassLoaders.) But this has been reported to not actually work, leading to class-not-found exceptions.\n\nThe workaround for now is to include the hadoop jars you want to run with on Druid's main classpath.\n"}, {"user": "xvrl", "commits": {}, "labels": ["Bug"], "created": "2014-08-16 00:32:52", "title": "Broker should wait for BrokerServerView to be populated before announcing itself", "url": "https://github.com/apache/druid/issues/687", "closed": "2014-09-23 22:43:43", "ttf": 38.000277777777775, "commitsDetails": [], "body": ""}, {"user": "xvrl", "commits": {}, "labels": ["Bug"], "created": "2014-08-07 21:05:32", "title": "Remove support for rejectionPolicyFactory in favor of rejectionPolicy", "url": "https://github.com/apache/druid/issues/669", "closed": "2015-02-13 19:25:56", "ttf": 189.00027777777777, "commitsDetails": [], "body": "Cleanup backwards compatibility fixed in https://github.com/metamx/druid/commit/91ebe45b4e9ec33f60e34a297cb301828210cc23.\n"}, {"user": "xvrl", "commits": {}, "labels": ["Bug"], "created": "2014-08-07 18:29:24", "title": "Remove support for deprecated ingestion schemas", "url": "https://github.com/apache/druid/issues/666", "closed": "2015-01-20 18:37:46", "ttf": 166.00027777777777, "commitsDetails": [], "body": "Remove cruft left for backwards compatibility as part of #416\n"}, {"user": "gianm", "commits": {"9bf835b84f59366265ee0b26f6ad3ecf7f695c86": {"commitGHEventType": "referenced", "commitUser": "dclim"}}, "changesInPackagesGIT": [], "labels": ["Area - Batch Ingestion", "Bug", "Priority - High"], "spoonStatsSummary": {}, "title": "Indexer does not merge properly when dimensions are provided in non-lexicographic order", "numCommits": 0, "created": "2014-08-05 18:27:00", "closed": "2016-01-21 23:42:31", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "filteredCommits": [], "url": "https://github.com/apache/druid/issues/658", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 534.0002777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "docs/content/ingestion/schema-design.md", "deletions": 3, "insertions": 1, "lines": 4}], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "", "authoredDateTime": "2018-11-30 14:53:57", "commitMessage": "remove #658 doc reference for Schema-less design (#6693)\n\n", "commitUser": "dclim", "commitDateTime": "2018-11-30 12:53:57", "commitParents": ["5fae522f127bd9e12c8b965ae152d3cea985104f"], "commitGHEventType": "referenced", "nameRev": "9bf835b84f59366265ee0b26f6ad3ecf7f695c86 tags/druid-0.14.0-incubating~186", "commitHash": "9bf835b84f59366265ee0b26f6ad3ecf7f695c86"}], "body": "If you give the indexer dimensions in some arbitrary order, the IncrementalIndex will sort its rows based on the values of dimensions in that order. Then, when the IndexMerger merges multiple indexes, it uses a MergeIterable that assumes rows are sorted on values of dimensions in lexicographic order instead of the order the underlying segments actually use. This causes the rowboats to come in out of order and not get combined properly, and the resulting segments have more rows than they should.\n\nWorkarounds include: Supply dimensions in lexicographic order in the indexer config. Avoid indexing with dimensionless schemas.\n\nI can think of two possible fixes:\n- IndexMerger could try to use arbitrary dimension orders from the underlying segment when possible. I think this is not possible in general while both getting correct rollup and avoiding re-sorting rows, since the underlying segments could use different row comparators. But it will be possible in many situations. The IndexMerger can also decide to sort rows in situations where it would need to.\n- IncrementalIndex could sort rows based on dimensions in lexicographic order instead of arbitrary order. This makes the IndexMerger's job easier but it means we can't control the row comparator, which could be useful if we ever decide to mess with it in order to do something like optimize size of the inverted indices.\n"}, {"user": "ethanocentricity", "commits": {}, "labels": ["Bug", "Ease of Use", "Priority - Medium", "Starter"], "created": "2014-06-25 19:07:46", "title": "Coordinator console should make use of druid.segmentCache.locations:maxSize for historical nodes", "url": "https://github.com/apache/druid/issues/622", "closed": "2019-09-25 22:29:09", "ttf": 1918.0002777777777, "commitsDetails": [], "body": "The rules for determining available space on a historical node for a segment seem to include both druid.server.maxSize and druid.segmentCache.locations:maxSize.\n\nHowever, the Coordinator Console shows the Max Size and Percent Used based only on druid.server.maxSize.\n\nIt would be useful if the console showed both values to help diagnose \"segment too large\" errors thrown by the OmniSegmentLoader (when people accidentally set them to different values).\n\nSee https://groups.google.com/forum/#!searchin/druid-development/segment$20too$20large/druid-development/w8Q65ZRusi4/kN0JybUtUfAJ\n"}, {"user": "xvrl", "commits": {}, "labels": ["Bug"], "created": "2014-06-17 16:06:19", "title": "DruidClusterBridgeTest is non-deterministic", "url": "https://github.com/apache/druid/issues/606", "closed": "2014-06-17 17:26:57", "ttf": 0.0002777777777777778, "commitsDetails": [], "body": "testRun(io.druid.server.bridge.DruidClusterBridgeTest): expected:118 but was:0\n"}, {"user": "gianm", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug", "Priority - Showstopper"], "created": "2014-06-05 01:46:06", "title": "Race in jar upload during hadoop indexing", "url": "https://github.com/apache/druid/issues/582", "closed": "2015-11-03 22:59:03", "ttf": 516.0002777777778, "commitsDetails": [], "body": "Hadoop indexing (through io.druid.indexer.JobHelper) does something like:\n\n```\nif (!fs.exists(jar)) {\n  upload jar using fs.create() to workingPath/classpath/jarName.jar\n}\n```\n\nThe same jar path is used for all druid jobs running on the same cluster. If two jobs start at the same time, both can find that the file doesn't exist, and the second one can overwrite the first one. This will cause the AM to fail because the hadoop AM is pretty particular about the mtime of its jar files.\n\nPossible solutions: (a) Better locking, or (b) have each job upload its jars to a separate directory.\n"}, {"user": "tarekrached", "commits": {"8b5de492cc760fcfd798404a7465fbde40836678": {"commitGHEventType": "closed", "commitUser": "drcrallen"}, "dc4ae59aafeb0b041426c9abe036aa2a70e01ada": {"commitGHEventType": "referenced", "commitUser": "drcrallen"}}, "changesInPackagesGIT": ["server/src/main/java/io/druid/server/http"], "labels": ["Bug", "Ease of Use", "Priority - Medium", "Starter"], "spoonStatsSummary": {"spoonFilesChanged": 1, "spoonMethodsChanged": 1, "INS": 8, "UPD": 7, "DEL": 5, "MOV": 10, "TOT": 30}, "title": "datasource segments summary shouldn't include replication", "numCommits": 1, "created": "2014-05-21 19:35:08", "closed": "2015-11-11 23:29:10", "gitStatsSummary": {"deletions": 14, "insertions": 27, "lines": 41, "gitFilesChange": 1}, "statsSkippedReason": "", "changesInPackagesSPOON": ["io.druid.server.http.DatasourcesResource.getSimpleDatasource(java.lang.String)"], "filteredCommits": ["8b5de492cc760fcfd798404a7465fbde40836678"], "url": "https://github.com/apache/druid/issues/560", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 539.0002777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "server/src/main/java/io/druid/server/http/DatasourcesResource.java", "deletions": 14, "insertions": 27, "lines": 41}, {"filePath": "server/src/test/java/io/druid/server/http/DatasourcesResourceTest.java", "deletions": 6, "insertions": 50, "lines": 56}], "commitSpoonAstDiffStats": [{"spoonFilePath": "DatasourcesResource.java", "spoonMethods": [{"INS": 8, "UPD": 7, "DEL": 5, "spoonMethodName": "io.druid.server.http.DatasourcesResource.getSimpleDatasource(java.lang.String)", "MOV": 10, "TOT": 30}]}, {"spoonFilePath": "DatasourcesResourceTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.http.DatasourcesResourceTest.testSimpleGetTheDataSourceManyTiers()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 3, "DEL": 0, "spoonMethodName": "io.druid.server.http.DatasourcesResourceTest.setUp()", "MOV": 0, "TOT": 3}, {"INS": 3, "UPD": 3, "DEL": 0, "spoonMethodName": "io.druid.server.http.DatasourcesResourceTest.testSimpleGetTheDataSource()", "MOV": 2, "TOT": 8}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2015-10-26 21:37:32", "commitMessage": "Fix #560 - datasource segments summary shouldn't include replication.\n", "commitUser": "drcrallen", "commitDateTime": "2015-11-07 21:43:47", "commitParents": ["61139b9dfa40bd04870252c33d301b387147bafe"], "commitGHEventType": "closed", "nameRev": "8b5de492cc760fcfd798404a7465fbde40836678 tags/druid-0.9.0-rc1~260^2", "commitHash": "8b5de492cc760fcfd798404a7465fbde40836678"}, {"commitGitStats": [{"filePath": "server/src/main/java/io/druid/server/http/DatasourcesResource.java", "deletions": 14, "insertions": 27, "lines": 41}, {"filePath": "server/src/test/java/io/druid/server/http/DatasourcesResourceTest.java", "deletions": 6, "insertions": 50, "lines": 56}], "commitSpoonAstDiffStats": [{"spoonFilePath": "DatasourcesResource.java", "spoonMethods": [{"INS": 8, "UPD": 7, "DEL": 5, "spoonMethodName": "io.druid.server.http.DatasourcesResource.getSimpleDatasource(java.lang.String)", "MOV": 10, "TOT": 30}]}, {"spoonFilePath": "DatasourcesResourceTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.server.http.DatasourcesResourceTest.testSimpleGetTheDataSourceManyTiers()", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 3, "DEL": 0, "spoonMethodName": "io.druid.server.http.DatasourcesResourceTest.setUp()", "MOV": 0, "TOT": 3}, {"INS": 3, "UPD": 3, "DEL": 0, "spoonMethodName": "io.druid.server.http.DatasourcesResourceTest.testSimpleGetTheDataSource()", "MOV": 2, "TOT": 8}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2015-11-11 15:29:08", "commitMessage": "Merge pull request #1865 from noddi/bugfix/issue-560\n\nFix #560 - datasource segments summary shouldn't include replication.", "commitUser": "drcrallen", "commitDateTime": "2015-11-11 15:29:08", "commitParents": ["9f7859867201678ee29d3dac681bc864f7920764", "8b5de492cc760fcfd798404a7465fbde40836678"], "commitGHEventType": "referenced", "nameRev": "dc4ae59aafeb0b041426c9abe036aa2a70e01ada tags/druid-0.9.0-rc1~260", "commitHash": "dc4ae59aafeb0b041426c9abe036aa2a70e01ada"}], "body": "/datasources/[DATASOURCE]?simple appears to include replication in the segments bit of the response, and it shouldn't (this information is in the tiers)\n\nthis was an uncaught part of #438 \nfor instance:\n\n```\nhttp://10.151.79.16:8080/druid/coordinator/v1/datasources/dash_track\n```\n\nreturns:\n\n```\n{\n  \"tiers\": {\n    \"_default_tier\": {\n      \"segmentCount\": 12,\n      \"size\": 568436361\n    },\n    \"hot\": {\n      \"segmentCount\": 10,\n      \"size\": 568436361\n    }\n  },\n  \"segments\": {\n    \"minTime\": \"2014-01-28T21:00:00.000Z\",\n    \"count\": 22,\n    \"maxTime\": \"2014-05-21T20:00:00.000Z\",\n    \"size\": 1136872722\n  }\n}\n```\n\nnote that segments.size = tiers._default_tier.size + tiers.hot.size\n\nin this case, segments.size should equal tiers._default_tier.size and segments.count should be 12 (or 10, whatever)\n"}, {"user": "gianm", "commits": {}, "labels": ["Area - Segment Balancing/Coordination", "Bug", "Priority - High"], "created": "2014-05-13 17:51:45", "title": "Broken segments cannot be replaced easily", "url": "https://github.com/apache/druid/issues/540", "closed": "2016-06-02 14:36:29", "ttf": 750.0002777777778, "commitsDetails": [], "body": "If a completely broken segment is inserted into the DB (where \"broken\" means \"historicals cannot load it\") then the coordinator will not be able to recognize that a replacement exists. This means that even if a replacement is generated through a new indexing job, the old one must manually be marked unused.\n"}, {"user": "fjy", "commits": {}, "labels": ["Bug"], "created": "2014-05-09 15:02:48", "title": "If coordinator start delay is too low, it can lead to a potential racey condition", "url": "https://github.com/apache/druid/issues/536", "closed": "2015-06-07 17:54:30", "ttf": 394.0002777777778, "commitsDetails": [], "body": "If the coordinator start delay is <= 60s, there can be a racey condition where the polling for segment metadata can occur after the coordinator begins to assign new work.\n"}, {"user": "fjy", "commits": {}, "labels": ["Bug"], "created": "2014-04-24 05:18:39", "title": "BrokerServerViews can have inconsistent state", "url": "https://github.com/apache/druid/issues/509", "closed": "2014-06-20 05:38:26", "ttf": 57.000277777777775, "commitsDetails": [], "body": "It is possible for brokers to issue queries down to historical nodes for certain segments, but before the query begins executing, the state of the cluster has changed (those same segments were moved), thus leading to exceptions.\n"}, {"user": "vogievetsky", "commits": {}, "labels": ["Bug", "Question"], "created": "2014-04-24 04:20:29", "title": "Druid maxTime sometimes returns [ ] with code 200", "url": "https://github.com/apache/druid/issues/506", "closed": "2014-08-07 17:15:19", "ttf": 105.00027777777778, "commitsDetails": [], "body": "What to make of this? Would retrying likely help?\nShould I disable caching on maxTime?\n\nI have this dataSource it is called `ret*******_***_seg****`, there are also others. I can not say exactly what it is but you get the gist.\n\nI have retry logic but this bypasses it because of the code 200 should I modify my retry logic to retry on this error? \nIdeally I would love to have a non 200 error code if something went wrong.\n"}, {"user": "gianm", "commits": {}, "labels": ["Bug"], "created": "2014-04-14 22:02:51", "title": "Casing bug in indexing service", "url": "https://github.com/apache/druid/issues/479", "closed": "2014-04-24 05:25:02", "ttf": 9.000277777777777, "commitsDetails": [], "body": "Reported on the mailing list: https://groups.google.com/forum/#!topic/druid-development/x7wxQAuyeJI\n\nAn IndexTask with a non-lowercase dataSource eventually yields an \"ISE: Segments not covered by locks for task\", apparently because by the time the SegmentInsertAction has been created, the datasource gets toLowerCased.\n"}, {"user": "prepor", "commits": {}, "labels": ["Bug", "Feature"], "created": "2014-04-08 11:46:42", "title": "hyperUnique and groupBy", "url": "https://github.com/apache/druid/issues/468", "closed": "2014-04-22 20:20:48", "ttf": 14.000277777777777, "commitsDetails": [], "body": "Hello. Thanks for great product.\n\nBut i have some issues with hyperUnique, maybe because i don't understand something.\n\nThen i make Timeseries request it works like a charm:\n\n``` json\n{ \n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"emails\",\n  \"granularity\": \"all\",\n  \"filter\": { \"type\": \"selector\", \"dimension\": \"event\", \"value\": \"open\" },\n  \"aggregations\" : [\n    {\"type\" : \"hyperUnique\", \"name\" : \"uniq\", \"fieldName\" : \"unique_emails\"}],\n\n  \"intervals\": [ \"2014-01-01T00:00:00.000/2014-05-01T00:00:00.000\" ]\n}\n```\n\n=>\n\n``` json\n[\n    {\n        \"timestamp\": \"2014-01-01T00:00:00.000Z\",\n        \"result\": {\n            \"uniq\": 1981674.7607874644\n        }\n    }\n]\n```\n\nBut with groupBy (and TopN) i get results like this:\n\n``` json\n\n{ \n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"emails\",\n  \"granularity\": \"all\",\n  \"dimensions\": [],\n  \"filter\": { \"type\": \"selector\", \"dimension\": \"event\", \"value\": \"open\" },\n  \"aggregations\" : [\n    {\"type\" : \"hyperUnique\", \"name\" : \"uniq\", \"fieldName\" : \"unique_emails\"}],\n\n  \"intervals\": [ \"2014-01-01T00:00:00.000/2014-05-01T00:00:00.000\" ]\n}\n```\n\n=>\n\n``` json\n[\n    {\n        \"version\": \"v1\",\n        \"timestamp\": \"2014-01-01T00:00:00.000Z\",\n        \"event\": {\n            \"uniq\": 2.000977198748901\n        }\n    }\n]\n```\n\nWhy?\nAlso I don't understand purpose of hyperUniqueCardinality.\n"}, {"user": "pgkiran", "commits": {"84290a233296e29be3e7d0424ebbd95cff381887": {"commitGHEventType": "referenced", "commitUser": "suneet-s"}}, "labels": ["Bug"], "title": "Support for mixed case dimension names", "created": "2014-03-20 17:27:11", "closed": "2015-01-20 18:40:55", "spoonStatsSummary": {}, "statsSkippedReason": "", "filteredCommits": [], "url": "https://github.com/apache/druid/issues/435", "filteredCommitsReason": {"multipleIssueFixes": 0, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 1, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 306.0002777777778, "commitsDetails": [{"commitGitStats": [], "commitSpoonAstDiffStats": [], "spoonStatsSkippedReason": "", "authoredDateTime": "", "commitMessage": "", "commitUser": "suneet-s", "commitDateTime": "", "commitParents": [], "commitGHEventType": "referenced", "nameRev": "", "commitHash": "84290a233296e29be3e7d0424ebbd95cff381887"}], "body": "Unable to use upper case characters in dimension names. For now converting all our dimension names to use lower case. \n"}, {"user": "xvrl", "commits": {}, "labels": ["Bug"], "created": "2014-02-25 23:39:51", "title": "uppercase dimension / column names cause the hadoop index task to fail", "url": "https://github.com/apache/druid/issues/408", "closed": "2014-03-24 20:02:41", "ttf": 26.00027777777778, "commitsDetails": [], "body": "uppercase column and or dimension names cause the hadoop index task to fail at the \"determine partitions\" stage when sharding is enabled using either \"random\" or \"singleDimension\" partitionsSpec.\n"}, {"user": "xvrl", "commits": {"1ed5254d5b53ca05b89245ed0378122bddca440c": {"commitGHEventType": "referenced", "commitUser": "nishantmonu51"}}, "changesInPackagesGIT": ["indexing-hadoop/src/main/java/io/druid/indexer"], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 1, "spoonMethodsChanged": 3, "INS": 2, "UPD": 1, "DEL": 0, "MOV": 1, "TOT": 4}, "title": "DetermineHashedPartitions should use more than one reducer", "numCommits": 1, "created": "2014-02-24 22:16:32", "closed": "2014-03-24 20:01:59", "gitStatsSummary": {"deletions": 1, "insertions": 10, "lines": 11, "gitFilesChange": 1}, "statsSkippedReason": "", "changesInPackagesSPOON": ["io.druid.indexer.DetermineHashedPartitionsJob.run()", "io.druid.indexer.DetermineHashedPartitionsJob.DetermineCardinalityReducer.getDataBytes(org.apache.hadoop.io.BytesWritable)", "io.druid.indexer.DetermineHashedPartitionsJob.DetermineCardinalityReducer.reduce(org.apache.hadoop.io.LongWritable,java.lang.Iterable,io.druid.indexer.Context)"], "filteredCommits": ["1ed5254d5b53ca05b89245ed0378122bddca440c"], "url": "https://github.com/apache/druid/issues/404", "filteredCommitsReason": {"multipleIssueFixes": 0, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 27.00027777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "indexing-hadoop/src/main/java/io/druid/indexer/DetermineHashedPartitionsJob.java", "deletions": 1, "insertions": 10, "lines": 11}], "commitSpoonAstDiffStats": [{"spoonFilePath": "DetermineHashedPartitionsJob.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "io.druid.indexer.DetermineHashedPartitionsJob.DetermineCardinalityReducer.reduce(org.apache.hadoop.io.LongWritable,java.lang.Iterable,io.druid.indexer.Context)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexer.DetermineHashedPartitionsJob.DetermineCardinalityReducer.getDataBytes(org.apache.hadoop.io.BytesWritable)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.indexer.DetermineHashedPartitionsJob.run()", "MOV": 1, "TOT": 2}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2014-02-26 02:39:45", "commitMessage": "improvements\n\n1) Number of reducers use 1 only when intervals are to be determined\n2) Read only useful bytes from BytesWritable\n", "commitUser": "nishantmonu51", "commitDateTime": "2014-02-26 02:39:45", "commitParents": ["8af63005a6eb1a136a8ba14f3add1704b0d92e04"], "commitGHEventType": "referenced", "nameRev": "1ed5254d5b53ca05b89245ed0378122bddca440c tags/druid-0.6.62~8^2", "commitHash": "1ed5254d5b53ca05b89245ed0378122bddca440c"}], "body": "`DetermineHashedPartitions` is currently set to have [only one reducer](https://github.com/metamx/druid/blob/master/indexing-hadoop/src/main/java/io/druid/indexer/DetermineHashedPartitionsJob.java#L102), which is suboptimal when indexing a large number of time intervals.\n\nWe should add a custom partitioner and set the number of reducers to the number of time buckets, similar to what the [dim-selection job in `DeterminePartitionsJob`](https://github.com/metamx/druid/blob/master/indexing-hadoop/src/main/java/io/druid/indexer/DeterminePartitionsJob.java#L185-186) does\n"}, {"user": "fjy", "commits": {}, "labels": ["Area - Querying", "Bug", "Priority - Medium"], "created": "2014-02-11 06:12:16", "title": "TimeBoundary problems with incomplete segments", "url": "https://github.com/apache/druid/issues/391", "closed": "2019-06-19 20:27:17", "ttf": 1954.0002777777777, "commitsDetails": [], "body": "Let's say we have segments from 12-02T00 to 12-02T01, 12-02T01 to 12-02T02 ... 12-02T11 to 12-02T12 on version 1\nand a segment , 12-02T00 to 12-03T00 on version 0\n\nIssuing a time boundary query will cause the VIT to look up and correctly return segments:\n12-02T00 to 12-02T01, 12-02T01 to 12-02T02 ... 12-02T11 to 12-02T12, 12-02T12 to 12-03T00\n\nAfter time boundary filtering, we are left to check two segments:  12-02T00 to 12-02T01 and 12-02T12 to 12-03T00\n\nTimeboundary query will pull and merge the first and last timestamps from the timestamp column from each of these segments.\n\nNow if the segment from 12-02T00 to 12-03T00 actually only has data with timestamps 12-02T00, an incorrect max time will be returned.\n"}, {"user": "textractor", "commits": {}, "labels": ["Bug"], "created": "2014-01-21 00:43:02", "title": "Wikipedia example looking for wrong JAR", "url": "https://github.com/apache/druid/issues/365", "closed": "2014-04-03 00:54:31", "ttf": 72.00027777777778, "commitsDetails": [], "body": "Trying to run it:\n\n```\n$ ./run_example_server.sh \nThis will run a stand-alone version of Druid\nPlease specify an example type.\nExamples availables:\nrabbitmq rand twitter webstream wikipedia\n> wikipedia\n\nls: /Users/igallevy/Downloads/druid/druid-services-0.6.52/../target/druid-examples-*-selfcontained.jar: No such file or directory\nls: /Users/igallevy/Downloads/druid/druid-services-0.6.52/../../services/target/druid-services-*-selfcontained.jar: No such file or directory\nRunning command:\n+ java -Xmx256m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -Ddruid.realtime.specFile=/Users/igallevy/Downloads/druid/druid-services-0.6.52/examples/wikipedia/wikipedia_realtime.spec -classpath '/Users/igallevy/Downloads/druid/druid-services-0.6.52/examples/wikipedia:::/Users/igallevy/Downloads/druid/druid-services-0.6.52/../config/realtime:/Users/igallevy/Downloads/druid/druid-services-0.6.52/lib/*:/Users/igallevy/Downloads/druid/druid-services-0.6.52/config/realtime' io.druid.cli.Main example realtime\n...\n```\n\nI think it should be looking for `$DRUID_HOME/lib/druid-services-0.6.52.jar`.\n"}, {"user": "maxd", "commits": {}, "labels": ["Bug"], "created": "2013-12-04 11:06:24", "title": "Validation of realtime.properties doesn't work", "url": "https://github.com/apache/druid/issues/317", "closed": "2015-03-02 23:04:21", "ttf": 453.0002777777778, "commitsDetails": [], "body": "Hello All,\n\nInstead of information about validation problems I see the following exception:\n\n```\n2013-12-04 11:02:11,579 INFO [main] io.druid.guice.JsonConfigurator - Loaded class[class io.druid.server.metrics.MonitorsConfig] from props[druid.monitoring.] as [MonitorsConfig{monitors=[]}]\nException in thread \"main\" java.lang.NoClassDefFoundError: javax/el/PropertyNotFoundException\n        at org.hibernate.validator.messageinterpolation.ResourceBundleMessageInterpolator.interpolateExpression(ResourceBundleMessageInterpolator.java:227)\n        at org.hibernate.validator.messageinterpolation.ResourceBundleMessageInterpolator.interpolateMessage(ResourceBundleMessageInterpolator.java:187)\n        at org.hibernate.validator.messageinterpolation.ResourceBundleMessageInterpolator.interpolate(ResourceBundleMessageInterpolator.java:115)\n        at org.hibernate.validator.internal.engine.ValidationContext.interpolate(ValidationContext.java:370)\n        at org.hibernate.validator.internal.engine.ValidationContext.createConstraintViolation(ValidationContext.java:284)\n        at org.hibernate.validator.internal.engine.ValidationContext.createConstraintViolations(ValidationContext.java:246)\n        at org.hibernate.validator.internal.engine.constraintvalidation.ConstraintTree.validateSingleConstraint(ConstraintTree.java:289)\n        at org.hibernate.validator.internal.engine.constraintvalidation.ConstraintTree.validateConstraints(ConstraintTree.java:133)\n        at org.hibernate.validator.internal.engine.constraintvalidation.ConstraintTree.validateConstraints(ConstraintTree.java:91)\n        at org.hibernate.validator.internal.metadata.core.MetaConstraint.validateConstraint(MetaConstraint.java:85)\n        at org.hibernate.validator.internal.engine.ValidatorImpl.validateConstraint(ValidatorImpl.java:478)\n        at org.hibernate.validator.internal.engine.ValidatorImpl.validateConstraintsForDefaultGroup(ValidatorImpl.java:424)\n        at org.hibernate.validator.internal.engine.ValidatorImpl.validateConstraintsForCurrentGroup(ValidatorImpl.java:388)\n        at org.hibernate.validator.internal.engine.ValidatorImpl.validateInContext(ValidatorImpl.java:340)\n        at org.hibernate.validator.internal.engine.ValidatorImpl.validate(ValidatorImpl.java:158)\n        at io.druid.guice.JsonConfigurator.configurate(JsonConfigurator.java:90)\n        at io.druid.guice.JsonConfigProvider.get(JsonConfigProvider.java:181)\n        at io.druid.guice.JsonConfigProvider.get(JsonConfigProvider.java:60)\n        at com.google.inject.internal.ProviderInternalFactory.provision(ProviderInternalFactory.java:86)\n        at com.google.inject.internal.InternalFactoryToInitializableAdapter.provision(InternalFactoryToInitializableAdapter.java:55)\n        at com.google.inject.internal.ProviderInternalFactory.circularGet(ProviderInternalFactory.java:66)\n        at com.google.inject.internal.InternalFactoryToInitializableAdapter.get(InternalFactoryToInitializableAdapter.java:47)\n        at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46)\n        at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1058)\n        at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)\n        at com.google.inject.Scopes$1$1.get(Scopes.java:65)\n\n...\n\n        at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46)\n        at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1058)\n        at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)\n        at com.google.inject.Scopes$1$1.get(Scopes.java:65)\n        at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:41)\n        at com.google.inject.internal.InternalInjectorCreator$1.call(InternalInjectorCreator.java:205)\n        at com.google.inject.internal.InternalInjectorCreator$1.call(InternalInjectorCreator.java:199)\n        at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1051)\n        at com.google.inject.internal.InternalInjectorCreator.loadEagerSingletons(InternalInjectorCreator.java:199)\n        at com.google.inject.internal.InternalInjectorCreator.injectDynamically(InternalInjectorCreator.java:180)\n        at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:110)\n        at com.google.inject.Guice.createInjector(Guice.java:96)\n        at com.google.inject.Guice.createInjector(Guice.java:73)\n        at com.google.inject.Guice.createInjector(Guice.java:62)\n        at io.druid.initialization.Initialization.makeInjectorWithModules(Initialization.java:317)\n        at io.druid.cli.GuiceRunnable.makeInjector(GuiceRunnable.java:56)\n        at io.druid.cli.ServerRunnable.run(ServerRunnable.java:39)\n        at io.druid.cli.Main.main(Main.java:88)\nCaused by: java.lang.ClassNotFoundException: javax.el.PropertyNotFoundException\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n\n...\n\n```\n\nI have added the following dependencies to the `pom.xml` and `services/pom.xml` files and rebuilded project:\n\n```\n        <dependency>\n            <groupId>javax.el</groupId>\n            <artifactId>javax.el-api</artifactId>\n            <version>3.0.0</version>\n        </dependency>\n        <dependency>\n            <groupId>com.sun.el</groupId>\n            <artifactId>el-ri</artifactId>\n            <version>1.0</version>\n        </dependency>\n```\n\nIt fixed problem. Now I see approprimate validation messages:\n\n```\nException in thread \"main\" com.google.inject.CreationException: Guice creation errors:\n\n1) druid.host - may not be null\n  at io.druid.guice.JsonConfigProvider.bind(JsonConfigProvider.java:112)\n  at io.druid.guice.JsonConfigProvider.bind(JsonConfigProvider.java:112)\n  while locating com.google.common.base.Supplier<io.druid.server.DruidNode> annotated with @io.druid.guice.annotations.Self()\n  at io.druid.server.initialization.EmitterModule.getServiceEmitter(EmitterModule.java:80)\n  at io.druid.server.initialization.EmitterModule.getServiceEmitter(EmitterModule.java:80)\n  while locating com.metamx.emitter.service.ServiceEmitter\n  at io.druid.server.metrics.MetricsModule.getMonitorScheduler(MetricsModule.java:80)\n  at io.druid.server.metrics.MetricsModule.getMonitorScheduler(MetricsModule.java:80)\n  while locating com.metamx.metrics.MonitorScheduler\n  at io.druid.server.metrics.MetricsModule.configure(MetricsModule.java:65)\n  while locating com.metamx.metrics.MonitorScheduler annotated with @com.google.inject.name.Named(value=ForTheEagerness)\n\n2) druid.port - must be greater than or equal to 0\n  at io.druid.guice.JsonConfigProvider.bind(JsonConfigProvider.java:112)\n  at io.druid.guice.JsonConfigProvider.bind(JsonConfigProvider.java:112)\n  while locating com.google.common.base.Supplier<io.druid.server.DruidNode> annotated with @io.druid.guice.annotations.Self()\n  at io.druid.server.initialization.EmitterModule.getServiceEmitter(EmitterModule.java:80)\n  at io.druid.server.initialization.EmitterModule.getServiceEmitter(EmitterModule.java:80)\n  while locating com.metamx.emitter.service.ServiceEmitter\n  at io.druid.server.metrics.MetricsModule.getMonitorScheduler(MetricsModule.java:80)\n  at io.druid.server.metrics.MetricsModule.getMonitorScheduler(MetricsModule.java:80)\n  while locating com.metamx.metrics.MonitorScheduler\n  at io.druid.server.metrics.MetricsModule.configure(MetricsModule.java:65)\n  while locating com.metamx.metrics.MonitorScheduler annotated with @com.google.inject.name.Named(value=ForTheEagerness)\n\n3) druid.service - may not be null\n  at io.druid.guice.JsonConfigProvider.bind(JsonConfigProvider.java:112)\n  at io.druid.guice.JsonConfigProvider.bind(JsonConfigProvider.java:112)\n  while locating com.google.common.base.Supplier<io.druid.server.DruidNode> annotated with @io.druid.guice.annotations.Self()\n  at io.druid.server.initialization.EmitterModule.getServiceEmitter(EmitterModule.java:80)\n  at io.druid.server.initialization.EmitterModule.getServiceEmitter(EmitterModule.java:80)\n  while locating com.metamx.emitter.service.ServiceEmitter\n  at io.druid.server.metrics.MetricsModule.getMonitorScheduler(MetricsModule.java:80)\n  at io.druid.server.metrics.MetricsModule.getMonitorScheduler(MetricsModule.java:80)\n  while locating com.metamx.metrics.MonitorScheduler\n  at io.druid.server.metrics.MetricsModule.configure(MetricsModule.java:65)\n  while locating com.metamx.metrics.MonitorScheduler annotated with @com.google.inject.name.Named(value=ForTheEagerness)\n\n3 errors\n```\n\nI am not a sure that I choose correct module for these dependencies. Could you please check it and add them?\n\nWorkaround:\n  Add JARs to lib/ directory:\n- el-ri-1.0.jar\n- javax.el-api-3.0.0.jar\n\nP.S.\n  Druid 0.6.24\n  jdk-7u45-linux-x64\n"}, {"user": "sixtus", "commits": {}, "labels": ["Area - Batch Ingestion", "Bug"], "created": "2013-08-22 11:25:19", "title": "hadoop importer parses period wrong if TZ!=utc", "url": "https://github.com/apache/druid/issues/228", "closed": "2017-08-22 17:19:36", "ttf": 1461.0002777777777, "commitsDetails": [], "body": "This shouldn't happen, but\n\n\"intervals\": [\"2013-08-19T00:00:00Z/2013-08-20T00:00:00Z\"]\n\nresults in\n\nindex-generator-[2013-08-19T00:00:00.000+02:00/2013-08-20T00:00:00.000+02:00]\n\ni.e. \"Z\" is interpreted as local time zone, and it sure shouldn't be.\n"}, {"user": "sharrissf", "commits": {}, "labels": ["Bug"], "created": "2013-05-06 18:22:36", "title": "Shade is pulling in way more stuff than it should when building Jar", "url": "https://github.com/apache/druid/issues/139", "closed": "2013-12-18 01:17:20", "ttf": 225.00027777777777, "commitsDetails": [], "body": "I noticed that the shared jar is pulling in things like findbugs and eclipse jdt etc. \n"}, {"user": "cheddar", "commits": {}, "labels": ["Bug"], "created": "2013-04-17 17:33:29", "title": "Master has issues assigning segments and logging on 0.3.27.2", "url": "https://github.com/apache/druid/issues/124", "closed": "2013-05-16 23:10:27", "ttf": 29.00027777777778, "commitsDetails": [], "body": "One user experienced a bunch of logs that look like\n\n```\n[2013-04-17 17:22:56,670] WARN Not enough _default_tier servers or node capacity to assign segment[rb_flow_2013-04-17T17:12:00.000Z_2013-04-17T17:13:00.000Z_2013-04-17T17:12:00.000Z_-1]! Expected Replicants[2] (com.metamx.druid.master.rules.LoadRule)\n[2013-04-17 17:22:56,670] WARN Not enough _default_tier servers or node capacity to assign segment[rb_flow_2013-04-17T17:09:00.000Z_2013-04-17T17:10:00.000Z_2013-04-17T17:09:00.000Z_-1]! Expected Replicants[2] (com.metamx.druid.master.rules.LoadRule)\n```\n\nAnd then \n\n```\n[2013-04-17 17:22:56,673] ERROR Caught exception, ignoring so that schedule keeps going.: {class=com.metamx.druid.master.DruidMaster, exceptionType=class java.lang.NullPointerException, exceptionMessage=null} (com.metamx.druid.master.DruidMaster)\njava.lang.NullPointerException\n        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)\n        at com.google.common.collect.Maps$TransformedEntriesMap.<init>(Maps.java:1165)\n        at com.google.common.collect.Maps.transformEntries(Maps.java:1064)\n        at com.metamx.druid.master.DruidMasterLogger.run(DruidMasterLogger.java:97)\n        at com.metamx.druid.master.DruidMaster$MasterRunnable.run(DruidMaster.java:617)\n        at com.metamx.druid.master.DruidMaster$3.call(DruidMaster.java:459)\n        at com.metamx.druid.master.DruidMaster$3.call(DruidMaster.java:452)\n        at com.metamx.common.concurrent.ScheduledExecutors$2.run(ScheduledExecutors.java:99)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:166)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\n        at java.lang.Thread.run(Thread.java:722)\n```\n\nWhen running the master.  They are only running a single compute node, so it cannot fully replicate, but this should not stop the Master from assigning segments.\n\nIt also should not produce an NPE when logging (I'm actually not sure if that is causing the NPE or not, but it seems like a decent avenue to pursue).\n"}, {"user": "cheddar", "commits": {}, "labels": ["Bug"], "created": "2013-04-17 16:26:17", "title": "Realtime node cannot start back up if there is a \"merged\" directory on disk ", "url": "https://github.com/apache/druid/issues/123", "closed": "2013-06-14 05:19:36", "ttf": 57.000277777777775, "commitsDetails": [], "body": "The realtime nodes periodically merge data and store it in a local directory for pushing up to deep storage.  When these directories are left lying around, the screw up the start up logic with exceptions like:\n\n[2013-04-17 15:58:43,178] ERROR RuntimeException aborted realtime processing[rb_flow]: {class=com.metamx.druid.realtime.RealtimeManager, exceptionType=class java.lang.NumberFormatException, exceptionMessage=For input string: \"merged\"} (co\njava.lang.NumberFormatException: For input string: \"merged\"\n        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n        at java.lang.Integer.parseInt(Integer.java:492)\n        at java.lang.Integer.parseInt(Integer.java:527)\n        at com.metamx.druid.realtime.plumber.RealtimePlumberSchool$1.bootstrapSinksFromDisk(RealtimePlumberSchool.java:377)\n        at com.metamx.druid.realtime.plumber.RealtimePlumberSchool$1.startJob(RealtimePlumberSchool.java:188)\n        at com.metamx.druid.realtime.RealtimeManager$FireChief.run(RealtimeManager.java:159)\n\nWe need to fix this in two ways:\n\n1) Make the restart logic resilient to this stuff\n2) Store the merged directories somewhere else so that they don't get in the way in the first place\n"}, {"user": "cheddar", "commits": {}, "labels": ["Bug"], "created": "2013-03-26 14:27:51", "title": "Casing bug for dataSource in realtime", "url": "https://github.com/apache/druid/issues/111", "closed": "2015-02-26 01:17:10", "ttf": 701.0002777777778, "commitsDetails": [], "body": "Nebrera on the IRC channel today reported that he was ingesting data but couldn't query it.  His real-time schema had some capital letters in the dataSource and his query matched it but he couldn't get results.  I had him adjust it to be all lowercase and it started working.  So, we've got a casing bug in the real-time nodes right now.\n"}, {"user": "cheddar", "commits": {}, "labels": ["Bug"], "created": "2013-03-06 18:55:49", "title": "Failure to parse a timestamp kills Realtime consumer", "url": "https://github.com/apache/druid/issues/99", "closed": "2013-08-07 00:39:01", "ttf": 153.00027777777777, "commitsDetails": [], "body": "When a timestamp cannot be properly parsed, it fails the whole realtime consumer because it is throwing an IllegalArgumentException instead of a ParseException like the code is currently looking for.  We need to fix that.\n"}, {"user": "sixtus", "commits": {}, "labels": ["Bug"], "created": "2013-02-20 14:56:17", "title": "BUG: compute node doesn't give up deleted file handles", "url": "https://github.com/apache/druid/issues/89", "closed": "2013-08-06 20:59:40", "ttf": 167.00027777777777, "commitsDetails": [], "body": "I just ran out of disk space on a compute node:\n- the node was the only one up, so it got all the segments\n- the other nodes came up, so it deleted some\n- however, the files weren't closed properly so the filesystem didn't release the space\n\nI had to restart the compute node to release the space on the filesystem\n"}, {"user": "fjy", "commits": {"6998d604a293f752af8649c7f12b31f2801decaa": {"commitGHEventType": "closed", "commitUser": "xvrl"}}, "changesInPackagesGIT": ["client/src/main/java/com/metamx/druid/query/timeboundary"], "labels": ["Bug"], "spoonStatsSummary": {}, "title": "TimeBoundary and MaxTime queries hit every segment in an interval instead of the edge segments", "numCommits": 1, "created": "2013-02-07 17:55:55", "closed": "2013-02-15 19:43:43", "gitStatsSummary": {"deletions": 1, "insertions": 0, "lines": 1, "gitFilesChange": 1}, "statsSkippedReason": "", "filteredCommits": ["6998d604a293f752af8649c7f12b31f2801decaa"], "url": "https://github.com/apache/druid/issues/80", "filteredCommitsReason": {"multipleIssueFixes": 0, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 8.000277777777777, "commitsDetails": [{"commitGitStats": [{"filePath": "client/src/main/java/com/metamx/druid/query/timeboundary/TimeBoundaryQueryQueryToolChest.java", "deletions": 1, "insertions": 0, "lines": 1}], "commitSpoonAstDiffStats": [{"spoonFilePath": "TimeBoundaryQueryQueryToolChest.java", "spoonMethods": []}], "spoonStatsSkippedReason": "", "authoredDateTime": "2013-02-14 12:41:34", "commitMessage": "fixes #80\n", "commitUser": "xvrl", "commitDateTime": "2013-02-14 12:41:34", "commitParents": ["98de0382d5c4a0f4c1b514aa1e0feb0868ddab84"], "commitGHEventType": "closed", "nameRev": "6998d604a293f752af8649c7f12b31f2801decaa tags/druid-0.3.0~2^2~2", "commitHash": "6998d604a293f752af8649c7f12b31f2801decaa"}], "body": "TimeBoundary and maxTime queries do not need to access and merge data from every segment in an interval, they only need information from the most and least recent segments.\n"}, {"user": "cheddar", "commits": {}, "labels": ["Bug"], "created": "2013-01-30 17:28:04", "title": "And/Or with only one field doesn't work", "url": "https://github.com/apache/druid/issues/73", "closed": "2013-04-30 18:16:59", "ttf": 90.00027777777778, "commitsDetails": [], "body": "Sixtus42 from the IRC channel reports that And/Or filters with only one field do not actually work properly.  This is pretty annoying and we should be able to make it work as if the and/or just wasn't there.\n"}, {"user": "cheddar", "commits": {"55648c47a76f69699e18c28890e4a0b288ac2d14": {"commitGHEventType": "referenced", "commitUser": "cheddar"}}, "changesInPackagesGIT": [], "labels": ["Bug"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "PostAggregations in GroupBy being computed twice?", "numCommits": 0, "created": "2013-01-30 17:26:47", "closed": "2013-04-30 17:50:13", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/72", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 90.00027777777778, "commitsDetails": [{"commitGitStats": [{"filePath": "server/src/main/java/com/metamx/druid/query/group/GroupByQueryRunnerFactoryConfig.java", "deletions": 0, "insertions": 14, "lines": 14}, {"filePath": "server/src/main/java/com/metamx/druid/query/group/GroupByQueryRunnerFactory.java", "deletions": 43, "insertions": 56, "lines": 99}, {"filePath": "server/src/test/java/com/metamx/druid/query/group/GroupByTimeseriesQueryRunnerTest.java", "deletions": 1, "insertions": 4, "lines": 5}, {"filePath": "client/src/main/java/com/metamx/druid/query/group/GroupByQueryQueryToolChest.java", "deletions": 67, "insertions": 79, "lines": 146}, {"filePath": "client/src/main/java/com/metamx/druid/query/group/GroupByQuery.java", "deletions": 2, "insertions": 4, "lines": 6}, {"filePath": "server/src/main/java/com/metamx/druid/initialization/ServerInit.java", "deletions": 1, "insertions": 3, "lines": 4}, {"filePath": "client/src/main/java/com/metamx/druid/QueryableNode.java", "deletions": 6, "insertions": 9, "lines": 15}, {"filePath": "common/src/main/java/com/metamx/druid/input/Rows.java", "deletions": 32, "insertions": 16, "lines": 48}, {"filePath": "server/src/test/java/com/metamx/druid/query/group/GroupByQueryRunnerTest.java", "deletions": 1, "insertions": 2, "lines": 3}], "commitSpoonAstDiffStats": [{"spoonFilePath": "QueryableNode.java", "spoonMethods": [{"INS": 5, "UPD": 0, "DEL": 1, "spoonMethodName": "com.metamx.druid.QueryableNode.initializeEmitter()", "MOV": 3, "TOT": 9}]}, {"spoonFilePath": "ServerInit.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.initialization.ServerInit.initDefaultQueryTypes(org.skife.config.ConfigurationObjectFactory,com.metamx.druid.collect.StupidPool)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "GroupByQueryRunnerFactoryConfig.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryRunnerFactoryConfig", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "GroupByQueryQueryToolChest.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryQueryToolChest.mergeResults(com.metamx.druid.query.QueryRunner).2.run(com.metamx.druid.Query).1", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryQueryToolChest.mergeResults(com.metamx.druid.query.QueryRunner).2.run(com.metamx.druid.Query).2", "MOV": 0, "TOT": 1}, {"INS": 2, "UPD": 0, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryQueryToolChest", "MOV": 0, "TOT": 2}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryQueryToolChest.mergeResults(com.metamx.druid.query.QueryRunner).2.run(com.metamx.druid.Query).3.accumulate(com.metamx.druid.index.v1.IncrementalIndex,com.metamx.druid.input.Row)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryQueryToolChest.mergeResults(com.metamx.druid.query.QueryRunner).2.run(com.metamx.druid.Query).4", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 4, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryQueryToolChest.mergeResults(com.metamx.druid.query.QueryRunner).2.run(com.metamx.druid.Query)", "MOV": 3, "TOT": 8}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryQueryToolChest.mergeResults(com.metamx.druid.query.QueryRunner).2.run(com.metamx.druid.Query).3", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "GroupByQueryRunnerFactory.java", "spoonMethods": [{"INS": 1, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryRunnerFactory.mergeRunners(java.util.concurrent.ExecutorService,java.lang.Iterable)", "MOV": 1, "TOT": 3}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryRunnerFactory.RowOrdering.compare(com.metamx.druid.input.Row,com.metamx.druid.input.Row)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryRunnerFactory.mergeRunners(java.util.concurrent.ExecutorService,java.lang.Iterable).2", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryRunnerFactory.mergeRunners(java.util.concurrent.ExecutorService,java.lang.Iterable).2.apply(com.metamx.druid.query.QueryRunner).1.run(com.metamx.druid.Query)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryRunnerFactory.1", "MOV": 1, "TOT": 2}, {"INS": 0, "UPD": 7, "DEL": 1, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryRunnerFactory.1.mergeResults(com.metamx.druid.query.QueryRunner)", "MOV": 3, "TOT": 11}, {"INS": 3, "UPD": 0, "DEL": 2, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryRunnerFactory", "MOV": 5, "TOT": 10}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryRunnerFactory.RowOrdering", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "GroupByQuery.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQuery.withOverriddenContext(java.util.Map)", "MOV": 0, "TOT": 1}, {"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQuery", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQuery.withQuerySegmentSpec(com.metamx.druid.query.segment.QuerySegmentSpec)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "GroupByQueryRunnerTest.java", "spoonMethods": [{"INS": 0, "UPD": 4, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryRunnerTest.constructorFeeder()", "MOV": 0, "TOT": 4}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryRunnerTest.constructorFeeder().1", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryRunnerTest.testMergeResults().4", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryRunnerTest.constructorFeeder().2", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByQueryRunnerTest.constructorFeeder().3", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "GroupByTimeseriesQueryRunnerTest.java", "spoonMethods": [{"INS": 0, "UPD": 3, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByTimeseriesQueryRunnerTest.constructorFeeder()", "MOV": 0, "TOT": 3}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByTimeseriesQueryRunnerTest.constructorFeeder().2", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByTimeseriesQueryRunnerTest.constructorFeeder().3", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByTimeseriesQueryRunnerTest.constructorFeeder().1", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.query.group.GroupByTimeseriesQueryRunnerTest.constructorFeeder().3.run(com.metamx.druid.Query)", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "Rows.java", "spoonMethods": [{"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.input.Rows.toInputRow(com.metamx.druid.input.Row,java.util.List)", "MOV": 0, "TOT": 1}, {"INS": 0, "UPD": 2, "DEL": 0, "spoonMethodName": "com.metamx.druid.input.Rows.toInputRow(com.metamx.druid.input.Row,java.util.List).1.getFloatMetric(java.lang.String)", "MOV": 2, "TOT": 4}, {"INS": 0, "UPD": 1, "DEL": 0, "spoonMethodName": "com.metamx.druid.input.Rows.toInputRow(com.metamx.druid.input.Row,java.util.List).1.toString()", "MOV": 1, "TOT": 2}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2013-04-09 19:13:08", "commitMessage": "1) Adjust the GroupByQuery to also be able to merge results on the local node.  Fixes #116\n2) Make the GroupByQuery operate in a multi-threaded fashion by default (this is configurable via druid.query.groupBy.singleThreaded).  Fixes #96\n3) Fix up some post aggregation computation stuff.  I believe #72 is fixed\n4) Fix case sensitivity issue with post aggregations on GroupBy queries\n", "commitUser": "cheddar", "commitDateTime": "2013-04-09 19:13:08", "commitParents": ["a678f08b677cfb65587679b6858e32f3ac49151e"], "commitGHEventType": "referenced", "nameRev": "55648c47a76f69699e18c28890e4a0b288ac2d14 tags/druid-0.3.35~4", "commitHash": "55648c47a76f69699e18c28890e4a0b288ac2d14"}], "body": "Sixtus42 on the IRC channel reports that PostAggregations with shadowed names in GroupBy queries appear to be computed twice.\n"}, {"user": "cheddar", "commits": {}, "labels": ["Bug"], "created": "2013-01-24 16:44:55", "title": "HadoopDruidIndexer breaks when timestampColumn is not all lowercase", "url": "https://github.com/apache/druid/issues/65", "closed": "2013-07-23 23:36:46", "ttf": 180.00027777777777, "commitsDetails": [], "body": "The Parser converts all the field names into lowercase values, but the HadoopDruidIndexerConfig is not doing the same with the timestampColumn field, so when it looks for the actual column, it breaks with an exception like\n\n Caused by: java.lang.IllegalArgumentException: null timestamp\n    at com.google.common.base.Preconditions.checkArgument(Preconditions.java:88)\n    at com.metamx.common.parsers.TimestampParser$5.apply(TimestampParser.java:119)\n    at com.metamx.common.parsers.TimestampParser$5.apply(TimestampParser.java:115)\n    at com.metamx.druid.indexer.IndexGeneratorJob$IndexGeneratorMapper.map(IndexGeneratorJob.java:189)\n"}, {"user": "cheddar", "commits": {"be8c6fafb048e622a313ab9b5b40a00f412d44ff": {"commitGHEventType": "referenced", "commitUser": "drcrallen"}}, "changesInPackagesGIT": [], "labels": ["Area - Batch Ingestion", "Bug", "Priority - High", "Starter"], "spoonStatsSummary": {"spoonFilesChanged": 0, "spoonMethodsChanged": 0, "INS": 0, "UPD": 0, "DEL": 0, "MOV": 0, "TOT": 0}, "title": "IncrementalIndex.add() barfs when InputRow.getDimensions() has duplicates", "numCommits": 0, "created": "2013-01-23 19:34:14", "closed": "2015-12-07 21:39:54", "gitStatsSummary": {"deletions": 0, "insertions": 0, "lines": 0, "gitFilesChange": 0}, "statsSkippedReason": "", "changesInPackagesSPOON": [], "filteredCommits": [], "url": "https://github.com/apache/druid/issues/63", "filteredCommitsReason": {"multipleIssueFixes": 1, "mergeCommitUsed": 0, "unavailable": 0, "duplicated": 0, "alsoFixesPhrase": 0, "moreThanOneParent": 0}, "ttf": 1048.0002777777777, "commitsDetails": [{"commitGitStats": [{"filePath": "processing/src/test/java/io/druid/segment/incremental/IncrementalIndexTest.java", "deletions": 0, "insertions": 147, "lines": 147}, {"filePath": "processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java", "deletions": 0, "insertions": 11, "lines": 11}], "commitSpoonAstDiffStats": [{"spoonFilePath": "IncrementalIndexTest.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.incremental.IncrementalIndexTest", "MOV": 0, "TOT": 1}]}, {"spoonFilePath": "IncrementalIndex.java", "spoonMethods": [{"INS": 1, "UPD": 0, "DEL": 0, "spoonMethodName": "io.druid.segment.incremental.IncrementalIndex.add(io.druid.data.input.InputRow)", "MOV": 1, "TOT": 2}]}], "spoonStatsSkippedReason": "", "authoredDateTime": "2015-12-03 18:01:11", "commitMessage": "Merge pull request #2017 from tubemogul/issue/63\n\nfixes issue #63", "commitUser": "drcrallen", "commitDateTime": "2015-12-03 18:01:11", "commitParents": ["045df54404bc265a9a3f5ef35bd7d613428c8ac6", "b6cc2428e17ad5e49bcf780e32f122166c10fd44"], "commitGHEventType": "referenced", "nameRev": "be8c6fafb048e622a313ab9b5b40a00f412d44ff tags/druid-0.9.0-rc1~225", "commitHash": "be8c6fafb048e622a313ab9b5b40a00f412d44ff"}], "body": "If InputRow.getDimensions() has duplicates, IncrementalIndex.add() fails with \n\njava.lang.ArrayIndexOutOfBoundsException: 4\n         at com.metamx.druid.index.v1.IncrementalIndex.add(IncrementalIndex.java:148)\n         at com.metamx.druid.realtime.Sink.add(Sink.java:98)\n         at com.metamx.druid.realtime.RealtimeManager$FireChief.run(RealtimeManager.java:176)\n[12:29pm]\n"}, {"user": "fjy", "commits": {}, "labels": ["Bug"], "created": "2012-12-21 19:16:15", "title": "GroupBy query does not finalize metrics", "url": "https://github.com/apache/druid/issues/48", "closed": "2013-04-30 18:15:07", "ttf": 129.00027777777777, "commitsDetails": [], "body": "It seems like group by queries are not finalizing metrics.\n"}], "captureTime": "Sun Jul  5 13:26:41 2020"}